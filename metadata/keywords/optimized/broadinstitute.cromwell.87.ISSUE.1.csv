quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Availability,"Good question! In the context of the docker://uri, pull and build are almost identical. Both will retrieve layers from Docker Hub (or a registry you target), tar.gz files, and dump them into a binary image. So these two commands do the same thing:. ```bash; singularity build docker://<container>; singularity pull docker://<container>; ```; Build is different if you do it in an environment where you can use sudo, because you can target a build recipe instead, e.g.,. ```bash; sudo singularity build <container>.sif Singularity; ```; and pull is different too if you target some non-docker pull source:. ```bash; singularity pull shub://vsoch/singularity-images; ```; You can look at the codebase to confirm this - a pull of a docker uri [comes down to calling build](https://github.com/sylabs/singularity/blob/03072a88e108966d50fd61b0e6a51e6dbc62ff20/internal/pkg/libexec/pull.go#L42). ```bash; func PullOciImage(path, uri string, opts types.Options) {; 	b, err := build.NewBuild(uri, path, ""sif"", """", """", opts); 	if err != nil {; 		sylog.Fatalf(""Unable to pull %v: %v"", uri, err); 	}. 	if err := b.Full(); err != nil {; 		sylog.Fatalf(""Unable to pull %v: %v"", uri, err); 	}; }; ```; so choose whichever word you like better :) In the context of docker, without sudo, they do the same thing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461796569:747,down,down,747,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461796569,1,['down'],['down']
Availability,Google Cloud backend v2beta - Hello World documentation outdated + failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:67,failure,failure,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['failure'],['failure']
Availability,"Google Genomics fails when transferring the String output from a task to a bucket (Cromwell 25, using the wdl_runner pipeline). Here is the offending WDL file:; ```yaml; task Print_String {; command {; echo hello; }; runtime {; docker: ""ubuntu:14.04""; cpu: ""1""; memory: ""1 GB""; disks: ""local-disk "" + 10 + "" HDD""; }; output {; String out_string = read_string(stdout()); }; }. workflow My_Workflow {; call Print_String {; }; }; ```. This results in the error log:; ```; 2017-03-31 03:39:42,716 wdl_runner INFO: Workflow output files = [u'hello']; 2017-03-31 03:39:42,716 file_util INFO: Copying [u'hello'] to gs://ccdg-100-samples-trios-pilot-crams-mgi/outputs/string-out-1/; 2017-03-31 03:39:45,722 file_util WARNING: Copy [u'hello'] to gs://ccdg-100-samples-trios-pilot-crams-mgi/outputs/string-out-1/ failed: attempt 0; 2017-03-31 03:39:48,695 file_util WARNING: Copy [u'hello'] to gs://ccdg-100-samples-trios-pilot-crams-mgi/outputs/string-out-1/ failed: attempt 1; 2017-03-31 03:39:51,643 file_util WARNING: Copy [u'hello'] to gs://ccdg-100-samples-trios-pilot-crams-mgi/outputs/string-out-1/ failed: attempt 2; ERROR: copying files from [u'hello'] to gs://ccdg-100-samples-trios-pilot-crams-mgi/outputs/string-out-1/ failed: CommandException: No URLs matched: hello. (exit status 1); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2115:202,echo,echo,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2115,3,"['ERROR', 'echo', 'error']","['ERROR', 'echo', 'error']"
Availability,"Got a centaur failure with `Failed to upload auth file` caused by the following exception, not considered retryable:. ```; 017-04-18 21:11:49,413 cromwell-system-akka.dispatchers.engine-dispatcher-64 ERROR - WorkflowManagerActor Workflow 6e23463e-3fc6-4b18-aeb0-fc7c920cd758 failed (during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.applyOrElse(JesInitializationActor.scala:63); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.applyOrElse(JesInitializationActor.scala:62); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.core.CromwellFatalException: com.google.cloud.storage.StorageExc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183:14,failure,failure,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183,4,"['ERROR', 'failure', 'recover']","['ERROR', 'failure', 'recoverWith']"
Availability,Got the same error in https://github.com/aws-samples/aws-refarch-wordpress,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4674#issuecomment-489183672:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4674#issuecomment-489183672,1,['error'],['error']
Availability,Graceful shut down is expecting a `Ctrl-C` or SIGINT (possibly SIGKILL too?). Docker stop just turns off the lights. We should document how to stop a container in order to take advantage of the graceful shutdown process.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2562:14,down,down,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2562,1,['down'],['down']
Availability,Graceful shutdown of cromwell wrt workflow state and heartbeats,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4242:53,heartbeat,heartbeats,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4242,1,['heartbeat'],['heartbeats']
Availability,Great. Building the code instead of downloading the latest release works like a charm :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-247018681:36,down,downloading,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406#issuecomment-247018681,1,['down'],['downloading']
Availability,"Green team is seeing many errors detailed [here](https://partnerissuetracker.corp.google.com/issues/122571609):. The workaround is:. >... cromwell should migrate to pipelines-io, but in the meantime, if you want to make a quick fix for this you can do:. ` rm -f $HOME/.config/gcloud/gce`. > immediately before invoking gsutil inside your retry loop. This would save approximately 10% of the failures being seen in Green Team efforts per @tbl3rd and the bug",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4641:26,error,errors,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4641,2,"['error', 'failure']","['errors', 'failures']"
Availability,GreenTeam saw 3 406 errors when polling our workflows for status. These occurred on 5/3/2016. Error hitting REST API: https://<REDACTED>/api/workflows/v1/ca25d78b-3d4c-4336-b9b8-c64e8ea0dc43/status => Unexpected response code: 406; Error hitting REST API: https://<REDACTED>/api/workflows/v1/ca25d78b-3d4c-4336-b9b8-c64e8ea0dc43/status => Unexpected response code: 406; Error hitting REST API: https://<REDACTED>/api/workflows/v1/ca25d78b-3d4c-4336-b9b8-c64e8ea0dc43/status => Unexpected response code: 406,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775:20,error,errors,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775,4,"['Error', 'error']","['Error', 'errors']"
Availability,"Guys, you should update mysql connector! Most of my workflow failures were because of mysql connection loss, it is such a pain to have a pipeline running for >1 day and having stuff crashed because ""cromwell lost connection to mysql""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4690#issuecomment-468618807:61,failure,failures,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4690#issuecomment-468618807,1,['failure'],['failures']
Availability,"HI @markjschreiber,. Thanks for getting in touch. I had a look at the code and believe I came up with a work around, albeit a bit of a hack. Im building our system in Terraform so have a bit more control over the underlying infrastructure. Im able to mount additional volumes to the underlying ec2 instance, I just needed to mount them inside the container. I managed this with the following runner config:. backend {; default = AWSBatch; providers {; AWSBatch {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; ...; default-runtime-attributes {; queueArn: ""${default_batch_queue}""; scriptBucketName: ""${script_bucket}""; disks: ""local-disk,/bin/bash,${static_ref_snapshot_mount}""; zones: ""${aws_batch_availability_zone}""; }; }; }; }. Specifically `default-runtime-attributes.disks`. The first entry in this list always defaults to the working dir (this is the volume I have set up to autoscale), but after that subsequent values correspond to locations on the host. The only downside is that there is no way to change the mount location in the container. Whatever the location on the host has to be the same location in the container. As I control both I can make sure they are in sync. It feels a bit clunky but seems to be working. I will close this issue but would suggest some minor modifications to the AWS volume handling could really add to the systems flexibility /:-). Best,; Jon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919472151:1019,down,downside,1019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919472151,1,['down'],['downside']
Availability,"HPC; # Singularity+Slurm: and an example on Slurm; # udocker: another rootless container solution; # udocker+slurm: also exemplified on slurm; # HtCondor: workload manager at UW-Madison; # LSF: the Platform Load Sharing Facility backend; # SGE: Sun Grid Engine; # SLURM: workload manager. # Note that these other backend examples will need tweaking and configuration.; # Please open an issue https://www.github.com/broadinstitute/cromwell if you have any questions; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions"". concurrent-job-limit = 10; # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; ## Warning: If set, Cromwell will run 'check-alive' for every job at this interval; exit-code-timeout-seconds = 360; filesystems {; local {; localization: [; # soft link does not work for docker with --contain. Hard links won't work; # across file systems; ""copy"", ""hard-link"", ""soft-link""; ]; caching {; duplication-strategy: [""copy"", ""hard-link"", ""soft-link""]; hashing-strategy: ""file""; }; }; }. #; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 3; Int requested_memory_mb_per_core = 8000; Int memory_mb = 40000; String? docker; String? partition; String? account; String? IMAGE; """""". submit = """"""; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${out} \; --error=${err} \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""/b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:6638,alive,alive,6638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,2,['alive'],['alive']
Availability,"Had a brief chat w/ @dshiga - I was wondering if all slowness was directly tied to reading from metadata or general slowness and it was the latter (not that it couldn't be _caused_ by reading from MD, but it's across the board slowness). Three thoughts:; - We were talking yesterday about how someone not named me should try setting up typesafe monitor and use it to debug/profile/analyze. No time like the present!; - I can't find the ticket (@kcibul - do you know the right string to search for?) but we should look into the thread pools/dispatchers. This could be a case where something is bringing down the whole system but bulkheading would keep everything else responsive; - A while back we talked about streamlining submission such that WF submissions get a ""Submitted"" status but aren't necessarily immediately launched, and the system would pull them - allowing us to tune the rate at which we pull. Perhaps time to resurrect this one?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772:602,down,down,602,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772,1,['down'],['down']
Availability,"Hah, mea culpa! That *is* Yossi's error!. I misread the logs earlier and had been unwittingly sitting at the same step that he's on. It just takes a lot wider of a scatter on my machine than the one he's on apparently. We have reason to believe that that value can't be fully static, but it might be able to be locked in after construction.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114757:34,error,error,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277114757,1,['error'],['error']
Availability,Handle 'insufficient data written' errors from the JES API Query Manager,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2535:35,error,errors,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2535,1,['error'],['errors']
Availability,Handle invalid inputs parsing error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4375:30,error,error,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4375,1,['error'],['error']
Availability,"Happened again last night:. ```; The code passed to eventually never returned normally. Attempted 30 times over 3.33454509745 minutes. Last failure message: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp).; ```. ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 30 times over 3.33454509745 minutes. Last failure message: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp).; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventually(HealthMonitorServiceActorSpec.scala:20); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.eventualStatus(HealthMonitorServiceActorSpec.scala:32); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$new$5(HealthMonitorServiceActorSpec.scala:81); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.withFixture(HealthMonitorServiceActorSpec.scala:20); at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680); at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692); at org.scalatest.Su",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:140,failure,failure,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,2,['failure'],['failure']
Availability,"Happy for a future PR to propose catching SOE, or even other `Error`s. In past lives I've been indoctrinated that `java.lang.Error` means ""stop and shutdown, something is horribly wrong"". For those who want to read debates on the issue:; https://www.google.com/search?q=java+catch+stack+overflow+error+site:stackoverflow.com",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5042#issuecomment-505061221:62,Error,Error,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5042#issuecomment-505061221,3,"['Error', 'error']","['Error', 'error']"
Availability,"Have not not found anywhere a complete example showing it working, including what should be int eh .conf file. If such an example exists, please point us to it,. Command:; nohup java -Dconfig.file=My.conf -jar cromwell-87-5448b85-SNAP-pre-edits.jar run ~/MemoryRetryTest.wdl 2>&1 > nohup.out. MemoryRetryTest.wdl:; workflow MemoryRetryTest {; 	String message = ""Killed""; 	; 	call TestOutOfMemoryRetry {}; 	call TestBadCommandRetry {}; }. task TestOutOfMemoryRetry {; 	command <<<; 		free -h; 		df -h; 		cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		tail /dev/zero; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; 	; }. task TestBadCommandRetry {; 	command <<<; free -h; df -h; cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		bedtools intersect nothing with nothing; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; }. My.conf:. include required(classpath(""application"")). system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"". system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; config {; project = ""$my_project""; root = ""$my_bucket""; name-for-call-caching-purposes: PAPI; slow-job-warning-time: 24 hours; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600. # Setup GCP to give more memory with each retry; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; ; # Number of workers to assign to PAPI requests; request-workers = 3. virtual-private-cloud {; network-label-key = ""network-key""; network-name = ""network-name""; subnetwork-name = ""subnetwork-name""; auth = ""auth""; }; pipeline-timeout = 7 days; genomics {; auth = ""auth""; compute-service-account = ""$my_account"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7451:1023,error,error-keys,1023,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451,2,"['Error', 'error']","['Error', 'error-keys']"
Availability,"Having default runtime attributes in jes config caused faulty WARN messages about ""Unrecognized configuration key(s) for Jes"". This PR should fix those.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3662:55,fault,faulty,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3662,1,['fault'],['faulty']
Availability,"Hello @bpsommerville,. The CI has a hard limit of 180 minutes. Often when we see tests hitting the limit it is because a change has inadvertently introduced behavior wherein Cromwell retries forever. Random failures are certainly possible also. I have restarted the tests on your behalf. To set expectations, our bandwidth to help with AWS is limited, so it would be up to you to check that your PR is covered by existing tests, or to add a test yourself. Best,; Adam",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563424439:207,failure,failures,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-563424439,1,['failure'],['failures']
Availability,"Hello @notestaff . This sounds like a good feature request, but not something that has been addressed yet and it might be a few months before we get to it. For now, is it possible that the output files can be bundled into a zipped file and passed onto a downstream task possibly? As an attempt to consolidate the outputs. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-444615940:254,down,downstream,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4216#issuecomment-444615940,1,['down'],['downstream']
Availability,"Hello Cromwell People, . Currently I believe Cromwell has retry logic for I/O issues or preemptible VMs issues for Google Cloud,. However, when Cromwell jobs that are executed via GridEngine dispatcher will fail with no re-try if the return code is deemed as a error code,. I am submitting a potential patch where Cromwell can retry failed jobs running on GridEngine with user specified retries (""backend.max-job-retries""), . I'm not sure how the configurations should be organized but here is my starting point; let me know what you guys think. Thanks,; Paul",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176:261,error,error,261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176,1,['error'],['error']
Availability,"Hello Cromwell Team, . Our bioinformatics team have been reporting a single retry after preemptible attempts have been exhausted. They've added logic in the task itself that introspects the vm in the event the job ends up on a non-preemptible VM and promptly exists. This isn't ideal as starting a VM still incurs cost. . I've made the follow changes in:; ```diff; --- a/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala; +++ b/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiAsyncBackendJobExecutionActor.scala; @@ -882,8 +882,11 @@ class PipelinesApiAsyncBackendJobExecutionActor(override val standardParams: Sta; else {; val msg = s""$baseMsg The maximum number of preemptible attempts ($maxPreemption) has been reached. The "" +; s""call will be restarted with a non-preemptible VM. Error code $errorCode.$prettyPrintedError)""; - FailedRetryableExecutionHandle(StandardException(; - errorCode, msg, jobTag, jobReturnCode, standardPaths.error), jobReturnCode, kvPairsToSave = Option(preemptionAndUnexpectedRetryCountsKvPairs)); + FailedNonRetryableExecutionHandle(; + StandardException(errorCode, msg, jobTag, jobReturnCode, standardPaths.error),; + jobReturnCode,; + kvPairsToSave = Option(preemptionAndUnexpectedRetryCountsKvPairs); + ); }; ```. and tested with a trivial WDL and tasks such as (trying out multiple premptible / maxRetries):. ```wdl; task crash {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" && sleep infinity ; }; output {; String message = read_string(stdout()); }; runtime {; preemptible: 3; maxRetries: 0; docker: ""ubuntu:latest""; }; }. workflow wf_preempt {; call crash. output {; crash.message; }; }. ```. Let me know if I'm going in the right direction for a pull request.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6666:942,Error,Error,942,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6666,7,"['Error', 'echo', 'error']","['Error', 'echo', 'error', 'errorCode']"
Availability,"Hello I am trying to re-use an existing workflow for Mutect2 available here: https://app.terra.bio/#workspaces/terra-outreach/CHIP-Detection-Mutect2 to run on SLURM with Singularity configuration. There are multiple steps similar to Mutect2 public workflow available here: https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl , but still attaching the modified WDL with additional steps. . So when we run this with the given configuration using the following; export SINGULARITY_CACHEDIR=$PWD/singularity_cache; export SINGULARITY_TMPDIR=$PWD/tmpdir; module load singularity; rm -rf nohup.out && nohup java -Dconfig.file=$PWD/cromwell_singularity.conf -jar $PWD/cromwell-84.jar run $PWD/mutect2_modified.wdl --inputs $PWD/inputs.json &. The issue is that the first step of splitting intervals runs fine, but as it starts mutect2, it starts copying of the complete execution directory making here is the directory structure. cromwell-executions/; └── Mutect2; └── e5769b79-5e02-44a5-a4f8-38745e152beb; ├── call-M2; │ └── shard-0; │ ├── execution; │ └── inputs; │ ├── -1816294717; │ ├── 1855713868; │ │ └── run_cromwell_only.tmp; │ │ └── cromwell-executions; │ │ └── Mutect2; │ │ └── e5769b79-5e02-44a5-a4f8-38745e152beb; │ ├── 2035192126; │ └── 891763929; └── call-SplitIntervals; ├── execution; │ ├── glob-0fc990c5ca95eebc97c4c204e3e303e1; │ └── interval-files; ├── inputs; │ └── -1816294717; └── tmp.c9d96672. As you can see that run_cromwell_only.tmp is being made and that happens to fall in an endless loop and eventually, it errors stating the file name is too long to copy. Can you help me how to avoid this behavior of making circular paths when copying files for execution? Also, note it does not happen in the first step of SplitIntervals but happens in the Mutect2 call. [mutect2_gatk.wdl.txt](https://github.com/broadinstitute/cromwell/files/9813528/mutect2_gatk.wdl.txt); [cromwell_singularity.conf.txt](https://github.com/broadinstitute/cromwell/files/981352",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6934:61,avail,available,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6934,2,['avail'],['available']
Availability,"Hello again,. Just an update: error persists even when I set the `project` name.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435543802:30,error,error,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435543802,1,['error'],['error']
Availability,"Hello and thanks for your question. We do not have any current plans to support SQLite (though I personally think it's a fantastic product!). If you can help us understand the scenario in which you're running Cromwell we may be able to offer some advice. In particular, it's surprising that a MySQL database is ""not available or difficult to get""; the typical scenario we envision for running Cromwell with persistence is a Cromwell Docker plus a MySQL Docker.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-561682355:316,avail,available,316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-561682355,1,['avail'],['available']
Availability,"Hello cromwell dev team,. I'm currently working with the reference-disks option using GCPBatch as my backend. I executed the create_images.sh script from the documentation (https://cromwell.readthedocs.io/en/develop/backends/GCPBatch/) to generate my reference-disk-localization-manifests. I'm also using Cromwell v87, as specified in the same documentation. I also tested with the current `develop` build. While the manifest is correctly configured in the Cromwell config, and the reference disk appears to be mounting successfully, I’m encountering a failure with the umount command. I’m not sure why this command is being invoked in the first place. Mount Image; <img width=""573"" alt=""Screenshot 2024-09-26 at 16 07 46"" src=""https://github.com/user-attachments/assets/ce43be4a-132b-4c87-a27d-718a376171f7"">. **Error Message:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. Below is an example of what I have included in my Cromwell configuration (cromwell.conf). . ```; reference-disk-localization-manifests = [; {; ""imageIdentifier"" : ""projects/private-test-cromwell/global/images/omics-reference-disk-image"",; ""diskSizeGb"" : 10,; ""files"" : [ ; {; ""path"" : ""test-cromwell-references/hg38/v0/Homo_sapiens_assembly38.dict"",; ""crc32c"" : 2158779318; },; {; ""path"" : ""test-cromwell-references/hg38/v0/Homo_sapiens_assembly38.fasta"",; ""crc32c"" : 420322484; },; {; ""path"" : ""test-cromwell-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai"",; ""crc32c"" : 1970999569; }; ]; }; ]; ```. @mcovarr and @aednichols, I have seen issue [#7502](https://github.com/broadinstitute/cromwell/pull/7502)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7548:553,failure,failure,553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7548,2,"['Error', 'failure']","['Error', 'failure']"
Availability,"Hello guys. I met some problem when I import a wdl in another one. The following code is part of the wf2.wdl:. import ""wf1.wdl"" as wf1. workflow wf2{. input{; String token; }; ; call wf1.t1{; input:; token = token; }; output { ; File final_file = t1.file; }; }. The error message told me:. `Workflow input processing failed:; Failed to import 'wf1.wdl' (reason 1 of 1): Failed to resolve 'wf1.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path`. wf1.wdl and wf2.wdl are in the same path, accounting to official documentation，"" import 'wf1.wdl' as wf1"" is valid. ; WOMtool also gives the same view. Furthermore, I have tried relative and absolute path. However both of them did not work. Therefore, any ideas about it? Thank you for your comment!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7101:266,error,error,266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7101,1,['error'],['error']
Availability,"Hello! I am trying to create a submit-docker block for use with docker containers, according to instructions from [here](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/). This is the how my config block looks:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 1; Int requested_memory_mb_per_core = 8000; Int memory_mb = 4000; String queue = ""short""; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpu} \; --mem ${memory_mb} \; --wrap ""/bin/bash ${script}""; """"""; ; submit-docker = """"""; docker pull ${docker}. sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpu} \; --mem ${memory_mb} \; --wrap ""docker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; ```; But, I get an error from each node:. /bin/bash: /data/og/wgs/cromwell-executions/HaplotypeCallerGvcf_GATK4/98c2fe18-92b7-49d4-b490-623ed61e3dfc/call-HaplotypeCaller/shard-41/execution/script: No such file or directory. As far as I can tell, it is trying to reach the script local on the main machine, and not in the docker container. Is this expected behavior, or am I missing something? I know I can replace ${script} with ${docker_cwd}/execution/script but I am unsure why I need this change that is not according to your documentation. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5768:1016,alive,alive,1016,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5768,2,"['alive', 'error']","['alive', 'error']"
Availability,"Hello! I used this snippet of code successfully on the Cromwell with Google backend:. ```; task fastqc {; input {; File f1; File? f2; Int? cpu=1; Int? machine_mem_gb; Int? preemptible_attempts; String mode. Float f2size = if (mode == ""PE"") then size(f2, ""GB"") else 0.0; Int disk_space_gb = ceil(size(f1, ""GB"") + f2size) + 20; }. command {; fastqc -t ~{cpu} --outdir $PWD ~{f1} ~{f2}; }. output {; Array[File] html = glob(""*html""); Array[File] zip = glob(""*zip""); }. runtime {; docker: ""biocontainers/fastqc:v0.11.5_cv2""; memory: select_first([machine_mem_gb, 4]) + "" GB""; cpu: cpu; disks: ""local-disk "" + disk_space_gb + "" HDD""; preemptible: select_first([preemptible_attempts, 3]); }; }; ```; On local backend I got this error:; _Failed to evaluate input '__disk_space_gb' (reason 1 of 1): Sorry! Operation + is not supported on empty optional values. You might resolve this using select_first([optional, default]) to guarantee that you have a filled value._. I tried following the advice (even though I don't see what is missing); Float f2size = select_first([if (mode == ""PE"") then size(f2, ""GB"") else 0.0]); And then I got an even more confusing error:; _Failed to process task definition 'fastqc' (reason 1 of 1): Failed to process expression 'select_first([f2size, select_first([if (mode == ""PE"") then size(f2, ""GB"") else 0.0])])' (reason 1 of 1): Invalid parameter 'ArrayLiteral(Vector(TernaryIf(Equals(IdentifierLookup(mode),StringLiteral(PE)),Size(IdentifierLookup(f2),Some(StringLiteral(GB))),PrimitiveLiteralExpressionElement(WomFloat(0.0)))))'. Expected an array of optional values (eg 'Array[X?]') but got 'Array[Float]+'_. Can you help me understand? It all passes Womtools validation.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5694:722,error,error,722,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5694,2,['error'],['error']
Availability,"Hello! We seem to be running into a similar issue on cromwell `34-bda9485`. After humming along without a problem for a while, we all of a sudden stopped being able to run workflows with zipped WDL imports. Looking at the metadata, we get:; ```json; ""failures"": [; {; ""message"": ""Workflow input processing failed"",; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""pipelines%2Fdna_seq%2FUnmappedBamToAlignedBam.wdl: Name or service not known""; },; {; ""causedBy"": [],; ""message"": ""java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method)""; },...; ```; Is this the same issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-457239244:251,failure,failures,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-457239244,1,['failure'],['failures']
Availability,"Hello!. I am running cromwell with docker on macOS (Big Sur v 11.2.3). When executing the following command from a simple WDL:. ```. docker run --cidfile /cromwell-executions/VarCallAndTable/00a1e01d-4341-4ea4-8225-a9fdaab526c1/call-HaplotypeCaller/execution/docker_cid -i --entrypoint /bin/bash -v /cromwell-executions/VarCallAndTable/00a1e01d-4341-4ea4-8225-a9fdaab526c1/call-HaplotypeCaller:/cromwell-executions/VarCallAndTable/00a1e01d-4341-4ea4-8225-a9fdaab526c1/call-HaplotypeCaller:delegated pegi3s/gatk-4@sha256:8415200fe87b4d4eb295e9decb207727e9f75aad73c860a8d0ffe3385a8eaa7a /cromwell-executions/VarCallAndTable/00a1e01d-4341-4ea4-8225-a9fdaab526c1/call-HaplotypeCaller/execution/script; ```. I get the error: . ```. /bin/bash: /cromwell-executions/VarCallAndTable/5a729547-b927-431f-9793-8a2ac48035ea/call-HaplotypeCaller/execution/script: No such file or directory. ```. After some digging, my guess is that the `docker run` command is not mounting the contents of `/cromwell-executions/VarCallAndTable/00a1e01d-4341-4ea4-8225-a9fdaab526c1/call-HaplotypeCaller`.; For that reason the execution script is not found. Has anyone experieced this? ; Are there any settings I should edit for it to work?. Really appreciate the help!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6353:713,error,error,713,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353,1,['error'],['error']
Availability,"Hello, I am new to WDL and have met the same problem recently. I defined a `struct` like this:; ```; struct Fastp {; File reportHtml; File reportJson; Array[File]+ fqs; }; ```; and try to return it as the output in a `task`:; ```; output {; Fastp out = {; ""reportHtml"": ""QC/fastp.html"",; ""reportJson"": ""QC/fastp.json"",; ""fqs"": if flag then [""QC/clean_1.fq.gz"",""QC/clean_2.fq.gz""] else [""QC/clean_1.fq.gz""]; }; }; ```; `womtool validate` was applied to check the language specification and everything went fine, but finally got the error when trying to run my workflow using Cromwell. Here is part of the error report:; ```; java.lang.UnsupportedOperationException: Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(QC/fastp.html), WomString(QC/fastp.json), [""QC/clean_1.fq.gz"", ""QC/clean_2.fq.gz""]]; ``` ; Any solution to this problem now?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-885644093:531,error,error,531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-885644093,2,['error'],['error']
Availability,"Hello, I am trying to deploy and use cromwell as a docker swarm stack and I have some serious problems with it. ; My configuration that I have in production is [https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml](https://github.com/antonkulaga/cromwell-client/blob/master/services/pipelines.yml) where I have /pipelines folder (with cromwell-executions and data/mysql folders inside of it); As current broad container does not have docker inside (and thus cannot spawn tasks with docker runtimes) I had to:; * make a custom cromwell container that inherits from the official one and contains docker ( https://github.com/antonkulaga/cromwell-client/tree/master/services/cromwell ); * use ; ```yml; volumes:; - /var/run/docker.sock:/var/run/docker.sock; ```; trick, to spawn docker containers as sibling to cromwell container.; Unfortunately when running this setup I discovered that when I configure cromwell execution directory to an absolute path, like ""/pipelines/cromwell-executions"" the the script file that was generated by cromwell for each task still used /cromwell-executions I had to mount an extra volume to /cromwell-executions to trick it.; The other problem is that it constantly having errors like this:; ```; Workflow failed. WorkflowFailure(Unable to determine that 190 is alive, and /pipelines/cromwell-executions/vsearch/1ab35317-b0ae-4e7b-8b09-3403cdaff125/call-global_search/execution/rc does not exist.,List()); ```; while rc file does exist (checked it both on host system and volume). My bet is that it is somehow related with having cromwell inside docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3910:1231,error,errors,1231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3910,2,"['alive', 'error']","['alive', 'errors']"
Availability,"Hello, I executed the following workflow using chromwell behind a proxy . $ java \; -Djava.net.useSystemProxies=true \; -Dhttp.proxyHost=202.241.78.237 -Dhttp.proxyPort=8080 \; -Dhttps.proxyHost=202.241.78.237 -Dhttps.proxyPort=8080 \; -jar cromwell-85.jar run public_health_bacterial_genomics/workflows/wf_theiaprok_illumina_pe.wdl -i input.json. but it caused the following error, indicating connection with quay.io/50.17.122.58:443 timed out. 2023-05-11 10:01:42,43] [info] Request threw an exception on attempt #1. Retrying after 596 milliseconds; org.http4s.client.ConnectionFailure: Error connecting to https://quay.io using address quay.io:443 (unresolved: false); at org.http4s.client.blaze.Http1Support.$anonfun$buildPipeline$1(Http1Support.scala:90); at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:477); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:834); at async @ org.http4s.internal.package$.$anonfun$fromFuture$1(package.scala:144); at flatMap @ org.http4s.internal.package$.fromFuture(package.scala:139); at flatMap @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119); at shift @ org.http4s.client.PoolManager.$anonfun$createConnection$2(PoolManager.scala:119); at uncancelable @ org.http4s.client.ConnectionManager$.pool(ConnectionManager.scala:83); at unsafeRunSync @ cromwell.docker.DockerInfoActor.preStart(DockerInfoActor.scala:172); Caused by: java.net.SocketTimeoutException: An attempt to establish connection with quay.io/50.17.122.58:443 timed out after 10 seconds.; at org.http4s.blaze.channel.nio2.ClientChannelFactory$$anon$1.run(ClientChannelFactory.scala:66); at org.http4s.blaze.util.Execution$$anon$3.execute(Execution.scala:80); at org.http4s.blaze.util.TickWheelExecutor$Node.run(TickWheelExecutor.scala:271); at org.http4s.blaz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7136:376,error,error,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7136,2,"['Error', 'error']","['Error', 'error']"
Availability,"Hello, I posted this bug because the validator does NOT do a check. IE I run the validator on every WDL I submit, but the validator did not catch this error. The error happens, as you said, after many tasks have run already.; [wot.wdl.txt](https://github.com/broadinstitute/cromwell/files/2168605/wot.wdl.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402886702:151,error,error,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402886702,2,['error'],['error']
Availability,"Hello, Im running it on SLURM HPC and i got the same error. [2024-01-20 12:06:05,87] [[38;5;220mwarn[0m] SLURM [[38;5;2mfdf21dfc[0m]: Key/s [memory, zones] is/are not supported by backend. Unsupported attributes will not be part of job executions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1902056409:53,error,error,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1902056409,1,['error'],['error']
Availability,"Hello,. I am having a problem that has been already discussed but I haven't been able to solve it using the suggestions. Basically, In the wdl workflow, I have two tasks (at the moment). The first works fine but the second is not starting because the output of the first task cannot be 'linked' or 'copied'. This cause the workflow to fail. The interesting part is that in the input folder of the second task there are two subfolders: 1 is empty named as `13016223` and the other is not accessible `-1976550098`. The workflow to run needs installed:; `cutadapt` and the script named `moveBarcodeToID.pl` that can be downloaded from here:. https://drive.google.com/open?id=1AizxTwjOEhL5XA7rsx-wbY97p0duB1nw. input fastq files can be retrieved here (they are small ~10000 reads each):. https://drive.google.com/file/d/1-c14Tja4zY3lyr6icFWT06stznR_-Zqr/view?usp=sharing; https://drive.google.com/file/d/1oJd_U9MjTllL0_kpNivw8I_LtSyvqpXH/view?usp=sharing. How can I solve this issue and make the workflow running smoothly?. ### Which backend are you running? ; I am running locally the workflow for now (because I am in the first phase of the development). ### Workflow is this:; ```; #workflow validated before running with: wdltool validate example.wdl and womtool validate scMeth_v2.wdl.sh -i scMeth_input_3.json. workflow scMeth {; # information for trimming the cell barcode; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName. # information for trimming the adapters and low quality reads; File file_format; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; call trimCellBarcode {; input:; sampleName=sampleName,; bases=bases,; input_fastq1=input_fastq1,; input_fastq2=input_fastq2,; command=command; }; call trimAdapters {; input:; file_format=file_format,; input_r1 = trimCellBarcode.fastq_debarcoded_R1,; input_r2 = trimCellBarcode.fastq_debarcod",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:616,down,downloaded,616,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['down'],['downloaded']
Availability,"Hello,. I am trying to use our on premise cromwell instance to submit jobs to GCP via the genomics api. I seem to have everything working except the VPC network name. We have a naming convention for the VPC network names configured in our GCP project and don't have the flexibility to change the name. However, cromwell seems to expect the VPC network name to be ""default"" and this does not seem to be configurable. Here is the error I am receiving when trying to submit the workflow:. [error] WorkflowManagerActor Workflow 8f55bf4d-9389-40e6-a469-dbd434394dd8 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 3. Invalid value for field 'resource.networkInterfaces[0].network': 'https://www.googleapis.com/compute/v1/projects/projectxyz/global/networks/default'. The referenced network resource cannot be found. Is there a way for me to pass a different network name? If not, where can I request this feature be added to a future version?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4005:428,error,error,428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4005,3,['error'],['error']
Availability,"Hello,. I can better assist you with errors like this over on the [WDL website](https://software.broadinstitute.org/wdl/index.php), specifically the [Ask the WDL team](http://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team) section. Oftentimes, you can find a solution to your error, or perhaps you will find documents that answer your question on the website. As such, we prefer to answer questions, or file bug reports (if needed) from the forum. In your particular case, I would suggest asking a new question in the Ask the WDL team forum. Let me know if you need anything else.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1071#issuecomment-229115974:37,error,errors,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071#issuecomment-229115974,2,['error'],"['error', 'errors']"
Availability,"Hello,. I found that womtool validate can't ignore ""#"" in json file and will report 'Unexpected input provided'. I think this is a bug in womtool? Example listed below. The ""#ValidateBamsWf.ValidateBAM.machine_mem_gb"" line should be ignored as it starts with a ""#""?. ```; #all files downloaded from latest github version at https://github.com/gatk-workflows/seq-format-validation.git; java -jar womtool-48.jar validate validate-bam.wdl -i validate-bam.inputs.json . ```. > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.machine_mem_gb (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); > WARNING: Unexpected input provided: #ValidateBamsWf.ValidateBAM.validation_mode (expected inputs: [ValidateBamsWf.ValidateBAM.disk_space_gb, ValidateBamsWf.gatk_docker_override, ValidateBamsWf.ValidateBAM.validation_mode, ValidateBamsWf.gatk_path_override, ValidateBamsWf.ValidateBAM.machine_mem_gb, ValidateBamsWf.bam_array]); ......",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5434:283,down,downloaded,283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5434,1,['down'],['downloaded']
Availability,"Hello,. I posted a couple of issues on the bcbio-nextgen tracker and I thought I'd link them here as well, in case anyone can help, as these seem to be Cromwell-specific errors. Running bcbio-nextgen with IPython parallel instead of Cromwell works fine for the scenario I've set up. In summary, I'm trying to run variant calling with bcbio-nextgen using Cromwell and 100 small FASTQ files (50-100 reads each), for testing, on an SGE cluster. The main issue seems to be that the master node is coming up under heavy load, for some reason, and, at some point, the Cromwell jobs stop getting scheduled. . This is described in more detail here: https://github.com/bcbio/bcbio-nextgen/issues/2721. A second issue, which might be related, is with the file `cromwell_work/persist/metadata.lobs`. It gets quite big, at 36 GB, for the scenario that I've described earlier, and I'm wondering if this is normal. . I'm also seeing some error messages (`java.sql.BatchUpdateException: lob is no longer valid`) when running bcbio-nextgen with Cromwell, in the same scenario as above. Could this somehow be related to the high cluster load issues as well? . More details are here: https://github.com/bcbio/bcbio-nextgen/issues/2722.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4732:170,error,errors,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4732,2,['error'],"['error', 'errors']"
Availability,"Hello,. I'am running a wdl worflow that I have created and I want to make a conditional statement. This because according to which sequencing library is made the user wants to trim the cell barcodes or not. So in case the user is providing the metadata with the name of the barcodes in the 4th column the task of trimming the barcode should be ""on"". In contrary, it should be off. This, of course, depends on whether the barcodes are provided in the metadata or not. What I am trying to do is to make the string that is in the barcode with the condition ""?"" (please see the workflow below in the scatter). When the scatter is reading the metadata, in the case in which there is no barcode the WDL is interrupted with this error: . ```; ""message"": ""Failed to evaluate 'scMethTask3.barcode' (reason 1 of 1): Evaluating files_and_metadata_row[3] failed: Failed to find index Success(WomInteger(3)); on array:\n\nSuccess([\""SRR5395068\"", \""SRR5395068_1.fastq.gz\"", \""SRR5395068_2.fastq.gz\""])\n\n3"",; ""causedBy"": []; ```; How can i avoid this? Or is there a way to accomplish what I am trying to do?. ### Which backend are you running? ; Unix terminal within slurm scheduler. ### Example meta_data files:; 1) without barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz	; ```; 2) with barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz ATCGCT	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz ATCGGA; ```; ### Below my workflow:. workflow scMethTask3 {. #information about the monitoring scrip and the number of samples; File? monitoring_script; File meta_data. #information for trimming the cell barcode; File command; Int bases; ; #information for trimming the adapters and low quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:722,error,error,722,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,1,['error'],['error']
Availability,"Hello,. I'm seeing this error with glob. Looks like the glob-xxx.list file is not create for some reason. The file it's looking for has a couple commas (,) in them, could that be the issue?. wdl: /humgen/gsa-hpprojects/dev/tsato/palantir/Analysis/451_MedicallyRelevantCoverageWorkflow/medically-relevant-coverage.wdl.copy; json: /humgen/gsa-hpprojects/dev/tsato/palantir/Analysis/451_MedicallyRelevantCoverageWorkflow/medically-relevant-coverage.json; cromwell server: gsa5:8003 (configured for SGE); metadata: http://gsa5:8003/api/workflows/v2/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/metadata?expandSubWorkflows=false. ""message"": ""Failed to evaluate outputs.: Could not evaluate GetHSMetrics.per_target_coverage = glob(\""*.per_target_coverage\"")[0]\n\tFile not found /humgen/gsa-hpprojects/dev/tsato/wdl/cromwell-executions/MedicallyRelevantCoverage/e97e55f5-f0e0-42dd-9a7d-ae4edd66dc19/call-GetHSMetrics/shard-1/execution/glob-66c7ddc0219653b72a61bd2dae8bb454.list""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1980:24,error,error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1980,1,['error'],['error']
Availability,"Hello,. I'm using Cromwell with Google Pipelines as backend and sometimes (maybe when more than 30 analysis running at same time) I'm getting workflow error (~2 of the 30). When inspecting the metadata for the workflow I can see a error message that contains ""ServiceException: 401 Requester pays bucket access requires authentication."". **Edit:** Using Cromwell 35. Has anyone had a similar problem? Here are the WDL task that are affected (from Broad's five dollar genome workflow):. ```wdl; task BaseRecalibrator {; File input_bam; File input_bam_index; String recalibration_report_filename; Array[String] sequence_group_interval; File dbSNP_vcf; File dbSNP_vcf_index; Array[File] known_indels_sites_VCFs; Array[File] known_indels_sites_indices; File ref_dict; File ref_fasta; File ref_fasta_index; Int disk_size; Int preemptible_tries. command {; /usr/gitc/gatk4/gatk-launch --javaOptions ""-XX:GCTimeLimit=50 -XX:GCHeapFreeLimit=10 -XX:+PrintFlagsFinal \; -XX:+PrintGCTimeStamps -XX:+PrintGCDateStamps -XX:+PrintGCDetails \; -Xloggc:gc_log.log -Xms4000m"" \; BaseRecalibrator \; -R ${ref_fasta} \; -I ${input_bam} \; --useOriginalQualities \; -O ${recalibration_report_filename} \; -knownSites ${dbSNP_vcf} \; -knownSites ${sep="" -knownSites "" known_indels_sites_VCFs} \; -L ${sep="" -L "" sequence_group_interval}; }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.2-1510681135""; memory: ""6 GB""; disks: ""local-disk "" + disk_size + "" HDD""; preemptible: preemptible_tries; }; output {; File recalibration_report = ""${recalibration_report_filename}""; }; }; ```. And here is my cromwell server config:. ```scala; include required(classpath(""application"")). webservice {; port = 8000; }. system {; workflow-restart = true; }. engine {; filesystems {. gcs {; auth = ""service-account""; }. http {}. local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }. backend {; default = ""Local""; providers {. Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBack",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336:151,error,error,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336,2,['error'],['error']
Availability,"Hello,. I'm working on making our cromwell server work with GCP Batch and running in our private VPC network. However, after following [this tutorial](https://cromwell.readthedocs.io/en/develop/backends/GCPBatch/), I encounter the following error:. ```; com.google.api.gax.rpc.InvalidArgumentException: io.grpc.StatusRuntimeException: INVALID_ARGUMENT: network field is invalid. network: projects/${project_id}/global/networks/${network_id}/ is not matching the expected format: global/networks/([a-z]([-a-z0-9]*[a-z0-9])?)$; 	at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:92); 	at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:41); 	at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:86); 	at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:66); 	at com.google.api.gax.grpc.GrpcExceptionCallable$ExceptionTransformingFuture.onFailure(GrpcExceptionCallable.java:97); 	at com.google.api.core.ApiFutures$1.onFailure(ApiFutures.java:84); 	at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1133); 	at com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:31); 	at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:1277); 	at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:1038); 	at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:808); 	at io.grpc.stub.ClientCalls$GrpcFuture.setException(ClientCalls.java:574); 	at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:544); 	at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39); 	at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23); 	at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7500:241,error,error,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500,1,['error'],['error']
Availability,"Hello,. when I try to run my workflow using a config file using. `$ java -Dconfig.file=../config/LSF.conf cromwell.jar cromwell run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json`. I get the following message; ```; Error: Could not find or load main class cromwell.jar; Caused by: java.lang.ClassNotFoundException: cromwell.jar; ```. Without specifying a config file, the pipeline runs without any problems. ; I installed Cromwell (version 79) using conda. I also tried the following:. `$ java -Dconfig.file=../config/LSF.conf cromwell-79.jar run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json `. ```; Error: Could not find or load main class cromwell-79.jar; Caused by: java.lang.ClassNotFoundException: cromwell-79.jar; ```. I also checked where the cromwell.jar file is saved in my conda environment and tried the following:. `; java -Dconfig.file=./LSF.conf /path/to/env/share/cromwell/cromwell.jar cromwell run ../pipelines/bismark_pid.wdl -i ../pipelines/bismark_wgbs_pid.json `. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. This is the config file LSF.config:; ```; include required(classpath(""application"")). backend {. # Override the default backend.; default = LSF. # The list of providers. Copy paste the contents of a backend provider in this section; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; 	submit = ""bsub -J ${job_name} -cwd ${cwd} -o ${out} -e ${err} /usr/bin/env bash ${script}""; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; # Second backend provider would be copy pasted here!. }; }; ```. I have not much experienced with cromwell and would be very grateful for help. Thank you,; Johannes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6796:235,Error,Error,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6796,3,"['Error', 'alive']","['Error', 'alive']"
Availability,"Hello. Here is the simple example of workflow with optional input: . ```; workflow wf {; String? inn; call tsk_grep { input: input_381012639=inn }; }. task tsk_grep {; String? input_381012639; command { ps aux | grep j${""a"" + input_381012639}; }; }; ```. When i do not specify any input for workflow(json file below):. ```; {; }; ```. i get an the error:; `wdl4s.WdlExpressionException: Could not resolve inn as a scatter variable, namespace, call, or declaration`. But when my workflow looks like this:. ```; workflow wf {; call tsk_grep; }. task tsk_grep {; String? input_381012639; command { ps aux | grep j${""a"" + input_381012639}; }; }; ```. Cromwell works as expected executing `ps aux | grep j`.; Is this a bug or i can only use optional variables inside of **task scope**?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1176:348,error,error,348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1176,1,['error'],['error']
Availability,"Hello.; Is this tool supposed to run on Windows?; i get the following error connected with illegal character in files path:. > [2016-06-17 19:46:12,739] [error] BackendCallExecutionActor [3a480614:testTask]: Illegal char <:> at index 130: D:\Cromwell\cromwell-ex ecutions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask \D:\Cromwell\other.bam; > java.nio.file.InvalidPathException: Illegal char <:> at index 130: D:\Cromwell\workflow-engine\cromwell-executions\test\3a480614-1f47-4101-a869-20b3cd836f38\call-testTask\ D:\Cromwell\test.bam; > at sun.nio.fs.WindowsPathParser.normalize(WindowsPathParser.java:182); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:153); > at sun.nio.fs.WindowsPathParser.parse(WindowsPathParser.java:77); > at sun.nio.fs.WindowsPath.parse(WindowsPath.java:94); > at sun.nio.fs.WindowsFileSystem.getPath(WindowsFileSystem.java:255); > at java.nio.file.Paths.get(Paths.java:84); > at cromwell.engine.backend.local.SharedFileSystem$class.toCallPath$1(Sha redFileSystem.scala:214); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$15.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(Sha redFileSystem.scala:259); > at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue (SharedFileSystem.scala:267); > at cromwell.engine.backend.local.LocalBackend.localizeWdlValue(LocalBack end.scala:94); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(Shar edFileSystem.scala:219); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:221); > at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(Shar edFileSystem.scala:220); > at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1016:70,error,error,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1016,2,['error'],['error']
Availability,"Hello， Could someone tell me how to fix this problem? Thanks!!!. **task hello** {; String name. command {; echo 'Hello ${name}!' > /Users/00.wdl/QC_new/QC/hello.out1; }; output {; File out1 = ""/Users/00.wdl/QC_new/QC/hello.out1""; }; }. **task hello2** {; 	File in1. 	command {; 		cat ${in1}; 	}. 	output {; 		File	response2 = stdout(); 	}. }. **workflow test** {; call hello {input name=""World"" }; call hello2 {input in1=hello.out1 }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5059:107,echo,echo,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5059,1,['echo'],['echo']
Availability,"Here is a slightly more general script (it assumes the lock file and saved image are in the current directory). It also does the pull into a temp file with a rename into the destination name at the end so that for a large image the -f check won't trigger for a partial download. I do some work here to deduce a filename that should match (as I understand the rules anyway) the one that the pull would create. I also include an option to force the path, since in my automation I tend to wish to define everything for my self. The derived or given image filename is echoed at the end. YOUR_HOST is the name of the sregistry host (this is the context I'm doing this in). ```; #!/bin/bash . lock_dir=. if [[ $# -ne 1 && $# -ne 2 ]] ; then; echo ""Usage: $0 image-name [output-file]"" 1>&2; exit 1; fi; name=$1; output_file=$2. if [[ ""$output_file"" = """" ]] ; then. # deduce filename . output_file=`basename $name`; if [[ $output_file =~ (.*):([^:]+)$ ]] ; then; base=${BASH_REMATCH[1]}; tag=${BASH_REMATCH[2]}; else; base=$output_file; tag=latest; fi; output_file=""${base}_$tag.sif""; fi. url=shub://YOUR_HOST/$name. # declare a very similar path (.lock) where Cromwell can access ; lock=$lock_dir/$output_file.lock; tmp=$output_file.tmp. if [ ! -f ""$output_file"" ]; then # If we already have the image, skip everything ; (; flock --exclusive 200; if [ ! -f ""$output_file"" ]; then # do a second check once the lock has been released ; singularity pull --nohttps $tmp $url; mv $tmp $output_file; fi; ) 200>$lock; fi. rm -f $lock. echo $output_file; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921:269,down,download,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537238921,4,"['down', 'echo']","['download', 'echo', 'echoed']"
Availability,"Here is a very simple WDL (test.wdl):; ```; version 1.0. workflow test {; input {; Array[File] list; }. call test {; input:; list = list; }; }. task test {; input {; Array[File] list; String docker = ""ubuntu:20.04""; }. File lines = write_lines(list). command <<<; cat ~{lines}; cat ~{write_lines(list)}; echo -e ""~{sep=""\\n"" list}""; >>>. runtime {; docker: docker; }; }; ```. Here is a basic input for the WDL (test.json):; ```; {; ""test.list"": [""/tmp/1"", ""/tmp/2"", ""/tmp/3""]; }; ```. When I run the workflow on my laptop with the following command:; ```; java -jar cromwell-51.jar run test.wdl -i test.json; ```; I get the following stdout output:; ```; /tmp/1; /tmp/2; /tmp/3; /cromwell-executions/test/00112233-4455-6677-8899-aabbccddeeff/call-test/inputs/1515144/1; /cromwell-executions/test/00112233-4455-6677-8899-aabbccddeeff/call-test/inputs/1515144/2; /cromwell-executions/test/00112233-4455-6677-8899-aabbccddeeff/call-test/inputs/1515144/3; /cromwell-executions/test/00112233-4455-6677-8899-aabbccddeeff/call-test/inputs/1515144/1; /cromwell-executions/test/00112233-4455-6677-8899-aabbccddeeff/call-test/inputs/1515144/2; /cromwell-executions/test/00112233-4455-6677-8899-aabbccddeeff/call-test/inputs/1515144/3; ```; Which shows that the absolute paths of the files passed to the test workflow have been exposed. I cannot imagine this being the expected behavior. How do I get to have `write_lines(...)` behave more like `~{sep="" "" ...}` even when it is not run inside a command <<< ... >>> instance?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5540:304,echo,echo,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5540,1,['echo'],['echo']
Availability,"Here is an example error for a task failing since there is less space than needed:. ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Task exceed_disk_size.simple_localize_and_fetch_size:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: action 9: unexpected exit status 1 was not ignored\n[Localization] Input name: input_file - Unexpected exit status 1 while running \""/bin/sh -c python -c 'import base64; print(base64.b64decode(\\\""IyEvYmluL2Jhc2gKCmZvciBpIGluICQoc2VxIDMpOyBkbwogICgKICAgIGdzdXRpbCAgY3AgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvcmFuZG9tX2ZpbGVfb3Zlcl8xX2diLnR4dCAvY3JvbXdlbGxfcm9vdC9zc19jcm9td2VsbF9idWNrZXQvcmFuZG9tX2ZpbGVfb3Zlcl8xX2diLnR4dCA+IGdzdXRpbF9vdXRwdXQudHh0IDI+JjEKIyBSZWNvcmQgdGhlIGV4aXQgY29kZSBvZiB0aGUgZ3N1dGlsIGNvbW1hbmQgd2l0aG91dCBwcm9qZWN0IGZsYWcKUkNfR1NVVElMPSQ/CmlmIFsgIiRSQ19HU1VUSUwiICE9ICIwIiBdOyB0aGVuCiAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgZ3N1dGlsXCBcIGNwXCBnczovL3NzX2Nyb213ZWxsX2J1Y2tldC9yYW5kb21fZmlsZV9vdmVyXzFfZ2IudHh0XCAvY3JvbXdlbGxfcm9vdC9zc19jcm9td2VsbF9idWNrZXQvcmFuZG9tX2ZpbGVfb3Zlcl8xX2diLnR4dFwgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CiAgCiAgIyBDaGVjayBpZiBpdCBtYXRjaGVzIHRoZSBCdWNrZXRJc1JlcXVlc3RlclBheXNFcnJvck1lc3NhZ2UKICBpZiBncmVwIC1xICJCdWNrZXQgaXMgcmVxdWVzdGVyIHBheXMgYnVja2V0IGJ1dCBubyB1c2VyIHByb2plY3QgcHJvdmlkZWQuIiBnc3V0aWxfb3V0cHV0LnR4dDsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgUmV0cnlpbmdcIHdpdGhcIHVzZXJcIHByb2plY3QKICAgIGdzdXRpbCAtdSBicm9hZC1kc2RlLWNyb213ZWxsLWRldiBjcCBnczovL3NzX2Nyb213ZWxsX2J1Y2tldC9yYW5kb21fZmlsZV9vdmVyXzFfZ2IudHh0IC9jcm9td2VsbF9yb290L3NzX2Nyb213ZWxsX2J1Y2tldC9yYW5kb21fZmlsZV9vdmVyXzFfZ2IudHh0CiAgZWxzZQogICAgZXhpdCAiJFJDX0dTVVRJTCIKICBmaQplbHNlCiAgZXhpdCAwCmZpCiAgKQogIFJDPSQ/CiAgaWYgWyAiJFJDIiA9ICIwIiBdOyB0aGVuCiAgICBicmVhawogIGZpCiAgaWYgWyAkaSAtbHQgMyBdOyB0aGVuCiAgICBwcmludGYgJyVzICVzXG4nICIkKGRhdGUgLXUgJyslWS8lbS8lZCAlSDolTTolUycpIiBXYWl0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4603:19,error,error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4603,3,"['error', 'failure']","['error', 'failures']"
Availability,"Here is another error that a user will have trouble determining if the whole workflow failed:. ```; [2016-10-28 14:37:09,70] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:37:09,70] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:2:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:37:35,25] [error] Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:16,error,error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,3,"['error', 'failure']","['error', 'errors', 'failures']"
Availability,"Here is another one (only appeared once), but the workflow keeps going... I am not even sure if I need to quit... ```; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:13:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:13:1] Status change from - to Initializing; [2016-10-27 13:47:19,26] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:9:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:9:1] Status change from - to Initializing; [2016-10-27 13:47:19,27] [info] JesAsyncBackendJobExecutionActor [fd2fcb78case_gatk_acnv_workflow.HetPulldown:15:1]: JesAsyncBackendJobExecutionActor [fd2fcb78:case_gatk_acnv_workflow.HetPulldown:15:1] Status change from - to Initializing; [2016-10-27 13:47:24,90] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChann",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647:863,error,error,863,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256645647,1,['error'],['error']
Availability,"Here is the current situation from Google from https://partnerissuetracker.corp.google.com/issues/71697449:; > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #1 Jan 8, 2018 09:25AM ; > Reported Issue; > I don't have specific numbers at this time, but over the past several weeks our production operations staff started noticing an odd behavior that we originally thought was just normal preemption. Normally we see preemption showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". T",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:486,Error,Error,486,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,2,"['Error', 'error']","['Error', 'error']"
Availability,"Here is the size optimization that deletes untested jython files:. https://github.com/broadinstitute/heterodon/blob/b54010d4f1fe9395f854ab62e4b66c203bf3f45d/build.sh#L82-L84. If we come up with a CI regression case for Windows (x64?) then the appropriate files could be excluded from the filter and tested. Re: the borked Cromwell, we could catch-and-box the thrown `java.lang.Error` into a `java.lang.Exception` and Cromwell would handle this particular error more gracefully.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-481487720:377,Error,Error,377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4802#issuecomment-481487720,2,"['Error', 'error']","['Error', 'error']"
Availability,"Here is the terminal output, for posterity:. The first command with internet, confirms we are using a cached image:; ```bash; $ singularity exec docker://busybox ls; INFO: Using cached SIF image; CHANGELOG.md LICENSE README.md dist paper qme.egg-info setup.py; Dockerfile MANIFEST.in build docs qme setup.cfg tests; ```; then I took off my wireless :scream: and ran the same - we know the image is in the cache:. ```bash; $ singularity exec docker://busybox ls; FATAL: Unable to handle docker://busybox uri: failed to get checksum for docker://busybox: error pinging docker registry registry-1.docker.io: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io: no such host; ```; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046:553,error,error,553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631206046,2,"['error', 'ping']","['error', 'pinging']"
Availability,"Here is the wdl file used for the test:. ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous_task_out = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous_task_out = print_nach_nachman.out; }. output{; File out = print_nach_nachman_meuman.out; }; }. task print_nach{. String output_file = ""task1.out""; command{; echo ""nach"" > ${output_file}; }; output{; File out = output_file; }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; File previous_task_out; Array[String] previous_content = read_lines(previous_task_out); String output_file = ""task2.out""; command{; echo ${sep=' ' previous_content} "" nachman"" > ${output_file}; }; output{; File out = output_file; }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; ; }. task print_nach_nachman_meuman{; File previous_task_out; Array[String] previous_content = read_lines(previous_task_out); String output_file = ""three_task_sequence.out""; command{; echo ${sep=' ' previous_content} "" meuman"" > ${output_file}; }; output{; File out = output_file; }; runtime {; docker: ""ubuntu:latest""; maxRetries: 3; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-468214328:375,echo,echo,375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686#issuecomment-468214328,3,['echo'],['echo']
Availability,"Here's a log for a similar workflow that had the same issue but recovered after restart. ```; 2017-02-13 16:50:09,104 INFO - MaterializeWorkflowDescriptorActor [UUID(3d01da76)]: Call-to-Backend assignments: test.hello -> JES; 2017-02-13 16:50:09,534 INFO - JES [UUID(3d01da76)]: Creating authentication file for workflow 3d01da76-98f9-4751-a3c0-efc61ef67030 at ; gs://cromwell-auth-broad-dsde-alpha/3d01da76-98f9-4751-a3c0-efc61ef67030_auth.json; 2017-02-13 16:50:10,063 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Starting calls: test.hello:NA:1; 2017-02-13 16:50:11,006 INFO - JesRun [UUID(3d01da76)test.hello:NA:1]: JES Run ID is operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:11,006 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: job id: operations/EJ7jhsOjKxiXht2Ej-qXrHAg9vy4-dodKg9wcm9kdWN0aW9uUXVldWU; 2017-02-13 16:50:16,621 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from - to Initializing; 2017-02-13 16:51:01,890 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Initializing to Running; 2017-02-13 16:51:38,243 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; 2017-02-13 16:51:38,977 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; {; ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; }; 2017-02-13 16:51:39,178 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953:64,recover,recovered,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279495953,1,['recover'],['recovered']
Availability,Here's the URL to the EPAM pipeline builder: http://pb.opensource.epam.com/. I assume this can be closed but crommers can feel free to reopen if I am in error,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-320522599:153,error,error,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-320522599,1,['error'],['error']
Availability,"Here's the error message:; ```; ""Unknown status"" did not start with ""Timed out"" (HealthMonitorServiceActorSpec.scala:76; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3391:11,error,error,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3391,1,['error'],['error']
Availability,"Here's the full metadata @danbills:. ```; {; ""submittedFiles"": {; ""workflow"": ""import \""ss2_single_sample.wdl\"" as ss2\nimport \""submit.wdl\"" as submit_wdl\n\n\ntask GetInputs {\n String bundle_uuid\n String bundle_version\n String dss_url\n Int retry_seconds\n Int timeout_seconds\n\n command <<<\n python <<CODE\n from pipeline_tools import utils\n\n # Get bundle manifest\n uuid = '${bundle_uuid}'\n version = '${bundle_version}'\n dss_url = '${dss_url}'\n retry_seconds = ${retry_seconds}\n timeout_seconds = ${timeout_seconds}\n print('Getting bundle manifest for id {0}, version {1}'.format(uuid, version))\n manifest_files = utils.get_manifest_files(uuid, version, dss_url, timeout_seconds, retry_seconds)\n\n print('Downloading assay.json')\n assay_json_uuid = manifest_files['name_to_meta']['assay.json']['uuid']\n assay_json = utils.get_file_by_uuid(assay_json_uuid, dss_url)\n\n sample_id = assay_json['sample_id']\n fastq_1_name = assay_json['seq']['lanes'][0]['r1']\n fastq_2_name = assay_json['seq']['lanes'][0]['r2']\n fastq_1_url = manifest_files['name_to_meta'][fastq_1_name]['url']\n fastq_2_url = manifest_files['name_to_meta'][fastq_2_name]['url']\n\n print('Creating input map')\n with open('inputs.tsv', 'w') as f:\n f.write('fastq_1\\tfastq_2\\tsample_id\\n')\n f.write('{0}\\t{1}\\t{2}\\n'.format(fastq_1_url, fastq_2_url, sample_id))\n print('Wrote input map')\n CODE\n >>>\n runtime {\n docker: \""humancellatlas/pipeline-tools:0.1.4\""\n }\n output {\n Object inputs = read_object(\""inputs.tsv\"")\n }\n}\n\nworkflow AdapterSs2RsemSingleSample {\n String bundle_uuid\n String bundle_version\n\n File gtf\n File ref_fasta\n File rrna_interval\n File ref_flat\n String star_genome\n String rsem_genome\n String reference_bundle\n\n # Submission\n File format_map\n String dss_url\n String submit_url\n String method\n String schema_version\n String run_type\n Int retry_seconds\n Int timeout_seconds\n\n # Set runtime environment such as \""dev\"" or \""staging\"" or \""prod\"" so sub",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3060#issuecomment-351777550:724,Down,Downloading,724,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060#issuecomment-351777550,1,['Down'],['Downloading']
Availability,"Hey @DivyaThottappilly do you still have this issue? I'm trying to get up and running a basic Hello World but keeps getting an S3Exception null error (301). . It seems like you've already past that stage and if you don't mind, could you help me setup this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379:144,error,error,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379,1,['error'],['error']
Availability,"Hey @OgnjenMilicevic, looks like there is an error in that page. I believe it's supposed to be `${docker_script}` instead of `${script}`. (Edit, I've opened a PR to address this). For context, here is my config I use for Slurm + Singularity: https://gist.github.com/illusional/b70f870fa0e2f8e7a0ba0a9e71d568f5",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5768#issuecomment-676817660:45,error,error,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5768#issuecomment-676817660,1,['error'],['error']
Availability,"Hey @ParkvilleData, just make sure you're formatting your code with three backticks, then starting the code on the next line, like:. ````; ```; <code starts here/>; ```; ````. The single backticks `` ` `` are for in-line entries, eg: `` `code` `` -> `code`. Can you post the actual error you're seeing, or can you confirm none of your tasks are returning a valid value for ""docker"" in the runtime section.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694157851:282,error,error,282,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694157851,1,['error'],['error']
Availability,"Hey @TMiguelT @vsoch, we noticed that on a system without `mksquashfs` in its path, the `singularity exec` from Dockerhub fails. This seems to be backed up here: http://singularity.lbl.gov/install-linux. > Note that when you configure, squashfs-tools is not required, however it is required for full functionality. You will see this message after the configuration:; > `mksquashfs from squash-tools is required for full functionality`; > If you choose not to install squashfs-tools, you will hit an error when you try a pull from Docker Hub, for example. I get slightly conflicting information from the Singularity 3 docs which just says: ; > Note that squashfs-tools is an image build dependency only and is not required for Singularity build and run commands.; (https://www.sylabs.io/guides/3.0/user-guide/quick_start.html?highlight=squashfs). We did install `squashfs` and it's in our `$PATH`, but it seems Singularity is only looking at:; - `/bin/mksquashfs`; - `/usr/bin/mksquashfs`; - `/sbin/mksquashfs`; - `/usr/sbin/mksquashfs`; - `/usr/local/bin/mksquashfs`; - `/usr/local/sbin/mksquashfs`. Any thoughts here, as you are almost always required to pull from docker hub (it's kind of the default). ___. I also noticed with some testing that in the udocker submit, if you exclude the `--rm` it will run a bit quicker. @danbills, am I able to make changes since the review?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702:499,error,error,499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-468087702,1,['error'],['error']
Availability,"Hey @Xophmeister, sorry for the slow response time!. This error message is actually coming from our SFS (shared filesystem) backend (so I'll ping @kshakir). I'm not familiar with the `mounts` attribute in the SFS. However, I think the answer to your question is that none of the attributes asked for by the SFS backend are arrays, and so arrays are not a supported attribute type. . I actually could only find reference to the `mounts` attribute outside of the SFS backend in places like BCS and Google cloud. I wonder whether you just need to move this attribute out of your configuration file and into your WDL task itself?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4685#issuecomment-481024411:58,error,error,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685#issuecomment-481024411,2,"['error', 'ping']","['error', 'ping']"
Availability,"Hey @aednichols, I tested this to make sure, and as expected, running the test with `version 1.0` fails with errors:. ```; Failed to read task definition at line 3 column 6 (reason 1 of 1): Failed to read outputs section (reason 1 of 1): Failed to read declaration (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; Failed to read task definition at line 13 column 6 (reason 1 of 1): Failed to parse expression (reason 1 of 1): Unknown engine function: 'sep'; ```. I've fixed the corresponding tests in https://github.com/broadinstitute/cromwell/pull/5494/commits/40851b7423de68bda7ff9aaa47a37fbf7a0a70a3. So this should still be good to merge!. Edit: I see the test is failing, but looks like an unrelated error:. ```; [error] java.lang.RuntimeException: The vault token file ""/home/travis/.vault-token"" does not exist. Be sure to login using the instructions on https://hub.docker.com/r/broadinstitute/dsde-toolbox/ under ""Authenticating to vault"".; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247:109,error,errors,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-654019247,3,['error'],"['error', 'errors']"
Availability,"Hey @antonkulaga, are you running this in Cromwell 30?. The good news: this was indeed a known issue for a long time but I believe it's finally been fixed as of https://github.com/broadinstitute/cromwell/pull/3175. ; The bad news: that won't be available until the next Cromwell release. If you're comfortable building from develop you're welcome to do that and try it out!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367441100:245,avail,available,245,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367441100,1,['avail'],['available']
Availability,"Hey @antonkulaga, these aren't exactly what you're after but there are two things you could have a look at that should help:; - You can use the `concurrent-job-limit` for the local backend to limit how many jobs (i.e. calls being run) are happening at any given time. That should cause things to slow down naturally without having to manually pause/resume them, which might help. In the config:; ```; backend {; ...; providers {; BackendName {; actor-factory = ...; config {; concurrent-job-limit = 5; ```. - The second item (not re-running early tasks) should be helped by [call caching](https://github.com/broadinstitute/cromwell#call-caching). As long as nothing changes in the intermediate steps, Cromwell should be able to detect and re-use your previous results.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527:301,down,down,301,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2023#issuecomment-283395527,1,['down'],['down']
Availability,"Hey @gdlex4015 . We expect IO failures from time to time. There's a place in the Cromwell config to increate the number of retries on IO actions, so possibly bumping that up will help. . https://github.com/broadinstitute/cromwell/blob/develop/cromwell.examples.conf#L721. Closing this for now but please re-open if you see this at a high frequency. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4439#issuecomment-466136579:30,failure,failures,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4439#issuecomment-466136579,1,['failure'],['failures']
Availability,"Hey @geoffjentry I think this is good to go now. The coveralls went down (I think) because I'm now testing against my mock actor rather than the actual actor, so the coverage in that went down. I left the CromwellServer stuff alone in this PR so we could talk about how we wanted to consolidate that, Boot and Main into one(?) thing.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/22#issuecomment-104481650:68,down,down,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/22#issuecomment-104481650,2,['down'],['down']
Availability,"Hey @geoffjentry. The formula downloads the .jar from the Github releases page and [creates a little wrapper script](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L11) that's used to make `cromwell` available as a command-line tool. There's no need to compile from source. In order to update the formula for future releases, you can just submit a PR to Homebrew by updating the [url](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L4), [SHA](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L5), and [install steps](https://github.com/slnovak/homebrew/blob/cromwell-0.14/Library/Formula/cromwell.rb#L9-L12). I'd be happy to do this in the future for future releases -- just include this step in whatever release checklist you may use. Homebrew is a pretty well-established community, so there's not much to contribute on that end. Cheers!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166033076:30,down,downloads,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/335#issuecomment-166033076,2,"['avail', 'down']","['available', 'downloads']"
Availability,"Hey @leepc12 it turns out that you do have a bug in your WDL and that Cromwell 29 was at fault for not highlighting it too. I'll submit a PR to include a better error message, which will be along the lines of:; ```; Unable to build WOM node for If '$if_2': Unable to build WOM node for Scatter '$scatter_2': Unable to build WOM node for WdlTaskCall 't3': Invalid indexing target. You cannot index a value of type'Array[Int]?'; ```. Notice that in order to access `t2.out` you're looking up inside another `if` block, which means that the output has to be treated as optional. . - Given the structure of *this* workflow you could move the `if ( b1 && b2 )` inside the `if (b1)` (and simplify the conditional expression). ; - If that's not possible in your real workflow you can use `select_first` to get the value out, eg `call t0 as t3 { input: i=select_first([t2.out])[i] }` (NB this is only valid because `if (b1 && b2)` implies `if (b1)` must have been run, so the `select_first` is known to succeed)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3007#issuecomment-349689182:89,fault,fault,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007#issuecomment-349689182,2,"['error', 'fault']","['error', 'fault']"
Availability,"Hey @lmtani . Thanks for reporting this. Another [issue](https://github.com/broadinstitute/cromwell/issues/4640) is designed to fix the transient failure mode you're describing. When that issue is closed, you should see this failure mode drop. Closing this as it's a duplicate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-466138620:146,failure,failure,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-466138620,2,['failure'],['failure']
Availability,"Hey @mr-c - I'm looking to update things. But this raises another question - since CWL 1.0.X is by definition backwards compatible, why would our reliance on an older version of `schema-salad` be an error on the tester? It shouldn't matter? . For instance we've talked in the past about embedding a particular version of the python code in our JAR just to ensure stability, but if I'm understanding what's causing issues on the CI (and I'm not sure I **am**, but it's my current theory at least) then if we did such a thing this would come up regardless.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-411546386:199,error,error,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-411546386,1,['error'],['error']
Availability,"Hey @nvanaja-. Can you rebase this PR? Hopefully, there aren't any major conflicts since the main branch has continued to diverge. Sorry, we have another set of (currently internal) CI tests that are failing on your branch. The team has [(also internally) discussed](https://broadinstitute.slack.com/archives/C1EH66VCM/p1612377943138400?thread_ts=1612377834.138300&cid=C1EH66VCM) the cryptic docker errors with `mysql-client`, and no one on the team has any quick concrete suggestions at this second other than rebasing. 🤞. Thanks again for your patience while we juggle your PR with our other work!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-773513271:399,error,errors,399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6157#issuecomment-773513271,1,['error'],['errors']
Availability,"Hey @rexwangcc -- a requirement for any task to be considered a ""success"" is the generation of stdout/stderr log file. From my experience, I've seen in the past that when there's an intermittent docker issues, the ""docker logs"" (which include stdout/stderr) aren't copied out -- so even if some of the outputs produced by your task exist, the fact that stdout/stderr don't exist is considered a failure. I'm not sure we want to change this behavior as not being able to capture docker logs means not all task outputs were created, thus a failure. In cases like this, what will be helpful is an option to retry transient task failures (such as what you describe here), which is work in review [here](https://github.com/broadinstitute/cromwell/pull/3596). Let me know if there's something I'm misunderstanding here!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3615#issuecomment-390845973:395,failure,failure,395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3615#issuecomment-390845973,3,['failure'],"['failure', 'failures']"
Availability,"Hey @sona1111, I took your workflow and was able to approximately get your error. Your issue is that you have trailing whitespace after the first line: `voila tsv \ `:. ![image](https://user-images.githubusercontent.com/22381693/71853318-17c17600-312f-11ea-9e6b-5b85ae692ce2.png). This is a bash thing. You can replicate this problem by just running the following command inside your container:. ```; voila tsv \ f1; ```. This returned me the error:. ```; voila tsv: error: argument files: cannot find ""/cromwell-executions/myWorkflow1/591dc02e-f9e4-48c6-8498-df17242fe217/call-task_voila_tsv/execution/ ""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5285#issuecomment-571342815:75,error,error,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5285#issuecomment-571342815,3,['error'],['error']
Availability,"Hey @vsoch, in our submit script, we're opting to pull and build our docker images on the head node as @TMiguelT suggested. If the build exists, singularity will ask to overwrite the existing build. Just wondering if there's an elegant way to skip the build if it exists (and it's up to date) or whether we should be forcing a rebuild every time. I could skip it like this, but it's not as friendly./; ```; echo 'n' | singularity build --sandbox $IMAGE docker://${docker}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463861596:407,echo,echo,407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463861596,1,['echo'],['echo']
Availability,"Hey @ylipacbio, I'm not from Cromwell but wanted to throw out a comment. For Cromwell to pull files, a [_Filesystem_](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/) needs to be implemented in Scala. . As far as I'm aware, Amazon's EFS is **NOT** implemented, and cannot be configured to work with Cromwell. The available filesystems are ftp, s3, demo-dos, gcs, oss, http (and unix). If you know some scala, [this](https://github.com/broadinstitute/cromwell/tree/develop/filesystems) might be good place to start on how to implement one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4602#issuecomment-489482319:332,avail,available,332,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4602#issuecomment-489482319,1,['avail'],['available']
Availability,"Hey Conrad - Thanks, this is awesome. To give some insight on how things are playing out in the hopefully-not-too-long-term, we're planning on cutting an alpha release of the PBE stuff imminently (perhaps today?) which is something we feel is stable enough to start poking at but is missing a few features we need in our production use cases (restart/recovery, call caching), backends other than local/JES, and with some known warts we need to hammer out. I'm guessing you're looking at roughly a month for something more stable than that, although I'm famous for my ""about a month"" predictions. However, since you're already pretty up to speed with what's going on, I'd say that the 0.20 alpha should be stable enough to work up a backend. It'd at least be a good test case as someone who _did_ figure out how to make one in the old system if the new system is inscrutable or not. In terms of what to do with this PR, I'll somewhat leave it up to you. We're hoping to close the 0.19 books as much as possible once the alpha thing is out, but if you feel like it'll provide value to folks over the next month or so I'm happy to take some time to review it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813:351,recover,recovery,351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1106#issuecomment-229977813,1,['recover'],['recovery']
Availability,"Hey Patrick, I just ran a tiny test and was able to confirm jobs getting aborted. ; - How many jobs were started from your workflow, and did any of the jobs from your workflow abort?; - Do you have a general sense at the stage your jobs were on when they were aborted? Were they all mostly executing the command when you aborted them? ; - Did Cromwell ever report the workflow to have been successfully Aborted? Any errors thrown in the server logs?. Would you mind posting the operation metadata from one of the jobs that you tried aborting using the rest endpoint? Or simply the events reported for that operation?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673:416,error,errors,416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3758#issuecomment-396002673,1,['error'],['errors']
Availability,"Hey Thibault, can you let me know your timeline for this? I'm going on vacation in a week but want to be available to you to discuss anything you have questions about and also available to give feedback.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289546870:105,avail,available,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-289546870,2,['avail'],['available']
Availability,"Hey, did you ever manage to get a workaround for this error?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-514934007:54,error,error,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-514934007,1,['error'],['error']
Availability,"Hey, not part of the Cromwell team but thought I'd try to help out. To clarify, you've:; - Built a Docker container with SGE + mysql; - Where `qsub` is not available through your `$PATH`, but installed at `/opt/gridengine/bin/lx-amd64/qsub`; - A (_virtual?_) SGE cluster is running within the container; - Running Cromwell inside this container; - Asking the cluster inside your docker container to spin up another Docker container. If this is correct, I'm struggling to understand the motivations behind it, but a few pointers:. - What does intermittent errors mean?; - You should avoid running Docker-in-Docker (SO: [Is it ok to run docker from inside docker?](https://stackoverflow.com/questions/27879713/is-it-ok-to-run-docker-from-inside-docker)); - It might be more predictable add `qsub` to the docker's path.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484:156,avail,available,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5334#issuecomment-571316484,2,"['avail', 'error']","['available', 'errors']"
Availability,"Hey,. I am trying to use TESK as a backend for a cromwell server (version 51) just running a simple task to test if it works (echo ""Hello World"" using an alpine image) and it does not work. TESK receives the input from the server with the correct syntax, however, the script files and all other files generated by cromwell are pointing to a local directory which TESK does not have (TESK is running in a kubernetes cluster). Maybe I am missing something but I this behaviour with creating local files does not work with a kubernetes cluster. Can I change it by setting the config differently or what is a possible solution? Is there anyone who is experiences with Cromwell-TESK?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3928#issuecomment-654851201:126,echo,echo,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3928#issuecomment-654851201,1,['echo'],['echo']
Availability,"Hi , ; When submitting jobs requiring GPU, we specified in the runtime session: ; gpuCount: 2; gpuType: ""nvidia-tesla-k80""; the jobs failed with following errors:; 2019/05/03 14:40:50 E: command failed: nvidia-docker | 2019/05/03 14:40:50 Error: Could not load UVM kernel module. Is nvidia-modprobe installed?. The same WDL file (with same docker and runtime attributes) used to work before. Please help!. Thanks!. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4935:155,error,errors,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935,2,"['Error', 'error']","['Error', 'errors']"
Availability,"Hi ,; Im running the GATK [warp joint genotyping pipeline ](https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/joint_genotyping/JointGenotyping.wdl)cromwell on GCP backend . The pipeline fails because cromwell cannot localize files with certain data types like ` Array[Array[String]]`. This issue was reported on the [terra website](https://support.terra.bio/hc/en-us/community/posts/4409388371611-How-do-I-pass-an-array-array-file-to-another-task-) too and the workaround was to write a task to read file into an array, I have performed the workaround but are there plans to fix this issue in any upcoming releases. **Cromwell version tested :** 85. **Are you seeing something that looks like a bug? Please attach as much information as possible.** ; `""Failed to evaluate 'sample_name_map_lines' (reason 1 of 1): Evaluating read_tsv(sample_name_map) failed: Failed to read_tsv(\""gs://wgs/test/sample_map.txt\"") (reason 1 of 1): java.lang.IllegalArgumentException: Could not build the path \""gs://wgs/test/sample_map.txt\"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, LinuxFileSystem. Failures: \nHTTP: gs://wgs/test/sample_map.txt does not have an http or https scheme (IllegalArgumentException)\nLinuxFileSystem: Cannot build a local path from gs://wgs/test/sample_map.txt (RuntimeException)\n Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""`. **Which backend are you running?** ; GCP. **Link to the workflow if possible**; https://github.com/broadinstitute/warp/blob/develop/pipelines/broad/dna_seq/germline/joint_genotyping/JointGenotyping.wdl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7364:1181,Failure,Failures,1181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7364,1,['Failure'],['Failures']
Availability,"Hi - is this in 0.19 proper or 0.19_hotfix? I believe this was fixed already in the latter. If you're using the former try the latter, if you're using the latter then clearly I'm wrong :). I'll also point a finger at develop which is radically different (many problems fixed, will have its own new problems) which should be available as 0.20 this week",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1070#issuecomment-228541825:324,avail,available,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070#issuecomment-228541825,1,['avail'],['available']
Availability,"Hi - yes, I can confirm this. Cromwell has had WDL 1.0 support for not quite a year now. You're right that this should be updated (pinging @cjllanwarne )",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4678#issuecomment-466792777:131,ping,pinging,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4678#issuecomment-466792777,1,['ping'],['pinging']
Availability,"Hi @DSLituiev, thanks for your question and welcome to our repo!. I think I know the problem – the inputs in a call do not need the type declarations (like `File` and `String`). Give this a try:; ```; call touBam.unMap {; input:; mapped_bam=mapped_bam,; unmapped_base=bam_base; }; ```; Unfortunately the error is pretty confusing because when a workflow doesn't conform to the grammar, the parser has a very hard time describing what's wrong in meaningful terms (it just kind of freaks out).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5256#issuecomment-548825348:304,error,error,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5256#issuecomment-548825348,1,['error'],['error']
Availability,Hi @DadongZ - I can't replicate this. . I run: `java -jar cromwell-33.1.jar server` (downloaded from ; In my browser I go to `http://localhost:8000/swagger/index.html?url=/swagger/cromwell.yaml` and it works fine. Are you doing something differently than this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3869#issuecomment-403133874:85,down,downloaded,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3869#issuecomment-403133874,1,['down'],['downloaded']
Availability,"Hi @EvanTheB could you check something for me - you should be seeing a message like `Cromwell will watch for an rc file *and* double-check every {} seconds to make sure this job is still alive` when you start your job? (assuming `INFO` level logging is enabled). Then, with that background polling ongoing throughout the job run, if a full iteration of `exit-poll-timeout` has passed since the job stopped running, Cromwell will then mark the job as failed. If that gives you enough to put something more helpful into the docs that would be awesome! If not, I can maybe clarify a bit more? Otherwise we should hopefully be able to cycle round to improving this documentation _eventually_ (though unfortunately I can't make any stronger promises on an ETA than that!)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-485806172:187,alive,alive,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-485806172,1,['alive'],['alive']
Availability,"Hi @aednichols !. > Is it possible to add `500` to `AdditionalRetryableHttpCodes`?. It is possible, but it won't give the result we need.; These codes are used only for `StorageException`s, since other exceptions don't have `getCode` method. Therefore, if we add `500` to `AdditionalRetryableHttpCodes`, Cromwell won't retry IOException caused by `500 Internal Server Error`. > It's also possible that by all 500 errors @cjllanwarne means 5xx. We did not think about it :) I think you're right, but just in case we will wait for an answer.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521675399:368,Error,Error,368,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521675399,2,"['Error', 'error']","['Error', 'errors']"
Availability,"Hi @antonkulaga - as far as I know there is no provision in the WDL spec to subset a `Directory` like this (nor do I believe this was intended behavior). As such, I'm closing this issue because the error you describe indicates Cromwell is correctly implementing the WDL spec. . If I'm incorrect and there is indeed something in the spec allowing this please reopen.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5041#issuecomment-504764351:198,error,error,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5041#issuecomment-504764351,1,['error'],['error']
Availability,"Hi @chapmanb - CWL support was out of scope for the group who did the AWS Batch backend support. Some things might work, but others will not. In particular things like `cwl.inputs.json` and `cwl.output.json` with special control files definitely won't work as that requires special wiring on the part of a backend (i.e. not at the Cromwell engine/WOM layer) in order to be successful. We'll get to this eventually but is not on our immediate roadmap. We'd certainly welcome contributions if other groups were interested in more robust AWS/CWL support in Cromwell (that's more of a general comment to any potentially interested parties who see this)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-457697601:528,robust,robust,528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-457697601,1,['robust'],['robust']
Availability,"Hi @cjllanwarne - I'm now chunking my queries, sending small batches of workflow IDs instead of querying for the status of 8k workflow IDs in the same query and I confirm that this resolves the issue. Thanks!; I'd still be in favour of catching this error or even before trying to interpret the query, counting how many terms are there in the json structure and reject it if too large. That way it wouldn't be so easy to bring down the server either intentionally or by mistake :); Cheers!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423617226:250,error,error,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423617226,2,"['down', 'error']","['down', 'error']"
Availability,"Hi @cjllanwarne ; I'm not sure that I'm fully understood what you are wanting. > leave any other new IOExceptions un-retried. Do you mean that we should leave the existing case from PR #4272 as is? Instead of changing it, we should add a case for any throwable to check whether it contains ""500 Internal Server Error"", right?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521653694:311,Error,Error,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521653694,1,['Error'],['Error']
Availability,"Hi @cjllanwarne ; You are right, the file is indeed read twice. There is a problem with reading it only once. . TL;DR This is because reading that file requires an execution context. The metadata that the user sees is taken from the `getMessage` method of the exception (at least right now it's done that way). This means that the error file must be read when creating the `WrongReturnCode` exception and this information must be embedded in the exception object.; In the meantime, [`expandFailureReasons`](https://github.com/broadinstitute/cromwell/blob/c45343ec33f922c9784667a9aa1d42ed68f8c9ba/engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala#L323) tends to add the content of the error file to every `KnownJobFailureException` exception (`WrongReturnCode` is the subclass of it). This means that if we want to get rid of double reading an error file, we must read this file at the moment the exception is created. In that case, the `expandFailureReasons` will have guarantees that there is no need to read it again.; The ideal solution is to let every `KnownJobFailureException` read the error file automatically during creation. This would have solved the metadata issue and eliminated the need to read the file in the `expandFailureReasons`.; But there is a huge problem. Reading the error file requires an execution context. Which means creating such an exception would require an execution context. This may break existing code.; That is the essence of a problem. In order not to read the file twice, the `expandFailureReasons` method needs guarantees that the file has already been read. To give these guarantees, we need to change the signature of the exception constructor so it will require execution context. But it may break existing code.; Although there is a workaround. I can add an `optionalErrorMessage` field to the `KnownJobFailureException` exception with the default value `None`. The `expandFailureReasons` method will check whether this field `Some` or",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519673809:331,error,error,331,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519673809,3,['error'],['error']
Availability,"Hi @cjllanwarne, thanks for the response! Actually, my examples passed validation by `wdltool`, but I justed tested them with the current version of `womtool` and they did not pass the validation, and the errors are meaningful. I think it's safe to dismiss the bug label now.; ```Unable to build WOM node for Scatter '$scatter_0': Unable to build WOM node for WdlTaskCall 'testtask': Cannot build expression for 'Test_optional.testtask.str = strings1[idx]': Invalid indexing target. You cannot index a value of type 'Array[String]?'```. ```Unable to build WOM node for Declaration 'num': Cannot build expression for 'Test_optional.num = length(strings1)': Unexpected arguments to function `length`. `length` takes a parameter of type Array but got: Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType)))```. ```Unable to build WOM node for Declaration 'string_pair': Cannot build expression for 'Test_optional.string_pair = zip(strings1, strings2)': Unexpected zip parameters: Vector(Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))), Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))))```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-428632555:205,error,errors,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-428632555,1,['error'],['errors']
Availability,Hi @cowmoo - this appears to be retrying across all backends no matter what the error was. Is that what you intended for this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-295843178:80,error,error,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-295843178,1,['error'],['error']
Availability,Hi @drkennetz - this seems quite similar in intent to an existing (opt-in) option for checking exit-code timeouts (https://cromwell.readthedocs.io/en/develop/backends/HPC/#exit-code-timeout). Since the timeout function would catch all failures (eg even if the script doesn't get a chance to trap the signal) I suspect it's more generally useful. What do you think?. cc @EvanTheB and @rhpvorderman since they probably know at least as much about why we went down the `exit-code-timeout` route as I do... 😄,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956:235,failure,failures,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107#issuecomment-519602956,2,"['down', 'failure']","['down', 'failures']"
Availability,"Hi @drkennetz ; Thank you for the info. The error you get and the usage scenario are very similar to issue #5085. I'm not sure, but I think that your problem has the same reason.; I already made a PR #5104 that should fix this. ; Although I'm not sure that Cromwell’s team will accept it, you can try to use it. If you know how to assemble a project in a .jar file from source code, then do it from the branch of this PR and run your workflow. I think it should run normally.; Alternatively, you can try to run Cromwell (the one that you have) in a server mode and submit your workflow using REST API or Swagger (it provides nice GUI).; If you'll make any of this, let me know if that helped. If it didn't help, please give me some examples of what you're running so I can reproduce it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-519947699:44,error,error,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-519947699,1,['error'],['error']
Availability,"Hi @dspeck1 ,. Thanks for your reply. The is the cromwell outputs when things happened:. ```; =======================log start============. status_events {; description: ""Job state is set from QUEUED to SCHEDULED for job projects/A_JOB_ID.""; event_time {; seconds: 1713287682; nanos: 566509009; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from SCHEDULED to RUNNING for job projects/A_JOB_ID.""; event_time {; seconds: 1713287919; nanos: 96623968; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from RUNNING to FAILED for job projects/A_JOB_ID. Job failed due to task failures; . For example, task with index 0 failed, failed task event description is Task state is updated from RUNNING to FAILED on zones/A_INSTANCE_ID due to Spot VM; preemption with exit code 50001.""; event_time {; seconds: 1713288624; nanos: 767597866; }; type: ""STATUS_CHANGED""; }. task_groups {; key: ""group0""; value {; counts {; key: ""FAILED""; value: 1; }; instances {; machine_type: ""e2-standard-2""; provisioning_model: SPOT; task_pack: 1; boot_disk {; type: ""pd-balanced""; size_gb: 30; image: ""projects/batch-custom-image/global/images/batch-cos-stable-official-20240320-01-p00""; }; }; }; }; run_duration {; seconds: 705; nanos: 670973898; }. 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GcpBatchAsyncBackendJobExecutionActor [UUID(0c7363b7)Test.mergeTest:NA:1]: Status change fr; om Running to Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - isTerminal match terminal run status with Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for un",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:631,failure,failures,631,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630,1,['failure'],['failures']
Availability,"Hi @dtenenba , we fully appreciate the importance of call caching and are looking into this. can I confirm a few things:. * that this is occurring on different files each run?; * you are seeing it every run of non-trivial size; * You have experienced at least one call-cache success run of any workflow (including a trivial one). This will help me narrow down what is going on. . To be clear, this should be working and we are aware that hashing is not a manual process but a simple value lookup.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894:355,down,down,355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457307894,1,['down'],['down']
Availability,"Hi @ernoc - this appears to be caused by the number of constraints you're specifying in your workflow query. - Short term, could you try making fewer constraints in your queries? We believe this is happening because slick is recursing per constraint, and thus hitting stack overflow when you have too many? Alternatively you could increase the JVM stack-overflow limit when running Cromwell to accommodate the number of constraints you need.; - Medium term, we should catch this (either in advance or literally catch the exception) and return ""unsupported operation"", rather than allowing this to bring down the entire Cromwell server.; - Longer term, we could try to restructure the query to support this level of querying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423597827:603,down,down,603,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423597827,1,['down'],['down']
Availability,"Hi @ffinfo - thanks for contributing this!. My only concern as-is is that this has the potential to overload HPC clusters (see the discussions in https://github.com/broadinstitute/cromwell/issues/1499). In your case, since you're aware of the dangers and willing to go ahead regardless so I don't see why we shouldn't let you - but I don't think this should be the default behavior for unsuspecting users. . Would you be willing to make this a configurable option in the backend config (something like `check-alive-all-jobs`)?. Longer term if you start hitting HPC limits perhaps we can circle back round to the batching solutions hinted at in #1499",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-422877292:509,alive,alive-all-jobs,509,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-422877292,1,['alive'],['alive-all-jobs']
Availability,"Hi @ffinfo would you might rolling back the `RetryAbortedJobs` changes and submitting them again as a separate PR? . I suspect that you're conflating `abort` as something external vs `abort` as something that Cromwell does itself (eg from a REST request or while it's shutting itself down) - and we need to be careful to get all of those interactions right - especially if this affects other backends. In any case, I think it's worth having it properly reviewed as its own change (rather than having it delay an otherwise approved PR 😄).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-427009938:284,down,down,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-427009938,1,['down'],['down']
Availability,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:518,recover,recover,518,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,6,"['alive', 'recover']","['alive', 'recover']"
Availability,"Hi @geoffjentry . I'm currently trying to figure out how to fix this issue, but I'm just starting to understand this project. Therefore, I want to clarify a few things.; As far as I understand, the problem is that some special (ad hoc) files are placed in ""/cromwell_root/path/to/input_file.txt"", while Cromwell expects them to be in ""/cromwell_root/ad_hoc_file.txt"" in order to execute them.; But what exactly is wrong here? It seems like either adhoc files are placed in the wrong directory and must be moved to ""/cromwell_root"" or their location is correct and it's Cromwell's fault that it tries to find them in the wrong directory. Which of these options is correct?; Also, I see you assigned yourself to this issue. Does this mean that help is no longer needed?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-506510217:580,fault,fault,580,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-506510217,1,['fault'],['fault']
Availability,Hi @geoffjentry Thanks for your response. I was trying to walk throught `[Server Mode](http://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/)`. On the `Start the job` section I wasn't able to choose files. I downloaded 33.1 from [(https://github.com/broadinstitute/cromwell/releases)]. Here is the system version:; ```; $ lsb_release -a; No LSB modules are available.; Distributor ID:	Ubuntu; Description:	Ubuntu 16.04.4 LTS; Release:	16.04; Codename:	xenial; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3869#issuecomment-403045308:218,down,downloaded,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3869#issuecomment-403045308,2,"['avail', 'down']","['available', 'downloaded']"
Availability,"Hi @geoffjentry, got an example of a fork in the liquibase scripts? One workaround that I'm already using in a couple of places is having a separate `changeSet` specific to postgres - for example `addAutoIncrement` does not really work the way it does in MySQL. But mostly I'm just enforcing the existing table/column name case conventions (Postgres wants to convert them all to lower-case). The main problem right now is the `IMPORTS_ZIP` column in `WORKFLOW_STORE_ENTRY` - the migrations are working but as soon as I try to run a workflow I get this:; ```; ERROR: column ""IMPORTS_ZIP"" is of type bytea but expression is of type bigint at character 335; ```; Typing this as a `Blob` in Slick appears to be the root of the problem, but I'll keep poking at it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-474911248:559,ERROR,ERROR,559,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-474911248,1,['ERROR'],['ERROR']
Availability,"Hi @hkeward, would you mind updating your branch from our latest `develop`? We have some important test reliability fixes that need picking up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-574767372:104,reliab,reliability,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5273#issuecomment-574767372,1,['reliab'],['reliability']
Availability,"Hi @likeanowl, would you mind updating your branch from our latest develop? We have some important test reliability fixes that need picking up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-574777216:104,reliab,reliability,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-574777216,1,['reliab'],['reliability']
Availability,Hi @markjschreiber I'm also running into this error. I am using cromwell 53 with a custom cdk stack based on the CloudFormation infrastructure described here: https://docs.opendata.aws/genomics-workflows/. Are modifications needed for compatibility with newer versions of Cromwell? Are these documented somewhere?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-689558662:46,error,error,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-689558662,1,['error'],['error']
Availability,"Hi @meganshand, I just re-looked at the WDL and it looks like you're using string interpolation where it isn't currently supported. Right now it's only available in `command` blocks. I don't know whether that'll solve the issue you're seeing but it's almost certainly causing problems!. E.g. These `${...}` won't be expanded and will probably be passed in verbatim to bash, where who knows how they'll be interpreted: ; ```; Array[File] bams = [""/seq/picard_aggregation/${PROJECT}/${CLEAN_SAMPLE}/current/${CLEAN_SAMPLE}.bam"", ""/seq/picard_aggregation/${PROJECT}/${CLEAN_SAMPLE}/current/${CLEAN_SAMPLE}.bam""]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285374768:152,avail,available,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1826#issuecomment-285374768,1,['avail'],['available']
Availability,"Hi @mepowers, thanks for updating this for us. Two quick comments:. 1. It looks like this is now a match to the example in [cromwell.example.backends/slurm.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/slurm.conf) but if you could double check that they really match, that'd be very helpful.; 2. I believe that `mem-per-cpu` is a slurm-instance-specific option (ie it's not necessarily globally available)? If so, I wonder if there's any way to indicate that alongside the examples?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5151#issuecomment-527502667:437,avail,available,437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5151#issuecomment-527502667,1,['avail'],['available']
Availability,"Hi @myazinn, I have had trouble doing the same thing with cromwell 44 for CWL. I cannot import tools.zip to run workflows. . `java -jar ~/bin/cromwell-44.jar run echo_cat_wf.cwl -i in.yml -p tools.zip` gives me error:. ```; Workflow input processing failed:; Invalid workflow reference: echo_cat_wf.cwl; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-519931357:211,error,error,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-519931357,1,['error'],['error']
Availability,"Hi @myazinn, sorry for the slow response time, and I haven't really looked at this in detail, but it looks like we'll now be calling the same function twice for the same stderr file (once for the job message and once for the workflow message)? Is that right?. What I mean is, when a task within a workflow fails, do we now download the same stderr file twice?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519629045:323,down,download,323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5096#issuecomment-519629045,1,['down'],['download']
Availability,"Hi @natechols – this is not a request I've heard before (not to say that's conclusive in any way) but it seems reasonable. We don't have any current plans for work in this area so a PR would be helpful. In particular, it looks like a natural place could be the existing list of `export`s in the generated run script. ```; #!/bin/bash. cd /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution; tmpDir=$(mkdir -p ""/cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/tmp.7e77d324"" && echo ""/cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/tmp.7e77d324""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution. ); outb631bc66=""${tmpDir}/out.$$"" errb631bc66=""${tmpDir}/err.$$""; mkfifo ""$outb631bc66"" ""$errb631bc66""; trap 'rm ""$outb631bc66"" ""$errb631bc66""' EXIT; tee '/cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution/stdout' < ""$outb631bc66"" &; tee '/cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution/stderr' < ""$errb631bc66"" >&2 &; (; cd /cromwell-executions/aliased_subworkflows/f0c1deee-cdab-4ae4-8165-f053a3d7f164/call-subwfT/subworkflow.subwf/b631bc66-ba89-427b-a9ec-6fe7c9e5361b/call-increment/shard-2/execution. echo $(( 2 + 1 )); ) ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-561687838:723,echo,echo,723,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-561687838,1,['echo'],['echo']
Availability,"Hi @notestaff and @huangzhibo -; I'm currently investigating this issue and trying to reproduce the problem.; Since I'm not familiar with WDL, I took workflow examples from [here](https://cromwell.readthedocs.io/en/stable/SubWorkflows/) and put `sub_wdl.wdl` in a subdirectory.; Therefore, the `main.wdl` looks like ; ```; import ""sub_dir/sub_wdl.wdl"" as sub. workflow main_workflow {; call sub.hello_and_goodbye { input: hello_and_goodbye_input = ""sub world"" }; # call myTask { input: hello_and_goodbye.hello_output }; output {; String main_output = hello_and_goodbye.hello_output; }; }; ```; The body of `sub_wdl.wdl` wasn't changed; ```; task hello {; String addressee; command {; echo ""Hello ${addressee}!""; }; output {; String salutation = read_string(stdout()); }; }; task goodbye {; String addressee; command {; echo ""Goodbye ${addressee}!""; }; output {; String salutation = read_string(stdout()); }; }; workflow hello_and_goodbye {; String hello_and_goodbye_input; call hello {input: addressee = hello_and_goodbye_input }; call goodbye {input: addressee = hello_and_goodbye_input }; output {; String hello_output = hello.salutation; String goodbye_output = goodbye.salutation; }; }; ```; I put `sub_wdl.wdl` into a subdirectory; ```; mkdir sub_dir; mv sub_wdl.wdl sub_dir/sub_wdl.wdl; ```; zipped this subdirectory; ```; zip -r sub_dir.zip sub_dir/; ```; and ran a workflow with the following commad; ```; java -jar /home/path/to/cromwell/cromwell.jar run /home/path/to/files/main.wdl --imports /home/path/to/files/sub_dir.zip; ```; The workflow succeeded without any problem.; Maybe my workflow is not reproducing this issue because I'm doing something wrong. I tried to run this workflow using Cromwell 41 and 45. Workflows succeeded in both cases. Can you tell me what should be changed in order to reproduce your problem?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-519025813:684,echo,echo,684,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-519025813,2,['echo'],['echo']
Availability,"Hi @ruchim !. 1. Do you see anything in your logs that indicate db errors?. No. I see that the SGE job completed (`Status change from Running to Done`). 2. What does your db config look like?; ```; database {; db.url = ""jdbc:mysql://.../${CROMWELL_DB}?useSSL=false&rewriteBatchedStatements=true""; db.user = ...; db.password = ...; db.driver = ""com.mysql.jdbc.Driver""; profile = ""slick.jdbc.MySQLProfile$""; }; ```; backed by a MariaDB instance. 3. When you report the REST endpoint shows the workflow as 'Running', what about the executionStatus key in the metadata? Are some jobs marked as 'Running' as well?. The SGE job reported as running as well. I manually query the database, and I see no changes in the `JOB_STORE_ENTRY` table when the SGE job completes and the corresponding entry appears in the Cromwell logs (although not entirely sure I should see something). 4. Do you see this behavior only with large scatters (10K) or do you see it with smaller scatters as well? Or any other type of workflow shape?. I've only observed this behaviour with large scatters AND a file-of-file-names approach. I don't know exactly what combination of WDL features or what threshold of scatter width triggers it .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445788979:67,error,errors,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-445788979,1,['error'],['errors']
Availability,"Hi @ruchim ,; Thanks for asking.; For example, normally, in the alignment, we need to provide the big fasta files as input.; So, it will download from s3 for each single job.; We have all the reference files in our EFS. For our own usage, we mount the EFS into every job definition. So the batch job can access the EFS directly. They don't need to download every time from S3.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-493492027:137,down,download,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-493492027,2,['down'],['download']
Availability,"Hi @ruchim!. Regarding our current test setup:; We (Brian O., Alex B. and I) are currently using a very minimal test configuration:. Workflow:; GA4GH md5sum from Dockstore; https://dockstore.org/workflows/github.com/briandoconnor/dockstore-workflow-md5sum/dockstore-wdl-workflow-md5sum:1.4.0. Single File:; Source: UChicago Gen3 Data STAGE crai file; DRS URL: dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c; Size: 1.49mb. I just now ran this test configuration from scratch, starting with a new workspace, and it failed like all the others have:. Error:; ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	; ```. Log:; ```; 2019/07/16 20:23:02 Starting container setup.; 2019/07/16 20:23:10 Done container setup.; 2019/07/16 20:23:16 Starting localization.; 2019/07/16 20:23:22 Localizing input dos://dg.4503/2132c569-06e7-474c-8806-93aa116c5d1c -> /cromwell_root/topmed-irc-share/genomes/NWD844894.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. The name of this workspace is `mbaumann test md5sum 20190716` and I have shared it with you as Owner, in case you would like to investigate. Regarding successful runs in Commons in 2018:; The last reported success that I am aware of was by Moran Cabi ali (then Broad) in mid-2018, when she did demos of obtaining data from UChicago (Windmill) and UCSC (Boardwalk).; I didn't actually run the workflow myself.; There are still some of the demo workspaces from that time available in Terra, which I can access yet don't have permission to share. I don't know if you can access them or not. One such workspace is:; `Team Calcium July 1 Demo - Boardwalk-Windmill_WS`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334:551,Error,Error,551,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069#issuecomment-511990334,3,"['Error', 'avail', 'error']","['Error', 'available', 'error']"
Availability,"Hi @ruchim,. Personally, the reason I'm asking for Mesos support is because we have a Mesos cluster available, but no yarn. It's awesome cromwell already supports yarn, but it'd be great to have even more options. Cheers; M",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-416991555:100,avail,available,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-416991555,1,['avail'],['available']
Availability,"Hi @seandavi - That does seem like it should work. Thinking back in my past I've definitely encountered tools which expect TMPDIR to exist and aren't smart enough to create it themselves. Also in JES there shouldn't be any issues with permissions, etc. We'd certainly welcome a PR if you're game for it, either (or both) against `0.19_hotfix` or `develop`. On that note, I should point out that a new release (currently `develop`) is imminent and for all but one use case (call caching) we beliee it to be more robust/stable that 0.19. I'd personally recommend people who don't need call caching work with the new system, but I understand that some people aren't comfortable working with code which isn't yet released.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687458:511,robust,robust,511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687458,1,['robust'],['robust']
Availability,"Hi @tAndreani ,. Were you able to fix this error? I'm actually having a similar issue where I'm providing the files in a singularity container as an input to the workflow and localization via hard link and copy fails. I would appreciate your help. Thanks,; Chetana",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580533224:43,error,error,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580533224,1,['error'],['error']
Availability,"Hi @vsoch - it shouldn't require too much of a deep dive into the scala, we know that it already can be made to work with `udocker` by just changing the configuration like you've done. Let me know if you've not seen the udocker example and I'll track it down for you.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412505720:254,down,down,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412505720,1,['down'],['down']
Availability,"Hi @vsoch - the first problem to solve is how to represent the usage of singularity in one's WDL (not sure how CWL does it, will need to look). This is being discussed in the [OpenWDL group](https://github.com/openwdl/wdl/pull/237) so if you have thoughts here that'd be very welcome. . For instance, is there a way to express ""run this container"" but not be locking a downstream WDL user into Singularity vs Docker?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411568103:369,down,downstream,369,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411568103,1,['down'],['downstream']
Availability,"Hi @vsoch,. Lot's of good stuff here on first glance. I'll dive deeper over the weekend. For better or worse, depending on pricing, support, reliability, etc. etc. etc. we like to move around our CI. I personally also like being able to test scripts on my laptop as much as possible. To that end, I'm trying to advocate for bash scripts that are then invoked from whatever CI we choose. I haven't RTFM'ed enough of this PR nor CircleCI's manual yet to fully grasp what specific Circle features are being used here. Could a lot of the logic be separated from the `.circleci/config.yml` into a script, or multiple scripts if necessary?. On a related note, based on your expertise I may want to pick your brain to go over our [existing CI scripts](https://github.com/broadinstitute/cromwell/blob/develop/src/ci/bin/test.inc.sh#L38-L39) too as we move to Circle, or perhaps something even ~shinier~ [newer](https://news.ycombinator.com/item?id=17602838). Re your build failing: it wasn't anything in your PR. Based on the logs there was a weird connection issue between Travis and Github returning HTTP 5xx errors during the tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-413988228:141,reliab,reliability,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-413988228,2,"['error', 'reliab']","['errors', 'reliability']"
Availability,"Hi @wleepang , I looked at the `-proxy` logs for one of the jobs that failed with this error and saw (among other things):. ```; download failed: s3://fh-ctr-public-reference-data/workflow_testing_data/WDL/unpaired-panel-consensus-variants-human/smallTestData.unmapped.bam to ../cromwell_root/fh-ctr-public-reference-data/workflow_testing_data/WDL/unpaired-panel-consensus-variants-human/smallTestData.unmapped.bam [Errno 28] No space left on device; ```. So it seems like maybe this is a scratch space issue? I thought that Cromwell/AWS batch just automatically created more scratch space when it was needed, but that seems to not be happening. Any suggestions for troubleshooting the problem?. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468377844:87,error,error,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468377844,2,"['down', 'error']","['download', 'error']"
Availability,Hi @zhilizheng - Please post the output or error logs. We will review.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061418802:43,error,error,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061418802,1,['error'],['error']
Availability,"Hi All, . I am currently running Cromwell 36 in AWS batch (both Server and run mode) in one account, lets say ACCT1, and all of my resource files are cross account in another account (ACCT2). Intermittently jobs will fail due to a failure to download a reference file. Error seen through the Cromwell proxy logs:. > aws s3 cp --sse AES256 --no-progress s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt /cromwell_root//s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt download failed: s3://s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt to ../cromwell_root/s4-somaticgenomicsrd-benchmark-gatk4/database/1.0/ucsc.hg19.fasta.bwt (""Connection broken: error(104, 'Connection reset by peer')"", error(104, 'Connection reset by `peer')). I'm wondering if it is in the scope of Cromwell on AWS to have a file download retry, or check at least, rather than continuing the task with an empty file which may persist silently causing troubleshooting to be quite difficult. Best,; Adam",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4710:231,failure,failure,231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4710,7,"['Error', 'down', 'error', 'failure']","['Error', 'download', 'error', 'failure']"
Availability,"Hi All, just checking in on this issue, to see if it is still alive. Would love to see ecr supported for hash-lookup.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4171#issuecomment-1332235511:62,alive,alive,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4171#issuecomment-1332235511,1,['alive'],['alive']
Availability,"Hi All,. Im running into an issue with my deployment of Cromwell 65. I am running scripts connecting to a local MySQL(also tested on MariaDB). Upon running a reasonably complex pipeline I am receiving a number of database errors:. ```; java.sql.BatchUpdateException: Data truncation: Data too long for column 'METADATA_KEY' at row 6; 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:78); 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499); 	at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480); 	at com.mysql.cj.util.Util.handleNewInstance(Util.java:192); 	at com.mysql.cj.util.Util.getInstance(Util.java:167); 	at com.mysql.cj.util.Util.getInstance(Util.java:174); 	at com.mysql.cj.jdbc.exceptions.SQLError.createBatchUpdateException(SQLError.java:224); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchedInserts(ClientPreparedStatement.java:755); 	at com.mysql.cj.jdbc.ClientPreparedStatement.executeBatchInternal(ClientPreparedStatement.java:426); 	at com.mysql.cj.jdbc.StatementImpl.executeBatch(StatementImpl.java:796); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$18(JdbcActionComponent.scala:542); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:425); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:420); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:489); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6545:222,error,errors,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6545,1,['error'],['errors']
Availability,"Hi Atlas Team,. . I have installed the Atlas(apache-atlas-sources-2.1.0) in; our server by following in the link; ""https://atlas.apache.org/2.0.0/InstallationSteps.html. After all the setup; have been done, we ran the atlas-start.py and the atlas is running on port; 21000.When we are accessing the atlas, we are facing 503 Service Unavailable; Error. We checked the logs from application.log file. We got below issue . . 2020-12-13 06:29:02,309 WARN - [main:] ~ JMX is not enabled to receive; remote connections. Please see cassandra-env.sh for more info.; (CassandraDaemon:81). 2020-12-13 06:29:02,310 ERROR - [main:] ~ cassandra.jmx.local.port missing; from cassandra-env.sh, unable to start local JMX service.null; (CassandraDaemon:87). . We see there is no ""cassandra-env.sh"" file in atlas and we; tried other ways and didn't find any solution for the above error. . Could you please help us in rectify these problem so that it; will be very helpful to us.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6133:345,Error,Error,345,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6133,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"Hi Brad, thanks. I'm looking at the error and the paths seem to have the same pathological repetition of the root (`bcbiotest/gcp/work_cromwell/main-somatic.cwl/c21b8bf4-9f80-45a3-9a23-f345b4d8f295`) that I was hoping to have fixed in that PR I mentioned before. Can you double check you're running from the latest develop ?; I'm re-running it right now as well with a longer root path to see if I can reproduce it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4471#issuecomment-445918021:36,error,error,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4471#issuecomment-445918021,1,['error'],['error']
Availability,"Hi Brad,. Thanks for reporting this. This error definitely should not occur but I'm surprised that it's causing Cromwell to freeze. Have you seen any other error in the log further down ?. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386320045:42,error,error,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386320045,3,"['down', 'error']","['down', 'error']"
Availability,"Hi Chetana,. in my case, the problem was the variable""file_format"" that I was passing to cutadapt . `cutadapt -f ${file_format}`. in the json file, one of the input was: `""scMeth.file_format"": ""fastq""`, but cutadapt didn't like it. Therefore I have substituted the initial command above with:. `cutadapt -f fastq`. or I have substitute `File file_format` with `String file_fomat` in the first step of the pipeline. Basically I was passing a file but in reality, was just a string for cutadapt. I don't know if this might help. If you type the error from Cromwell maybe I can help you better. Best; Tommaso",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580619402:543,error,error,543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066#issuecomment-580619402,1,['error'],['error']
Availability,"Hi Chris, thanks for the reply. Both of these workarounds are good, and it's nice to have them written down. Having said this I think the issue is actually that Cromwell isn't correctly implementing the CWL spec, because the H3ABio workflow is technically correct to use relative imports. I think it would be good for us to have a way to solve this without having to change the CWL itself (which is why my two suggestions would only involve changing the Cromwell submission but not the CWL).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4505#issuecomment-449248258:103,down,down,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4505#issuecomment-449248258,1,['down'],['down']
Availability,"Hi Cromwell Team; ; I am writing in respect to an issue that I am having with using AWS + Cromwell + MySQL server. Not sure if this is the best place to ask because I’m not sure if the issue is Cromwell specific. It might be related to AWS backend configuration. But I couldn’t figure out the problem and I figured you might be able to provide some insight.; ; I have a docker image that has MySQL server installed. I’m using percorna-server-5.6 specifically because I need to use MySQL5.6. There is a Cromwell task which does the following:; 1. Start mysql server. The first line of the WDL task is literally `service mysql start`; 2. Initialize the database and load Vcf files into the database.; 3. Run some SQL query and perform some analysis; ; And the above Cromwell task need to be run for multiple samples. ; ; So, first I did step 1-3 manually without using WDL just to make sure that I can run multiple MySQL docker container just fine. And it worked. So then the next step is test the whole WDL using LOCAL backend. And it ran fine. But when I submit the same WDL script to AWS Batch, the first task will always succeed and the subsequent tasks will always fail with port connection error because all the containers are connecting to port 3306 and port 3306 is already used. Do you know why it is trying to connect to port 3306? My issue was that it worked when running locally. So, I’m wondering if there’s something with how the docker run command was submitted to AWS Batch or EC2 is configured?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4688:1194,error,error,1194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4688,1,['error'],['error']
Availability,"Hi Cromwell team,. I've heard rumors that lately you've been plagued with the same intermittent 503 errors using the GCS NIO library that have plagued the GATK. We have what we believe is a definitive fix for this issue up at https://github.com/GoogleCloudPlatform/google-cloud-java/pull/2281. To take advantage of the fix, client code needs to set the library-level retry/reopen settings to be sufficiently aggressive. The settings GATK uses (which have cleared up our 503 issues at scale completely) can be seen in the `setGlobalNIODefaultOptions()` method in this class:. https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/gcs/BucketUtils.java. Mentioning @LeeTL1220 on this ticket by request",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2495:100,error,errors,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2495,1,['error'],['errors']
Availability,"Hi I did try to build DAG for a task from WARP pipeline, and wom failed with an error; https://github.com/broadinstitute/warp/issues/438; Turns out it it no a problem of a pipeline, but possibly a bug in a wom. Best, Eugene",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6499:80,error,error,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6499,1,['error'],['error']
Availability,"Hi Jeff,. You've built this great Actor system, but it needs to be actors on both ends. The round-robin pool of actors is not an actor system anymore if you cannot pass a Promise/Future/ActorRef to the other side, even if the API/channel capacity is limited. The status response should happen in less than a second, not an hour. We both know that we can have [millions of Actors in Akka/Scala](http://doc.akka.io/docs/akka/snapshot/general/actor-systems.html#What_you_should_not_concern_yourself_with), and the throughput on the Google network is huge. Thus no API limits should be prohibitive. I suggested using Pub/Sub API here:. https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229703152. And if that's not an option, you can implement the whole Google Genomics Pipeline API super-easy as an ephemeral GCE instance, which really is just becomes a promise/future. I even broke it down in a [post here](https://groups.google.com/forum/#!topic/google-genomics-discuss/_ox9h-C0_50), specifically in the paragraph that starts with **_So you might ask what exactly is an Operation resource_**:. https://groups.google.com/forum/#!topic/google-genomics-discuss/_ox9h-C0_50. You know my philosophy, always build it yourself to bypass any limitations :). `p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260214027:901,down,down,901,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260214027,1,['down'],['down']
Availability,"Hi Jon,. This isn't directly supported right now. You could change the; CloudFormation template so that your customized mount is mounted to the EC2; nodes. It is also possible to mount EFS directly into an AWS Batch; container through the job definition but that would require changes in; Cromwell's AWS Batch backend. Using EFS with containers for Cromwell; workflows is something we are investigating but there are some stress tests; that we need to do at scale to see if sufficient IOPs are available. On Wed, May 5, 2021 at 11:29 AM microbioticajon ***@***.***>; wrote:. > Hi Guys,; >; > This is more of a question/request than a bug report. Apologies if this is; > not the place to ask.; >; > Im trying to run Cromwell with an AWS backend. A number of our workflows; > make extensive use of very large reference files. To avoid localising the; > same huge file over and over (wasting time and space) I want to copy these; > reference files to an additional volume during batch node initialisation; > and mount to each container (rather than using File arguments I would use a; > simple String argument to prevent localisation - I appreciate this is a; > hack). I am already doing this with a different pipeline framework with; > some success, however it requires the JobDefinition to specify the mount; > locations between the node(host) and job container; >; > Is it possible to provide additional mount/volume instructions to the aws; > batch backend in the cromwell.conf?; >; > If this is possible, I cannot see any specific examples in the Cromwell; > docs. If this is not currently possible, could I request adding the ability; > to define additional mount points as a feature request??; >; > Kind Regards,; > Jon; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6334>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EI7XBOPHMWSYW3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484:494,avail,available,494,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484,1,['avail'],['available']
Availability,"Hi Kris,. That is great news, and of course I totally agree with the notification approach as things scale up ;) Though the issue might be when you get into millions/billions of operations later on, then there are some things that would need to be tweaked for that. As the number of operations scale up, the logging could then also become a bottleneck, as those are also API requests - besides the ones coming from the Pipeline API - and usually is a positive multiplier greater than 1 of the number of operations, with their own Retry requests. I think you'll agree that it's usually better to be more modular, so that things can easily be tweaked and updated over time - such as the transition to Pluggable Backends, but in this case for the Pipeline API directly. I agree with the capability of having fine-grained informational log events, though Pub/Sub API has certain limitations to be aware of:. https://cloud.google.com/pubsub/quotas#other_limits. Don't get me wrong, I'm still excited to see how version 2.0 of the Pipeline API evolves, but there are some tricky scalability issues that might emerge which could make the Cromwell code unnecessarily complex down the line, if one has to work through too many limitations/edge-cases. Paul",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789:1167,down,down,1167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260279789,1,['down'],['down']
Availability,"Hi Luyu,. Thanks for the feedback. This is an interesting case. Normally if there is; a few minutes gap between workflows the instances will be terminated by; batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to; shut down the instances. Also because these files are written to a mounted disk they are not deleted; when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks,; Mark. On Sat, Oct 24, 2020 at 5:27 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; >; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space.; >; > I have checked Cromwell documents and some materials from AWS, as well as; > issue #4323 <https://github.com/broadinstitute/cromwell/issues/4323>. But; > none of them works for me. Thank you in advance for any suggestions.; >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5974>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443:315,down,down,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716230443,1,['down'],['down']
Availability,"Hi Richard,. The Cromwell server is responsible for updating the database. The general; flow of information is AWS Batch -> Cromwell AWS Batch Backend Module ->; Cromwell Metadata actor -> DB. Cromwell only becomes aware of a failure if AWS Batch Backend Module; detects a failure in Batch (usually a non zero return code for the job). I; haven't tested it but I think if you define a retry strategy in the job; definition then Cromwell will not even be aware of the retry unless all of; the retries fail. Any or all of the Metadata entries in the database can be deleted if you; observe weird caching behavior. You can even drop the whole DB and the; Cromwell server will regenerate it the next time it starts. On Thu, Nov 19, 2020 at 3:51 AM Richard Davison <notifications@github.com>; wrote:. > When does the database get notified of a job's failure?; >; > - the moment the job fails; >; > or; >; > - when AWS Batch finally gives up trying to run the job; >; > I'm asking because from what I can tell, once a workflow is in a terminal; > state, some records are deleted from the database, which means that it; > would be impossible to try to run a job in a failed state. This is; > precisely what I tested: I navigated to the failed job in AWS Batch, and; > then pressed the ""Clone Job"" button.; >; > Perhaps a better test would be to literally create a new Job Description; > revision (as you pointed out earlier) to see if Batch a failed attempt can; > be rerun without impacting the status of the workflow.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730224182>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EJLBXTJDA4SKYT4Y43SQTMADANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165:226,failure,failure,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-730462165,3,['failure'],['failure']
Availability,"Hi Ruchi,. 1. I have the metadata from a later run which failed in the same way, it's [here](https://gist.github.com/dtenenba/87ba6eaad666071625da6cfe98db9f61). 2. Yes, it seems like most of the BaseRecalibrator tasks transitioned from running to Done.; I see this in the logs, right before the error:. ```; 2018-12-17 17:11:25,368 cromwell-system-akka.dispatchers.backend-dispatcher-3452 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(2951ea9d)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:5:1]: Status change from Running to Succeeded; 2018-12-17 17:11:25,437 cromwell-system-akka.dispatchers.backend-dispatcher-3452 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(2951ea9d)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:10:1]: Status change from Running to Succeeded; 2018-12-17 17:11:27,559 cromwell-system-akka.dispatchers.backend-dispatcher-3452 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(2951ea9d)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Status change from Initializing to Running; 2018-12-17 17:11:36,844 cromwell-system-akka.dispatchers.backend-dispatcher-3452 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(2951ea9d)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:9:1]: Status change from Initializing to Running; 2018-12-17 17:11:51,970 cromwell-system-akka.dispatchers.backend-dispatcher-3452 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(2951ea9d)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:15:1]: Status change from Initializing to Running; 2018-12-17 17:11:53,801 cromwell-system-akka.dispatchers.backend-dispatcher-3452 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(2951ea9d)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:11:1]: Status change from Running to Succeeded; 2018-12-17 17:11:55,351 cromwell-system-akka.dispatchers.backend-dispatcher-3452 INFO - AwsBatchAsyncBackendJobExecutionActor [UUID(2951ea9d)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:7:1]: Status change ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496#issuecomment-447976105:295,error,error,295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496#issuecomment-447976105,1,['error'],['error']
Availability,"Hi Want to run my pipeline in gcp with nvidia-tesla-a100. Get errors for insert machine. Trake into the code since cromwell set n2-custom machine meanwhile a100 require a2-highgpu-1g. I guess it is not a big change but can extend capbilities. runtime {bootDiskSizeGb: 100; disks: ""/mnt 3000 HDD""; gpuType: ""nvidia-tesla-a100""; gpuCount: 1; nvidiaDriverVersion: ""418.87.00""; zones: [""us-central1-c""]; } (edited) . ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6558:62,error,errors,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6558,1,['error'],['errors']
Availability,"Hi Will,. I see, but unfortunately I still get an error - I ran the your updated workflow without the Docker portion and specifically sent an `SIGINT`, and it looks like it ended with an error. Below are the steps - hope this does not affect the launch schedule:. ``` Bash; $ cat error_continue.wdl; task hello {; String addressee; command {; echo ""Hello ${addressee}!"" && kill -SIGINT $BASHPID; }; output {; String salutation = read_string(stdout()); }; runtime {; continueOnReturnCode: true; }; }. workflow w {; call hello; }; $; $ java -jar cromwell.jar inputs error_continue.wdl; This functionality is deprecated and will be removed in 0.18. Please use wdltool: https://github.com/broadinstitute/wdltool; {; ""w.hello.addressee"": ""String""; }; $; $ java -jar cromwell.jar inputs error_continue.wdl > error_continue.json; This functionality is deprecated and will be removed in 0.18. Please use wdltool: https://github.com/broadinstitute/wdltool; $; $ java -jar cromwell.jar run error_continue.wdl error_continue.json; [2016-01-31 16:37:25,449] [info] RUN sub-command; [2016-01-31 16:37:25,469] [info] WDL file: error_continue.wdl; [2016-01-31 16:37:25,471] [info] Inputs: error_continue.json; [2016-01-31 16:37:25,989] [info] Slf4jLogger started; [2016-01-31 16:37:26,86] [info] SingleWorkflowRunnerActor: launching workflow; [2016-01-31 16:37:27,345] [info] Running with database db.url = jdbc:hsqldb:mem:748afb13-e3af-4e9d-af14-5c2b3bd209a9;shutdown=false;hsqldb.tx=mvcc; [2016-01-31 16:37:28,247] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 2a89a995-aa89-4172-a5e1-1054cbccd9e0; [2016-01-31 16:37:28,291] [info] WorkflowManagerActor Found no workflows to restart.; [2016-01-31 16:37:28,660] [info] WorkflowActor [2a89a995]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#-896492658])) message received; [2016-01-31 16:37:28,788] [info] WorkflowActor [2a89a995]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwell-system/user/SingleWorkfl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:50,error,error,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,3,"['echo', 'error']","['echo', 'error']"
Availability,"Hi Will,. Try it without the `&& exit 1` portion, as I feel you are exiting before it can complete the workflow. For instance I ran the following code without the Docker portion on a cluster, and it worked. Below are all the steps for it:. 1) The wdl file:. ``` Bash; $ cat error_continue.wdl; task hello {. String addressee. command {; echo ""Hello ${addressee}!""; }. output {; String salutation = read_string(stdout()); }. }. workflow w {; call hello; }; ```. 2) The json inputs file:. ``` Bash; $ java -jar cromwell.jar inputs error_continue.wdl; {; ""w.hello.addressee"": ""String""; }; $ java -jar cromwell.jar inputs error_continue.wdl > error_continue.json; $; ```. 3) And then I ran it:. ``` Bash; $ java -jar cromwell.jar run error_continue.wdl error_continue.json; [2016-01-25 18:25:31,77] [info] RUN sub-command; [2016-01-25 18:25:31,91] [info] WDL file: error_continue.wdl; [2016-01-25 18:25:31,92] [info] Inputs: error_continue.json; [2016-01-25 18:25:31,562] [info] Slf4jLogger started; [2016-01-25 18:25:31,649] [info] SingleWorkflowRunnerActor: launching workflow; [2016-01-25 18:25:32,777] [info] Running with database db.url = jdbc:hsqldb:mem:65a527dd-bc31-462e-bca9-05a545fea48a;shutdown=false;hsqldb.tx=mvcc; [2016-01-25 18:25:33,796] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 9cdf23a5-1eaa-420a-8fae-ea3e4623d4db; [2016-01-25 18:25:33,812] [info] WorkflowManagerActor Found no workflows to restart.; [2016-01-25 18:25:34,730] [info] WorkflowActor [9cdf23a5]: Start message received; [2016-01-25 18:25:34,997] [info] SingleWorkflowRunnerActor: workflow ID 9cdf23a5-1eaa-420a-8fae-ea3e4623d4db; [2016-01-25 18:25:34,999] [info] WorkflowActor [9cdf23a5]: ExecutionStoreCreated(Start) message received; [2016-01-25 18:25:35,11] [warn] SingleWorkflowRunnerActor: received unexpected message: CurrentState(Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-9cdf23a5-1eaa-420a-8fae-ea3e4623d4db#-1942530845],Submitted); [2016-01-25 18:25:35,",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-174729363:337,echo,echo,337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-174729363,1,['echo'],['echo']
Availability,"Hi all, . I try to modify & deploy my cloud function. . But I get this error:. ![image](https://user-images.githubusercontent.com/8334979/70418119-bcec1e80-1a9d-11ea-8f13-d6809efe63e7.png). Then I [check my bucket information](https://cloud.google.com/storage/docs/using-requester-pays#enable), the ""requester pays"" is disabled. Also, I try to create a new bucket & deploy a new cloud function. The error remains. So, I am wondering is there any other possibilities that could cause this situation?; (maybe MIS team accidentally change some related settings); I can not deploy any cloud function(s) successfully since early today. Please give me some suggestion, thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5311:71,error,error,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5311,2,['error'],['error']
Availability,"Hi all, . I'm encountering an issue when using the AWS Batch backend. I'm using the EFS (local) file system for the backend, not S3. I've got a workflow that downloads fastq files as an initial step. These jobs fail non-deterministically ; a fraction of the time. These jobs are a scatter over an input array of fastq files, and most of them generally complete. However, 20% of the shards might fail in any given scatter. A complete job will have the following outputs in the shard output folder:; ```; download_fastq-0-rc.txt ; download_fastq-0-stderr.log ; download_fastq-0-stdout.log ; input_fastq_specified_R1.fq.gz ; script ; tmp.71626c8d/; ```. When cromwell submits the job, it auto-generates a script to download the fastq. It's a very simple job, so here's an example script:. ```; #!/bin/bash. cd /EFSROOT/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; tmpDir=$(mkdir -p ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27"" && echo ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. ); outed746149=""${tmpDir}/out.$$"" erred746149=""${tmpDir}/err.$$""; mkfifo ""$outed746149"" ""$erred746149""; trap 'rm ""$outed746149"" ""$erred746149""' EXIT; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stdout.log' < ""$outed746149"" &; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stderr.log' < ""$erred746149"" >&2 &; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5421:158,down,downloads,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421,2,['down'],"['download', 'downloads']"
Availability,"Hi all. I've been playing the CWL ResourceRequirement with cromwell under SLURM env.; https://www.commonwl.org/v1.0/CommandLineTool.html#ResourceRequirement. So, if I submit a CWL with; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000; ramMax: 2000; ```; The cromwell will send the command to SLURM with a ridiculously high value of memory; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152000 \; ```; And most likely it will fail because we do not have such a beefy flavor node, since SLURM reads the memory in MB.; ```; sbatch: error: Batch job submission failed: Requested node configuration is not available; ```; More tests like using different units or numbers will lead to similar errors:; Submit; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2GB; ramMax: 2GB; ```; will give me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```; and; ```; - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2; ramMax: 2; ```; gives me; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2097152 \; ```; more for; ``` - class: ResourceRequirement; coresMin: 1; coresMax: 1; ramMin: 2000MB; ramMax: 2000MB; ```; it will give; ```; executing: sbatch --nodes 1 --ntasks 1 \; --cpus-per-task=1 --mem-per-cpu=2000000000 \; ```. So, it seems cromwell will read the numbers in MB and transfer it in bytes to SLURM. However, SLURM probably needs memory in MB as well. (I assume it is a bug when mapping CWL resource key to WDL). Currently, I manage to use an expression in the config `memoryMin/1000000` to circumvent the issue.; I don't think it is a very major issue nor a complex one, so feel free to fix it whenever. The cromwell version is the latest release, cromwell-34, and please find my working config if necessary. [cromwell.slurm.conf.gz](https://github.com/broadinstitute/cromwell/files/2368103/cromwell.slurm.conf.gz)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4080:592,error,error,592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4080,3,"['avail', 'error']","['available', 'error', 'errors']"
Availability,"Hi all;; I'm running into a database write error on some systems. Similar to #3584, this does not cause the pipeline to stop, but is quite noisy and presumably affecting the ability to re-run and query run status. A CWL pipeline that triggers this is `somatic` from this set of minimal CWL test sets (https://github.com/bcbio/test_bcbio_cwl) but it appears to be system specific, rather than data specific. The same pipeline works fine on most systems, but only reports this database write error on some. When running we get this message on the completion of tasks:; ```; [2018-05-09 10:16:59,44] [[38;5;1merror[0m] 1bd683a6:prep_samples_to_rec:-1:1: Failure writing to call cache: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; java.sql.BatchUpdateException: data exception: string data, right truncation; table: CALL_CACHING_HASH_ENTRY column: HASH_KEY; 	at org.hsqldb.jdbc.JDBCPreparedStatement.executeBatch(Unknown Source); 	at com.zaxxer.hikari.pool.ProxyStatement.executeBatch(ProxyStatement.java:128); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.executeBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:532); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:386); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:381); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:448); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:501); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$1.$anonfun$run$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3607:43,error,error,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607,3,"['Failure', 'error']","['Failure', 'error']"
Availability,"Hi all;; In testing release 35 with CWL inputs I've also been looking at supporting remote URL references. This is working correctly for GS URLs but not for http URLs. I've put together a test case that demonstrates the problem:. https://github.com/bcbio/test_bcbio_cwl/tree/master/gcp. The `somatic-workflow-http` CWL workflow uses http URLs and doesn't work, while the comparable `somatic-workflow` CWL workflow uses GS URLs referencing the same data and does work. The workflow fails with:; ```; java.io.FileNotFoundException: Cannot hash file https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genom; es/hg19/seq/hg19.fa; ```; when running tasks. The files get downloaded to the input directories but get numerical values instead of the original file names so never seem to sync over and get translated correctly to the workflow; ```; ls -lh cromwell_work/cromwell-executions/main-somatic.cwl/eaa632df-52a8-4aae-826f-647a42fa7145/call-prep_samples_to_rec/inputs/1515144/; total 136K; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 225050424226294657; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 2612405277530248055; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 503001634356675169; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 5802330287039666628; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 5809676514510180826; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 6090832304768530540; -rw------- 2 chapmanb chapmanb 43 Sep 26 14:07 6105514522473810611; -rw------- 3 chapmanb chapmanb 37K Sep 26 14:07 6807576659333162957; -rw------- 3 chapmanb chapmanb 150 Sep 26 14:07 6853384576121493061; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 7483350933664987331; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 7538690575330349970; -rw------- 3 chapmanb chapmanb 37K Sep 26 14:07 7691692211431528147; -rw------- 2 chapmanb chapmanb 292 Sep 26 14:07 7783203266940950463; -rw------- 3 chapmanb chapmanb 150 Sep 26 14:07 8389565043859020157; -rw------- 2 chapmanb chapmanb 43 Sep 26",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184:679,down,downloaded,679,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184,1,['down'],['downloaded']
Availability,"Hi cromwell developers,. I'm having persistent errors similar to those in #2034 : stderr shows a line like this.; ; bash: path/to/my/cwd/cromwell-executions/my_workflow_name/some_hexadecimal_garbage/call-my_task_name/execution/script: Permission denied. Can you suggest a diagnosis or a workaround for this problem? I've tried to configure cromwell with LSF, and I suspect that's where the issue lies. Here's my complete configuration file. Thanks for your help!. include required(classpath(""application"")); backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 16; ; runtime-attributes = """"""; Int cpu; Int nthreads; Float? memory_kb; """"""; ; submit = """"""; bsub \; -J ${job_name} \; -cwd ${cwd} \; -R rusage[mem=${memory_kb}] \; -n ${nthreads} \; -W ${cpu} \; -o ${out} \; -e ${err} \; ""${script} ""; """"""; ; kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}""; job-id-regex = ""Job <(\\d+)>.*""; }; }; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3185:47,error,errors,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3185,2,"['alive', 'error']","['alive', 'errors']"
Availability,"Hi everyone, Cromwell on Azure is now available here:. https://github.com/microsoft/CromwellOnAzure. Please let me know if you have any questions or issues. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-553080157:38,avail,available,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-553080157,1,['avail'],['available']
Availability,"Hi everyone, this is as far as I can get on my own trying to introduce SQLite support into Cromwell. I have highlighted reasons to support Sqlite on Cromwell elsewhere. This is as far as I could get without help:. + Migration scheme correctly implemented. All the necessary tables with all the correct constraints (foreign key, unique, primary key) are created on startup.; + Updated upstream liquibase in order to allow unique constraints to be defined properly.; + Made sure all the types were converted in SQLite types (TEXT, INTEGER, BLOB etc.); + Updated the testing to understand SQLite types properly. So far so good. Unfortunately the testing does not recognize the foreign key, primary key or unique constraints, even though they are defined (clearly visible in the sqlitebrowser). . Since the testing is just testing, I also decided to run cromwell with a workflow, but that does not work: ; ```; [ERROR] [07/20/2020 14:01:02.134] [cromwell-system-akka.dispatchers.engine-dispatcher-50] [akka://cromwell-system/user/cromwell-service/WorkflowStoreActor/WorkflowStoreEngineActor] Error trying to fetch new workflows; org.sqlite.SQLiteException: [SQLITE_ERROR] SQL error or missing database (near ""for"": syntax error); at org.sqlite.core.DB.newSQLException(DB.java:1010); at org.sqlite.core.DB.newSQLException(DB.java:1022); at org.sqlite.core.DB.throwex(DB.java:987); at org.sqlite.core.NativeDB.prepare_utf8(Native Method); at org.sqlite.core.NativeDB.prepare(NativeDB.java:134); at org.sqlite.core.DB.prepare(DB.java:264); at org.sqlite.core.CorePreparedStatement.<init>(CorePreparedStatement.java:45); at org.sqlite.jdbc3.JDBC3PreparedStatement.<init>(JDBC3PreparedStatement.java:30); at org.sqlite.jdbc4.JDBC4PreparedStatement.<init>(JDBC4PreparedStatement.java:19); at org.sqlite.jdbc4.JDBC4Connection.prepareStatement(JDBC4Connection.java:35); at org.sqlite.jdbc3.JDBC3Connection.prepareStatement(JDBC3Connection.java:241); at org.sqlite.jdbc3.JDBC3Connection.prepareStatement(JDBC3Conne",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5582:908,ERROR,ERROR,908,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5582,1,['ERROR'],['ERROR']
Availability,"Hi folks,; I am running cromwell 36 with AWS batch. Doing the hello world example from the following:. https://aws.amazon.com/blogs/compute/using-cromwell-with-aws-batch/. I am able to submit from the swagger UI and am getting the following erro:. `2018-10-30 00:39:25,929 INFO - jobQueueArn: arn:aws:batch:us-east-2:365166883642:job-queue/GenomicsHighPriorityQue-0c2108973103ca2; 2018-10-30 00:39:25,929 INFO - taskId: wf_hello.hello-None-1; 2018-10-30 00:39:25,929 INFO - hostpath root: wf_hello/hello/bcc91ab0-fd91-41a8-b3e6-cbf091cb511d/None/1; 2018-10-30 00:39:25,965 cromwell-system-akka.dispatchers.backend-dispatcher-229 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(bcc91ab0)wf_hello.hello:NA:1]: Error attempting to Execute; software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: batch.default.amazonaws.com: Name or service not known`. Any idea the source of this error?; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4334:629,ERROR,ERROR,629,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4334,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"Hi is this implemented in the latest version of cromwell? ; I am getting the following error for files > 5G with the latest version . 2019-10-31 04:31:17,243 cromwell-system-akka.dispatchers.engine-dispatcher-32 WARN - 85d92e7d-3017-4e8d-adac-551ebcd50165-EngineJobExecutionActor-jgi_meta.bbcms:NA:1 [UUID(85d92e7d)]: Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_jgi_meta.bbcms:-1:1 (EnhancedCromwellIoException: [Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: 1272B7BFF87110E8)), invalidating cache entry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-548974241:87,error,error,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4828#issuecomment-548974241,1,['error'],['error']
Availability,"Hi team,. I try to configure cromwell to run ExomeGermlineSingleSample_v3.1.9.wdl on Slurm, and I follow your guide, but I have an error that ${docker_script} : No such file or directory; /cromwell-executions/ExomeGermlineSingleSample/118135f5-ce0e-437b-9fd2-332dd614bded/call-GenerateSubsettedContaminationResources/execution/script : No such file or directory; I attached the run file; #!/bin/bash; #SBATCH --nodes=1; #SBATCH --time=2:00:00. module load jdk. java -Dconfig.file=/mainfs/wrgl/broadinstitute_warp_development/tutorials/cromwell-slurm_5.config \; -jar /mainfs/wrgl/broadinstitute_warp_development/tutorials/cromwell-85.jar \; run /mainfs/wrgl/broadinstitute_warp_development/warp/ExomeGermlineSingleSample_v3.1.9.wdl \; -i /mainfs/wrgl/broadinstitute_warp_development/tutorials/Exom_test.json. #### Configuration file ###. include required(classpath(""application"")). system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false; }. backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; temporary-directory = ""$(mktemp -d /tmp/tmp.XXXXXX)"". runtime-attributes = """"""; Int runtime_minutes = 60; Int cpu = 1; Int memory_mb = 3900; String? docker; """""". submit = """""" \; 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap ""/bin/bash ${script}""'; """""". submit-docker = """""" \. # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; module load apptainer; if [ -z $APPTAINER_CACHEDIR ];; then CACHE_DIR=$HOME/.apptainer/cache; else CACHE_DIR=$APPTAINER_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; LOCK_FILE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7086:131,error,error,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7086,1,['error'],['error']
Availability,"Hi there!. I'm new to both WDL and Cromwell. I'm trying to run a workflow and the main input is an Illumina run folder, which could be up to a Tb in size. So I don't want it to be copied or hard-linked (which is almost the same in time) but to be soft-linked. I changed the localization option in the backend but then I'm getting the following error:. `Cannot localize directory with symbolic links`. So, isn't this possible? Will I always have to hard-linked the directories? This is not an option in my case due to performance issues. Everything is in the same shared file system, so I don't see the point in hard-linking. Also, if I call the same directory in different tasks, will it be hard-linked every time?. Thank you very much in advance!. Best,; Santiago",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3614:344,error,error,344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3614,1,['error'],['error']
Availability,"Hi there,. Biocontainers are incredibly popular tools in bioinformatics. Each time a [Bioconda recipe](https://bioconda.github.io/recipes.html) is made, a Biocontainer is automatically generated. This creates a collection of Dockerized tools immediately available to everybody. To reduce footprint, Biocontainers run on BusyBox, a Linux version tailored to embedded systems. Some tools therefore do not expose the full set of options of matching GNU tools on Ubuntu, CentOS, etc. Given the popularity of Biocontainers, it would be great if Cromwell could support them fully. There are a couple of small bugs related to `find` and `xargs` that should be easy to fix when one explores the file `stderr.background` for a task run from a Biocontainer:. ```; find: unrecognized: -empty; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: find [-HL] [PATH]... [OPTIONS] [ACTIONS]. Search for files and perform actions on them.; First failed action stops processing of current file.; Defaults: PATH is current directory, action is '-print'. 	-L,-follow	Follow symlinks; 	-H		...on command line only; 	-xdev		Don't descend directories on other filesystems; 	-maxdepth N	Descend at most N levels. -maxdepth 0 applies; 			actions to command line arguments only; 	-mindepth N	Don't act on first N levels; 	-depth		Act on directory *after* traversing it. Actions:; 	( ACTIONS )	Group actions for -o / -a; 	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4607:254,avail,available,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607,1,['avail'],['available']
Availability,"Hi there,. There is an issue posting a cromwell job, getting the id from the response, and then trying to run a GET call too soon after the initial post. Here is a simple example. `curl -v ""localhost:8000/api/workflows/v1"" -F workflowSource=@simple.wdl -F workflowInputs=@simple.json | python -c 'import json,sys,time;obj=json.load(sys.stdin);print obj[""id""];' | xargs -I {} curl ""http://localhost:8000/api/workflows/v1/{}/status""`. The get response was. `{; ""status"": ""fail"",; ""message"": ""Unrecognized workflow ID: 054f9462-3775-4745-8ca6-5a63c9c4492f""; }` . Even though the 054f9462 ID was gotten directly from the original response. The simple solution is to add a sleep statement between the post and get. `curl -v ""localhost:8000/api/workflows/v1"" -F workflowSource=@simple.wdl -F workflowInputs=@simple.json | python -c 'import json,sys,time;obj=json.load(sys.stdin);time.sleep(10);print obj[""id""];' | xargs -I {} curl ""http://localhost:8000/api/workflows/v1/{}/status""`. The response was . `{""status"":""Submitted"",""id"":""897e2f30-2e4d-4cf0-9bb3-d926ec42dc6a""}`. Is there a way to know for sure the job is ready for a GET post? Or is this a bug, and the response JSON should be returned after GETs are available?. Thanks for your time!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2671:1206,avail,available,1206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2671,1,['avail'],['available']
Availability,"Hi! cromwell. When I try to use the cromwell on AWS, I got the error in below. $ java -Dconfig.file=aws.conf -jar cromwell-latest.jar run hello.wdl -i hello.inputs; (Ref. https://cromwell.readthedocs.io/en/develop/tutorials/AwsBatch101/). [2018-10-23 04:06:40,39] [error] AwsBatchAsyncBackendJobExecutionActor [6d66572bwf_hello.hello:NA:1]: Error attempting to Execute. Please refer to attached image.; ![2018-10-23 13 07 21](https://user-images.githubusercontent.com/4966343/47335191-01173680-d6c5-11e8-8c51-97142bbf0d02.png). I guess this issue related to aws batch or setting.; How can I resolve this issue?. Thanks in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4294:63,error,error,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4294,3,"['Error', 'error']","['Error', 'error']"
Availability,"Hi!. I am attempting to run a tool that requires a directory structure as an input. One issue is that the tool will error out if there are files in the directory that are not the expected type. In our infrastructure we occasionally will have `.md5` files next to the important files which leads to the error in running the tool. To avoid this issue when launching Cromwell workflows I am attempting to list the good files in the listing attribute of the Directory input data type. The files I list are being staged in the inputs folder for the step but they are not being staged in the input directory folder path. Cromwell uses the empty input directory folder path as input to the tool which causes it to fail. . Example.cwl; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""ls""]; arguments: [""$(inputs.dir)""]. requirements:; - class: DockerRequirement; dockerPull: ""ubuntu:xenial"". inputs:; dir:; type: Directory. outputs:; example_out:; type: stdout. stdout: output.txt; ```; Input.yaml:; ```; dir: ; class: Directory; listing:; - class: File; path: ./data/1.txt; - class: File; path: ./data/2.txt; ```. staged files:; ```; => find inputs/; inputs/; inputs/1465754395; inputs/1465754395/2.txt; inputs/1465754395/1.txt; inputs/-143808698; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d/.file; ```. And the generated Cromwell command; ```; [d14c14d1example.cwl:NA:1]: 'ls' '/cromwell-executions/example.cwl/d14c14d1-ce96-44fc-9315-d1c431011f83/call-example.cwl/inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d'; ```. Tested using Cromwell 35 and 37. Cwltool works as expected after adding a basename to the input directory in the yaml. . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4670:116,error,error,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4670,2,['error'],['error']
Availability,"Hi!. I have been trying to make memory retry work on our system without sucess. ; Read all docs and previous issues I could find, but it still doesn't work for us. I have written a test wdl with two tasks, both write ""Killed"" to stderr, and supposed to get retried with more memory. The first task, **TestBadCommandRetry** is designed to fail regularly with rc 127, due to a bad command.; The purpose of this task is to prove the memory-retry mechanism is configured correctly in our system. Result of TestBadCommandRetry:; The memory-error-key is caught and memory is increased as defined in memory-retry-multiplier.; I also see this failure message in metadata.json:; _""message"": ""stderr for job `MemoryRetryTest.TestBadCommandRetry:NA:1` contained one of the `memory-retry-error-keys: [Killed]` specified in the Cromwell config. Job might have run out of memory.""_. Grepping metadata for memory of this job, I see the expected behaviour:; ""memory"": ""1 GB"",; ""memory"": ""2 GB"",. The second task, **TestOutOfMemoryRetry** is designed to fail do to real out of memory error.; The purpose of this task is to shoe that memory-retry mechanism is not working when a task runs out of memory, even if ""Killed"" is written to stderr. Result of TestOutOfMemoryRetry:; When this task is run, it fails but **the job is retried with the same amount of memory**.; This time I see the following failure message:; _""message"": ""Task MemoryRetryTest.TestOutOfMemoryRetry:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running \""/cromwell_root/script\"": unexpected exit status 137 was not ignored\n[UserAction] Unexpected exit status 137 while running \""/cromwell_root/script\"": Killed\n"",_. Grepping metadata for memory of this job, I see the memory expension is not working:; ""memory"": ""1 GB"",; ""memory"": ""1 GB"",; ; I have verified ""Killed"" is written correctly to stderr :; ```; gsutil cat gs://<out_bucket>/cromwell-execution/Me",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7205:535,error,error-key,535,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7205,3,"['error', 'failure']","['error-key', 'error-keys', 'failure']"
Availability,"Hi!. I'm having some trouble request s3 objects that are outside my current region (I get a Status Code: 301). . Backend: AWS Batch; Filesystem: S3; Region : `ap-southeast-2`. I'm attempting to run a small genomics pipeline that is trying to request some of the [`broad-reference` open data set](; https://registry.opendata.aws/broad-references) on AWS S3. I can see that open data set exists in `us-east-1`. . Specifically, I'm requesting (`s3://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta`) and I'm receiving the same error 5 times.; ```; [2019-03-12 11:27:21,50] [error] WorkflowManagerActor Workflow 434834fb-cb24-4bd2-ba44-8a1c929b11f5 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null); [Attempted 1 time(s)] - S3Exception: null (Service: S3Client; Status Code: 301; Request ID: null). 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:217); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:187); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:182); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4731:531,error,error,531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731,2,['error'],['error']
Availability,"Hi!. I've followed the instructions at https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/ and I've run into the following errors during 'sbt assembly': . [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:111:57: type mismatch; ; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:114:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:157:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; ja",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:149,error,errors,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,13,['error'],"['error', 'errors']"
Availability,"Hi, . I downloaded the jar file from the GitHub release page:. > wget https://github.com/broadinstitute/cromwell/releases/download/34/cromwell-34.jar. sha256sum results in the hash mentioned by @Horneth. Actually I have two java versions on my system:. ```; java version ""1.8.0_20""; Java(TM) SE Runtime Environment (build 1.8.0_20-b26); Java HotSpot(TM) 64-Bit Server VM (build 25.20-b23, mixed mode); ```; and. ```; openjdk version ""1.8.0_141""; OpenJDK Runtime Environment (build 1.8.0_141-b16); OpenJDK 64-Bit Server VM (build 25.141-b16, mixed mode); ```. It fails with the first one, however I can launch the server with the openjdk version.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4082#issuecomment-420540300:8,down,downloaded,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082#issuecomment-420540300,2,['down'],"['download', 'downloaded']"
Availability,"Hi, . I have been getting PoolTime out error (below) for all human jobs. Smaller ones e.g mouse genomes are a success. There are multiple steps in the wdl with outputs at every stage. Each output is copied from the S3//temp//cromwell_executions folder to S3://output folder successfully except the largest one i.e. .bam file. The .bam file is successfully copied from EC2 instance temp folder to S3://temp folder but it does copy from S3://temp/cromwell_executions to S3://output.; ; Both core environment and workflows have been set up using the templates provided by AWS genomics workflow. AWS batch jobs show a success notification, however it is only Cromwell that sends a status of ""failure"".. . Cromwell metadata : ; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""Timeout waiting for connection from pool"",; ""causedBy"": []; }; ],; ""message"": ""Unable to execute HTTP request: Timeout waiting for connection from pool""; }; ],; ""message"": ""software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool""; }; ],; ""message"": ""[Attempted 1 time(s)] - CompletionException: software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request: Timeout waiting for connection from pool""; }; ],; ```. Options.json; ```; ""final_workflow_outputs_dir"": ""s3://singleronbio-de-tmp/output/2023/Aug/2023-00578"",; ""final_call_logs_dir"": ""s3://singleronbio-de-tmp/log/2023/Aug/2023-00578/230719005"",; ""final_workflow_log_dir"": ""s3://singleronbio-de-tmp/workflow_log/2023/Aug/2023-00578/230719005"",; ""backend"": ""AWSBATCH"",; ""base_url"": ""XXXX"",; ""route_submit"": ""/api/workflows/v1"",; ""route_valid"": ""/api/womtool/v1/describe"",; ""route_status"": ""/api/workflows/v1/{id}/status"",; ""route_outputs"": ""/api/workflows/v1/{id}/outputs"",; ""write_to_cache"": true,; ""read_from_cache"": true; }; ```. Thank you, ; Lakshmi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7200:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7200,3,"['error', 'failure']","['error', 'failure', 'failures']"
Availability,"Hi, . I'm reporting an issue that seem to suggest File localization issues.; The execution bucket is [here](https://console.cloud.google.com/storage/browser/broad-dsde-methods/cromwell-execution-34/PindelSmallVariants/?project=broad-dsde-methods&organizationId=548622027621) using the DSDE-methods Cromwell server V.34. The errors I'm experiencing is described below. Resubmitting the job fixes the problem. ## failure for job ID ; * `2ebeed9a-8f42-418b-8569-8d80f5654d50` (shard-40), ; * `1cc79dda-9c6e-41b4-ac99-e1a422258039` (shard-20). ```; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; /bin/bash: /cromwell_root/script: No such file or directory; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; ```. ------------. ## failure for job ID ; * `4c5c8530-b79d-465b-8050-f7ba7368c057` (shard-26), ; * `5cbb2124-c526-4ca0-978e-4154ff4501cd` (shard-0). ```; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; Initializing parameters...; Pindel version 0.2.5b8, 20151210.; Loading reference genome ...; Loading reference genome done.; Initializing parameters done.; Please use samtools to index your reference file.; .fai is missing. sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; ```. ------------. ## failure for job ID ; * `7f219a29-69c1-4116-9c54-5d4656ee0124`. ```; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; sh: -q: unknown operand; Initializing parameters...; Pindel version 0.2.5b8, 20151210.; Loading reference genome ...; Error: fasta line starts with  instead of '>'. Aborting.; sh: -q: unknown operand;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4148:324,error,errors,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4148,3,"['error', 'failure']","['errors', 'failure']"
Availability,"Hi, ; I have a CWL workflow which works well when no output is specified but fails with the following when output is added:. ```; [2019-01-09 17:56:27,87] [error] WorkflowManagerActor Workflow b240bd3e-cdfd-45c8-be1e-046794929e90 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Workflow output parameters such as WorkflowOutputParameter(file:///var/folders/34/kzv1pzl57s92dbz54rvr1px00000gn/T/b240bd3e-cdfd-45c8-be1e-046794929e90.temp.4180597054306144316/b240bd3e-cdfd-45c8-be1e-046794929e90.cwl#outputwf,None,None,None,None,None,None,Some(Inr(Inl([Ljava.lang.String;@2ff95103))),None,Some(Inl(Inl(File)))) are not supported.; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:135); 	at akka.actor.FSM.akka$actor$FSM$$processMsg",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4538:156,error,error,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4538,1,['error'],['error']
Availability,"Hi, ; I understand cromwell that call caching will copy whatever it finished last time to current run if I keep the script unchanged. However, it seems to be not that case to me. I have run Mutect2, one of the GATK tool on fireclouds. one of the major step is it splits to 50 jobs based on some genomic interval and runs separately on different VM. Most jobs are successful done but a few fail because of some transient error. Once I relanuch the job, it splits to 50 jobs based on the same interval, but it is still running all 50 jobs simultaneously without knowing some of the jobs have run successfully last time (of course, I enabled call caching). So I ends up spending more and more money and time on it. Could you please advise on this ?. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3740:420,error,error,420,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3740,1,['error'],['error']
Availability,"Hi, ; To use Cromwell in AWS, may I have my own mount point?; If I use the following in my WDL file, I will got two mount points. But the problem is I can't provide my own Source Path. Cromwell will generate one for test1. If we can provide own source path, then we can use our reference files in EFS instead of downloading them each time from S3.; runtime {; docker: ""ubuntu:latest""; disks: ""local-disk, test1""; }. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4579:312,down,downloading,312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4579,1,['down'],['downloading']
Availability,"Hi, ; can we define Directory in workflow input section? I met error ""Cannot coerce expression of type 'String' to 'Directory'"".; my wdl file, cromwell 59 is used: ; ```; version development. workflow pipeline {; input {; Directory index_dir = ""/home/danny.gu/PycharmProjects/nestcmd/tests/testdata/index/""; }. call getFastqInfo{}. scatter (each in keys(getFastqInfo.fastq_info)) { ; String sample = each; File read1 = getFastqInfo.fastq_info[each][0][0]; File read2 = getFastqInfo.fastq_info[each][1][0]. call fastp {; input: ; read1 = read1,; read2 = read2; }. call salmon {; input: ; indexDir = index_dir,; read1 = fastp.out1,; read2 = fastp.out2; }. }. call MergeTranscriptTPM {; input: ; quants = salmon.outDir; }. call MergeTranscriptCount {; input: ; quants = salmon.outDir; }. meta {; name: ""PipelineExample""; desc: ""This is a simple pipeline for fast gene/transcript quantification. workflow = [fastq -> Fastp -> Salmon]""; author: ""unknown""; source: ""source URL for the tool""; version: ""unknown""; }. output{; Array[File] fastp_out1 = fastp.out1; Array[File] fastp_out2 = fastp.out2; Array[File] salmon_transcript = salmon.transcript; Array[Directory] salmon_outDir = salmon.outDir; File MergeTranscriptTPM_result = MergeTranscriptTPM.result; File MergeTranscriptCount_result = MergeTranscriptCount.result; }. }. task getFastqInfo{; input {; Array[Directory]? fastq_dirs; Array[File]? fastq_files; String r1_name = '(.*).read1.fastq.gz'; String r2_name = '(.*).read2.fastq.gz'; String docker = 'gudeqing/getfastqinfo:1.0'; }. command <<<; set -e; python /get_fastq_info.py \; ~{if defined(fastq_dirs) then ""-fastq_dirs "" else """"}~{sep="" "" fastq_dirs} \; ~{if defined(fastq_files) then ""-fastq_files "" else """"}~{sep="" "" fastq_files} \; -r1_name '~{r1_name}' \; -r2_name '~{r2_name}' \; -out fastq.info.json; >>>. output {; Map[String, Array[Array[File]]] fastq_info = read_json(""fastq.info.json""); File fastq_info_json = ""fastq.info.json""; }. runtime {; docker: docker; }. ; }; ; task fastp{; i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6501:63,error,error,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501,1,['error'],['error']
Availability,"Hi, I am using cromwell-59.jar, and run in local backend mode. ; Used command:; `java -Dconfig.file=cromwell.examples.conf -jar ~/softwares/cromwell-59.jar run example.wdl -i input_detail.json`. however, when I try to re-run a successed workflow to validate the cache calling function, I got the following information that confused me. very much. Would you be kind to give me some ideas on what's going on? . `a588e03e-a4fc-4809-b5f5-bb540cac9ca3-EngineJobExecutionActor-rnaseq_pipeline.fastp:NA:1 [UUID(a588e03e)]: Could not copy a suitable cache hit for a588e03e:rnaseq_pipeline.fastp:-1:1. No copy attempts were made.`. following is the source code related I fetched, but still cannot understand it. `if (data.cacheHitFailureCount > 0) {; val totalHits = data.cacheHitFailureCount; val copyFails = data.failedCopyAttempts; val blacklisted = totalHits - copyFails; workflowLogger.info(; s""Could not copy a suitable cache hit for $jobTag. "" +; s""EJEA attempted to copy $totalHits cache hits before failing. "" +; s""Of these $copyFails failed to copy and $blacklisted were already blacklisted from previous attempts). "" +; s""Falling back to running job.""; ); val template = s""BT-322 {} cache hit copying failure: {} failed copy attempts of maximum {} with {}.""; log.info(template, jobTag, data.failedCopyAttempts, callCachingParameters.maxFailedCopyAttempts, data.aggregatedHashString); } ; else {; log.info(s""BT-322 {} cache hit copying nomatch: could not find a suitable cache hit."", jobTag); workflowLogger.info(""Could not copy a suitable cache hit for {}. No copy attempts were made."", jobTag); }`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6484:1203,failure,failure,1203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6484,1,['failure'],['failure']
Availability,"Hi, I this is might be a little late, but I am having this issue too when running using Batch. I configured my core environment on my own (without using the CF templates). I have a bucket that is located in `us-west-2` and the instance running Cromwell (v59), and the Job Queue are located in `us-east-2`. When I run a job, I get the same error that @illusional was getting.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4731#issuecomment-927177699:339,error,error,339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4731#issuecomment-927177699,1,['error'],['error']
Availability,"Hi, I was trying to have a VCF related workflow, which involves gatk4, picard tools. As an example, lets say I want to call gatk4 first to get some VCF files, and use picard to sort them. if i have `gatk4.cwl` output as; ```; outputs:; vcf_list:; type: File[]; outputBinding:; glob: '*.vcf.gz'; secondaryFiles: [.tbi]; ```; and next `picard sort` has input array (w/ or w/o `secondaryFiles` here doesn’t matter from my tests. Neither works and will have the same error); ```; inputs:; vcf:; type:; type: array; items: File; inputBinding:; prefix: I=; separate: false; ```; After gatk4 finishes, the `execution` dir will look like; ``` ; drwx------ 3 root root 4.0K Jan 14 19:16 genomicsdb-0; -rw-r--r-- 3 root root 5.7K Jan 14 20:17 genomicsdb-0.vcf.gz; -rw-r--r-- 2 root root 105 Jan 14 20:17 genomicsdb-0.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 19:17 genomicsdb-1; -rw-r--r-- 3 root root 927K Jan 14 20:32 genomicsdb-1.vcf.gz; -rw-r--r-- 2 root root 7.6K Jan 14 20:32 genomicsdb-1.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 19:29 genomicsdb-2; -rw-r--r-- 3 root root 554K Jan 14 20:31 genomicsdb-2.vcf.gz; -rw-r--r-- 2 root root 11K Jan 14 20:31 genomicsdb-2.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 19:41 genomicsdb-3; -rw-r--r-- 3 root root 813K Jan 14 20:30 genomicsdb-3.vcf.gz; -rw-r--r-- 2 root root 11K Jan 14 20:30 genomicsdb-3.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 19:52 genomicsdb-4; -rw-r--r-- 3 root root 620K Jan 14 20:32 genomicsdb-4.vcf.gz; -rw-r--r-- 2 root root 12K Jan 14 20:32 genomicsdb-4.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 20:04 genomicsdb-5; -rw-r--r-- 3 root root 50K Jan 14 20:17 genomicsdb-5.vcf.gz; -rw-r--r-- 2 root root 746 Jan 14 20:17 genomicsdb-5.vcf.gz.tbi; drwx------ 3 root root 4.0K Jan 14 20:05 genomicsdb-6; -rw-r--r-- 3 root root 673K Jan 14 20:31 genomicsdb-6.vcf.gz; -rw-r--r-- 2 root root 13K Jan 14 20:31 genomicsdb-6.vcf.gz.tbi; drwxr-xr-x 2 root root 4.0K Jan 14 20:32 glob-330eecb06b4c0ad6b45febf0c8001b04; -rw-r--r--",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4546:463,error,error,463,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4546,1,['error'],['error']
Availability,"Hi, I'm now getting this behaviour always - not occasionally. . This is problematic because I can never get cromwell to submit any jobs: it always crashes before it can get to submit anything. . I've found that I only get this error while I'm polling the cromwell server - stopping the polling takes the problem away (but I need to poll for status). This is my scenario - may be used as steps to reproduce:; * I've got ~8k workflows in ""running"" status. Each workflow has a WDL 120 lines long + ~300 lines of WDL imports; * Another machine polls for the status of these workflows every minute, using the POST query method; * When I start the cromwell server, it starts to recap on pending work and parse running workflows WDLs, and then crashes before getting to resume any workflow. It appears to crash when I poll for workflow status. I'm querying the status for all ~8K workflow IDs at once - this may be related. This message precedes the stack overflow error message:; `Uncaught error from thread [cromwell-system-akka.dispatchers.api-dispatcher-30]: null, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094:227,error,error,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-423196094,5,"['down', 'error']","['down', 'error']"
Availability,"Hi, I've been running Cromwell with the file-based DB fine for a few weeks, but today had a seemingly unrelated problem and this seems to have corrupted the DB. Potentially unrelated error:; ```; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/cromwell-service/WorkflowStoreCoo; rdinatedAccessActor#1289452983]] after [60000 ms]. Message of type [cromwell.engine.workflow.workflowstore.WorkflowStor; eCoordinatedAccessActor$FetchStartableWorkflows]. A typical reason for `AskTimeoutException` is that the recipient acto; r didn't send a reply.; at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:183,error,error,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649,1,['error'],['error']
Availability,"Hi,. I am a novice BI trying to implement cromwell via GATK exomesinglesample.wdl on a slurm HPC.; I notice the Haplotype Caller stage tries to use all available memory on the system -1024 with:; available_memory_mb=$(free -m | awk '/^Mem/ {print $2}'); let java_memory_size_mb=available_memory_mb-1024. which is causing problems.; Is there a way of setting the maximum memory that this rule can grab in the config file?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7335:152,avail,available,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7335,1,['avail'],['available']
Availability,"Hi,. I encountered the same problem randomly. In a scattered task, some attempts passed while some attempts failed. ; As you mentioned, I think the script was not locally available on the worker at the moment of mounting, due to a network problem.; It would be better if Cromwell can re-try on this. . > When this occurs it is because the fetch_and_run.sh script is not available on the worker nodes of the batch compute environment so when docker mounts that it mounts as a directory because there is no file. Possible causes that I can think of: 1. The script is not available in the S3 bucket you used for the genomics workflow core setup 2. When you ran the Cromwell install you didn't use the exact same namespace that you used for the genomics workflow core so the required scripts are not available.; > […](#); > On Sat, Sep 19, 2020 at 9:26 AM openbioinfomatics for more people who need it ***@***.***> wrote: version: v53 backend: aws [image: image] <https://user-images.githubusercontent.com/45682016/93668392-8d535380-fabe-11ea-870e-36786c6c3d9d.png> i think this part code may go wrong. mount file indeed. [image: image] <https://user-images.githubusercontent.com/45682016/93668410-a1975080-fabe-11ea-9571-b7ce8b9080ef.png> — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5872>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EOOFWDEMRNZCUXJ5CDSGSWPNANCNFSM4RTAXSGA> .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-724031410:171,avail,available,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-724031410,4,['avail'],['available']
Availability,"Hi,. I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch; Cromwell version: 51; Error log:. Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; nmerged.bam)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977:292,Error,Error,292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977,2,"['Error', 'Failure']","['Error', 'Failure']"
Availability,"Hi,. I have built a WDL workflow which works well with SLURM but now I am trying to get it to be able to be run on a standalone server. . I have Slurm as my provider and have created one for Local. ` Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. run-in-background = true; exit-code-timeout-seconds = 300; workflow-reset = true; read_from_cache = true; write_to_cache = true; system.file-hash-cache=true; concurrent-job-limit = 2. runtime-attributes = """"""; String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg""; """""". submit = ""singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}"". filesystems {; local {; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; } ## end local; } ## end file systems; } ## end config; } ## End Local`. Oddly, when running the workflow I get a submit docker error. ie. as per below. I have no idea why it's looking for docker as I'm not knowingly using it. I'm not using docker in my run time parameters. I have been able to get standalone working on another workflow by passing a singularity container to each task command output but I was wondering if there was a more elegant solution I could use such as just changing to a pre-made provider. I have searched Google and through here but not found anything. I did find one issue here but they seemed to want to use docker where as I don't. . Thanks for the help!. `task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_scri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862:934,error,error,934,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862,1,['error'],['error']
Availability,"Hi,. I recently come across the same issue when using Cromwell with AWS Batch. However, it seems none of the reference links in this thread is available, except for the most recent link to `cromwell-aio.template.yaml` which also failed during the creation. So may I know where I should get help regarding this issue? Many thanks!. Sincerely,; Yiming",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-922097668:143,avail,available,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-922097668,1,['avail'],['available']
Availability,"Hi,. I recently ran a workflow on our dev server which was running cromwell 37. Right around that time dev-ops reverted it to 36. I wanted to confirm that it ran on 37, but couldn't find anything in the metadata to indicate the version of cromwell that was running when the workflow executed. I think it would be great to have that information available. Thanks. FYI - here's the metadata for the workflow in question:; https://cromwell.gotc-dev.broadinstitute.org/api/workflows/v1/0ac19869-db74-45ba-a785-9e3eebe4103d/metadata",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4668:344,avail,available,344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4668,1,['avail'],['available']
Availability,"Hi,. I was wondering if there's any interest to support a new backend for submitting jobs to a Hashicorp Nomad cluster?; I suppose this will be fairly similar to the AWS batch system. Thanks; Matthias. ping @tomiles",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6001:202,ping,ping,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6001,1,['ping'],['ping']
Availability,"Hi,. I'm trying to run through the Hello World example. I'm running Cromwell on OS X with Java 8. Any ideas on how to troubleshoot the below error?. Thanks!. ```; › cromwell run hello.wdl hello.json; [2015-12-18 08:43:13,222] [info] Slf4jLogger started; [2015-12-18 08:43:13,335] [info] Default backend: LOCAL; [2015-12-18 08:43:13,335] [info] RUN sub-command; [2015-12-18 08:43:13,336] [info] WDL file: hello.wdl; [2015-12-18 08:43:13,439] [info] Inputs: hello.json; [2015-12-18 08:43:13,560] [info] input: test.hello.name => ""world""; [2015-12-18 08:43:13,776] [info] SingleWorkflowRunnerActor: launching workflow; [2015-12-18 08:43:15,936] [info] Running with database db.url = jdbc:hsqldb:mem:86473284-494c-43d2-94fd-d00107a2a787;shutdown=false;hsqldb.tx=mvcc; [2015-12-18 08:43:17,516] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = e67af113-c3a7-41f4-9178-6640c1c652e9; [2015-12-18 08:43:17,592] [info] WorkflowManagerActor Found no workflows to restart.; [2015-12-18 08:43:18,816] [error] SingleWorkflowRunnerActor: Ask timed out on [Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312]] after [5000 ms]; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312]] after [5000 ms]; at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:334); at akka.actor.Scheduler$$anon$7.run(Scheduler.scala:117); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:599); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:597); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467); at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.sca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/334:141,error,error,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/334,1,['error'],['error']
Availability,"Hi,. I've noticed that the glob() function from WDL doesn't work on the AWS. Here is a sample workflow:. ```; workflow glob_test; {; call GlobTest; {; input:; m1 = ""msg1"",; m2 = ""msg2""; }; }. task GlobTest; {; String m1; String m2; command; <<<; echo ${m1} > foo.1.txt; echo ${m2} > bar.1.txt; echo ${m2} > foo.2.txt; >>>; runtime; {; cpu: 1; memory: ""128 MB""; docker: ""ubuntu:latest""; }; output; {; Array[File] o1 = glob(""*.1.txt""); Array[File] o2 = glob(""*.2.txt""); }; }; ```. The workflow finishes successfully, but in the AWS logs I see the following line:; ```; aws s3 cp --no-progress /cromwell_root/glob-6f3569c1e6d4f3f2cfb3469eb61bd5a0/* s3://path/to/s3/; The user-provided path /cromwell_root/glob-6f3569c1e6d4f3f2cfb3469eb61bd5a0/* does not exist. ; ```. I've checked and there is no trace of directory with the name ""glob-*"" anywhere starting from root dir. Command used to run this workflow on aws:; `java -Dconfig.file=aws.conf -jar cromwell-36.jar run test.wdl`. Thank you,; Timur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4353:246,echo,echo,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4353,3,['echo'],['echo']
Availability,"Hi,. In Cromwell 52 we updated the S3 module to perform multithreaded, multipart; copies to improve the size of results that may be cached. There are also; additional improvements that have recently been merged into dev and should; appear in the next release version (or you could build from source). v52+ requires a new AWS configuration. Instructions are in; https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf. On Sat, Oct 24, 2020 at 8:27 PM Luyu <notifications@github.com> wrote:. > Hi,; >; > I got a timeout exception during cache copying on AWS S3. The cache file; > size is 133GB. Given the file size, more time should be allowed for cache; > copying. Is there any config option that can tune this? Thank you in; > advance for any suggestions.; >; > Backend: AWS Batch; > Cromwell version: 51; > Error log:; >; > Failure copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed; > out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; >; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136; > /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to; > s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488; >; > 8c0031/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/8df24f46-2f4f-4557-a662-d630ac443736/call-SamToFastqAndBwaMem/shard-0/cacheCopy/39T_R.u; > nmerged.bam); >; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5977>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EMWLDPLNV7UM35OWWLSMNWFNANCNFSM4S56ELLQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310:855,Error,Error,855,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-716229310,2,"['Error', 'Failure']","['Error', 'Failure']"
Availability,"Hi,. Is it possible to use an SQLite database instead of a MySQL server to allow for call-caching?. This post comes close to saying there isn't a way without a lot of work (https://github.com/broadinstitute/cromwell/issues/3786). Is this still the position?. I know you can use a MySQL database, but sometimes the extra resources are not available or difficult to get. An option for a local persistence database would be very useful. Thanks for your help,",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271:338,avail,available,338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271,1,['avail'],['available']
Availability,"Hi,. Running a workflow on WSL/Ubuntu 20.04 using conda-installed Cromwell:. `cromwell run ngs-ubuntu-20-04/iletisim/warp/pipelines/broad/dna_seq/germline/single_sample/exome/local_newGCP_ExomeGermlineSingleSample_deneme6_bcftools.wdl -i ngs-ubuntu-20-04/iletisim/json/S736Nr1.json -o ngs-ubuntu-20-04/iletisim/json/options2.json`. Getting the error:. ```; [2023-02-04 08:55:00,61] [info] Running with database db.url = jdbc:hsqldb:mem:bc9ad7e3-efc7-4f37-aecb-b283b104cbcd;shutdown=false;hsqldb.tx=mvcc; [2023-02-04 08:55:06,54] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2023-02-04 08:55:06,55] [info] [RenameWorkflowOptionsInMetadata] 100%; [2023-02-04 08:55:06,64] [info] Running with database db.url = jdbc:hsqldb:mem:a487ea75-b617-4523-a254-d0e694e68ff9;shutdown=false;hsqldb.tx=mvcc; [2023-02-04 08:55:06,92] [info] Slf4jLogger started; [2023-02-04 08:55:07,18] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-b625dba"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2023-02-04 08:55:07,22] [info] Metadata summary refreshing every 2 seconds.; [2023-02-04 08:55:07,26] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2023-02-04 08:55:07,26] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2023-02-04 08:55:07,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2023-02-04 08:55:07,63] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2023-02-04 08:55:07,64] [info] SingleWorkflowRunnerActor: Version 34-unknown-SNAP; [2023-02-04 08:55:07,65] [info] SingleWorkflowRunnerActor: Submitting workflow; [2023-02-04 08:55:07,68] [info] Unspecified type (Unspecified version) workflow 48f62f22-25fe-4f0f-b5fe-21191f035abd submitted; [2023-02-04 08:55:07,72] [info] S",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:344,error,error,344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,1,['error'],['error']
Availability,"Hi,. Sorry for submitting an issue here but I'm consistently getting a ""Something has gone wrong"" error trying to log in to your Jira. I'm hoping someone can offer some guidance for an issue I'm having running a CWL workflow with Cromwell on GCP. I'm using bcbio to generate CWL to do joint calling. This worked fine when I tested it with a single sample to shake out any issues with the pipeline. However when scaling up to a 20 sample batch there's an issue with the get_parallel_regions_jointvc step. This step appears to be localizing multiple copies of the reference genome data (one for each sample) to the same disk. This really blows up the storage requirements as the number of samples increase and ends up exhausting the storage allocated to the worker instance. Is this expected behaviour or is there some kind of configuration I'm missing that would avoid this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5131:98,error,error,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5131,1,['error'],['error']
Availability,"Hi,. Switching from version 51 to 52/53 I get the error:. ```; Task XYZ has an invalid runtime attribute memory = !! NOT FOUND !!; ```. I could not find any docs describing the change in required runtime attributes. I cannot add it to config as it raises another error. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5817:50,error,error,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5817,2,['error'],['error']
Availability,"Hi,. The task doesn't fail when the output file can't be found, hence, cromwell tries to run the next job. Here is a small sample workflow:. ```; workflow should_fail; {; call Task1; {; input:; out_base = ""foo""; }; }. task Task1; {; String out_base. command; <<<; #typo simulation, one extra 't'; echo ""Hello World!"" > ${out_base}.ttxt; >>>. runtime; {; cpu: 1; memory: ""128 MB""; docker: ""ubuntu:latest""; }. output; {; File out = ""${out_base}.txt""; }; }; ```. command used to run the workflow on the aws: ; `java -Dconfig.file=aws.conf -jar cromwell-36.jar run test.wdl`. Thank you,; Timur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4354:297,echo,echo,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4354,1,['echo'],['echo']
Availability,"Hi,. This looks like a bug in the generation of the script. Any chance you can; share the WDL file with me?. On Mon, Nov 30, 2020 at 7:35 AM henriqueribeiro <notifications@github.com>; wrote:. > I'm running gatk-sv workflows with AWS backend and I'm facing some issues; > on scatter tasks. It seems that for some of the tasks, the; > reconfigured-script is bad constructed. Below is an excerpt from the script:; >; > #!/bin/bash; >; > {echo '*** LOCALIZING INPUTS ***'if [ ! -d /tmp/scratch ]; then mkdir /tmp/scratch && chmod 777 /tmp/scratch; ficd /tmp/scratch; > /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-48/SR00c.NA20802.txt.gz.tbi /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-48/SR00c.NA20802.txt.gz.tbi; > /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-31/SR00c.NA19661.txt.gz /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-31/SR00c.NA19661.txt.gz; > /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-32/SR00c.NA19678.txt.gz.tbi /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/ca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6106#issuecomment-738953857:436,echo,echo,436,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6106#issuecomment-738953857,1,['echo'],['echo']
Availability,"Hi,. What is the way to specify resource requirements (cores, ram etc) to AWSBatch? . I am executing a CWL workflow with AWS batch backend. ; Each task is submitted by cromwell and I can verify on AWS console that all jobs ended successfully.; However, for jobs that I have set coresMin and coresMax requirements I get the warning:. ```; [warn] AwsBatchAsyncBackendJobExecutionActor [6bd79e09fastqc_1:NA:1]: Unrecognized runtime attribute keys: cpuMax; ```. and at the end of the workflow the error:; ```; [error] Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.validatedRuntimeAttributes(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:493,error,error,493,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,2,['error'],['error']
Availability,"Hi,. sorry for the late response. It seemed that this caused the error. Best,; Flo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4082#issuecomment-427867227:65,error,error,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4082#issuecomment-427867227,1,['error'],['error']
Availability,"Hi,; I am trying to create two task. MarkDuplicates followed up Samtools index of that MarkDuplicates. Both from separate dockers. The issue I think is that when samtools indexes a bam file it does it in the same directory. So if the input to samtools is output from MarkDuplicates then I get this output error:. java.io.FileNotFoundException: Could not process output, file not found: /home/coyote/cromwell/WGS/cromwell-executions/AlignBwaMem/87cfebcc-b103-4e15-8313-0f39a8a959d5/call-md/shard-1/execution/SRR13481471.md.bam.bai. because for some reason both SRR13481471.md.bam and SRR13481471.md.bam.bai and in the folder inputs for samtools to index, i.e. samtools is indexing the input folder bam and the resulting index goes to that inputs folder and not the execution folder. How do we handle this? There must be a way since this must happen alot. Why does cromwell think the output index file should be in the MarkDuplictes folder ""md"" in my case. Here is my calls and tasks:; ```; call md_bqsr.markdupsIndiv as md {; 	input :; 		bam = samsort.outputBam,; 		outputPrefix = s.outputPrefix,; 		runtime_params = standard_runtime_gatk; }. call samtools.index as samindex {; 	input :; 		bam = md.bamMD,; 		runtime_params = standard_runtime_samtools; }. task markdupsIndiv {; 	input {; 		File bam; 		String outputPrefix. 		# runtime; 		RuntimeGATK runtime_params; 	}; 	; 	command {; 		set -e -o pipefail; 		export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. 		gatk --java-options ""-Xmx~{runtime_params.memory}m"" MarkDuplicates \; 			-I ~{bam} \; 			-O ~{outputPrefix}.md.bam \; 			-M ~{outputPrefix}.md.metrics; 	}. 	runtime {; 		maxRetries: runtime_params.max_retries; 		memory: runtime_params.memory + "" MB""; 		cpu: runtime_params.cpu; 		docker: runtime_params.gatk_docker; 	}; 	; 	output {; 		File bamMD = ""~{outputPrefix}.md.bam""; 		File metrics = ""~{outputPrefix}.md.metrics""; 	}	; }. task index {; 	input {; 		File bam; 		; 		# runtime; 		RuntimeSamtools runtime_pa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6182:305,error,error,305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6182,1,['error'],['error']
Availability,"Hi,; I am trying to run a workflow on AWS Batch using the genomics-ami.; The ami was built following the instructions in the relevant pages and i have confirmed that it contains a /cromwell-root mount point and has rw access to the bucket we use.; The AWS batch backpoint was tested with the hello.wdl workflow and it went through. When running the workflow on the local filesystem it completes without errors but when running it using the AWS Batch backend the first step fails with the following error:. ```; 2019-01-11 20:27:06,80] [error] WorkflowManagerActor Workflow 8fa7a9e4-f30d-4c19-b8cb-68be6442f317 failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; Caused by: java.io.IOException: Could not read from s3://bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt: s3://s3.amazonaws.com/bucket/cwl_temp_file_8fa7a9e4-f30d-4c19-b8cb-68be6442f317.cwl/8fa7a9e4-f30d-4c19-b8cb-68be6442f317/call-bbmap/bbmap-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542:403,error,errors,403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542,3,['error'],"['error', 'errors']"
Availability,"Hi,; I run cromwell for warp pipeline using slurm HPC and apptainer, and I got errors of localization:. (i) hard-link try to bind the cwd with wrong name, the real one has a suffix (.tmp); (ii) copy and cashe-copy have error: file-name too long (I can see the generated file path has loops, that makes the file name too long). B.W.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7097:79,error,errors,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7097,2,['error'],"['error', 'errors']"
Availability,"Hi,; I'm seeing a number of compilation errors with head dev etc. I've attached the output from debug. Scala version; scala-2.12.0-1.noarch. Java version; java 10.0.1 2018-04-17; Java(TM) SE Runtime Environment 18.3 (build 10.0.1+10); Java HotSpot(TM) 64-Bit Server VM 18.3 (build 10.0.1+10, mixed mode). CentOS Linux release 7.5.1804 (Core). Error output attached. Any thoughts ? Could be something obvious as I'm new to scala.; Thanks. [comp.log](https://github.com/broadinstitute/cromwell/files/2109138/comp.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3791:40,error,errors,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3791,2,"['Error', 'error']","['Error', 'errors']"
Availability,"Hi,; following the example on how to use AWS Batch I am able to run the tests using the suggested edits to aws.conf. I then try to specify the output directories using the following `options.json` file. ```; {; ""final_workflow_outputs_dir"": ""s3://bucket/cromwell/outputs"",; ""final_call_logs_dir"": ""s3:/bucket/cromwell/call_logs"",; ""final_workflow_log_dir"": ""s3://bucket/cromwell/wf_logs""; }; ```. When running cromwell : `java -Dconfig.file=awsbatch/aws.conf -jar cromwell-36.jar run awsbatch/hello.wdl -i awsbatch/hello.inputs -o options.json`. it results with the error:. ```; [2019-01-12 00:31:03,94] [info] $a [866d19d0]: Copying workflow logs from /rcecloud/kmavrommatis/workspace/Workflows/cromwell/cromwell-workflow-logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log to s3://celgene-rnd-riku-researchanalytics/cromwell/wf_logs/workflow.866d19d0-64da-45c3-9b69-830f0475ba12.log; [2019-01-12 00:31:04,03] [error] Key cannot be empty; java.lang.IllegalArgumentException: Key cannot be empty; 	at software.amazon.awssdk.core.util.ValidationUtils.assertStringNotEmpty(ValidationUtils.java:111); 	at software.amazon.awssdk.core.runtime.transform.PathMarshallers$GreedyPathMarshaller.marshall(PathMarshallers.java:109); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:87); 	at software.amazon.awssdk.services.s3.transform.HeadObjectRequestMarshaller.marshall(HeadObjectRequestMarshaller.java:31); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:88); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.headObject(DefaultS3Client.java:1628); 	at org.lerch.s3fs.util.S3Utils.getS3ObjectSummary(S3Utils.java:47); 	at org.lerch.s3fs.S3FileSystemProvider.exists(S3FileSystemProvider.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4541:566,error,error,566,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4541,2,['error'],['error']
Availability,"Hi. I have a workflow named `main_wf.wdl` which imports a subworkflow named `sub_wf.wdl` like the following:; ```; import ""sub_wf.wdl"" as sub; ```; I am able to run `main_wf.wdl` in run mode. But when I run a cromwell server and uses the swagger UI to commit the above workflow, I get the following error:. ```; 2018-12-17 11:47:12,214 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-12-17 11:47:12,235 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-12-17 11:47:13,348 cromwell-system-akka.dispatchers.engine-dispatcher-28 ERROR - WorkflowManagerActor Workflow 3bd35e44-d8a7-41ed-8271-c773949e8c5c failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; error in opening zip file; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:214); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:184); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:179); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:135); ```; I put the `sub_wf.wdl` into a `workflow.zip` file like below:; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4509:299,error,error,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4509,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Hi. I have been using WDL/Cromwell/Terra for over a year. It works great for ""simple"" pipeline. It has saved me a lot of time and money. I need to run something more complicated. I want to use the scatter gather pattern. I wrote a simple workflow to figure out how to implement scatter-gather. My scatter task works as expected however, my gather task does not. I have each of my tasks in separate files. This made it easier to debug my task separately. The only way I could iterate over an Array was to set the first line of my wdl file to be 'version 1.0' and use the following syntax. ```; version 1.0; workflow loopWorkflow {; call loopTask; }. task loopTask {; Array[String] csvParts = [""a"", ""b"", ""c""]; Int n = 123. command <<<; set -euxo pipefail. exec 2>&1; echo ""shell: $SHELL""; $SHELL -h. echo ""AEDWIP The value of N should be 123 n = ~{n}"". for part in ~{sep=' ' csvParts}; do; echo ""AEDWIP $part AEDWIP""; done. echo ""\n\n********* create a bashArray""; bashArray=('~{sep=""' '"" csvParts}'); for part in ""${bashArray[@]}""; do; echo ""bashArray loop aedwip $part aedwip""; done. ls -l >results.csv; >>>. output {; File results_csv='results.csv'; }; }; ```. setting the version to 1 prevents me from being able to import my loopTask into my workflow. ```; [2023-03-20 19:23:37,02] [info] MaterializeWorkflowDescriptorActor [c74751ef]: Parsing workflow as WDL draft-2; [2023-03-20 19:23:37,26] [info] WorkflowManagerActor: Workflow c74751ef-5494-46d1-8064-25e7d4334405 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; ERROR: Finished parsing without consuming all tokens. version 1.0; ^; ```. Also when I set the version to 1.0 I am no longer able to use womtool-85 to generate template input. . Also I can not use cromwell-85 run with --inputs with version 1. comments and suggestions appreciated. Kind regards. Andy",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7093:765,echo,echo,765,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7093,6,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability,"Hi. I'm trying to enable call caching using a local file database and I can't seem to get it to work. Everything that I try does not seem to make a difference, and each run always starts from the first task. I'm running cromwell in run mode from the command line, and I am testing on both cromwell 43 and cromwell 47. I also have write-to-cache and read-from-cache set to true in my options.json (although I understand that is the default behaviour). I am unable to use a mySQL or postgres database at this current time. Is there something that I'm missing? Is there any additional information that is needed to help diagnose this?. My cromwell.conf is as follows:. backend {; default = LSF; providers {; LSF {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; exit-code-timeout-seconds = 600. runtime-attributes = """"""; Int cpus; Float memory_mb; String lsf_queue; String lsf_project; """""". submit = """"""; bsub \; -q ${lsf_queue} \; -P ${lsf_project} \; -J ${job_name} \; -cwd ${cwd} \; -o ${out} \; -e ${err} \; -n ${cpus} \; -R rusage[mem=${memory_mb}] \; /usr/bin/env bash ${script}; """""". job-id-regex = ""Job <(\\d+)>.*"". kill = ""bkill ${job_id}""; check-alive = ""bjobs ${job_id}"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [; ""soft-link"", ""copy"", ""hard-link""; ]; hashing-strategy: ""path""; }; }; }; }; }; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=100000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 86400000; numThreads = 1; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5370:1203,alive,alive,1203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5370,1,['alive'],['alive']
Availability,"Hi; We are trying to setup the cromwell + wdl for genomic analyses at the [National Computational Infrastructure HPC facility](https://nci.org.au/systems-services/peak-system/raijin/) in Australia. This HPC runs bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4967:409,error,errors,409,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967,1,['error'],['errors']
Availability,"Hi;; I'm running Cromwell 34 and hitting an issue when reading from a cache. The problematic CWL step uses a variable, `min_allele_fraction` that is initially an array of longs:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L101. and then gets converted into a record with individual long elements:. https://github.com/bcbio/test_bcbio_cwl/blob/eae685b8023126b7f159d3048f4ab4dd1a1833d6/prealign/prealign-workflow/steps/batch_for_variantcall.cwl#L309. This all works fine on the first run of a pipeline, but when reading the step from the cache we get an error about not supporting `Long`:; ```; [2018-08-21 10:26:40,02] [info] WorkflowExecutionActor-3e76006c-870a-4c34-9f21-949eee1a5b33 [3e76006c]: Starting batch_for_variantcall; [2018-08-21 10:26:41,10] [error] unrecognized simpleton WOM type: Long; java.lang.RuntimeException: unrecognized simpleton WOM type: Long; at cromwell.Simpletons$.toSimpleton(Simpletons.scala:30); at cromwell.Simpletons$.toSimpleton(Simpletons.scala:16); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResults",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4023:654,error,error,654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023,2,['error'],['error']
Availability,"Hi，I have learned that cromwell does not support 【cpu and memory】runtime attributes for local backends（see https://cromwell.readthedocs.io/en/develop/RuntimeAttributes/）. Thus, when running a workflow locally， How can we avoid concurent jobs that may crash the workflow by running out of memory？ I know that maximum job number can be limited, and some jobs can be parallelized wildly for they require only little resources , however, some jobs should not be paralleized for need of large memory. So, we need to check available resource before submit a job. . best wishes!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6458:517,avail,available,517,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6458,1,['avail'],['available']
Availability,"Hm. This looks like a conf bug on our side, but [is your config file importing application.conf](https://cromwell.readthedocs.io/en/stable/tutorials/ConfigurationFiles/#creating-your-first-configuration-file)? That file contains other overrides that cromwell should have over the default `akka` configuration. The bug here is that application.conf is only supposed to contain overrides, while reference.conf should contain newly defined resources. Since the `services` block are cromwell's services, they should be newly defined in reference.conf. That would then allow anyone who accidentally doesn't pick up our application.conf to *at least* have the reference `services`, plus the original `akka` values with degraded cromwell performance.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577#issuecomment-457755274:713,degraded,degraded,713,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577#issuecomment-457755274,1,['degraded'],['degraded']
Availability,"Hmm I don't know, I don't think so. Those failures started popping up recently I believe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298935140:42,failure,failures,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298935140,1,['failure'],['failures']
Availability,"Hmm that is definitely different from the ""task rejected from queue"" errors. And anyway 28 has the larger default metadata batch size changes, so if this really was a different symptom of that problem it shouldn't be happening on 28. . I don't see much different between develop and 28_hotfix that could legitimately explain fixes in the vicinity of Slick. 😕",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315451894:69,error,errors,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2447#issuecomment-315451894,1,['error'],['errors']
Availability,"Hmm we would still need to cache at least the ""unevaluated expression"" I think, whatever that means. Otherwise what about this : . ```; task t1 {; String a; String b = ""hello"" + a; command {; echo ${b}; }; }. task t1 {; String a; String b = a + ""hello""; command {; echo ${b}; }; }. workflow w {; call t1 { input: a = ""a"" }; call t2 { input: a = ""a"" }; }; ```. Same input / command but the result will be different.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065035:192,echo,echo,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2309#issuecomment-306065035,2,['echo'],['echo']
Availability,"Hmm, that's interesting on the google side. So I'm clear you're saying that Cromwell is showing Running when they were not in Google? If so, how long did that stay the case - was it in perpetuity? I ask because as the number of jobs increases the average latency between a state change on Google's side and Cromwell detecting it increases due to QPS limitations. We're always trying to work with them to find ways to make that faster but we're limited on how many things we can query about at once, so we round robin them through. As an example the other day I submitted 200k single call workflows which each only slept for a couple of seconds but it took upwards of an hour for Cromwell to know that everything was complete due to that. I'm still going to look into the root cause of the exceptions you saw, i've been seeing those a lot myself (but had reason to believe it was an artifact of my not-at-all-standard setup, glad you chimed in to fix that for me) and wanted to make sure they weren't masking something more fundamentally wrong. re the logging aspect, I agree completely - this has always been an issue and is growing the more the people start adopting Cromwell. I found it amusing that just hours prior I said I should change that one to be less frightening and then it frightened someone ;) In general I think that logging is always a a dark art but answering the ""who is the log for?"" is even harder here as we intentionally designed cromwell to satisfy multiple use cases all of whom have different things they want to see. It's something that we're looking to work on over the next several months.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711:1000,mask,masking,1000,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260127711,1,['mask'],['masking']
Availability,"Hmmm, I don't even know how I would grant it at the project level. I pretty much used this:; ```; for role in lifesciences.workflowsRunner iam.serviceAccountUser storage.objectAdmin; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:MY-NUMBER-compute@developer.gserviceaccount.com --role roles/$role; done; ```; Maybe if `iam.serviceAccounts.actAs` is granted only once I might have missed it as I was not able to download the whole log file. Do you know why occasionally `storage.buckets.get` is requested and what actually happens to Cromwell if it is not granted to the service account?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686096916:449,down,download,449,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686096916,1,['down'],['download']
Availability,"Hmmm, still stuck on this - any updates from your guys' end? I tried cloning and resubmitting, still getting the same error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-840874050:118,error,error,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-840874050,1,['error'],['error']
Availability,"Honestly I didn't go very far down that road once I realized that Travis and Jenkins tests finished if I set it to ONLY ""copy"" mode. I don't think there's any technical reason why we couldn't at least test symlink and copy mode.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/196#issuecomment-142703490:30,down,down,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/196#issuecomment-142703490,1,['down'],['down']
Availability,"Honestly I'd really like it if that validation were removed from wdl4s. Even though it ""makes sense"" for it to be there because ""the spec says memory should be formatted like this so we should validate it down at the wdl4s level"". I still would rather change the spec to be a suggestion rather than something that's mandatory so we don't have this one outlier which actually makes the code a lot harder to write. If we do change that now, I'll shed a tear because I have to rebase those changes on my super long lived branch that we were going to get to once PBE was finished.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212551388:205,down,down,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/708#issuecomment-212551388,1,['down'],['down']
Availability,Hopefully addresses issues with missed summarizations by filtering on the client. This appears to have the same 3 PAPI v2 failures that are on develop related to GPUs and slightly less memory than expected in a monitoring log assertion. 🤷‍♂,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5125:122,failure,failures,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125,1,['failure'],['failures']
Availability,Hoping @geoffjentry can provide some insight here as to how to get this pack up and running. Perhaps @jacarey can be second reviewer if available.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/211#issuecomment-144851621:136,avail,available,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/211#issuecomment-144851621,1,['avail'],['available']
Availability,How about Centaur tests that submitting pictures of Gumby now produces 4xx errors (and whatever else this fixes)?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318488237:75,error,errors,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500#issuecomment-318488237,1,['error'],['errors']
Availability,"How about if we just drop the structure and flattened all the messages:; ```; ""failures"": [{; ""message"": ""connect timed out""; ""message"": ""Failed to upload authentication file""; ""message"": ""Error getting access token for service account: ""; }]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717:79,failure,failures,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282801717,2,"['Error', 'failure']","['Error', 'failures']"
Availability,How does Cromwell deal with rate limits on JES? I have some largish batch jobs that seem to result in triggering JES (google genomics api) rate limits when submitted via Cromwell. Is Cromwell robust to this?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1663:192,robust,robust,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1663,1,['robust'],['robust']
Availability,How does it work? Does Cromwell still have the chance to evaluate failure retries against e.g. exit codes?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7476:66,failure,failure,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7476,1,['failure'],['failure']
Availability,"How to evaluate engine functions in a Pluggable Backends World?. **Acceptance Criteria**:. Use the following WDLs files to test isolated components of engine function implementation:. Read/Write file at a task level when running against the Local Backend:. ```; task t {; Array[String] a = [""a"", ""b"", ""c""]; File x = write_lines(a); String y = read_string(""/foo/bar/some-public-object.txt""); command {; cat ${x}; echo ${y}; }; }. workflow w { call t }; ```. read functions from the task output section when running against the Local Backend:. ```; task t {; command {; echo foo; echo bar; }; output {; Array[String] array = read_lines(stdout()); }; }. workflow w { call t }; ```. **Unanswered Questions**. What do we do when a user wants to call `write_lines` from the workflow level? If multiple filesystems are supported, then it's unclear where this gets written to. Perhaps all of them? If `x` is used by a JES job, then the file _must_ be written to at least GCS. ```; workflow w {; String x = write_lines([""a"", ""b"", ""c""]); }; ```. *\* Decision **; We should make this operation unsupported until we have a concrete use case for it, and likely this will be easier when we have filesystem separated out entirely from backend.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/654:412,echo,echo,412,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/654,3,['echo'],['echo']
Availability,How to troubleshoot the backend failure?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6904:32,failure,failure,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6904,1,['failure'],['failure']
Availability,"However, AWS Batch backend ignores script-epilogue as unrecognized. Do you have any suggestions?. > Yes, the script epilogue is exactly where the change should be. The script is generated by AwsBatchJob.scala; > […](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isn’t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark … <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. — You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probabl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:590,down,down,590,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383,1,['down'],['down']
Availability,Huh. Did not realise the return-code file checking rate is also bound by the exit-code-checking poll rate. That slows down the development cycle somewhat; we have it set pretty long (~120 minutes) to prevent overloading the queque systems.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488996990:118,down,down,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488996990,1,['down'],['down']
Availability,"I *think* that it errored out while I was still testing local backend, so I; never tested JES. On Thu, Dec 15, 2016 at 11:28 AM, Ruchi <notifications@github.com> wrote:. > @LeeTL1220-- was your example WDL run locally or on JES?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267372631>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk7xuRrOQ7Sac2mRulJkYLAIMPnD_ks5rIWqQgaJpZM4JmxQ5>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267410928:18,error,errored,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-267410928,1,['error'],['errored']
Availability,I accidentally submitted a WDL and inputs to the batch submission endpoint instead of the regular submission endpoint. Cromwell did not respond with an error and eventually just timed out.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1270:152,error,error,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1270,1,['error'],['error']
Availability,"I actually was not able to reproduce the failure with newer GATK versions, but the test does confirm that we now choose a larger boot disk size than before the changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5790#issuecomment-679261370:41,failure,failure,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5790#issuecomment-679261370,1,['failure'],['failure']
Availability,"I added a workaround since #891. The output block now makes a File output and then reads from it. Unfortunately, this version still doesn't work!. Here's a slightly different workflow:. ```; task MakeMeAFile {; Int c = 6. command <<<; echo FILECONTENT > output.txt; >>>. runtime {; docker: ""ubuntu:latest""; memory: ""1 GB""; preemptible: 3; }. output {; File outFile = ""output.txt""; String out = read_string(outFile); }; }. task ReadMeAFile {; File infile. command <<<; cat ${infile}; >>>. runtime {; docker: ""ubuntu:latest""; memory: ""1 GB""; preemptible: 3; }. output {; File out = read_string(stdout()); }; }. workflow FileMakeAndRead {; Int a = 5; call MakeMeAFile; call ReadMeAFile { input: infile = MakeMeAFile.out }; }; ```. And the new failure:. ```; ""failures"": [; ""Item not found: cromwell-dev/cromwell_execution/chrisl/FileMakeAndRead/000adc78-7dc7-4073-b5f6-83e499c13706/call-MakeMeAFile/cromwell-dev/cromwell_execution/chrisl/FileMakeAndRead/000adc78-7dc7-4073-b5f6-83e499c13706/call-MakeMeAFile/output.txt""; ],; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/892:235,echo,echo,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/892,3,"['echo', 'failure']","['echo', 'failure', 'failures']"
Availability,I agree that metrics on checksum failures would be nice but that does seem to be beyond the scope of the ticket as currently written; perhaps a follow-on ticket?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6683#issuecomment-1051019488:33,failure,failures,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6683#issuecomment-1051019488,1,['failure'],['failures']
Availability,"I agree with Lee here. I think we could do much better at getting people rolling rather than pointing them at a mega-file inside of our source tree. For example -- slimming down what a user needs to have in their conf file, and also providing template conf files for common configurations (SGE, JES, Local, etc). This issue needs more refinement before being ready for development",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255497178:173,down,down,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1590#issuecomment-255497178,1,['down'],['down']
Availability,I agree with not worrying about this CircleCI failure because we still have that coverage from Travis CI.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6214#issuecomment-801188054:46,failure,failure,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6214#issuecomment-801188054,1,['failure'],['failure']
Availability,"I agree with what Chris said above about making this ""opt-in"" and adding a Centaur test. It also looks like those Travis `sbt` failures are real so those tests would need to be fixed as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4379#issuecomment-439111507:127,failure,failures,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4379#issuecomment-439111507,1,['failure'],['failures']
Availability,"I agree, it looks like introducing `s3.amazonaws.com` before the bucket path breaks the path. What version of Cromwell are you on?; ```; > aws s3 ls ""s3://aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/""; 2019-04-23 15:41:46 0 ; 2019-04-23 15:46:35 2 local_disk-rc.txt; 2019-04-23 15:46:36 0 local_disk-stderr.log; 2019-04-23 15:46:35 1304 local_disk-stdout.log; 2019-04-23 15:41:46 1117 script; ```; ```; > aws s3 ls ""s3://s3.amazonaws.com/aen-test/cromwell-execution/test/1e346768-e95f-415c-9afd-5b1e8886ff02/call-local_disk/"". An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied; ```; I think what the code is going for is [introducing an endpoint](https://docs.aws.amazon.com/general/latest/gr/s3.html) which is supported when using HTTP/REST but apparently not with the `s3://` scheme.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185:572,error,error,572,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-926897185,1,['error'],['error']
Availability,I also agree with not worrying about this CircleCI failure for now since the same tests are still running in Travis so the coverage is unchanged.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6214#issuecomment-801192405:51,failure,failure,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6214#issuecomment-801192405,1,['failure'],['failure']
Availability,I also encountered the same error. Could anyone make some comments?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6744#issuecomment-2118191458:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6744#issuecomment-2118191458,1,['error'],['error']
Availability,I also hit this while experimenting with using singularity with cromwell. Just replicating https://github.com/kundajelab/atac-seq-pipeline/blob/master/docs/tutorial_local_singularity.md locally. Get the error with cromwell-37 and with a fresh build of develop branch. Works with cromwell-36.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-464943989:203,error,error,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-464943989,1,['error'],['error']
Availability,"I also met this error, can you tell me how to solve it? please",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4186#issuecomment-1895225269:16,error,error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4186#issuecomment-1895225269,1,['error'],['error']
Availability,"I also played around with bolting on the docker hashing too. To be clear, I like @mcovarr's PR here better, as it's much cleaner, and has tests! Still, here's some overlapping [code](https://github.com/broadinstitute/cromwell/compare/job_avoidance...ks_hash_docker_image) to look at, especially the first commit with an alternative way to get an `ActorSystem` down into the `BackendCall`. A few issues left though, but some/most of these can be logged as new tickets, and we can get basic wiring in for the moment via this PR. Biggest issue-- 10 seconds is right on the edge for testing _and_ checking the docker server for the hash, so different docker tests currently timeout intermittently. Among other issues I saw, `Future` exception handling may be different due to refactoring. For example converting `Future { /* big block */ }` to `/* big block */ hashFuture.map(hash => ...)` allows exceptions within the block to not get caught (as expected?). Also I wasn't sure yet how we want to handle some `Failure` cases, specifically when the docker server doesn't return a hash. I assume that means that we should just run again from scratch, and NOT go to a `FailedExecution` state in the database. Or maybe we should go to `Failure`, and just retry a particular operations later. With ~~Gatling~~ Tyburn load testing, perhaps we can log any docker client errors now, and start to distinguish them with custom error handling code as they pop up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702:360,down,down,360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/322#issuecomment-164760702,5,"['Failure', 'down', 'error']","['Failure', 'down', 'error', 'errors']"
Availability,"I also saw this problem. The VM is not a preemptible and I'm using Cromwell v32. There's a lot of shards spending 10 minutes in ""Waiting for quota"" when this problem happens. The instance that gives PAPI Error Code 10 was able to get a virtual machine, though. Maybe there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.Batching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:204,Error,Error,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,4,"['Error', 'error']","['Error', 'error']"
Availability,"I also want to bind FSx in ECS container.Like @hurchu ,I hava refrence in FSx, I don't want to download every time from S3.; This is my vision, EC2 has two additional filesystems, /fsx /cromwell_root(ebs autoscaling), Container on EC2 mount this two filesystems in itself.Now there is only cromwell_root(ebs filesystem) in container.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-765076130:95,down,download,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-765076130,1,['down'],['download']
Availability,"I am a developer of one of the Cromwell UI clients ( https://github.com/antonkulaga/cromwell-client ) and I use /api/womtool/{version}/describe call to let user check if there are errors in his code before running the workflow (so if something is wrong it is possible to figure out without bloating cromwell-executions history with failed workflows). However, right now it does not check if files in input.json can be resolved that is checked only when you attempt to run, it would be nice to allow optional parameters to check if the files mentioned in input.json can be resolved in describe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6142:180,error,errors,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6142,1,['error'],['errors']
Availability,"I am aiming to run several thousand samples using Cromwell server on AWS. In testing, when submitting more than a few jobs to batch, I see many errors like:. ```; Caused by: software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: null; Status Code: 429; Request ID: b8d3f02a-deee-11e8-8f12-b5957ed5827f); ```. I think this has to do with AWS batch limits. The recommendation, it appears, is to set the concurrent-job-limit in cromwell config to some number like 16. (Setting to 100 results in the errors above). While that fixes the errors, it seems to limit the scale of what can be done on AWS. So, a few questions:. 1. Does the job submission mechanism (batch vs single) affect this behavior?; 2. Would the use of scatter-gather over large batches of otherwise independent samples help?; 3. Are there limit increases that can or should be requested from AWS to make batch more amenable to large numbers of workflows?; 4. Are the best practices for AWS different than for GCP with respect to workflow authoring and submission?. Thanks for any thoughts and sorry if I missed something obvious in the docs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4355:144,error,errors,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4355,3,['error'],['errors']
Availability,"I am also having this issue. . I don't understand what I am going wrong because it seems to be I am following the basic use case explained in the cromwell manual. I have basicaly took the published paired-fastq-to-unmapped-bam.wdl. can have added inputs and parsing to that each umapped bam file created get processed first by ; processing-for-variant-discovery-local-gatk4.wdl; and then the processed bam file gets processed by ; haplotypecaller-gvcf-gatk4.wdl. So I import the 2 workflows on the top. ```; import ""processing-for-variant-discovery-local-gatk4.wdl"" as process_bam; import ""haplotypecaller-gvcf-gatk4_no_docker.wdl"" as haplotype_caller; ```. and then use them on the ubam outputs. When I run it through validate I get the same error. ```; java -jar ~/src/cromwell-36/womtool-36.jar validate my_pipeline.wdl. ERROR: Missing value or call: Couldn't find value or call with name 'haplotype_caller' in workflow (line 131):. Array[File] HaplotypeCalls_output_vcfs = haplotype_caller.HaplotypeCallerGvcf_GATK4.output_vcf; ^. java -jar ~/src/cromwell-36/womtool-36.jar validate my_pipeline.wdl. ERROR: Missing value or call: Couldn't find value or call with name 'haplotype_caller' in workflow (line 131):. Array[File] HaplotypeCalls_output_vcfs = haplotype_caller.output_vcf; ^; ```. I thought it might be that 2 subworkflows was not supported so I took out the last one and the error become about the first:. ```; java -jar ~/src/cromwell-36/womtool-36.jar validate my_pipeline_no_variant_calling.wdl. ERROR: Missing value or call: Couldn't find value or call with name 'sub' in workflow (line 110):. Array[File] ProcessedBams_duplication_metrics = sub.duplication_metrics; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2756#issuecomment-435006736:743,error,error,743,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2756#issuecomment-435006736,5,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I am brand new to cromwell and sorry if this question looks so basic.; Following the tutorial of ""Server Mode"", I open a webpage `localhost:8000` but I didn't find a button of `choose file` at `workflowSource` or at `workflowInputs`. I did see a red error sign at the bottom showing `ERROR {...}` and there is a message in it `{""schemaValidationMessages"":[{""level"":""error"",""message"":""Can't read from file /swagger/cromwell.yaml""}]}`; Any suggestions?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3869:250,error,error,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3869,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I am constantly having issues when I run docker-compose of cromwell (I use https://github.com/broadinstitute/cromwell/tree/develop/scripts/docker-compose-mysql where I provide my configutation). Even though I do not have docker-user in my application.conf file when I run my pipelines, I get:; ```; cromwell_1 | [ERROR] [05/20/2017 12:57:46.015] [cromwell-system-akka.dispatchers.engine-dispatcher-22] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-f7fdd305-d137-4128-bf68-7c39fd6c834b/WorkflowInitializationActor-f7fdd305-d137-4128-bf68-7c39fd6c834b/Local] Error parsing generated wdl:; cromwell_1 | task submit {; cromwell_1 | ; cromwell_1 | String job_id; cromwell_1 | String job_name; cromwell_1 | String cwd; cromwell_1 | String out; cromwell_1 | String err; cromwell_1 | String script; cromwell_1 | String? docker Int? max_runtime = 2; cromwell_1 | command {; cromwell_1 | /bin/bash ${script}; cromwell_1 | }; cromwell_1 | }; cromwell_1 | ; cromwell_1 | ; cromwell_1 | task submit_docker {; cromwell_1 | ; cromwell_1 | String job_id; cromwell_1 | String job_name; cromwell_1 | String cwd; cromwell_1 | String out; cromwell_1 | String err; cromwell_1 | String script; cromwell_1 | ; cromwell_1 | String docker_cwd; cromwell_1 | String? docker Int? max_runtime = 2; cromwell_1 | command {; cromwell_1 | ; cromwell_1 | docker run \; cromwell_1 | --rm -i \; cromwell_1 | ${""--user "" + docker_user} \; cromwell_1 | --entrypoint /bin/bash \; cromwell_1 | -v ${cwd}:${docker_cwd} \; cromwell_1 | ${docker} ${script}; cromwell_1 | ; cromwell_1 | }; cromwell_1 | }; cromwell_1 | java.lang.RuntimeException: Error parsing generated wdl:; cromwell_1 | task submit {; cromwell_1 | ; cromwell_1 | String job_id; cromwell_1 | String job_name; cromwell_1 | String cwd; cromwell_1 | String out; cromwell_1 | String err; cromwell_1 | String script; cromwell_1 | String? docker Int? max_runtime = 2; cromwell_1 | command {; cromwell_1 | /bin/bash ${script}; cromwell_1 | }; cromw",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2284:313,ERROR,ERROR,313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2284,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"I am developing a pipeline and want to share google storage buckets for genome data (bowtie2 index tar ball and other big files) with users (Google authenticated) but want users to pay for the network traffic to download genome data. My pipeline works fine if storage bucket for genome data is set as ""owner pays"". But if I set it ""requester pays"" then I get the following error `BadRequestException: 400 Bucket is requester pays bucket but no user project provided`. I googled it and found that [`gsutil` must use JSON API (not CLI)](https://cloud.google.com/storage/docs/requester-pays) for ""requester pays"" buckets. Is there any plan to support ""requester pays"" buckets for JES backend? . ```; [2017-11-18 16:01:09,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: python $(which encode_bowtie2.py) \; /cromwell_root/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/d57a5f97-8542-4fcc-89c4-b7c487957dea/call-trim_adapter/shard-0/glob-019a547c7b0dda79121d0398158a07d0/ENCFF439VSY.trim.merged.R1.fastq.gz \; \; --multimapping 4 \; \; --nth 4; [2017-11-18 16:01:09,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: python $(which encode_bowtie2.py) \; /cromwell_root/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/d57a5f97-8542-4fcc-89c4-b7c487957dea/call-trim_adapter/shard-1/glob-019a547c7b0dda79121d0398158a07d0/ENCFF463QCX.trim.merged.R1.fastq.gz \; \; --multimapping 4 \; \; --nth 4; [2017-11-18 16:01:18,97] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: job id: operations/EJSf0Yz9Kxjs8__E9aKWivQBILWN-vrbGyoPcHJvZHVjdGlvblF1ZXVl; [2017-11-18 16:01:18,97] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: job id: operations/EJWg0Yz9KxiXpeC4gsSenC4gtY36-tsbKg9wcm9kdWN0aW9uUXVldWU; [2017-11-18 16:01",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916:212,down,download,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916,2,"['down', 'error']","['download', 'error']"
Availability,"I am encountering call caching issues with images from google artifact registry (gar). . When I use image directly from dockerhub or gcr I have no call caching issues and see this in the logs. > 2024-07-19 14:35:24 cromwell-system-akka.dispatchers.engine-dispatcher-26890 INFO - BT-322 61ba2acc:garTest.simpleLs:-1:1 cache hit copying nomatch: could not find a suitable cache hit.; 2024-07-19 14:35:24 cromwell-system-akka.dispatchers.engine-dispatcher-26890 INFO - 61ba2acc-4274-423b-818a-8cf1da67cd44-EngineJobExecutionActor-garTest.simpleLs:NA:1 [UUID(61ba2acc)]: Could not copy a suitable cache hit for 61ba2acc:garTest.simpleLs:-1:1. No copy attempts were made. However, when I copy the same image to my access controlled google artifact registry I get this authentication error. > 2024-07-19 14:31:44 cromwell-system-akka.dispatchers.engine-dispatcher-3006 WARN - BackendPreparationActor_for_f20da4b8:garTest.simpleLs:-1:1 [UUID(f20da4b8)]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for us-central1-docker.pkg.dev/xxx/yyy/aaa Request failed with status 403 and body {""errors"":[{""code"":""DENIED"",""message"":""Unauthenticated request. Unauthenticated requests do not have permission \""artifactregistry.repositories.downloadArtifacts\"" on resource \""projects/xxx/locations/us-central1/repositories/yyy\"" (or it may not exist)""}]}. The workflow completes successfully regardless of this error but call caching doesn't work when a gar image is used.; The service account I am using with the cromwell server has ""Artifact Registry Reader"" IAM role.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7473:778,error,error,778,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7473,4,"['down', 'error']","['downloadArtifacts', 'error', 'errors']"
Availability,"I am experiencing an issue that may also be related to this, using WDL draft-2 spec and cromwell-39.; Here is a dummy example I created off a real error I received, it is minimal but hopefully descriptive enough:. ```WDL; task example {; Map[String, File] sample_files; Array[Array[String]]? tax_id_and_name; String? summary_report_name. String default_summary_report = select_first([summary_report_name, 'summary_report.txt']). command <<<; set -ex; example_command \; -o ${default_summary_report} \; -i ${write_json(sample_files)} \; ${ if defined(tax_id_and_name) then '-t ' + write_tsv(tax_id_and_name) else '' }; >>>; runtime {; docker: ""<local or private image name with the custom `example_command` installed>""; }; output {; File summary_report = ""${default_summary_report}""; }; }; ```; The run fails and the offending log output from Cromwell says:. ```commandline; example_command -o summary_report.txt -i /cromwell-executions/test_example_workflow/1d0ebc28-df3c-4e8c-9ade-7cae41513fcc/call-example/execution/write_json_b428b2ef25b3a99656256ecf58545736.tmp -t /Users/myuser/projects/wdl_example/cromwell-executions/test_example_workflow/1d0ebc28-df3c-4e8c-9ade-7cae41513fcc/call-example/execution/write_tsv_c317cbd4e3102b89210776bbc6430eeb.tmp; E Unable to open file /Users/myuser/projects/wdl_example/cromwell-executions/test_example_workflow/1d0ebc28-df3c-4e8c-9ade-7cae41513fcc/call-example/execution/write_tsv_c317cbd4e3102b89210776bbc6430eeb.tmp for reading (No such file or directory). Stopped at /usr/bin/example_command line 192.; ```. `write_json()` has no issue creating a path within the container, while `write_tsv()` returns a host path which is not found within the container.; I am able to workaround this at the moment by using `basename(write_tsv())` since the file is still in the execution directory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032#issuecomment-484095411:147,error,error,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032#issuecomment-484095411,1,['error'],['error']
Availability,"I am facing the same problem, because I am using alpine images and bash is not available. Could it be possible that this is fix in the near future?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2593#issuecomment-364071889:79,avail,available,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2593#issuecomment-364071889,1,['avail'],['available']
Availability,"I am following up on a [report](https://gatkforums.broadinstitute.org/wdl/discussion/23250/wdl-1-0-wont-let-me-call-an-optional-task-with-an-optional-input) (by someone else) filed over a year ago.; Now I have a slightly different need, that is, the task 2 will take the optional input `input_2_opt` for its required input.; So, the following is what I want to achieve. ```; version 1.0. workflow my_workflow {; input {; File input_1; File? input_2_opt; }. call task1 {; input:; input_1 = input_1; }. if (defined(input_2_opt)) {; call task2 {; input:; input_2 = input_2_opt; }; }. output {; File output_1 = task1.output_1; File? output_2 = task2.output_2; }; }. task task1 {; input{; File input_1; }; command {; echo ""Hello, world!"" > hello.txt; }; output {; File output_1 = ""hello.txt""; }; }. task task2 {; input{; File input_2; }; command {; cat ${input_2} > goodbye.txt; }; output {; File output_2 = ""goodbye.txt""; }; }. ```. Running `womtool validate` on this gives. ```; Failed to process workflow definition 'my_workflow' (reason 1 of 1): Failed to process 'call task2' (reason 1 of 1): Failed to supply input input_2 = input_2_opt (reason 1 of 1): Cannot coerce expression of type 'File?' to 'File'; ```. But like in the original post, if I take out the version specification and the `input` braces in the workflow and tasks, womtool thinks the WDL is OK. Can you please explain what is the cause? And is there a solution on my end?. Thanks. ----------------------; ### The Jira interface is way too overwhelming ; ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5354:712,echo,echo,712,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5354,1,['echo'],['echo']
Availability,I am getting java.lang.ClassNotFoundException: languages.cwl.CwlV1_0LanguageFactory while using Cromwell 85 and 86 jar. ; [cromwell-85.log](https://github.com/broadinstitute/cromwell/files/13271608/cromwell-85.log); PFA for cromwell.log; Cromwell jar build completes without any errors. While using the jar to start Cromwell on Premise system we get the CWL error (command: nohup java -jar -Dconfig.file=/fastdata/02/genomics/cromwell/reference-84.conf /fastdata/02/genomics/cromwell/cromwell-85-f34251c-SNAP-original.jar server 2>&1 >> cromwell.log &).; https://github.com/broadinstitute/cromwell/releases notes for Cromwell 85 says: CWL implementation removed :This release removes the cwl top-level artifact. Some nonfunctional references may remain and will be addressed over time. Looks like some references is causing the issue?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7247:279,error,errors,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7247,2,['error'],"['error', 'errors']"
Availability,"I am having an issue when trying to use `--imports tasks.zip` with Cromwell v31. I assume it is related to this issue. I can't seem to make it work with imports. I assume the parameter `--workflow-root` may be an alternative to using `imports`? However, instead of using zip imports, I tried using `--workflow-root` and still did not work. Thanks in advance for any help. Details of my scenario:. In my workflow I have:. ```; import ""tasks/task-1.wdl"" as task1; import ""tasks/task-2.wdl"" as task2; ```. my zip file looks like this:. ```; c4b301bb01ef:Desktop gonzalezma$ unzip -l tasks.zip ; Archive: tasks.zip; Length Date Time Name; -------- ---- ---- ----; 128 03-17-18 11:57 tasks/task-1.wdl; 119 03-17-18 11:57 tasks/task-2.wdl; -------- -------; 247 2 files; ```. Cromwell fails and says it can't find the task wdl files. How do I make this work? . The detailed error is as follows:. ```; [2018-03-17 14:23:42,03] [error] WorkflowManagerActor Workflow 6d61108d-3a3c-4850-8bd9-2862660f953c failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; /var/folders/zm/35r081w17gn1nw2kksqjlp6dyhcq01/T/7591430002708022127.zip7406634406151397777/tasks/task-1.wdl; cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; /var/folders/zm/35r081w17gn1nw2kksqjlp6dyhcq01/T/7591430002708022127.zip7406634406151397777/tasks/task-1.wdl; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:203); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:173); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3297#issuecomment-373942284:868,error,error,868,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3297#issuecomment-373942284,1,['error'],['error']
Availability,"I am having the same error with the example ""Using Data on S3"" on https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-examples/ . I have changed the S3 bucket name in the .json file to my bucket name, but the run still failed. After reporting running failure, I have got the same error message. I am using cromwell-48. The S3 bucket has all public access, and I was logged in as the Admin in two terminal windows, one running the server and the other submitting the job. The previous two hello-world example were successful. There is no log file in the bucket and in the cromwell-execution, the only file create was the script. There is no rc or stderr or stdout created.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-597868610:21,error,error,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-597868610,3,"['error', 'failure']","['error', 'failure']"
Availability,I am having the same issue. Was there a solution for this error?; cromwell version: 47; MySQL version: 5.5.64-MariaDB; centos-release-7-7.1908.0.el7.centos.x86_64,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084#issuecomment-625457700:58,error,error,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084#issuecomment-625457700,1,['error'],['error']
Availability,"I am not familiar with that error message. From a bit of Googling it looks like [this](https://medium.com/@byronwhitlock/gcp-trusted-image-policy-1dbce98410c9) may be relevant. Assuming `cloud-lifesciences` is Google's project hosting the image that Cloud Life Sciences is trying to use to spin up the worker VM, you may need to add `projects/cloud-lifesciences` to your organization's [trusted image projects](https://medium.com/@byronwhitlock/gcp-trusted-image-policy-1dbce98410c9#:~:text=the%20trusted%20image-,projects,-.).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905450292:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905450292,1,['error'],['error']
Availability,"I am reading your tutorial on [HPC](https://cromwell.readthedocs.io/en/stable/backends/HPC/#:~:text=FileSystems-,Shared%20FileSystem,providers.) and I have a question that could be very uneducated. The shared filesystem section talks about the localization strategies for inputs, which is certainly an issue, but the outputs are not mentioned. Let's say I have several nodes in a cluster and a single shared volume between them, either physical or software one (like Lustre). I am using Slurm backend and any node can end up running any task based on internal Slurm scheduling. Ideally I would want each task to copy the inputs from the shared volume to a local folder, create outputs, and then copy outputs to the shared volume. I know one can output final outputs anywhere, but how can one control what happens to intermediate files? The problem would arise if the subsequent tasks in the workflow are done on different nodes, but is enforcing (one node)/(one wf execution) even possible? Even if it is it beats the point of scheduling resources by availability. The solution I can see is running Cromwell FROM the shared volume, but then everything would happen there and tiny inputs and outputs would choke the job and possibly cause wear on hardware. Unless I can set a temp directory while the outputs are written?. I am asking because I am not experienced and would like to know if there are solutions I am missing before I end up doing development on my own. ; Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5802:1051,avail,availability,1051,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5802,1,['avail'],['availability']
Availability,"I am running into the same bug on Terra. Not sure what version of Cromwell it is using. My wdl validates however, I get the a run time error. ```; Failed to evaluate input 'fastq1' (reason 1 of 1): No coercion defined from wom value(s) '""gs://fc-secure-46b3886a-473a-49ef-8073-022230a526ac/6463b025-27cf-4649-b6d0-59f860bdf18b/bam2FastQStarAlignWorkflow/a4a0d2f2-cc8b-41d8-a5b5-61cf6c2d0bd4/call-bamToFastq/cacheCopy/GTEX-1192X-0011-R10a-SM-DO941.1.fastq.gz""' of type 'File' to 'Array[File]'.; ```. adding '[' and ']' resolved the run time issue; ```; call starWorkflow.star_fastq_list {; input:; star_index = starIndex,; fastq1 = [ bamToFastq.firstEndFastq ],; fastq2 = [ bamToFastq.secondEndFastq ],; prefix = sampleId; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4550#issuecomment-1148945607:135,error,error,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550#issuecomment-1148945607,1,['error'],['error']
Availability,"I am running on a local backend (laptop) and have noticed that the number of tasks running simultaneously appears to be unlimited? If this is the case, would it be possible to add config parameters to local backends to limit cpu and memory usage to that available?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365:254,avail,available,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365,1,['avail'],['available']
Availability,"I am running the program [deFuse](https://bitbucket.org/dranew/defuse) version 0.8.1 using a local backend. When I run the progam locally inside a docker container, the program completes successfully. When I run it using Cromwell/WDL, it raises the following error:; ```; Starting defuse command:; /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl #<1 > #>1; Reasons:; /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakp; oints.split.001.fa.cdna.psl missing; Failure for defuse command:; /usr/local/bin/gmap -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a; -ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa > /cromwell-executions/detectFusions/96242; 9bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa.cdna.psl.tmp; Reason:; Job command with nonzero return code; Return codes: 139; Job output:; Running on 2ecb3961d54d; Note: /usr/local/bin/gmap.avx2 does not exist. For faster speed, may want to compile package on an AVX2 machine; GMAP version 2018-07-04 called with args: /usr/local/bin/gmap.sse42 -D defuse-data/gmap -d cdna -f psl /cromwell-executions/detectFusions/962429bb-ddfa-456a-ab35-c29cf554e409/call-deFuse/execution/OUT/jobs/breakpoints.split.001.fa; Checking compiler assumptions for SSE2: 6B8B4567 327B23C6 xor=59F066A1; Checking compiler assumptions for SSE4.1: -103 -58 max=198 => compiler zero extends; Checking compiler options for SSE4.2: 6B8B4567 __builtin_clz=1 __builtin_ctz=0 _mm_popcnt_u32=17 __builtin_popcount=17 ; Finished checking compiler assumptions; Pre-loading compressed genome (oligos)......done (78,222,840 bytes, 19098 pages, 0.00 sec); Pre-loading compressed genome (bits)......done (78,222,864 bytes, 19098 pages, 0.02 sec); Looking for index files in directory defuse-data/gmap/cdna; Pointers file is cdna.ref153offsets64meta; Offsets file is cdna.ref153offsets64strm; Positions file is cdna.re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:259,error,error,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,2,"['Failure', 'error']","['Failure', 'error']"
Availability,"I am struggling to make read_json work on any data that is more complex than a flat json-map. For instance, a json like; ```json; [{""series"":""GSE69360"",""name"":""Biochain_Adult_Liver"",""path"":""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",""model"":""Illumina HiSeq 2000"",""run"":""SRR2014238"",""gsm"":""GSM1698568"",""characteristics"":""number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male"",""strategy"":""RNA-Seq"",""organism"":""Homo sapiens"",""layout"":""PAIRED"",""title"":""Biochain_Adult_Liver""}]; ```; cannot be read by ; ```; Array[Map[String,String]] runs = read_json(path_to_json_file); ```; even through it is clearly Array[Map[String, String]]; I get the following failure:; ```; Workflow failed; WorkflowFailure(Failed to evaluate job outputs,List(WorkflowFailure(Bad output 'get_gsm.runs': Failed to read_json(""/data/cromwell-executions/test/f8f591dc-3797-46de-9846-dbd2a902ff65/call-get_gsm/execution/GSM1698568_runs.json"") (reason 1 of 1): No coercion defined from '[{""series"":""GSE69360"",""name"":""Biochain_Adult_Liver"",""path"":""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",""model"":""Illumina HiSeq 2000"",""run"":""SRR2014238"",""gsm"":""GSM1698568"",""characteristics"":""number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male"",""strategy"":""RNA-Seq"",""organism"":""Homo sapiens"",""layout"":""PAIRED"",""title"":""Biochain_Adult_Liver""}]' of type 'spray.json.JsArray' to 'Object'.,List()))); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4518:202,down,download,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4518,3,"['down', 'failure']","['download', 'failure']"
Availability,"I am trying to adapt a WDL workflow originally developed for DNAnexus, to work in AWS Batch. I am running from ""develop"" branch on Mac, server mode. The workflow seems to run on AWS, but then fails when checking for output logs in S3... inputs: [demux_plus_inputs.json.txt](https://github.com/broadinstitute/cromwell/files/2099495/demux_plus_inputs.json.txt); workflow: [demux_only.wdl.txt](https://github.com/broadinstitute/cromwell/files/2099496/demux_only.wdl.txt); resource file: [workflows.zip](https://github.com/broadinstitute/cromwell/files/2099470/workflows.zip); config file, which shows some attempts to add the local filesystem, since I get an error about that: ; [aws.conf.txt](https://github.com/broadinstitute/cromwell/files/2099528/aws.conf.txt). ```; 2018-06-13 14:29:27,796 cromwell-system-akka.dispatchers.api-dispatcher-72 INFO - Unspecified type (Unspecified version) workflow a67833cb-b894-4790-872f-9f3104cab60c submitted; 2018-06-13 14:29:44,760 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - 1 new workflows fetched; 2018-06-13 14:29:44,761 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(a67833cb-b894-4790-872f-9f3104cab60c); 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-a67833cb-b894-4790-872f-9f3104cab60c; 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-13 14:29:44,774 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-13 14:29:45,255 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Parsing workflow as WDL draft-2; 2018-06-13 14:29:46,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:656,error,error,656,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['error'],['error']
Availability,"I am trying to build this branch but got the error below. Both local (macOS) and cromwell-dev Docker image. ```bash; sbt assembly; ```. ```; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:113:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.links.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:116:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:159:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /code/wom/src/main/scala/wom/views/GraphPrint.scala:168:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] four errors found; ```. Any suggestion?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479:45,error,error,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6204#issuecomment-948637479,23,['error'],"['error', 'errors']"
Availability,"I am trying to reproduce one of the [examples](https://github.com/broadinstitute/wdl/blob/develop/SPEC.md#example-5-word-count) in the spec. It doesn't work out of the box since `wc` command produces a string with filename, and not just integer. Thus, I tried to substitute all non-digit characters with function `sub`:; ```; output {; Int count = read_int(sub(stdout(), ""\D"", """"); }; ```; However, I get an error:; ```; Unable to load namespace from workflow: Unrecognized token on line 7, column 40:. Int count = read_int(sub(stdout(), ""\D"", """")); ^; cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3$$anon$1: Workflow input processing failed:; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1908:408,error,error,408,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1908,1,['error'],['error']
Availability,"I am trying to run a workflow with a subworkflow that has some workflow level inputs. Somethign along the lines of ; ```; Workflow subWorkflow {. File f; String s; ...; ...; }; ```. ```; import ""file/path/subworkflow.wdl"" as sub. workflow root {; call sub.subWorkflow as aliasSub; }; ```. When I try to pass the values for `File f` and `String s` from the inputs json I get an failure message. To make sure I was giving the workflow the correct inputs json I first ran it with bad inputs on purpose and got expected failures; ```; status: ""Failed"",; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_pac' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.agg_preemptible_tries' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_ann' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.wgs_coverage_interval_list' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_fasta_index' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.unmapped_bam_suffix' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_ud' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_amb' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.preemptible_tries' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_sa' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.known_indels_sites_VCFs' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_mu' not",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2912:377,failure,failure,377,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2912,3,['failure'],"['failure', 'failures']"
Availability,"I am trying to run the GATK4 workflow on Cromwell using the Spark backend. The WDL is from the Broad's GitHub repo:; https://github.com/broadinstitute/gatk4-data-processing/blob/master/processing-for-variant-discovery-gatk4.wdl. The following is my cromwell conf file where cromwell is running on the namenode of the spark cluster:; ```json; include required(classpath(""application"")). backend {; default = ""Spark""; providers {; Spark {; actor-factory = ""cromwell.backend.impl.spark.SparkBackendFactory""; config {; root: ""/mnt/data/cromwell"". filesystems {; local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; caching {; duplication-strategy: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }; master: ""yarn""; deployMode: ""cilent""; }; }; }; }; ```. It looks like the shell script cromwell is generating to submit the job to spark using spark-submit has a syntax error in it, so the workflow fails immediately. ```bash; [ec2-user@ip-10-66-51-33 execution]$ pwd; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ ls -l; total 8; -rwxr--r-- 1 root root 1182 Feb 4 19:37 script; -rw-r--r-- 1 root root 296 Feb 4 19:37 stderr; -rw-r--r-- 1 root root 0 Feb 4 19:37 stdout; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ cat stderr; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: 3: /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: Syntax error: ""("" unexpected; ```. ```; [ec2-user@ip-10-66-51-33 execution]$ cat script ; #!/bin/sh; cd /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; spark-submit --master yarn --total-executor-cores 1 --deploy-mode cilent --class GATK4 --executor-memory 1gb InstantiatedCommand(# Not setti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4611:871,error,error,871,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4611,1,['error'],['error']
Availability,"I am trying to test my wdl and docker image on my local computer before publishing. ```; [2022-02-09 11:16:10,66] [warn] BackendPreparationActor_for_35c8738b:deseq_one_vs_all.one_vs_all:-1:1 [35c8738b]: Docker lookup failed; java.lang.Exception: Unauthorized to get docker hash aedavids/test-1vs-all-2:latest; at cromwell.engine.workflow.WorkflowDockerLookupActor.cromwell$engine$workflow$WorkflowDockerLookupActor$$handleLookupFailure(WorkflowDockerLookupActor.scala:222); ```. using docker image I see that my image exists. I am able to start the container using docker run. I also tried using the sha instead of the tag name. same error. I run Cromwell as follows; ```; $ java -jar ${WDL_TOOLS}/cromwell-74.jar run --inputs ../1vsAllTask.wdl.inputs.json ../1vsAllTask.wdl; ```. ```; $ cat ../1vsAllTask.wdl.inputs.json ; {; stuff deleted; ""deseq_one_vs_all.one_vs_all.dockerImg"": ""aedavids/test-1vs-all-2""; }; ```. ```; $ docker images |grep aedavids; aedavids/test-1vs-all-2 latest 0d33407a54e3 19 hours ago 6.28GB; ```; Any idea what my issue might be?. Andy. fyi https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team is a bad URL. I not have access to Jira on broadworkbench.atlassian.net.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6674:634,error,error,634,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6674,1,['error'],['error']
Availability,"I am trying to use this. I switch to the ""aws_backend"" branch, checked it out, figured out how to build it, and now I have run into:. [2018-06-04 06:34:20,69] [error] java.lang.IllegalArgumentException: s3://atbiofx-cromwell/cromwell-execution exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: MacOSXFileSystem. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: s3://atbiofx-cromwell/cromwell-execution exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: MacOSXFileSystem. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3427#issuecomment-394509732:160,error,error,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3427#issuecomment-394509732,1,['error'],['error']
Availability,"I am trying to use udocker to locally run the gatk4-germline-snps-indels, using the wdl and json file offered by its GitHub page: https://github.com/gatk-workflows/gatk4-germline-snps-indels.; I am locally running it with docker, until now seems working well, no fault report.; I read udocker intro, it said I can use it to pull or run docker image (maybe my understand is wrong). ; What should I do to use udocker replace docker for this task?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413196352:263,fault,fault,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413196352,1,['fault'],['fault']
Availability,"I am using exactly the wdl and json offered by gatk GitHub page for gatk4-germline-snps-indels, locally, I got this error, intervals-hg38.even.handcurated.20k.intervals is larger than 128000 Bytes. Maximum read limits can be adjusted in the configuration under system.input-read-limits.; I tried to change it via type this in command line: java -Dsystem.input-read-limits=500000 -jar /cromwell-34.jar ; Didn't work.; Who can tell me how to fix it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-413173960:116,error,error,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-413173960,1,['error'],['error']
Availability,I am working on code in the branch `issue\5004` that will remove the need for the proxy container and might make this redundant. It would be good to discuss and see if there is a way to kill two birds with one stone.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-626967088:118,redundant,redundant,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5447#issuecomment-626967088,1,['redundant'],['redundant']
Availability,"I asked your question to PAPI and here is the response:. > This detail is not something that should be counted on in a containerized environment.; That said: the /dev/disk/by-id/* system is simply a convenient alias. The underlying block storage doesn't change (eg, /dev/disk/by-id/google-local-disk is a symlink to a block device, in this case, /dev/sdb). So they should be able to continue monitoring if they want, it will just be harder to recover the mapping.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230:443,recover,recover,443,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230,1,['recover'],['recover']
Availability,"I assigned this to myself 12 (!) days ago but haven't started, I am taking this as an indication I am busy and should make it available for others to pick up",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4774#issuecomment-486406119:126,avail,available,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4774#issuecomment-486406119,1,['avail'],['available']
Availability,"I assume this error has to do with my config, but not particularly clear what is going on... Here is the workflow, inputs, and config; I am running from swagger:; [myWorkflow_awsbatch.wdl.txt](https://github.com/broadinstitute/cromwell/files/2077985/myWorkflow_awsbatch.wdl.txt). [aws.conf.txt](https://github.com/broadinstitute/cromwell/files/2077989/aws.conf.txt). [hello.inputs.txt](https://github.com/broadinstitute/cromwell/files/2078033/hello.inputs.txt). ```; 2018-06-06 16:18:30,442 cromwell-system-akka.dispatchers.api-dispatcher-215 INFO - WDL (Unspecified version) workflow 948bf608-f91b-46a7-b892-86454be067fd submitted; 2018-06-06 16:18:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - 1 new workflows fetched; 2018-06-06 16:18:47,222 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Starting workflow UUID(948bf608-f91b-46a7-b892-86454be067fd); 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowManagerActor Successfully started WorkflowActor-948bf608-f91b-46a7-b892-86454be067fd; 2018-06-06 16:18:47,223 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-06 16:18:47,229 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(948bf608)]: Parsing workflow as WDL draft-2; 2018-06-06 16:18:47,232 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow 948bf608-f91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['error'],['error']
Availability,I backed out the name-mangling change because it was redundant in fixing the actual bug and had far-reaching consequences.; - The upgrade script was very broken because it makes extensive use of anonymous node names to come up with real names for what to put in the WDL; - String concatenation and string comparison feel like gross tools to use when we have types at our disposal... i.e. evaluating `.isInstanceOf[AnonymousExpressionNode]`. I can imagine a future where we have a `canLinkWith` function that evaluates name and type to return a boolean,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4075#issuecomment-420680044:53,redundant,redundant,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4075#issuecomment-420680044,1,['redundant'],['redundant']
Availability,"I believe that I am running into this problem with a batch of workflows.; I have a Cromwell instance running on GCE (launched via `docker-compose ... up`). Cromwell had gotten stuck accepting new workflow requests, so I shut it down and it didn't go down cleanly. After restarting Cromwell, I see:. ```; cromwell_1 | 2018-05-24 16:03:29,668 cromwell-system-akka.dispatchers.engine-dispatcher-27 ERROR - Error trying to fetch new workflows; cromwell_1 | common.exception.AggregatedMessageException: Error(s):; cromwell_1 | Workflow a07583dd-f571-44bf-abb7-5bf281dfd249 in state Running and restarted = false cannot be started and should not have been fetched.; ```. with a lengthy list of workflows listed with the same error message. All of these workflows came to a stop. In fact, querying cromwell, I saw:. ```; $ curl http://localhost:8000/engine/v1/stats; {""workflows"":0,""jobs"":0}; ```. I have been able to now restart cromwell and submit new workflows and get them running, but these other workflows were fairly well along. I would like to get them started again. What is the best way to do this?. ```; $ curl http://localhost:8000/engine/v1/version; {""cromwell"":""32-c07d8d9-SNAP""}; ```. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3673#issuecomment-391771784:228,down,down,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673#issuecomment-391771784,6,"['ERROR', 'Error', 'down', 'error']","['ERROR', 'Error', 'down', 'error']"
Availability,"I believe the one Travis failure is unrelated, an intermittent failure in SprayDockerRegistryApiClientSpec.scala that might be a GCR hiccup?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/348#issuecomment-168850405:25,failure,failure,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/348#issuecomment-168850405,2,['failure'],['failure']
Availability,I believe the patch coverage check failure is due to an incidental fix to the formatting of a log statement.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7555#issuecomment-2376759111:35,failure,failure,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7555#issuecomment-2376759111,1,['failure'],['failure']
Availability,I believe this might be the intended behavior. Since we treat subworkflow calls the same as a task call -- the outputs of a call can't be used until the call completes and only then can downstream calls continue. I can see the advantages of doing things differently for a subworkflow call -- @geoffjentry @danbills thoughts?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3814#issuecomment-400446797:186,down,downstream,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3814#issuecomment-400446797,1,['down'],['downstream']
Availability,"I came here to make a comment but as I started typing that made me wonder if what I was about to suggest might have been old behavior and explain why (we think) things went from fail slow to fail fast. Or, alternatively it was _always_ fail fast w/o us meaning to do so and this is all bollocks ... At one point startRunnableCalls was called findRunnableCalls, and then it'd kick everything off. I was going to suggest that one thing you could do is find runnable calls, and if the set is empty you know that either the workflow has failed or succeeded based on if the state of all of the calls (all success = success, any failure = failure, some not run ... uh oh!). Now that I go back and look at your code this seems more or less what you're doing, at least in spirit. I was mainly just excited that this might explain the slow -> fast transition (again, assuming it ever actually happened in the first place!)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/436#issuecomment-182178955:623,failure,failure,623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/436#issuecomment-182178955,2,['failure'],['failure']
Availability,"I can reproduce this on my system. If I do not include `--type cwl` it fails to recognize echo.cwl as a valid workflow input, if I do include it the tool succeeds. Furthermore, if I run an actual workflow with the tools inside the directory (so no import) it succeeds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5085#issuecomment-519932595:90,echo,echo,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085#issuecomment-519932595,1,['echo'],['echo']
Availability,"I can see these workflows failing in the Cromwell logs, but I do not have the permissions to find out more from within the application (tried via Swagger). ```; [anichols@caas-cromwell-prod101 ~]$ docker logs caas_cromwell_1 | grep 7541e4e2-f74c-43f7-82af-8df891a27520; 2018-11-05 21:05:08,784 cromwell-system-akka.dispatchers.api-dispatcher-378 INFO - Unspecified type (Unspecified version) workflow 7541e4e2-f74c-43f7-82af-8df891a27520 submitted; 2018-11-06 04:55:58,066 cromwell-system-akka.dispatchers.engine-dispatcher-98 INFO - Status changed to 'Submitted' for 7541e4e2-f74c-43f7-82af-8df891a27520; 2018-11-06 04:56:01,019 cromwell-system-akka.dispatchers.engine-dispatcher-123 INFO - WorkflowManagerActor Starting workflow UUID(7541e4e2-f74c-43f7-82af-8df891a27520); 2018-11-06 04:56:01,019 cromwell-system-akka.dispatchers.engine-dispatcher-123 INFO - WorkflowManagerActor Successfully started WorkflowActor-7541e4e2-f74c-43f7-82af-8df891a27520; 2018-11-06 04:56:01,068 cromwell-system-akka.dispatchers.engine-dispatcher-49 ERROR - WorkflowManagerActor Workflow 7541e4e2-f74c-43f7-82af-8df891a27520 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; 2018-11-06 04:56:01,068 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - WorkflowManagerActor WorkflowActor-7541e4e2-f74c-43f7-82af-8df891a27520 is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-437481659:1033,ERROR,ERROR,1033,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-437481659,1,['ERROR'],['ERROR']
Availability,"I can't fault the code, but as you say, at the moment it doesn't complete the ticket. Can we add another ticket to actually wire this up?. Otherwise, 👍 . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/834/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/834#issuecomment-219721904:8,fault,fault,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/834#issuecomment-219721904,1,['fault'],['fault']
Availability,"I can't reproduce this error with hash 437d1b592ca606cdd96276a1cf85bf84594c31eb on develop, though I do still see the TMPDIR bug. I'll work on fixing that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743#issuecomment-395613110:23,error,error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743#issuecomment-395613110,1,['error'],['error']
Availability,"I can't reproduce this. I ran the following WDL without issues:. ```; task test {; command {; echo ""hello""; }; output {; String o = read_string(stdout()); }; }. workflow workflow_test {; call test as aliased_test; output {; aliased_test.*; }; }; ```. @yfarjoun do you happen to have the full WDL ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-305608096:94,echo,echo,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2125#issuecomment-305608096,1,['echo'],['echo']
Availability,"I can't run anything from the command line on this branch:. ```; [error] /Users/sfrazer/projects/cromwell/src/main/scala/cromwell/Main.scala:19: value BackendyString is not a member of object cromwell.engine.backend.CromwellBackend; [error] import cromwell.engine.backend.CromwellBackend.BackendyString; [error] ^; [error] /Users/sfrazer/projects/cromwell/src/main/scala/cromwell/Main.scala:352: value toBackendType is not a member of String; [error] val backendType = args(1).toBackendType; [error] ^; [error] two errors found; [error] (compile:compileIncremental) Compilation failed; [error] Total time: 21 s, completed Dec 14, 2015 5:10:50 PM; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164576757:66,error,error,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164576757,10,['error'],"['error', 'errors']"
Availability,I can't tell from this fragment what the problem you're seeing is. This workflow worked as expected for me. I tried running it using `miniwdl run` and `java -jar cromwell.jar run`.; ```; version 1.0. task T {; command {; echo hello world; >&2 echo another world; }; output {; File out = stdout(); File err = stderr(); }; }. workflow W {; call T; output {; File out = T.out; File err = T.err; }; }; ```; Another common form is `String s = read_string(stdout())` which puts the command block `stdout` in a string result. Sometimes this is easier to use than opening a file.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1055722482:221,echo,echo,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686#issuecomment-1055722482,2,['echo'],['echo']
Availability,"I cant get the sra filesystem to work. Here is the error:. ```; [2020-08-21 11:08:59,62] [info] WorkflowManagerActor Workflow dbd5cdc0-c79a-42cd-b929-56ddb1115467 failed (during InitializingWorkflowState): common.exception.AggregatedMessageException: Failed to instantiate backend filesystem:; Cannot find a filesystem with name sra in the configuration. Available filesystems: ftp, s3, gcs, oss, drs, http; 	at common.validation.Validation$ValidationChecked$.$anonfun$unsafe$2(Validation.scala:98); 	at cats.syntax.EitherOps$.valueOr$extension(either.scala:66); 	at common.validation.Validation$ValidationChecked$.unsafe$extension(Validation.scala:98); 	at cromwell.backend.BackendConfigurationDescriptor.configuredPathBuilderFactories$lzycompute(backend.scala:109); 	at cromwell.backend.BackendConfigurationDescriptor.configuredPathBuilderFactories(backend.scala:108); 	at cromwell.backend.BackendConfigurationDescriptor.pathBuilders(backend.scala:120); 	at cromwell.backend.standard.StandardInitializationActor.pathBuilders$lzycompute(StandardInitializationActor.scala:62); 	at cromwell.backend.standard.StandardInitializationActor.pathBuilders(StandardInitializationActor.scala:62); 	at cromwell.backend.google.pipelines.common.PipelinesApiInitializationActor.$anonfun$workflowPaths$2(PipelinesApiInitializationActor.scala:137); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(Abst",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793:51,error,error,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793,2,"['Avail', 'error']","['Available', 'error']"
Availability,"I confirmed with EB that the file used here does not need to be protected, so that should make things easier. Whoever takes this ticket... if you find it takes too long to run still let me know and we can work together to slim down the use case even further I think (but it may be just fine the way it is)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/820#issuecomment-218557707:227,down,down,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/820#issuecomment-218557707,1,['down'],['down']
Availability,"I could not find on [WDL spec](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md) something saying that structs could not be used as outputs... so I decide to report here. This is an example I prepared:. ```wdl; version 1.0. struct Test {; String name; File path; }. struct Collection {; Array[Test] samples; }. task GenerateComplexObject {; input {; Int items; }. command <<<; python <<CODE; import sys; import os; import json; items = []; for item in range(0, ~{items}):; name = f""test{item}.txt""; os.system(f""echo 'some content' > {name}""); items.append({""name"": f""item-{item}"", ""path"": name}). with open(""results.json"", ""w"") as fh:; json.dump({'samples': items}, fh); CODE; >>>. runtime {; docker: ""python:3.8""; memory: ""1 GB""; cpu: 1; preemptible: 3; disks: ""local-disk "" + 10 + "" HDD""; }. output {; Collection results = read_json(""results.json""); }; }. workflow TestStruct {; input {; Int items; }. call GenerateComplexObject {; input:; items=items; }. output {; Collection out = GenerateComplexObject.results; }; }; ```. When using local backend I have no problem, but when using PAPIv2 (`cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory`) the files from `Test` struct (path) do not delocalize. ```bash; gsutil ls gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/GenerateComplexObject.log; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/gcs_delocalization.sh; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/gcs_localization.sh; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/gcs_transfer.sh; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/rc; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/results.json; gs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5592:524,echo,echo,524,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592,1,['echo'],['echo']
Availability,"I did not know of this thread.; At our institute we have solved this differently. We use `singularity exec` and no specific pull command. This will try to locate the image in the cache whis is located in `SINGULARITY_CACHEDIR` (env variable). If it is already there it will use it. If not, it will download it. This will lead to race condition if it is used in a scatter. We use https://github.com/biowdl/prepull-singularity to pull the images beforehand, so no race conditions occur. I am also thinking of adding a `docker_pull` thing to the config, so you can do `singularity exec {image} echo done!` or something similar to make sure the cache is populated at workflow initialization time. I have no ETA on this though, for now the prepull singularity script works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756:298,down,download,298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627146756,2,"['down', 'echo']","['download', 'echo']"
Availability,"I did some benchmarking over the weekend. I ran a 1000+ job workflow on both the HSQL database with overflow file, and with sqlite. The Sqlite database creates 99M in files. The HSQLDB creates 9.5G in files, that is 100 times more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specifi",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:501,down,down,501,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906,2,['down'],['down']
Availability,"I did that and began encountering the following error:; ```; 2019-02-25 18:17:52,693 cromwell-system-akka.actor.default-dispatcher-29 ERROR - No configuration setting found for key 'services'; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor: exception during creation; 	at akka.actor.ActorInitializationException$.apply(Actor.scala:193); 	at akka.actor.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services'; 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156); 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorF",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:48,error,error,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,I didn't drill in but that is a lot of build failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617305109:45,failure,failures,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617305109,1,['failure'],['failures']
Availability,"I do not really know how to give you more relevant information, but IntelliJ is complaining that the WDL plugin has an error ... ```; Cyclic TextAttributesKey dependency found: BAD_CHARACTER->BAD_CHARACTER; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2276:119,error,error,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2276,1,['error'],['error']
Availability,I do this all the time and never have a problem. It's possible that there's some pathological state things can be in which causes problem but we'd need a reproducible example to track it down and we don't have one. I vote to close this.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328217375:187,down,down,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1495#issuecomment-328217375,1,['down'],['down']
Availability,"I don't actually know what the fix is because I don't know the intent behind exporting custom `TMPDIR` into the shell environment. I could just delete that line — it seems redundant to me, I can't think why the command shell `TMPDIR` has to equal `java.io.tmpdir` — but I don't know if it's there to fix some other issue that I don't know about.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2035#issuecomment-282901047:172,redundant,redundant,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2035#issuecomment-282901047,1,['redundant'],['redundant']
Availability,I don't disagree in spirit but I think this is a dangerous road to start going down and we'd need to manage it carefully. There are a *lot* of potential variations in the structure of xSV files (example: http://stat.ethz.ch/R-manual/R-devel/library/utils/html/read.table.html) and there aren't too many people who manage to handle it gracefully - I don't have faith that we'll be one of the few who find the holy grail there.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1962#issuecomment-278176656:79,down,down,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1962#issuecomment-278176656,1,['down'],['down']
Availability,"I don't know about the actual difference. I also have to admit that there is another possible variable here which is that when we ran this originally the farm was free and clear so the jobs ran very quickly. Now the farm is quite busy which is slowing down the workflow, but this is not the fault of cromwell. . Maybe @yfarjoun would have a better sense of timing boost.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275:252,down,down,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-289041275,2,"['down', 'fault']","['down', 'fault']"
Availability,"I don't know what's going on with the labels. Ruchi, Gemma and I discussed this when I was Acting Delivery Czar for a day in your absence. It definitely was in the sprint at one point. A/C: at a minimum emulate the exception above and confirm that a running WorkflowActor does not crash and hang forever without making progress. Some very nice-to-haves would be understanding what actually is going on here; could we be failing faster with conspicuous broken credentials? Also de-scary any error messages a la the other `OneForOneStrategy` tickets as appropriate.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4916#issuecomment-494544562:490,error,error,490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916#issuecomment-494544562,1,['error'],['error']
Availability,"I don't quite understand why this has failed, github actions suggests that this build was working before and my change caused it to crash. FWIW, I find the travis test logs extremely hard to navigate. . I tried to download the log locally and with a couple of greps found this: . ```; - should successfully run hello_google_legacy_machine_selection *** FAILED *** (6 minutes, 33 seconds); centaur.test.CentaurTestException: Invalid metadata response:; -Missing key: calls.wf_hello.hello.jes.machineType; at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); at centaur.test.Operations$$anon$28.checkDiff$1(Test.scala:737); at centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$8(Test.scala:779); at map @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$6(Test.scala:777); at flatMap @ centaur.test.Operations$$anon$28.$anonfun$validateMetadata$5(Test.scala:776); at unsafeToFuture @ centaur.api.CentaurCromwellClient$.$anonfun$retryRequest$3(CentaurCromwellClient.scala:151); at timeout @ cromwell.api.model.package$EnhancedFailureResponseOrT$.timeout$extension(package.scala:61); at fromFuture @ cromwell.api.model.package$EnhancedFutureHttpResponse$.asFailureResponseOrT$extension(package.scala:38); ...; ```. Any help would be appreciated :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690:214,down,download,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-749303690,1,['down'],['download']
Availability,"I don't see any error logging associated with this cromwell hash. However, I did see this:. ```; 2016-05-03 10:14:45,314 cromwell-system-akka.actor.default-dispatcher-17 ERROR - BackendCallExecutionActor [UUID(643d3c46):CollectUnsortedReadgroupBamQualityMetrics:22]: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; {; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala:191) ~[cromwell.jar:0.19]; at cromwell.e",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991:16,error,error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216661991,8,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error', 'errors']"
Availability,I don't think it closes anything no. It should be enough for the release though ? We'll see if/where we need more retries in the logs if this error pops up again.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/625#issuecomment-202554850:142,error,error,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/625#issuecomment-202554850,1,['error'],['error']
Availability,I don't think it's been improved no. I have no idea how often users encounter this. It could be added as a low hanging fruit for User improvement though as it's not a big deal to make the error message more useful.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-329784391:188,error,error,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-329784391,1,['error'],['error']
Availability,I don't think so but it's the first layer of the multi-failures test I was trying to fix. The rest is not ready yet but I though I'd PR that already in case other tests fail because of it,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3457#issuecomment-376602036:55,failure,failures,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3457#issuecomment-376602036,1,['failure'],['failures']
Availability,"I don't think the errors above correlate with DB backup times, when the database would be unavailable, but here are the times. (At least I think I have the correct terraformed `sfmt6…` db). <img width=""664"" alt=""terraform-sfmt6-backups"" src=""https://user-images.githubusercontent.com/791985/48174854-a04c4880-e2d7-11e8-9ea5-48a455567f65.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4360#issuecomment-436855808:18,error,errors,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4360#issuecomment-436855808,1,['error'],['errors']
Availability,I don't think these issues block anybody - they just lead to constant questions and give a bad impression of our reliability. People are often worried they are still spending money because it looks that way. I could do a query to probably find how often aborts don't work if that helps. . There isn't a workaround to either issue - only that we tell users it's ok after we dig in to find out that it is and they just deal with the inconsistency.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334480164:113,reliab,reliability,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-334480164,1,['reliab'],['reliability']
Availability,I don't understand the Travis CI failure - is this unrelated? Is there a way for me to re-run the test?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505582077:33,failure,failure,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505582077,1,['failure'],['failure']
Availability,"I don't understand the travis build failure, it looks to be something unrelated to my changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493515502:36,failure,failure,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493515502,1,['failure'],['failure']
Availability,"I dropped my database and call caching sped up. On May 2, 2017 16:30, ""Thib"" <notifications@github.com> wrote:. > [ERROR] [05/01/2017 21:06:41.921] [cromwell-system-akka.dispatchers.engine-dispatcher-106] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor] WorkflowManagerActor Workflow; > 67fdb82c-72bb-4d33-a74b-441a8db2a780 failed (during ExecutingWorkflowState): Task m2.Mutect2.M2:108:1 failed. JES error code 10. Message: 15: Gsutil failed: failed to upload logs for ""gs:/; > /broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full_dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19; > ec38f93/call-M2/shard-108/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://broad-dsde-methods/lichtens/cromwell-executions-test-dl-oxoq-full/full; > _dl_ob_training_with_m2/67fdb82c-72bb-4d33-a74b-441a8db2a780/call-m2_nt/shard-37/Mutect2/71720e5e-1769-46e7-a2b8-98d19ec38f93/call-M2/shard-108/, command failed: Traceback (most recent call; > last):; > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 75, in <module>; > main(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/gsutil.py"", line 22, in main; > project, account = bootstrapping.GetActiveProjectAndAccount(); > File ""/usr/local/share/google/google-cloud-sdk/bin/bootstrapping/bootstrapping.py"", line 205, in GetActiveProjectAndAccount; > project_name = properties.VALUES.core.project.Get(validate=False); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1221, in Get; > required); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1501, in _GetProperty; > value = _GetPropertyWithoutDefault(prop, properties_file); > File ""/usr/local/share/google/google-cloud-sdk/lib/googlecloudsdk/core/properties.py"", line 1539, in _GetPropertyWithoutDefault; > value = callbac",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027:115,ERROR,ERROR,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298887027,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I dug up the source for my statement about autocommit:. >If autocommit mode is enabled, each SQL statement forms a single transaction on its own. https://dev.mysql.com/doc/refman/5.7/en/innodb-autocommit-commit-rollback.html",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4508#issuecomment-452773467:211,rollback,rollback,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4508#issuecomment-452773467,1,['rollback'],['rollback']
Availability,"I encountered a need for this improvement while developing WDLs intended for FireCloud. Cromwell support for optional Docker runtimes would enable me to write FireCloud WDLs with quicker development iterations. It would also enable faster and more resilient automated testing (i.e., unit testing) of such WDLs. My current approach to developing WDLs intended for FireCloud is to add Cromwell and my WDL (`foo.wdl`) into the Docker image that contains my workflow dependencies (e.g. Python and R code). ; Then, from my local machine, I execute in my Docker container an equivalent version of my FireCloud WDL with a command like `docker exec $containerId java -jar cromwell-36.1.jar run foo_test.wdl --inputs test_inputs.json --options options.json`. Without a way to override the `runtime` attribute (or ignore its `docker` key) in `foo.wdl`, I resort to commenting out the attribute and copying the content to `foo_test.wdl`. This enables fast development and unit testing, but requires manually syncing `foo.wdl` and `foo_test.wdl`. That, of course, has poor maintainability -- my approach is a kludge. Adam (@aednichols) and I investigated better ways to do this, but found none. [See Slack](https://broadinstitute.slack.com/archives/CA2URMDPX/p1551723156019500) for more details about my issue. In summary, as an engineer using Cromwell to develop and test FireCloud WDLs, support for optional Docker runtimes as proposed here strikes me as the best option for my use case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941:248,resilien,resilient,248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-469426941,1,['resilien'],['resilient']
Availability,"I encountered this error in Cromwell v26, while running a large workflow that appears to have failed. I am not sure whether it is the cause of the failure; I have not yet been able to locate what job this corresponds to. ; ```; [ERROR] [05/22/2017 00:14:05.821] [cromwell-system-akka.dispatchers.engine-dispatcher-38] [akka.dispatch.Dispatcher] null; java.lang.NullPointerException; 	at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:22); 	at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2286:19,error,error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2286,3,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,"I encountered this error when running a WDL:; ```message: Runtime validation failed; causedBy: ; message: Task hello has an invalid runtime attribute docker = !! NOT FOUND !!; ```. I understand that it requires a docker attribute. The issue is that the error gets found at runtime. This should be caught when validating the WDL. . The risk is that users could ""run half their tasks and only find out mid-workflow that one needs an extra parameter"" (ChrisL's words).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2932:19,error,error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2932,2,['error'],['error']
Availability,I explored `rewriteBatchedStatements` as per https://github.com/slick/slick/issues/1272 as I thought this might be the magic we're supposed to ask @dvoet about. It didn't seem to have an effect but there could be some combination of operator error and our slick code confounding this. I did note that Rawls is only using this in their test `reference.conf` so perhaps this isn't what he was talking about. I'll also note one of the last comments in that issue states that it munges the return count. I didn't look but I wouldn't be surprised if we have code checking the # of inserted entries.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269833846:242,error,error,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1582#issuecomment-269833846,1,['error'],['error']
Availability,"I feel like overall WDL developers need to have some way to control that docker images be cached within the WDL. Tasks that are not scattered are likely not relevant here, as one single download is unlikely to incur large egress charges. On the other hand for scattered tasks there should be a way for the WDL developer to demand caching, rather than relying on the user to do the right thing. Ideally this should all be handled by Google and container images, when downloaded, should be cached for a pre-determined amount of time. There is something called `mirror.gcr.io` but I did not fully understand how it works and whether it could be part of the solution here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788:186,down,download,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788,2,['down'],"['download', 'downloaded']"
Availability,"I fixed the regex. It turned out that this also fixed any issues. > Thanks for finding and fixing this!. Thank you for trusting me with push access on this repository. It makes it easier for me as all tests run immediately, also the tests that need private variables. Also I can restart jobs on travis now that looks like they are failed due to some intermittent connection error. I had to restart one for this PR, and it indeed turned green on the retry. This makes it easier for me to fix any bugs I find. The trust is much appreciated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836:374,error,error,374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836,1,['error'],['error']
Availability,"I found two logback.xml files in cromwell. They were the exact same, but my PR only updates the one under engine. Is because I did not update the other one as well - the reason that travis is failing with the following error?. [error] 1 error was encountered during merge; …java.lang.RuntimeException: deduplicate: different file contents found in the following:; logback.xml; logback.xml",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546:219,error,error,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692#issuecomment-261663546,3,['error'],['error']
Availability,"I gave Cromwell the following JSON when I had to specify the pair workflow:. {; ""CNVSomaticPairWorkflow.common_sites"": ""gs://broad-dsde-methods/mkanaszn/iota_small_exac_grch38.interval_list"",; ""CNVSomaticPairWorkflow.gatk_docker"": ""samuelklee/gatk:sl_wgs_acnv_headers"",; ""CNVSomaticPairWorkflow.intervals"": ""gs://shlee-dev/CNV/intervals/pad_parmY_int_agilent_1kg_exotars_logrch38.tsv"",; ""CNVSomaticPairWorkflow.normal_bam"": ""gs://shlee-dev/hcc/hcc1143_N_clean.bam"",; ""CNVSomaticPairWorkflow.normal_bam_idx"": ""gs://shlee-dev/hcc/hcc1143_N_clean.bai"",; ""CNVSomaticPairWorkflow.read_count_pon"": ""gs://broad-dsde-methods/cromwell-execution-29/CNVSomaticPanelWorkflow/b36a6e5a-d31e-45e1-9935-b44b87690b6e/call-CreateReadCountPanelOfNormals/1kg_tutorial.pon.hdf5"",; ""CNVSomaticPairWorkflow.ref_fasta_dict"": ""gs://shlee-dev/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.dict"",; ""CNVSomaticPairWorkflow.ref_fasta_fai"": ""gs://shlee-dev/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa.fai"",; ""CNVSomaticPairWorkflow.ref_fasta"": ""gs://shlee-dev/ref/GRCh38_1kg/GRCh38_full_analysis_set_plus_decoy_hla.fa"",; ""CNVSomaticPairWorkflow.tumor_bam"": ""gs://shlee-dev/hcc/hcc1143_T_clean.bam"",; ""CNVSomaticPairWorkflow.tumor_bam_idx"": ""gs://shlee-dev/hcc/hcc1143_T_clean.bai"",; ""CNVSomaticPanelWorkflow.PreprocessIntervals.bin_length"": ""0"",; ""CNVSomaticPanelWorkflow.PreprocessIntervals.padding"": ""250""; }. As you can see, I accidentally wrote ""CNVSomaticPanelWorkflow"" instead of ""CNVSomaticPairWorkflow"" in the last two entries. I didn't get an error for this, which might have been useful though. This might be a duplicate issue but I didn't find another one related to this problem. Sorry if it is.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3120:1545,error,error,1545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3120,1,['error'],['error']
Availability,"I gave your travis test a nudge since I don't think it's your fault that that specific test case failed. I don't know where circle CI came from, but since the error is ""there's no configuration"" I think it's safe to ignore that one too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027:62,fault,fault,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-754102027,2,"['error', 'fault']","['error', 'fault']"
Availability,"I get a lot of coercion errors area just when I want to get a file inside the Directory, like dir + ""/"" +""filename"" that makes the Directory totally useless for the pipelines for me.; the error is:; ```; Workflow failed. WorkflowFailure(Failed to evaluate job outputs,List(WorkflowFailure(Bad output 'methylation_extraction.out': IllegalArgumentException: No coercion defined from wom value(s) '""/data/cromwell-executions/bs_map_fast/1ea51f16-2197-4703-be02-ee3e59c448c1/call-methylation_search/execution/output/output_CpG.bedGraph""' of type 'Directory' to 'File'.,List()))); ```; I enclose the pipeline ( main WDL there is bs_map_run_fast.wdl) and the input; [bs-seq.zip](https://github.com/broadinstitute/cromwell/files/3317934/bs-seq.zip)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5041:24,error,errors,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5041,2,['error'],"['error', 'errors']"
Availability,"I get an `ArrayIndexOutOfBoundsException` error when starting the latest `develop` branch in server mode with a mysql (`mariadb-10.3.12-5.fc30.x86_64`) backend.; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-56b0390-SNAP.jar server; 2019-01-31 18:29:28,169 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:29:32,763 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:32,786 INFO - CREATE TABLE cromwell.DATABASECHANGELOGLOCK (ID INT NOT NULL, `LOCKED` BIT(1) NOT NULL, LOCKGRANTED datetime NULL, LOCKEDBY VARCHAR(255) NULL, CONSTRAINT PK_DATABASECHANGELOGLOCK PRIMARY KEY (ID)); 2019-01-31 18:29:32,845 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:32,853 INFO - DELETE FROM cromwell.DATABASECHANGELOGLOCK; 2019-01-31 18:29:32,854 INFO - INSERT INTO cromwell.DATABASECHANGELOGLOCK (ID, `LOCKED`) VALUES (1, 0); 2019-01-31 18:29:32,869 INFO - SELECT `LOCKED` FROM cromwell.DATABASECHANGELOGLOCK WHERE ID=1; 2019-01-31 18:29:32,888 INFO - Successfully acquired change log lock; 2019-01-31 18:29:35,077 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,078 INFO - CREATE TABLE cromwell.DATABASECHANGELOG (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED datetime NOT NULL, ORDEREXECUTED INT NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35) NULL, `DESCRIPTION` VARCHAR(255) NULL, COMMENTS VARCHAR(255) NULL, TAG VARCHAR(255) NULL, LIQUIBASE VARCHAR(20) NULL, CONTEXTS VARCHAR(255) NULL, LABELS VARCHAR(255) NULL, DEPLOYMENT_ID VARCHAR(10) NULL); 2019-01-31 18:29:35,271 INFO - SELECT COUNT(*) FROM cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,279 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:29:35,280 INFO - SELECT * FROM cromwell.DATABASECHANGELOG ORDER BY DATEEX",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:42,error,error,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['error'],['error']
Availability,"I get cyclic dependency errors in places where I do not see a reason for them. I enclose the wdl and input.json; [cyclic_error.zip](https://github.com/broadinstitute/cromwell/files/1654348/cyclic_error.zip). Here is also the error log:; ```; Workflow input processing failed; WorkflowFailure(This workflow contains a cyclic dependency on quality_de_novo_with_download.copy_initial_quality_reports,List())WorkflowFailure(wdl.Scope.childGraphNodesSorted(Scope.scala:51),List())WorkflowFailure(wdl.Scope.childGraphNodesSorted$(Scope.scala:42),List())WorkflowFailure(wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63),List())WorkflowFailure(wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63),List())WorkflowFailure(wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140),List())WorkflowFailure(wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73),List())WorkflowFailure(wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27),List())WorkflowFailure(wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:97),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491),List())WorkflowFailure(scala.util.Either.flatMap(Either.scala:338),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490),List())WorkflowFailure(cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3176:24,error,errors,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3176,2,['error'],"['error', 'errors']"
Availability,"I get the same error message when e.g. renaming an arbitrary `.txt` file to `.zip` - that's not necessarily what's happening, but a clue that the zip itself may be bad.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4509#issuecomment-451259859:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4509#issuecomment-451259859,1,['error'],['error']
Availability,I got the same error.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-685331409:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-685331409,1,['error'],['error']
Availability,"I got this error while running JES on single workflow mode. ; However, I really do not know what happened. It seems like some jobs ran fine. Cromwell hangs and won't shutdown properly after a Ctl-C... ```; [2016-10-31 19:16:34,92] [error] 5a34e38c:crsp_validation_workflow.clinical_sensitivity_run_create_seg_gt_table:0:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File gt_seg_file hash. Caused by 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:51); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor$$anonfun$4.applyOrElse(EngineJobHashingActor.scala:45); at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167); at akka.actor.FSM$class.processEvent(FSM.scala:666); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobHashingActor.scala:18); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.processEvent(EngineJobHashingActor.scala:18); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.engine.workflow.lifecycle.execution.callcaching.EngineJobHashingActor.aroundReceive(EngineJobHashingActor.scala:18); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1637:11,error,error,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637,4,['error'],"['error', 'errors']"
Availability,"I had accidently left my `filesystems` stanza to `refresh-token` mode instead of what it was supposed to be: `application-default`. The error was very cryptic and left me guessing. ```; [2016-07-13 10:12:45,36] [info] Slf4jLogger started; [2016-07-13 10:12:45,59] [info] RUN sub-command; [2016-07-13 10:12:45,59] [info] WDL file: 3step.wdl; [2016-07-13 10:12:45,59] [info] Inputs: 3step.inputs; [2016-07-13 10:12:45,59] [info] Workflow Options: 3step.options; [2016-07-13 10:12:45,61] [info] Slf4jLogger started; [2016-07-13 10:12:45,63] [info] SingleWorkflowRunnerActor: Launching workflow; [2016-07-13 10:12:45,63] [info] WorkflowManagerActor Starting workflow dacbcd34-2045-4a93-b3b8-ff4ca83e1259; [2016-07-13 10:12:45,64] [info] SingleWorkflowRunnerActor: Workflow submitted dacbcd34-2045-4a93-b3b8-ff4ca83e1259; [2016-07-13 10:12:45,64] [info] WorkflowManagerActor Successfully started WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259; [2016-07-13 10:12:45,67] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [2016-07-13 10:12:46,07] [info] Running with database db.url = jdbc:hsqldb:mem:937e84db-703a-4f18-8e6d-1a2a18227cf5;shutdown=false;hsqldb.tx=mvcc; [2016-07-13 10:12:46,43] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: Call-to-Backend assignments: three_step.ps -> JES, three_step.cgrep -> JES, three_step.wc -> JES; [2016-07-13 10:12:46,44] [info] MaterializeWorkflowDescriptorActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [2016-07-13 10:12:46,45] [info] WorkflowActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: transitioning from MaterializingWorkflowDescriptorState to InitializingWorkflowState; [2016-07-13 10:12:46,46] [info] WorkflowInitializationActor-dacbcd34-2045-4a93-b3b8-ff4ca83e1259 [dacbcd34]: State is transitioning ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1156:136,error,error,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1156,1,['error'],['error']
Availability,"I had actually assumed that the singularity pull/caching mechanism would handle simultaneous downloads properly (by allowing only one to progress to fill the cache), but it doesn't appear to.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537242867:93,down,downloads,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537242867,1,['down'],['downloads']
Availability,"I had the same issue. I got the same error message:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; ```; I had set up my credentials with:; ```; export GOOGLE_APPLICATION_CREDENTIALS=sa.json; ```; and had this configuration in `google.conf` copied from the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/):; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }; ```; That clearly did not work. I tried to follow the logic in this post. I followed Horneth suggestion to use `service-account`'s authorization and I took the [auths](https://cromwell.readthedocs.io/en/develop/backends/Google/) configuration and changed `pem-file` to `json-file` in `google.conf` as follows:; ```; google {; application-name = ""cromwell""; auths = [; {; name = ""service_account""; scheme = ""service_account""; service-account-id = ""xxx@xxx.iam.gserviceaccount.com""; json-file = ""sa.json""; }; ]; }. engine {; filesystems {; gcs {; auth = ""service_account""; project = ""xxx""; }; }; }; ```; And I have replaced every other instance of `auth = ""application-default""` with `auth = ""service_account""`. Now when I run Cromwell:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; I don't get the error anymore. I do get a different error:; ```; [2020-07-27 22:54:56,48] [info] WorkflowManagerActor Workflow 0fb5e69d-7d70-407e-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906:37,error,error,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690#issuecomment-664753906,3,"['Error', 'error']","['Error', 'error']"
Availability,"I had the same problem. The code that generates `stdout` and `stderr` is included in [StandardAsyncExecutionActor.scala](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala):; ```; // The `tee` trickery below is to be able to redirect to known filenames for CWL while also streaming; // stdout and stderr for PAPI to periodically upload to cloud storage.; // https://stackoverflow.com/questions/692000/how-do-i-write-stderr-to-a-file-while-using-tee-with-a-pipe; (errorOrDirectoryOutputs, errorOrGlobFiles).mapN((directoryOutputs, globFiles) =>; s""""""|#!$jobShell; |DOCKER_OUTPUT_DIR_LINK; |cd ${cwd.pathAsString}; |tmpDir=$temporaryDirectory; |$tmpDirPermissionsAdjustment; |export _JAVA_OPTIONS=-Djava.io.tmpdir=""$$tmpDir""; |export TMPDIR=""$$tmpDir""; |export HOME=""$home""; |(; |cd ${cwd.pathAsString}; |SCRIPT_PREAMBLE; |); |$out=""$${tmpDir}/out.$$$$"" $err=""$${tmpDir}/err.$$$$""; |mkfifo ""$$$out"" ""$$$err""; |trap 'rm ""$$$out"" ""$$$err""' EXIT; |touch $stdoutRedirection $stderrRedirection; |tee $stdoutRedirection < ""$$$out"" &; |tee $stderrRedirection < ""$$$err"" >&2 &; |(; |cd ${cwd.pathAsString}; |ENVIRONMENT_VARIABLES; |INSTANTIATED_COMMAND; |) $stdinRedirection > ""$$$out"" 2> ""$$$err""; |echo $$? > $rcTmpPath; |$emptyDirectoryFillCommand; |(; |cd ${cwd.pathAsString}; |SCRIPT_EPILOGUE; |${globScripts(globFiles)}; |${directoryScripts(directoryOutputs)}; |); |mv $rcTmpPath $rcPath; |"""""".stripMargin; .replace(""SCRIPT_PREAMBLE"", scriptPreamble); .replace(""ENVIRONMENT_VARIABLES"", environmentVariables); .replace(""INSTANTIATED_COMMAND"", commandString); .replace(""SCRIPT_EPILOGUE"", scriptEpilogue); .replace(""DOCKER_OUTPUT_DIR_LINK"", dockerOutputDir)); }; ```; With `SCRIPT_EPILOGUE` set to default to `sync` and modifiable with the `script-epilogue` variable in the configuration (this is not explained in the Cromwell documentation but it is explained [here](https://github.com/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:584,error,errorOrDirectoryOutputs,584,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,2,['error'],"['errorOrDirectoryOutputs', 'errorOrGlobFiles']"
Availability,"I havd submit a job , but submit failed. The job input.json file size is large, longer than the maximum of 8388608。. How can i submit the large job successfully ?. The submit error is below:. {; ""status"": ""fail"",; ""message"": ""Request too large: Request was longer than the maximum of 8388608""; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7426:175,error,error,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7426,1,['error'],['error']
Availability,"I have a WDL with command block `samtools view -s ${frac} -b ${bampath} -o ${outname}.bam`, where `frac` is a `Float` type. I'm running in FireCloud. Downstream, in the PAPI script, this had turned into `samtools view -s 5.0E-5 -b ...<etc>`, which rather confused samtools. Someone somewhere decided to render `0.00005` as `5.0E-5`, and it wasn't me. Can you help?. (Adam N said to reference WDL draft-2, so I am hereby doing that.). WDL follows:. ```; task downsample {; File bampath; File baipath; File refpath; File refipath; Float frac; String outname; ; Int disk_size = ceil((size(bampath, ""GB"") + size(refpath, ""GB"") + size(refipath, ""GB"")) * 3). command {; samtools view -s ${frac} -b ${bampath} -o ${outname}.bam; }; ; output {; 	File outcram = ""${outname}.bam""; }. runtime {; docker : ""mshand/genomesinthecloud:samtools.1.8""; disks: ""local-disk ${disk_size} HDD""; memory: ""4GB""; }; }. workflow dswf {; call downsample; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4049:150,Down,Downstream,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4049,3,"['Down', 'down']","['Downstream', 'downsample']"
Availability,"I have a command that creates an output file, and prints the name of that file to a known file. EG `./program` puts its output in 1234.txt, and prints '1234' into output_name. I thought I could do this in my task:. ```; command <<<; ./program; >>>; String filename = read_string('output_name'); output {; File output = ""~{filename}.txt""; }; ```. However this fails, because apparently read_string is executed before command. (no such file error, cromwell never calls the backend to execute the command). I tried replacing `File output = ""~{filename}.txt""` with `File output = ""~{read_string('filename')}.txt""`. This does not work as arbitrary expressions are not yet allowed in string interpolation:. `Unable to start job due to: No implementation of FileEvaluator[StringExpression]`. Is there anything I am doing wrong? Can I make read_string execute after command?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4092:439,error,error,439,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4092,1,['error'],['error']
Availability,"I have a few cases in Firecloud (using cromwell .19) where it appears like some scatter jobs are not being launched. I have tracked it down to a problem with outputs from the prepare call not being registered as actual outputs. According to the logs and the gcs bucket all the outputs from the prepare step are right, but according to cromwell the output array is missing a couple entries. Looking in [workflow metadata](https://github.com/broadinstitute/cromwell/files/557271/metadata.txt) at the CallingGroup_Workflow.mone_prepare call, the outputs for split_base.06.interval_list and split_base.22.interval_list are missing. According to the [google bucket listing of the glob dir](https://github.com/broadinstitute/cromwell/files/557270/glob_listing.txt), the files are there. The JES logs for the prepare step show the upload happens successfully. To summarize, the call CallingGroup_Workflow.mone_prepare successfully generates and upload 25 interval files but the output section of the metadata shows only 23 (6 and 22 are missing). This means that only 23 downstream calls run which is bad. [workflow log](https://github.com/broadinstitute/cromwell/files/557272/workflow.9b9e6b19-aac4-451f-bf24-a9132c0e5408.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1633:135,down,down,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1633,2,['down'],"['down', 'downstream']"
Availability,"I have a following pipeline which fails validation, and I cannot see what can be wrong with it:. ```; version 1.0. ## ; # Git URL import; import ""https://raw.githubusercontent.com/DSLituiev/five-dollar-genome-analysis-pipeline/master/tasks/to_uBam.wdl"" as touBam. # WORKFLOW DEFINITION; workflow WGUnMap {; input {; File mapped_bam; String bam_base; }. call touBam.unMap {; input:; File mapped_bam=mapped_bam,; String unmapped_base=bam_base; }. # Outputs that will be retained when execution is complete; output {; File unmapped_bam = touBam.out; }; }; ```. The error is:; ```; java -jar `which womtool.jar` validate unmap.wdl; ERROR: Unexpected symbol (line 46, col 7) when parsing '_gen19'. Expected rbrace, got ""File"". File mapped_bam=mapped_bam,; ^. $call_body = :input :colon $_gen19 -> CallBody( inputs=$2 ); ```. Any ideas? Why brace is expected to be closed right after the colon?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5256:562,error,error,562,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5256,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I have a generic (run-anywhere) wdl now that exhibits the behaviour (runs under previous develop and release 22 & 23 versions, but not with 340a5cf): ; ```; workflow dna_mapping_38 {. call createInputs. scatter (arg in createInputs.alignedReadGroup) {; call mapping { input: inFile=arg }; }. call groupItemsByKey as groupArgsByLibrary {; input:; keys=createInputs.library,; items=mapping.outFile; }. scatter (libset in groupArgsByLibrary.groups) {; call markDup as libraryMerge {; input:; inputBams=libset.right,; outputBam=""library_${libset.left}.bam""; }; }. output {; Array[File] libMerged = libraryMerge.markDupedBam; }; }. #########; # TASKS #; #########. task createInputs {; command {; for i in `seq 1 5`; do echo ""lib1""; touch arg$i; done; }; output {; Array[File] alignedReadGroup = glob(""arg*""); Array[String] library = read_lines(stdout()); }; }. task mapping {; File inFile; command {; echo ""dummy mapping""; }; output {; File outFile=inFile; }; }. task groupItemsByKey {. Array[String] keys; Array[String] items. meta {; description: ""return pairs of (key, all-items-with-that-key)""; }. command <<<; python <<CODE; import itertools; import sys; keys = ""${sep='\t' keys}"".split(""\t""); items = ""${sep='\t' items}"".split(""\t""); assert len(items) == len(keys); theKey = lambda x: x[0]; theItem = lambda x: x[1]; data = sorted(zip(keys, items), key=theKey); for key, group in itertools.groupby(data, theKey):; sys.stderr.write(key + ""\n""); sys.stdout.write(""\t"".join(theItem(i) for i in group) + ""\n""); CODE; >>>. output {; Array[Pair[String, Array[String]]] groups = zip(read_lines(stderr()), read_tsv(stdout())); }; }. task markDup {. Array[File] inputBams; String outputBam. command {; echo ""dummy marking duplicates""; touch ${outputBam}; }. output {; File markDupedBam = ""${outputBam}""; }; }; ```; running:; ```; java -jar workspace/cromwell/target/scala-2.11/cromwell-24-5155e6f-SNAP.jar run scatterTest.wdl - - - -; ```. succeeds but running with new version: ; ```; java -jar workspace/cr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512:715,echo,echo,715,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1802#issuecomment-268422512,2,['echo'],['echo']
Availability,"I have a job that ended up in a weird state. It's a series of 3 tasks, where each one depends on the output of the previous one. The workflow finished, but the middle job is listed as still running. It's workflow. Seems like an error state. See https://cromwell-v30.dsde-methods.broadinstitute.org/api/workflows/v1/01c7d76f-5b2b-48cd-be08-ce75b923666e/timing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483:228,error,error,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483,1,['error'],['error']
Availability,"I have a large WDL file. At some point, I write the below WDL and get an error:. ```; Array[File] mybams. if (run_my_bams) {; scatter (i in range(length(mybams))) {; ......; }; }; ```; ```; Unable to build WOM node for Scatter '$scatter_0': Unable to build WOM node for If '$if_1': Two or more nodes have the same FullyQualifiedName: ^.mybams""; ```. To fix, I randomly tried the below and it worked:; ```; Array[File] mybams; Int num_mybams = length(mybams). if (run_my_bams) {; scatter (i in range(num_mybams)) {; ......; }; }; ```. Given that this is equivalent WDL, there should not have been an error message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3284:73,error,error,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3284,2,['error'],['error']
Availability,"I have a scatter block in a WDL (see below). I had one task that called a subworkflow and it was fine (m2.Mutect2). So I decided to add another task to the scatter block and now I get a parsing error. I'm really trying to find the issue here, but can't. Basically, p.left.left works for earlier calls in the same scatter block but not the later ones. ```. import ""dl_ob_training_m2.wdl"" as dl_ob_training_m2; import ""mutect2.wdl"" as m2. workflow dl_testing_m2 {; File tumor_bam_file_list; Array[File] tumor_bam_files = read_lines(tumor_bam_file_list). File tumor_bam_file_index_list; Array[File] tumor_bam_indices = read_lines(tumor_bam_file_index_list). File normal_bam_file_list; Array[File] normal_bam_files = read_lines(normal_bam_file_list). File normal_bam_file_index_list; Array[File] normal_bam_indices = read_lines(normal_bam_file_index_list). # pair.left and pair.right; Array[Pair[File, File]] tumor_bam_pair = zip(tumor_bam_files, tumor_bam_indices); Array[Pair[File, File]] normal_bam_pair = zip(normal_bam_files, normal_bam_indices). Array[Pair[Pair[File, File], Pair[File, File]]] tumor_normal_pairs = zip(tumor_bam_pair, normal_bam_pair). # ...........<snip>......... scatter (p in tumor_normal_pairs) {; # This call works fine; call m2.Mutect2 as m2_tn {; input:; gatk4_jar = ""/root/gatk-protected.jar"",; intervals = mutectIntervals,; ref_fasta = ref_fasta,; ref_fasta_index = ref_fai,; ref_dict = ref_dict,; tumor_bam = p.left.left,; tumor_bam_index = p.left.right,; tumor_sample_name = sub(sub(p.left.left, ""[/]*.*/"", """"), ""\\.bam$"", """"),; normal_bam = p.right.left,; normal_bam_index = p.right.right,; normal_sample_name = sub(sub(p.right.left, ""[/]*.*/"", """"), ""\\.bam$"", """"),; scatter_count = scatter_count,; dbsnp = dbSNPVCF,; dbsnp_index = dbsnp_index,; cosmic = cosmicVCF,; cosmic_index = cosmic_index,; is_run_orientation_bias_filter = true,; is_run_oncotator = false,; oncotator_docker = ""broadinstitute/oncotator:1.9.2.0-eval-gatk-protected"",; m2_docker = ""broadinstitute/ga",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2334:194,error,error,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2334,1,['error'],['error']
Availability,"I have a simple workflow with the following input cache structure:. ```json; {; ""String sampleName"": ""1B5BE2D031348FBA1A6E8624811B57E3"",; ""File reference"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reference_bwt"": ""1f6b1b1dff750c107f19d3fbc4c7ac90"",; ""File reads"": [; ""fa22ef528d4abd40315c885e784ff6c2"",; ""df337314b38af64554899eb5ebe81c74""; ]; }; ```. When I rerun the workflow I purely get a `cacheMiss`, but the metadata comparison between two of the inputs gives the following error:. ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]\nFailed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call A (f9a2bfe7-a173-439f-8c39-4bca22552a22 / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""ec4ed7c97d38063d4ad0587812c034e8\"",\""083ce2cf30923ff510378b1c63feb0b6\""]""; }; },; {; ""JsString"": {; ""value"": ""Failed to extract relevant metadata for call B (e6f82c61-4d10-4c7e-9122-815658bb874c / BwaAligner.bwamem:-1) (reason 1 of 1): Cannot extract hashes for File reads. Expected JsString or JsObject but got JsArray [\""fa22ef528d4abd40315c885e784ff6c2\"",\""df337314b38af64554899eb5ebe81c74\""]""; }; }; ]; }; }; }; ```. I presume this means that `processField` [[CallCacheDiffActor.scala#L164-L168](https://github.com/broadinstitute/cromwell/blob/8415afa3ee7ffe83",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5348:481,error,error,481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5348,2,['error'],['error']
Availability,"I have a task with an input 'File' (WDL concept) that is actually a folder. I did not know what would happen, as I could not find explanation of the concept in either wdl or cromwell docs. . For the 'File's that are actually files, cromwell reports ```Localisation via hard-link has failed ... invalid cross device link``` as expected (different disks), and then seems to successfully soft-link (as expected). For the folder, the same hard link error appears, but then there is no soft link error, and the folder is (recursively) copied. The folder hard link would also fail due to folders not being hard-linkable. But the soft-link should probably succeed. I am running an SGE backend, with no modification of the 'localisation' settings https://cromwell.readthedocs.io/en/latest/backends/HPC/#shared-filesystem. What is the expected behaviour? Should I avoid using folders for WDL 'File's? Any advice would be appreciated. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3785:445,error,error,445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3785,2,['error'],['error']
Availability,"I have a wdl task that doesn't handle an optional parameter as it should.; The wdl validates (wdltool-0.8.jar), but when I submit to my server, an exception is thrown over and over again. ```; [ERROR] [01/10/2017 15:07:12.214] [cromwell-system-akka.actor.default-dispatcher-5] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-98777f84-7e; 14-4408-b31e-9b57db5d813b/WorkflowExecutionActor-98777f84-7e14-4408-b31e-9b57db5d813b/98777f84-7e14-4408-b31e-9b57db5d813b-EngineJobExecutionActor-snps.testHC:NA:1/98777f84-7e1; 4-4408-b31e-9b57db5d813b-BackendJobExecutionActor-98777f84:snps.testHC:-1:1/DispatchedConfigAsyncJobExecutionActor] DispatchedConfigAsyncJobExecutionActor [UUID(98777f84)snps.t; estHC:NA:1]: Error attempting to Execute; java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlStringType,None); at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:48); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at wdl4s.Task$$anonfun$instantiateCommand$1.apply(Task.scala:108); at wdl4s.Task$$anonfun$instantiateCommand$1.apply(Task.scala:108); at scala.util.Try$.apply(Try.scala:192); at wdl4s.Task.instantiateCommand(Task.scala:108); ...; ```. Restarting the service seems to be the only way of stoppin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1830:194,ERROR,ERROR,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1830,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"I have a working cromwell/AWS batch configuration.; I have a simple workflow called three_task_sequence.wdl which I am able to run on AWS backend, and see the outputs in s3. However, submitting this job to my cromwell server:; `curl -X POST ""http://172.20.1.67:8001/api/workflows/v1"" -H ""accept: application/json"" -F ""workflowSource=@three_task_sequence.wdl"" -F ""workflowOptions=@workflow_options.json""; `; Where workflow_options.json content is:; ```; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686:585,error,error,585,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I have already changed that setting, as it was causing a different error. my aws.conf:. ```; include required(classpath(""application"")). aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""us-east-1""; }. engine { filesystems { s3 { auth = ""default"" } } }. backend {; default = ""AWSBATCH""; providers {; AWSBATCH {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 3; numCreateDefinitionAttempts = 3; root = ""s3://concr-genomics-results/cromwell-execution""; auth = ""default""; concurrent-job-limit = 16; default-runtime-attributes { queueArn = ""arn:aws:batch:us-east-1:<##########>:job-queue/GenomicsDefaultQueue-<###########>"" }; filesystems { s3 { auth = ""default"" } }; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016997:67,error,error,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341#issuecomment-435016997,1,['error'],['error']
Availability,I have also sneakily added another commit that seems to fix persistent near-immediate failures to build Centaur TES.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3504#issuecomment-380422757:86,failure,failures,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3504#issuecomment-380422757,1,['failure'],['failures']
Availability,"I have an similar issue to this [https://github.com/broadinstitute/cromwell/issues/1306](https://github.com/broadinstitute/cromwell/issues/1306), but I don't think OP's solution fits whats going in my case since input files are not part of the issue. Using the hello-world example:. ```{wdl}; task hello {; String name. command {; echo 'Hello ${name}!'; }; output {; File response = stdout(); }; }. workflow test {; call hello; }; ```. with input; ```{json}; {; ""test.hello.name"": ""World""; }; ```; I run `$ cromwell run hello.wdl hello.json hello.out`. I get the error message:. > /Users/jasonweirather/Dropbox (Partners HealthCare)/projects/2017_08_FIRECLOUD/cromwell-executions/test/12ed39b6-cf8f-4ea1-a965-193cd89f99e9/call-hello/execution/stderr.background; -bash: syntax error near unexpected token `('. Seems it may be having troubles being run from a working directory with a space and parentheses. This was done on Mac OS X 10.12.6 in the bash terminal.; Version of cromwell is `cromwell: 28-5fd2237-SNAP`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2518:331,echo,echo,331,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2518,3,"['echo', 'error']","['echo', 'error']"
Availability,"I have been experimenting some random failures due to docker containers being killed for some reason on my system (not only https://github.com/broadinstitute/cromwell/issues/3370), but if I re-run the workflow with caching enabled then this calls end without failure and the pipeline can continue and work. Nevertheless, it is tedious to re-run a whole pipeline due to random failures and rely on caching for avoid re-computation. This is something that can be avoided by providing a configuration option for retry jobs (cromwell level) or add to some tasks a runtime attribute (WDL level) to set the number of retries that can be done per-task. Do you think that this is possible in the near future to avoid re-running a whole pipeline due to a random failure in a concrete task(s)?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3417:38,failure,failures,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417,4,['failure'],"['failure', 'failures']"
Availability,"I have been testing AWS Batch with Cromwell 36. Jobs are submitted, etc. However, I keep seeing **disk full errors** with any real workflows. I started looking into how the autoscaling of the filesystem is working. It appears that the autoscaling is set up to scale `/scratch`, but not `/cromwell_root`. I have tested that it is working in my custom AMI (created with the python script here: http://aws-genomics-workflows.s3-website-us-east-1.amazonaws.com/aws-batch/create-custom-ami/#create-a-custom-ami) by ssh'ing into a running container and creating some large files in `/scratch`. Doing the same in `/cromwell_root`, of course, does not result in growing the filesystem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4322:108,error,errors,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4322,1,['error'],['errors']
Availability,"I have caught cromwell red-handed! I did not realize that the cromwell timing was implemented in [TrumpScript](https://github.com/samshadwell/TrumpScript). **TL;DR** The timing diagram showed that my job took > 2 hours to run, even though half of that was spent on overhead in cromwell. ; ## Proof. Here is a snapshot of the timing diagram (see highlighted runtime below -- 2h 6m).; ![cromwell_snapshot_so_slow_lies](https://cloud.githubusercontent.com/assets/2152339/18798566/23db7738-81a1-11e6-8a39-43612a561aa7.png); Yet, when I check that job:. ```; $ head -5 cromwell-executions/case_gatk_acnv_workflow/0050770f-ad61-49e4-bc81-3b0e5f5e2203/call-TumorCalculateTargetCoverage/shard-10/execution/stdout. 17:06:18.839 INFO IntelGKLUtils - Trying to load Intel GKL library from:; jar:file:/root/gatk-protected/build/libs/gatk-protected-all-24e6bdc-SNAPSHOT-spark_standalone.jar!/com/intel/gkl/native/libIntelGKL.so; 17:06:19.327 INFO IntelGKLUtils - Intel GKL library loaded from classpath.; 17:06:19.353 INFO CalculateTargetCoverage - Defaults.BUFFER_SIZE : 131072; 17:06:19.355 INFO CalculateTargetCoverage - Defaults.COMPRESSION_LEVEL : 5. $ tail -5 cromwell-executions/case_gatk_acnv_workflow/0050770f-ad61-49e4-bc81-3b0e5f5e2203/call-TumorCalculateTargetCoverage/shard-10/execution/stdout; 18:10:04.122 INFO CalculateTargetCoverage - Writing counts ...; 18:10:05.250 INFO CalculateTargetCoverage - Writing counts done.; 18:10:05.250 INFO CalculateTargetCoverage - Shutting down engine; ```. The job finished in about 64 minutes (please note that timezones are not concordant between timing and log messages).; This will likely (de facto) be addressed once the md5 issue is resolved in issue #1483",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1484:1478,down,down,1478,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1484,1,['down'],['down']
Availability,"I have configured a SLURM backend for Cromwell and have encountered unusual behavior while trying to configure memory as a runtime attribute. . I define a runtime attribute Int with default value in my Cromwell configuration file and attempt to override this in my task WDL. Whether the override succeeds seems to depend on the variable name used! This is very confusing behavior; I expect to either receive a message that a variable name is not allowed, or the override should succeed. . Fails: ""memory_mb""; Succeeds: ""requested_memory_per_core"". I am verifying whether the override succeeds by checking the Cromwell output [task]/execution/script.submit. . ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int bsub_cpu = 1; Int memory_mb = 1000; String queue; """"""; ; submit = """"""; 			sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; 			${""-n "" + bsub_cpu} \; 			--mem-per-cpu=${memory_mb} \; 			--wrap ""/bin/bash ${script}""; 		""""""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ```. ```; workflow tutorialWorkflow{; 	call task_A { input : in=""testing"" }; 	call task_B { input : in=task_A.out }; 	call task_C { input : in=task_A.out }; }. task task_A{; 	String in. 	command{; 		echo 'This is task A ${in}.'; 	}	; 	output{; 		String out='This is task A ${in}'; 	}; 	runtime{; 		bsub_cpu: 1; 		runtime_minutes: 10; 		memory_mb: 100; 		queue: ""short""; 	}; }. task task_B{; 	String in; 	command{; 		echo 'This is task B ${in}.'; 	}; 	runtime{; 		bsub_cpu: 2; 		runtime_minutes: 15; 		memory_mb: 110; 		queue: ""short""; 	}; }. task task_C{; 	String in; 	command{; 		echo 'This is task C ${in}.'; 	}; 	runtime{; 		runtime_minutes: 25; 		memory_mb: 210; 		queue: ""short""; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2068:1123,alive,alive,1123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2068,4,"['alive', 'echo']","['alive', 'echo']"
Availability,"I have confirmed and worked-around by changing the configuration -o and -e parameters to ; ```; -o ${out}.cromwell; -e ${err}.cromwell; ```; identical duplicated files are now written to the work directory. (Identical except in the case of error, which is when I need these files anyway). My request is to remove the >(tee) lines, but I understand they probably exist to serve some other backend. The ability to turn them off would be appreciated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393064752:240,error,error,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705#issuecomment-393064752,1,['error'],['error']
Availability,"I have created a new lablels file and using that to pass the VPC/subnet info but still get the same error:; ```; $ grep -i label genomics.conf; network-label-key = ""my-private-network""; subnetwork-label-key = ""my-private-subnetwork""; $ cat labels.json; {; ""my-private-network"": ""xxxx"",; ""my-private-subnetwork"": ""xxxx""; }; ```; and updated my cromwell command to the following:; ```; java -Dconfig.file=genomics.conf -jar cromwell-66.jar run cumulus.wdl -i cumulus_inputs.json -l labels.json; ```; I still get the same error though. Is this even possible or am I missing something?. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905041782:100,error,error,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905041782,2,['error'],['error']
Availability,"I have encountered the same problem, either local mode or server mode. version : comwell41; wdl: version 1. - error info: . ```; ""submission"": ""2019-06-28T08:36:56.384Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""/tmp/imports_workflow_63e53e21-b200-46b2-9653-db79983d6c1d_3805297415647673436.zip4786068963842572955/bcftools-task/bcftoolsView.wdl""; }; ],; ""message"": ""Workflow input processing failed""; }; ],; ```. - my wdl header. ```; version 1.0. import ""bcftools-task/bcftoolsView.wdl"" as select; import ""beagle-task/prephasing.wdl"" as prephasing. ```. - unzip -v impute_human_beagle_v1.zip. ```; Archive: impute_human_beagle_v1.zip; Length Method Size Cmpr Date Time CRC-32 Name; -------- ------ ------- ---- ---------- ----- -------- ----; 655 Defl:N 319 51% 06-28-2019 15:52 d52896d1 bcftools-task/bcftoolsView.wdl; 694 Defl:N 384 45% 06-28-2019 14:24 552eeedb beagle-task/prephasing.wdl; -------- ------- --- -------; 1349 703 48% 2 files; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-506658137:110,error,error,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-506658137,2,"['error', 'failure']","['error', 'failures']"
Availability,"I have encountered this issue when running the GATK 4 [Merge BAM Alignment](https://github.com/gatk-workflows/gatk4-data-processing/blob/master/processing-for-variant-discovery-gatk4.wdl#L334) task, on Cromwell 36 running on Google Cloud. For some reason, it seems that the docker image `broadinstitute/gatk` is too large for the disk. What I'm not sure about is which disk this is referring to. Is it the machine running Cromwell, or is it the pipelines API worker machine? If it's the latter, how can I stop this error happening?. ```; cromwell_1 | 2018-10-25 06:53:52,612 cromwell-system-akka.dispatchers.engine-dispatcher-440 ERROR - WorkflowManagerActor Workflow 28605745-a8d2-43c4-ab02-70e5c5c032fe failed (during ExecutingWorkflowStat; e): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:0:1 failed. The job was stopped before the command finished. PAPI error code 5. 8: Failed to pull image broadinstitut; e/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71: ""docker pull broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71"" failed: exit status 1: sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71: Pulling from broadinstitute/gatk; cromwell_1 | ae79f2514705: Pulling fs layer; cromwell_1 | 5ad56d5fc149: Pulling fs layer; cromwell_1 | 170e558760e8: Pulling fs layer; cromwell_1 | 395460e233f5: Pulling fs layer; cromwell_1 | 6f01dc62e444: Pulling fs layer; cromwell_1 | 98db058f41f6: Pulling fs layer; [...]; cromwell_1 | failed to register layer: Error processing tar file(exit status 1): write /root/.cache/pip/http/5/1/d/8/2/51d82969228464b761a16257d5eefe8e2b3dde3c1ad733721353e785: no space left on device; cromwell_1 |; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsy",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337:515,error,error,515,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,I have just had a run with Cromwell 55 configured with the `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` Google API and which resulted in several `504 Gateway Timeout` errors while attempting to read `rc` and `stdout` output files. Is this something that should be looked into?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957:199,error,errors,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760291957,1,['error'],['errors']
Availability,"I have locally configured mariaDB my.cnf as follows：. > [mysqld]; max_connections=5024; thread_cache_size=1000; datadir=/zfsyt1/B2C_RD_P2/USER/fuxiangke/Cromwell/Data/; socket=/zfsyt1/B2C_RD_P2/USER/fuxiangke/Cromwell/socket/cromwell.sock; default-time-zone='+8:00'; port=3310; skip-character-set-client-handshake; init_connect='SET collation_connection = utf8mb4_unicode_ci'; init_connect='SET NAMES utf8mb4'; character-set-server=utf8mb4; collation-server=utf8mb4_unicode_ci; #open slow query logging; slow_query_log = ON; slow_query_log_file = /zfsyt1/B2C_RD_P2/USER/fuxiangke/Cromwell/log-files/slow_cromwell.log; long_query_time = 1; [mysqld_safe]; log-error=/zfsyt1/B2C_RD_P2/USER/fuxiangke/Cromwell/log-files/error.log; pid-file=/zfsyt1/B2C_RD_P2/USER/fuxiangke/Cromwell/pid/mysqld_cromwell.pid; [client]; default-character-set=utf8mb4; > . Then I configured the flow. The following error was reported when scatter 478 tasks were needed in one step：. > 2021-12-06 17:03:51,401 cromwell-system-akka.dispatchers.service-dispatcher-9 ERROR - Failed to summarize metadata. java.sql.SQLException: null; at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129); at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); at com.mysql.cj.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:2045); at com.zaxxer.hikari.pool.ProxyConnection.setAutoCommit(ProxyConnection.java:388); at com.zaxxer.hikari.pool.HikariProxyConnection.setAutoCommit(HikariProxyConnection.java); at slick.jdbc.JdbcBackend$BaseSession.startInTransaction(JdbcBackend.scala:511); at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:37); at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:34); at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276); at java.base/java.util.concurrent.ThreadPo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6583:658,error,error,658,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6583,3,['error'],['error']
Availability,"I have moved to cromwell v53.1 now. However the caching still doesn't work for me. I consistently get an error like this:. > 2020-11-07 17:54:51,634 cromwell-system-akka.dispatchers.engine-dispatcher-35 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 (EnhancedCromwellIoException: [Attempted 1 time(s)] - RejectedExecutionException: ); > 2020-11-07 17:54:51,635 cromwell-system-akka.dispatchers.engine-dispatcher-35 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Invalidating cache entry CallCachingEntryId(347) (Cache entry details: Some(7b292def-1477-4450-988a-e01627d61786:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0)); > 2020-11-07 17:54:51,673 cromwell-system-akka.dispatchers.backend-dispatcher-7385 WARN - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-BackendCacheHitCopyingActor-0123c178:GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:-1:1-5 [UUID(0123c178)GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1]: Unrecognized runtime attribute keys: preemptible; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Fa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:105,error,error,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,2,"['Failure', 'error']","['Failure', 'error']"
Availability,"I have not reproduced the issue, I will try when I get a chance here. At the time I was modifying my backend's 'runtime-attributes'. I made all those attributes optional. I also removed all the 'runtime' stanzas in the wdl file I was testing. I believe there was an error in the 'submit' syntax of my backend config. . Is it possible a config file that fails to parse will cause backends to default to the Local backend? I know parts of the config are not parsed until a job is submitted. I will try to isolate the problem. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3533#issuecomment-382918043:266,error,error,266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3533#issuecomment-382918043,1,['error'],['error']
Availability,"I have the exact same issue. First of all, both the [Configuration examples](https://cromwell.readthedocs.io/en/stable/Configuring/#configuration-examples) and [Local](https://cromwell.readthedocs.io/en/stable/backends/Local/) sections of the documentation point to non-existing file https://github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf while they should point to https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf like lozybean pointed out. This is still not fixed in the documentation. But then I have downloaded the [cromwell.examples.conf](https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf) file and used it as follows:; ```; wget https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/cromwell.examples.conf; sed -i 's/#concurrent-job-limit = 5/concurrent-job-limit = 5/' cromwell.examples.conf; java -Dconfig.file=cromwell.examples.conf -jar cromwell-51.jar run ...; ```. And Cromwell on my laptop still spawned 23 job tasks simultaneously. What do I have to do to limit the number of concurrent jobs? This would be very convenient for me to be able to speed up development of my own WDL. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302:584,down,downloaded,584,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5359#issuecomment-649697302,1,['down'],['downloaded']
Availability,I have to admit I'm among the guilty here - we make the `run` mode available to PacBio customers (via a Python wrapper that provides a friendlier CLI) who prefer to use the command line. Are there drawbacks to this from a black-box user perspective?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6211#issuecomment-794367473:67,avail,available,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6211#issuecomment-794367473,1,['avail'],['available']
Availability,"I have tried the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) to run Cromwell on Google Cloud. I did not get very far. I have followed the long set of instructions. I have logged in with my `<google-user-id>`, I have set my own `<google-project-id>`. I have created my own bucket. I have generate my service account key with the command:; ```; gcloud iam service-accounts keys create sa.json --iam-account ""$EMAIL""; ```. Then I run the hello.wdl with the command:; ```; GOOGLE_APPLICATION_CREDENTIALS=sa.json; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```. But I get the following error:; ```; [2020-07-27 18:34:00,37] [error] PipelinesApiAsyncBackendJobExecutionActor [3d2d7a27wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; Caused by: com.google.cloud.storage.StorageException: xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.create(HttpStorageRpc.java:308); 	at com.google.cloud.storage.StorageImpl$3.call(StorageImpl.java:213); 	at com.google.cloud.storage.StorageImpl$3.call(StorageImpl.java:210); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); 	at com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at com.google.cloud.storage.StorageImpl.internalCreate(StorageImpl.java:209); 	at com.google.cloud.storage.StorageImpl.create(StorageImpl.java:171); 	at cromwell.filesystems.gcs.GcsPath.request$1(GcsPathBuilder.scala:196); 	at cromwell.filesystems.gcs.GcsPath.$anonfun$writeContent$2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594:658,error,error,658,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594,3,"['Error', 'error']","['Error', 'error']"
Availability,"I have, really weird behavior for File? I expect that if file does not exist I should get null for File? (as there are no wld functions that actually check for file existance), however instead I get an error:; ```; Workflow failed; WorkflowFailure(,List(WorkflowFailure(Could not process output, file not found: /pipelines/scripts/cromwell-executions/quantification/87596324-9f68-4419-b3ce-6ff47fe381b4/call-prepare_samples/execution/not_exist,List()))); ```; ```wdl; task prepare_samples {; File samples; File references; File samples_folder. command {; /scripts/run.sc --samples ${samples} --references ${references} --cache ${samples_folder}; }. runtime {; docker: ""quay.io/comp-bio-aging/prepare-samples@sha256:9aaa223ff520634bb0357500ffb90aa80315729e0870ebbc7da4a4b31c382a2c""; }. output {; File? invalid = ""invalid.tsv""; File? novel = ""novel.tsv""; #File? cached = ""cached.tsv""; File? cached = ""not_exist"" #I expect it to be null if the file does not exist; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3367:202,error,error,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3367,1,['error'],['error']
Availability,I haven't been able to reproduce this on 34_hotfix or develop. Standard output and error filenames are `stdout` and `stderr` in both the metadata and IRL.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4166#issuecomment-425171148:83,error,error,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4166#issuecomment-425171148,1,['error'],['error']
Availability,I haven't seen this happen live but in theory 'take' can throw an exception if the underlying queue changes between checking `available` and running `dequeue`. That might happen if an unluckily timed `abort` removes an actor from the queue between those checks.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4165:126,avail,available,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4165,1,['avail'],['available']
Availability,I heard chatter about a 30.2 release ...is there any chance this change can make it in that release? It's mostly for FC users as the current failure logs are sent to the server logs and basically the user never sees call caching fail even though the job succeeds.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3182#issuecomment-360157283:141,failure,failure,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3182#issuecomment-360157283,1,['failure'],['failure']
Availability,I highly recommend using the dsde-toolbox method as it gives you access to the `vault-edit` command which is much less error prone,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2326#issuecomment-305899198:119,error,error,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2326#issuecomment-305899198,1,['error'],['error']
Availability,"I honestly have not really pulled docker images without Cromwell before, other than on my laptop for minimal testing. If I try to pull a docker manually I do get the same error, as you suggested, even if the Google VM and the GCR bucket are both running on the same Google Cloud network. Isn't this a bad design from Google though? How do I make my dockers available for my WDLs and on Terra while at the same time preventing actors running the same WDLs in Google Clouds in other continents from forcing me to incur egress charges? I must be missing something. I see two possible alternative partial solutions for this issue:. (i) is there a way to write a WDL so that it automatically detects whether it should use `us.gcr.io`, or `eu.gcr.io` or `asia.gcr.io` and so that it would automatically select the one that is closer (and free)? I suppose not, as this would be outside the specification of WDL. Curios what you think though. (ii) is there a way to prevent Cromwell running with PAPIv2 from having to pull a docker image multiple time? I wrote WDLs that run on large cohorts (biobank size) and they can scatter task arrays with ~1,000 shards. If this resulted in pulling a docker once, absorbing the cost would likely still be scalable, but as it is now it is very inefficient and it makes the cost of running the WDL almost dominated by the pulling of the dockers if egress costs are involved. [Notice also that someone from the VA run my WDL but I think that, since the computation was performed on an LSF HPC cluster, the docker image was pulled only once and then reused within the LSF HPC cluster, as I did not notice any significant egress costs when this happened]. @cjllanwarne thank you for reaching out to Google. I hope this spurs a broader discussion. I am not in urgent need for a fix, but I very much hope a solution is available in the long term.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702:171,error,error,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702,3,"['avail', 'error']","['available', 'error']"
Availability,I just downloaded the `womtool-84.jar` at <https://github.com/broadinstitute/cromwell/releases> I didn't need to build the source.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1237279136:7,down,downloaded,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902#issuecomment-1237279136,1,['down'],['downloaded']
Availability,"I just encountered this error, as well. I narrowed it down to this commit. https://github.com/broadinstitute/cromwell/commit/448067fd72fc1bde89c0e4291d8790a65ff5968f",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-492444503:24,error,error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4969#issuecomment-492444503,2,"['down', 'error']","['down', 'error']"
Availability,"I just gave this a try in my facility. It still fails. My current work around is to use cromwell version 0.32 (via conda package). Cheers. On Tue, 19 Feb 2019 at 15:53, Michael Franklin <notifications@github.com>; wrote:. > Able to replicate the break from v36 to v37, I switched my check-alive in; > my config to use scontrol and it succeeded:; >; > ""check-alive"": ""scontrol show job ${job_id}"",; >; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-464982252>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AAdrHHALXNpvY-NDgAF3uYIRY7EyP3zqks5vO4M_gaJpZM4aEezy>; > .; >. -- ; Nicholas Yue; Graphics - Arnold, Alembic, RenderMan, OpenGL, HDF5; Custom Dev - C++ porting, OSX, Linux, Windows; http://au.linkedin.com/in/nicholasyue; https://vimeo.com/channels/naiadtools",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-465869573:289,alive,alive,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-465869573,2,['alive'],['alive']
Availability,I just merged https://github.com/broadinstitute/cromwell/pull/7179 so I think this PR has become redundant.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7183#issuecomment-1644504893:97,redundant,redundant,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7183#issuecomment-1644504893,1,['redundant'],['redundant']
Availability,I just ran into the problem of an SGE node crash that we didn't find out until we wondered why certain jobs took so long to run. Having the `is-alive` command and check run at a configurable poll interval would be really usefull for me. Having a poll interval of 0 by default (i.e. turned off) and the value in hours/days could leave it configurable without overloading the queque masters. But as Uncle Ben said: With great power...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-413172161:144,alive,alive,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-413172161,1,['alive'],['alive']
Availability,"I just ran into this as well--cromwell does not work on OSes without `/bin/bash`. `/usr/bin/env bash` is more portable and IMHO the better option. The following fails to execute for me:. ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. Unfortunately this precludes me from using Cromwell.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3201#issuecomment-579545087:257,echo,echo,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3201#issuecomment-579545087,1,['echo'],['echo']
Availability,"I just realized that we essentially blocked ourselves because of the comment above, where we now have so many subworkflows that we can't stand up a job manager on that cromwell because it can't filter them out quickly enough on the JMUI server side to render the website. Our own fault, but we'll be happy to see this functionality go in!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3240#issuecomment-382108058:280,fault,fault,280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3240#issuecomment-382108058,1,['fault'],['fault']
Availability,"I just stood up a server locally. Submitted this workflow: https://raw.githubusercontent.com/bcbio/bcbio_validation_workflows/master/NA12878-chr20/NA12878-platinum-chr20-workflow/main-NA12878-platinum-chr20.cwl. Used this as inputs json; ```; {; ""checkerWorkflow.expectedMd5sum"": ""00579a00e3e7fa0674428ac7049423e2"",; ""checkerWorkflow.inputFile"": ""md5sum.input""; }; ```. Received this output:. ```; 2018-10-12 16:33:43,542 INFO - Pre-Processing /var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/cwl_temp_file_7193cfe7-2342-48cc-8d7c-bf7d3434c57f.cwl; 2018-10-12 16:33:58,870 cromwell-system-akka.dispatchers.engine-dispatcher-43 ERROR - WorkflowManagerActor Workflow 7193cfe7-2342-48cc-8d7c-bf7d3434c57f failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; running cwltool on file /var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/cwl_temp_file_7193cfe7-2342-48cc-8d7c-bf7d3434c57f.cwl failed with Traceback (most recent call last):; File ""/Users/jgentry/projects/cromwell/server/target/scala-2.12/cromwell-36-82fd70f-SNAP.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 12, in cwltool_salad; File ""/Users/jgentry/projects/cromwell/server/target/scala-2.12/cromwell-36-82fd70f-SNAP.jar/Lib/cwltool/load_tool.py"", line 279, in validate_document; File ""/Users/jgentry/projects/cromwell/server/target/scala-2.12/cromwell-36-82fd70f-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 915, in resolve_all; File ""/Users/jgentry/projects/cromwell/server/target/scala-2.12/cromwell-36-82fd70f-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 1087, in validate_links; schema_salad.validate.ValidationException: ../../../../var/folders/bs/wc6g67396rg8qnfj9qhvbvfsg7jhsw/T/cwl_temp_dir_822121598177295457/cwl_temp_file_7193cfe7-2342-48cc-8d7c-bf7d3434c57f.cwl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-429454574:659,ERROR,ERROR,659,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-429454574,1,['ERROR'],['ERROR']
Availability,"I just witnessed that too. I think it might be a problem with the docker container transiently not being able to authenticate calls to the google API, which results in failures to localize/delocalize. I'll bring it up to Google but for now the only workaround is to start the workflow again I'm afraid",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-437583991:168,failure,failures,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-437583991,1,['failure'],['failures']
Availability,"I know that this is bigger than what the issue says - I'm sorry about that. It just felt like now or never to rework a little bit our communication with GCS and remove some duplicate code in the process. This PR removes the `IoInterface`/`IoManager` and uses `java.nio.FileSystem` implementations instead to provide file system interactions.; The (now removed) `IoInterface` was nothing but a wrapper interface around a `FileSystem`.; The (also removed) `IoManager` was a wrapper of `IoInterface`s that selected the most appropriate interface according to the path to be processed. This functionality is now provided by the `cromwell.engine.backend.io.PathString.toPath` method which takes a list of available `FileSystem`s and tries to create a `java.nio.Path`. If it succeeds (ie if there is a suitable `FileSystem` able to parse the raw `String`), the created `Path` can now be used in an abstract way (with `Files.*`) or `better.files` as long as the corresponding `FileSystemProvider` implements the required methods. . The `WdlStandardLibraryImpl` now has a `List[FileSystem]` at its disposal and uses it to parse raw `String` paths with the `toPath` method. Most of the implementation can then be the same and rely entirely on the `FileSystem`s implementations. Engine functions can always be overridden by their respective backend implementations if a special treatment is needed. The functionalities previously in `GoogleCloudStorage` have been merged into `GcsFileSystemProvider`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/524:700,avail,available,700,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/524,1,['avail'],['available']
Availability,"I know there have been some issues with read_object/write_object and structs but I haven't been able to find anything mentioning this particular problem. If I define a struct with mixed types and assign a string and int value inline it works as expected (with cromwell-48):. ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""int"": 4; }. command {; echo -e ""hello ~{p.string} ~{p.int}""; }; }. workflow main {; call print_params; }. // hello abc 4; ```. If instead I just define a boolean value it also works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true; }. command {; echo -e ""hello ~{p.boolean}""; }; }. workflow main {; call print_params; }. // hello true; ```. However if I define a string and a boolean I get a wom conversion error:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""string"": ""abc"",; ""boolean"": true; }. command {; echo -e ""hello ~{p.string} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // Fails with no coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Similarly for boolean and float:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""boolean"": true,; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float} ~{p.boolean}""; }; }. workflow main {; call print_params; }. // No coercion defined from wom value(s) '""true""' of type 'String' to 'Boolean'.; ```. Though a float alone works:; ```; version 1.0. struct Params {; String? string; Int? int; Float? float; Boolean? boolean; }. task print_params {; Params p = {; ""float"": 0.04; }. command {; echo -e ""hello ~{p.float}""; }; }. workflow main {; call print_params; }. // hello 0.04; ```. If I try to define a float=0.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5414:448,echo,echo,448,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5414,3,"['echo', 'error']","['echo', 'error']"
Availability,"I know there is a more serious ticket for this issue, but it seems like a number of centaur local tests have been failing again lately due to `expected ""hello"" but got """"` kind of errors.; I found this https://linux.die.net/man/2/sync and have been restarting centaur local on this branch a few times and none of them have failed so far. I don't think it entirely fixes the issue though.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1974:180,error,errors,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1974,1,['error'],['errors']
Availability,"I know we've talked about it at length, but I don't know if it's written down anywhere - could you (either in the PR description or the ticket) lay out the WDL versions supported by Cromwell as of this PR, and what user-facing changes this PR introduces?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7105#issuecomment-1514780146:73,down,down,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7105#issuecomment-1514780146,1,['down'],['down']
Availability,I let Aaron know about this dockerhub failure and he'll add it to the ones being retried. I think we can close this for now.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4233#issuecomment-429909434:38,failure,failure,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4233#issuecomment-429909434,1,['failure'],['failure']
Availability,"I like the idea of a centralized rate limiter (proxy, supervisor, whatever) to more rationally avoid QPS issues, but these changes are more generally making the vassals robust to any sort of transient problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/268#issuecomment-153442613:169,robust,robust,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/268#issuecomment-153442613,1,['robust'],['robust']
Availability,"I love the failure type but since you've done so much, would it be another load of work to use `sttp` + cats-effect backend vs the source of our Future woes, akka?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4624#issuecomment-462491136:11,failure,failure,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4624#issuecomment-462491136,1,['failure'],['failure']
Availability,"I mentioned this to Jeff earlier, but I had some sort of git calamity that prevents me from squashing down these commits in the usual way. I'll fix this before the actual merge to sprint2 with a brute force patching of a new branch of sprint2 with these changes, but I'd like to hold off on doing that until this is ready for merge to avoid losing your comments.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/9#issuecomment-100315647:102,down,down,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/9#issuecomment-100315647,1,['down'],['down']
Availability,"I missed that the value for `client-payload` is in single quotes and does not interpolate. Consequently, `""version"": ""$CROMWELL_VERSION""` was passed as a literal to the `update-service` [action](https://github.com/broadinstitute/terra-helmfile/actions/runs/3190563369/jobs/5205838370). There, it was evaluated in double quotes `VERSION=""$CROMWELL_VERSION""`, found to be empty in that environment, and halted with `Error: empty version string`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6926:414,Error,Error,414,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6926,1,['Error'],['Error']
Availability,"I narrowed it down to the fact that I don't have an alt contig for the reference file -- i was leaving that blank in the wdl input file. If i just fake it by using the human alt from the Broad's human genome reference in their pipeline, the weird nesting-copying doesnt happen. I'll leave this open because I don't know if this is expected behavior or not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-541156513:14,down,down,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-541156513,1,['down'],['down']
Availability,"I need a red thumb because of some changes in WOM - specifically I needed some way to indicate that a not-really-a subworkflow call should not get its subworkflow name prepended. I'm also curious whether the ""make up a random UUID for the not-really-a-subworkflow name"" is going to cause problems down the road (eg does restart rely on stable subworkflow names?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3388#issuecomment-371860973:297,down,down,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3388#issuecomment-371860973,1,['down'],['down']
Availability,"I need this ability to label Google VMs for resource tracking, but have been thus far unable to have a VM labelled correctly. The jobs submit and run, but the labels do not show up. . From the documentation here (https://cromwell.readthedocs.io/en/develop/wf_options/Google/), it's not clear where the google-specific options are added, so I tried the following: ; ```; {; ""default_runtime_attributes"":{; ""zones"":""us-east1-b"", ; ""google_labels"": {""custom-label"":""custom-value""}; }; }; ```; I submit (Cromwell v42) with:; ```; curl -X POST ""<CROMWELL URL>/api/workflows/v1"" \; -H ""accept: application/json"" \; -H ""Content-Type: multipart/form-data"" \; -F ""workflowSource=@main.wdl"" ; -F ""workflowInputs=@inputs.json"" \; -F ""workflowOptions=@options.json"" \; -F ""workflowType=WDL"" \; -F ""workflowTypeVersion=draft-2""; ```. That submits/runs fine, but when I check the VM that spins up, I only see the two labels of `cromwell-workflow-id` and `wdl-task-name`. If I change the options JSON to anything else, e.g.; ```; {; ""default_runtime_attributes"":{""zones"":""us-east1-b""},; ""google_labels"": {""custom-label"":""custom-value""}; }; ```; then it fails to submit, returning:; ```; {; ""status"": ""fail"",; ""message"": ""Error(s): Invalid workflow options provided: Unsupported key/value pair in WorkflowOptions: google_labels -> {\""custom-label\"":\""custom-value\""}""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4825#issuecomment-500586533:1206,Error,Error,1206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4825#issuecomment-500586533,1,['Error'],['Error']
Availability,"I need to put this down for the moment to finish https://github.com/broadinstitute/cromwell/pull/7439 which is currently affecting users, hope to get back to it tomorrow morning.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2115768826:19,down,down,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2115768826,1,['down'],['down']
Availability,I noticed an error in your tutorial:; https://cromwell.readthedocs.io/en/jg_add_http_doc/tutorials/AwsBatch101/. The aws.config you supply is missing:; numSubmitAttempts = 6; numCreateDefinitionAttempts = 6; from the config portion:. It was generating a confusing error and should be updated.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4278:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4278,2,['error'],['error']
Availability,I noticed that I got a few failures using the default 1 second timeout here. . So this PR ups that to 10 seconds... 🤞,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4385:27,failure,failures,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4385,1,['failure'],['failures']
Availability,"I noticed that all the input files are always being copied, so I tried passing `-Dbackend.shared-filesystem.localization.0=hard-link` or `-Dbackend.shared-filesystem.localization.0=soft-link`. In both cases, `cromwell-0.19.jar` tries and fails to localize input files repeatedly. Localization by copy works. test.wdl:. ``` wdl; workflow test {; File in; call cat { input: in=in }; }. task cat {; File in; command {; cat ${in}; }; }; ```. test.inputs:. ``` json; {; ""test.in"": ""test.wdl""; }; ```. The command. ``` bash; java -Dbackend.shared-filesystem.localization.0=hard-link \; -jar ~/java/cromwell-0.19.jar \; run test.wdl test.inputs; ```. results in. ``` txt; 2016-06-24 21:57:15,613 ERROR - BackendCallExecutionActor [UUID(8a4e2219):cat]: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; cromwell.util.AggregatedException: Failures during localizationCould not localize test.wdl -> /Users/davids/work/wdl/cromwell-executions/test/8a4e2219-90e2-4210-a753-26b149c18b51/call-cat/Users/davids/work/wdl/test.wdl; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell-0.19.jar:0.19]; at cromwell.util.TryUtil$.sequence(TryUtil.scala:125) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustSharedInputPaths(SharedFileSystem.scala:228) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustSharedInputPaths(LocalBackend.scala:94) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.adjustInputPaths(LocalBackend.scala:96) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.LocalBackend.instantiateCommand(LocalBackend.scala:246) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCommand$lzycompute(JobDescriptor.scala:52) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.BackendCallJobDescriptor.instantiateCom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1070:689,ERROR,ERROR,689,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070,3,"['ERROR', 'Failure']","['ERROR', 'Failures']"
Availability,"I noticed that cromwell generates exec.sh setting HOME presumably incorrectly.; This is from GATK4's somatic CNV workflow executed by cromwell 31 with JES backend. Thanks!. ```; #!/bin/bash; tmpDir=$(; set -e; cd /cromwell_root; tmpDir=""$(mkdir -p ""/cromwell_root/infant-all-cnv-out/cromwell-execution/CNVSomaticPairWorkflow/aa10c071-5c42-4f6a-a4e0-3ba96cc54283/call-ModelSegmentsTumor/tmp.3efcbfd1"" && echo ""/cromwell_root/infant-all-cnv-out/cromwell-execution/CNVSomaticPairWorkflow/aa10c071-5c42-4f6a-a4e0-3ba96cc54283/call-ModelSegmentsTumor/tmp.3efcbfd1"")""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME$HOME""; (; cd /cromwell_root. ); ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3421:403,echo,echo,403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3421,2,['echo'],['echo']
Availability,"I now notice that both checking for the RC and the 'check-alive' checks are controlled by this setting. This seems a bit strange as the rc checking is comparatively very cheap compared to 'check-alive'. Initially the point of not running check-alive all the time was that the cost was high compared to rc file checking. But now the solution has been to slow down rc checking to the speed of the costly check-alive! I am a bit muddled on this, so am not sure if I am getting it. . Without exit-code-timeout-seconds at what interval is the rc file checked for?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-488542941:58,alive,alive,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-488542941,5,"['alive', 'down']","['alive', 'down']"
Availability,"I ran 123 workflows as part of a single submission in FireCloud which means, same WDL and identical inputs aside from the BAM and sample name. They all started around the same time. . 3 failed with this failure:. `Google credentials are invalid: connect timed out`. Here is the deep-link to the FireCloud run:. https://api.firecloud.org/api/workspaces/engle-macarthur-ccdd/genomes-reprocessing/submissions/c7af7e06-a435-44ec-8466-124ad8e1bcaf/workflows/a714b11b-0162-4585-afa5-abbd7433af51. Here is the full metadata for the failed workflow:. {; ""workflowName"": ""BamToUnmappedBams"",; ""submittedFiles"": {; ""inputs"": ""{\""BamToUnmappedBams.input_bam\"":\""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/batch04/S64-2_Illumina.bam\""}"",; ""workflow"": ""task RevertSam {\n File input_bam\n String revert_bam_name\n Int disk_size\n\n # TODO: why is SORT_ORDER=coordinate set below since we sort it again in the next step?\n # TODO: why did we need this line?\n # OUTPUT_MAP=${output_map} \\\n command {\n java -Xmx3000m -jar /usr/gitc/picard.jar \\\n RevertSam \\\n INPUT=${input_bam} \\\n OUTPUT=${revert_bam_name} \\\n VALIDATION_STRINGENCY=LENIENT \\\n ATTRIBUTE_TO_CLEAR=FT \\\n ATTRIBUTE_TO_CLEAR=XS \\\n SORT_ORDER=queryname \\\n MAX_RECORDS_IN_RAM=1000000 \n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File unmapped_bam = \""${revert_bam_name}\""\n }\n}\n\ntask SortSam {\n File input_bam\n String sorted_bam_name\n Int disk_size\n\n # TODO: why not use samtools sort as it is multi-threaded?\n command {\n java -Xmx3000m -jar /usr/gitc/picard.jar \\\n SortSam \\\n INPUT=${input_bam} \\\n OUTPUT=${sorted_bam_name} \\\n SORT_ORDER=queryname \\\n MAX_RECORDS_IN_RAM=1000000\n }\n runtime {\n docker: \""broadinstitute/genomes-in-the-cloud:2.2.3-1469027018\""\n disks: \""local-disk \"" + disk_size + \"" HDD\""\n memory: \""3500 MB\""\n }\n output {\n File sorted_bam = \""${sorted_bam_name}\""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886:203,failure,failure,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886,1,['failure'],['failure']
Availability,"I ran a WDL file locally, by first creating a gatk jar and building a Docker image on my computer and then executing the WDL using Cromwell. I ran the following commands:; ./gradlew shadowJar; docker build -t us.gcr.io/broad-dsde-methods/broad-gatk-snapshots:testimage1 --build-arg DRELEASE=false .; java -jar cromwell-30.2.jar run cnv_somatic_pair_workflow.wdl --inputs cnv_somatic_pair_wgs_no-gc_workflow.json. Although all the necessary tasks seem to have finished, I still got the following error message:; [2018-05-14 11:34:10,40] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-6-0-mergePreferred#1200284127]] terminated abruptly; [2018-05-14 11:34:10,40] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-2-0-mergePreferred#-1631704537]] terminated abruptly; [2018-05-14 11:34:10,43] [info] Automatic shutdown of the async connection; [2018-05-14 11:34:10,43] [info] Gracefully shutdown sentry threads.; [2018-05-14 11:34:10,43] [info] Shutdown finished.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618:495,error,error,495,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618,5,['error'],['error']
Availability,"I ran a couple workflows, against 0.19 and against develop (considering the workflow id wrapping issue is resolved). Here are some differences I found, there might be other that those workflow didn't catch.; - The ""Collector"" of a scatter is present in the metadata in develop, and wasn't in 0.19. Oddly it doesn't contain its output though : . ```; {; ""attempt"": 1,; ""executionStatus"": ""Done"",; ""shardIndex"": -1; }; ```; - `stdout` and `stderr` are missing (**Local only**); - `runtimeAttributes` is missing ; Completely missing on local.; On JES, only attributes in the WDL show up, those for which the default value was used are missing.; - `executionEvents` is missing (even if there is none, there is an attribute with an empty list in 0.19); - `cache` is missing (**Local only**); e.g. ```; ""cache"": {; ""allowResultReuse"": true; }; ```; - `inputs` at the call level is missing if there are no inputs (`""inputs"" : {}` was present in 0.19); - `inputs` at the workflow level is missing if there are no inputs (`""inputs"" : {}` was present in 0.19); - `outputs` at the workflow level is missing if there are no inputs (`""outputs"" : {}` was present in 0.19); - Scatter keys are shown in develop's metadata as a normal call:. ```; ""w.$scatter_0"": [; {; ""attempt"": 1,; ""executionStatus"": ""Done"",; ""shardIndex"": -1; }; ]; ```; - `submission` is missing; - In 0.19, all ""first level"" (non-shards) calls would appear in the metadata right away, with a `NotStarted` status and some basic available information:. ```; ""example.gatherUltimateAnalysis"": [; {; ""executionStatus"": ""NotStarted"",; ""shardIndex"": -1,; ""outputs"": {},; ""runtimeAttributes"": {},; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""array"": ""ultimateAnalysis.out""; },; ""backend"": ""JES"",; ""attempt"": 1,; ""executionEvents"": []; }; ]; ```. In develop, a call appears in the metadata only at runtime; - `backendStatus` has been renamed to `jesOperationStatus` (JES status)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/936#issuecomment-223718341:1482,avail,available,1482,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/936#issuecomment-223718341,1,['avail'],['available']
Availability,"I ran a super small WDL, then checked its status to make sure it was running. Then, I tried to abort it -- the request returned a 500: ""The server was not able to produce a timely response to your request."". However, when I check the status of the workflow, it says it has been successfully aborted. I included the WDL i ran against https://cromwell.gotc-int.broadinstitute.org/swagger/index.html?url=/swagger/cromwell.yaml in case you'd like to try it yourself. task echoHelloWorld {; command {; echo 'Hello, World!'; }; runtime {; docker: ""phusion/baseimage""; disks: ""local-disk 10 HDD""; memory: ""1 GB""; preemptible: 3; }; }. workflow printHelloAndGoodbye {; call echoHelloWorld; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253:468,echo,echoHelloWorld,468,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253,3,['echo'],"['echo', 'echoHelloWorld']"
Availability,"I ran into a related issue while running the ENCODE HiC pipeline via Caper on SLURM. I opened an issue there too. On our HPC I need to `module load cuda/11.7` to use the `nvcc` binary. I tried `--wrap='module load cuda/11.7'` but while this gets passed to the `sbatch` command it returns a script argument not permitted error, possibly because `module` isn't a binary but a bash function? Are there any other options for using Caper/Cromwell with the `module` system?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-1430297902:320,error,error,320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-1430297902,1,['error'],['error']
Availability,"I ran into the following problem when upgrading from cromwell 31.1 to 36. ; It looks like in cromwell 31.1, it is possible to pass a single File to a task that expects an Array[File]. In cromwell 36 however, this gives the following error: `Failed to evaluate input 'files' (reason 1 of 1): No coercion defined from wom value(s) '""cromwell-36.jar""' of type 'File' to 'Array[File]'.`; However, both womtool 31.1 and womtool 36 give no errors when validating the workflow. I'm not sure which of the two is the correct behaviour according to the WDL spec, but I think womtool and cromwell should agree on whether or not the wdl file is valid or not. Cromwell Womtool 31. $ java -jar womtool-31.jar validate wf.wdl ; ; $ java -jar cromwell-31.1.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:07:53,81] [info] Running with database db.url = jdbc:hsqldb:mem:6b74f862-dc18-4cf1-8e2e-0b9002bba0bf;shutdown=false;hsqldb.tx=mvcc; .; .; [2019-01-15 15:08:11,54] [info] WorkflowExecutionActor-977d0c47-9cf5-4893-8dcf-465c27da13d7 [977d0c47]: Workflow wf complete. Final Outputs:; {; ""wf.F"": [[""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-0/inputs/home/redmar/devel/wdl/test/issue/cromwell-31.1.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-1/inputs/home/redmar/devel/wdl/test/issue/cromwell-36.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-2/inputs/home/redmar/devel/wdl/test/issue/womtool-31.jar""], [""/home/redmar/devel/wdl/test/issue/cromwell-executions/wf/977d0c47-9cf5-4893-8dcf-465c27da13d7/call-ls/shard-3/inputs/home/redmar/devel/wdl/test/issue/womtool-36.jar""]]; }. Cromwell Womtool 36. $ java -jar womtool-36.jar validate wf.wdl ; ; $ java -jar cromwell-36.jar run --inputs wf.json wf.wdl ; [2019-01-15 15:09:17,10] [info] Running with database db.url = jdbc:hsqldb:mem:e77f2c21-f28a-4571-ba89-d9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4550:233,error,error,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4550,2,['error'],"['error', 'errors']"
Availability,"I ran into this again today, unless the server stdout is captured the errors don't make their way to workflow logs so it's particularly annoying to debug failed workflows. As a user, I would expect that either all logs (both pertaining to the workflow, or the server's execution of the workflow) are placed in the workflow log, or there is a separately configurable server log file for that stream of information.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-480382172:70,error,errors,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-480382172,1,['error'],['errors']
Availability,"I ran it using json ways and after configuring mysql and Call Caching it is still create a new project. here is my commands and config file. ```; java -jar -Dconfig.file=/work/share/ac7m4df1o5/bin/cromwell/3_config/udocker_slum.conf ../cromwell-84.jar run /work/share/ac7m4df1o5/bin/cromwell/1_pipeline/Exome_Germline_Single_Sample/ExomeGermlineSingleSample_v3.1.5.wdl -i D5327.NA12878.json -o ../options.json; ```. conf is. ```; include required(classpath(""application"")). docker {; hash-lookup {; enable = false; }; }; call-caching {; enabled = true; invalidate-bad-cache-results = true; }. backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; 	concurrent-job-limit = 5; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -t ${runtime_minutes} \; 	 -p wzhcexclu06 \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; # Pull the image using the head node, in case our workers don't have network access; # udocker pull ${docker}. sbatch \; -J ${job_name} \; -D ${cwd} \; -t ${runtime_minutes} \; 	 -p wzhcexclu06 \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }. ```. help pleas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6920:1521,alive,alive,1521,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6920,1,['alive'],['alive']
Availability,"I ran the WDL at the bottom of this issue using mock-jes. Initially it ran everything but then got stuck in a state with 2 jobs left and the 1 wf still ""running"". I looked and those 2 jobs had had 500 errors associated with them. . At a later point I restarted Cromwell, for a long (> 10 mins) period of time the stats endpoint was unresponsive although other endpoints were fine. After some period of time (< 1.5 hours) I came back and those 2 jobs had completed but the WF was still stuck as Running. task snooze {; command {; sleep 10 && ps > myfile.txt; }; output {; File procs = ""myfile.txt""; }; runtime {; docker: ""ubuntu:14.04""; preemptible: 3; cpu: 10; }; }. workflow one_step {; Array[Int] integers = range(20000); scatter(i in integers) {; call snooze; }; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1662:201,error,errors,201,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1662,1,['error'],['errors']
Availability,I realized that downstream libs won't automagically get the updated cats library so no need to relax this,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2577#issuecomment-324989698:16,down,downstream,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2577#issuecomment-324989698,1,['down'],['downstream']
Availability,I rebased and cleaned up the branch I have: https://github.com/broadinstitute/cromwell/tree/cromwell-2094. What's left is:. - Handle failure cases in `WorkflowDockerLookupActor` (see FIXMEs); - Figure out the right way to handle the tag/hash pair: Currently the runtime attribute value is overridden with the hash + we pass a `CallCacheEligible` object in the descriptor. This is probably too much. We could leave the runtime attribute as is and pass the hash only if needed and successfully retrieved ?; - Have backend report if it used the hash or the tag when a call runs. Note that this could affect call caching I think ? (We need to wait from the backend to know which was used before being able to compute the real call hash ? What if they used the tag ?); - Test it (unit ? centaur ?),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726:133,failure,failure,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2094#issuecomment-299049726,1,['failure'],['failure']
Availability,"I recently attempted to run a workflow with files in a separate non public google project as inputs. I submitted it using both Swagger and curl but neither gave very informative error messages. Here's the curl command and output:. ```; curl -v https://cromwell.gotc-dev.broadinstitute.org/api/workflows/v1 -F wdlSource=@PairedSingleSampleWf.wdl -F workflowInputs=@ExampleOfIncorrectProject.json; * Trying 104.197.140.34...; * Connected to cromwell.gotc-dev.broadinstitute.org (104.197.140.34) port 443 (#0); * TLS 1.2 connection using TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384; * Server certificate: cromwell.gotc-dev.broadinstitute.org; * Server certificate: InCommon RSA Server CA; * Server certificate: USERTrust RSA Certification Authority; * Server certificate: AddTrust External CA Root; > POST /api/workflows/v1 HTTP/1.1; > Host: cromwell.gotc-dev.broadinstitute.org; > User-Agent: curl/7.43.0; > Accept: */*; > Content-Length: 38237; > Expect: 100-continue; > Content-Type: multipart/form-data; boundary=------------------------c2956aa00c34148a; > ; < HTTP/1.1 100 Continue; < HTTP/1.1 500 Internal Server Error; < Date: Fri, 18 Mar 2016 17:59:04 GMT; < Server: spray-can/1.3.2; < X-Frame-Options: SAMEORIGIN; < Access-Control-Allow-Origin: *; < Access-Control-Allow-Headers: authorization,content-type,accept,origin; < Access-Control-Allow-Methods: GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD; < Access-Control-Max-Age: 1728000; < Content-Type: text/plain; charset=UTF-8; < Content-Length: 69; < Connection: close; < ; * Closing connection 0; The server was not able to produce a timely response to your request.%; ```. The issue was that I was using the broad-gotc-dev project to run the workflow, but two of my input files were in broad-gp-gotc-pilot, but I had trouble figuring that out from this error message. Happy to provide the wdl and json files as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/589:178,error,error,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/589,3,"['Error', 'error']","['Error', 'error']"
Availability,"I remember we added this for certain JES failures so the backend plumbing might already be there (although IIRC we called it ""unexpected failure"" and it was hard coded to 3)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358781998:41,failure,failures,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-358781998,2,['failure'],"['failure', 'failures']"
Availability,"I reproduced the problem locally. The problem is our default health monitor `StandardHealthMonitorServiceActor` which periodically tries to ping Docker Hub. I'm not sure what changed between 36 and 37 that this now manifests when previously it did not. There is a `cromwell.services.healthmonitor.impl.noop.NoopHealthMonitorServiceActor` that one should be able to swap in to turn off health monitoring, but unfortunately there's a bug in the 37 version of this class that causes it to crash during initialization. As you've noticed the workflow still runs successfully albeit with a lot of noise, we'll try to come up with a more satisfactory resolution.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462415697:140,ping,ping,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462415697,1,['ping'],['ping']
Availability,"I restarted Cromwell with one workflow running and one of my tasks failed with this exception: . 2016-04-07 20:32:28,188 cromwell-system-akka.actor.default-dispatcher-20 ERROR - WorkflowActor [UUID(03db4daf)]: Completion work failed for call CollectQualityYieldMetrics:9.; com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry '27-PairedEndSingleSampleWorkflow.CollectQualityYieldMetrics-metr' for key 'UK_SYM_WORKFLOW_EXECUTION_ID_SCOPE_NAME_ITERATION_IO'. the logs can be found on gotc-staging - 20160407-cromwell.log",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/684:170,ERROR,ERROR,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/684,1,['ERROR'],['ERROR']
Availability,"I reviewed my script and I realized that the scatter is done in a more complicated way. Here is a toy-example of what I am doing:. ```wdl; workflow TestWorkflow {; Array[Int] my_array; ; call GenerateMap {; input:; i = length(my_array); }; scatter (entry in GenerateMap.map_output) {; call CopyFile {; input:; file = entry.right; }; }; ; output {; Array[Pair[Int, Array[File]]] final_out = zip(my_array, CopyFile.out); }; }. task GenerateMap {; Int i; command <<<; for n in `seq 1 ${i}`; do \; touch $n.txt; \; echo -e ""$n\t$n.txt"" >> my_map.txt; \; done; >>>. output {; Map[Int, File] map_output = read_map(""my_map.txt""); }; }. task CopyFile {; File file. String copy_file = basename(file) + "".copy""; command <<<; cp ${file} ${copy_file}; >>>. output {; Array[File] out = [""${file}"", ""${copy_file}""]; }; }; ```. And my outputs after running with the input array with `[1,2,3,4,5,6,7,8,9]` are the following:. ```json; {; ""outputs"": {; ""TestWorkflow.final_out"": [{; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/10.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-0/execution/10.txt.copy""],; ""left"": 1; }, {; ""left"": 2,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/4.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-1/execution/4.txt.copy""]; }, {; ""left"": 3,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-GenerateMap/execution/6.txt"", ""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-486e-8c7d-2bfdc5c5f97d/call-CopyFile/shard-2/execution/6.txt.copy""]; }, {; ""left"": 4,; ""right"": [""/Users/daniel/Desktop/test-cromwell-map/execution/TestWorkflow/67295907-5ffe-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779:511,echo,echo,511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368445779,1,['echo'],['echo']
Availability,"I run a short bash pipeline like this : for CHROM in `cut -f1 $IN_FILE |grep -Pv '^@'| sort|uniq`; do. But I get this error : sort: cannot create temporary file in ‘/cromwell_root/tmp’: No such file or directory. when running in firecloud. should I ""mkdir -pv /cromwell_root/tmp"" first?. I see ""export TMPDIR=/cromwell_root/tmp"" in the exec.sh on the 3rd line.... I went ahead and issued before the pipeline starts the command ""mkdir -pv $TMPDIR"" and made the error go away!. Is it normal/expected to need to create TMPDIR ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/731:118,error,error,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/731,2,['error'],['error']
Availability,"I run cromwell on a SLURM cluster with a postgres database for call caching. Call caching mostly works fine, but there is no way to constrain the size of the cache. Eventually the available space (200GB) for the postgres database fills up, and all cromwell instances using the call cache fail. My workaround is to periodically stop all workflows and truncate the call caching database. . I imagine implementing an option to restrict the number of cache entries should be straightforward for a Guava-based cache; if someone could help roadmap, I may be able to submit a pull request in the future. . For my purposes, I assume that all cromwell instances agree on the size.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7560:180,avail,available,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7560,1,['avail'],['available']
Availability,"I saw a case where lack of Google Cloud resources caused a permanent workflow failure, for what seems like a transient condition that could be overcome by retries:. 2019-05-24 12:32:07,173 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.Ca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001:78,failure,failure,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001,4,"['ERROR', 'avail', 'error', 'failure']","['ERROR', 'available', 'error', 'failure']"
Availability,I see a unit test failure that looks like it could be the result of one of these upgrades:; ```; should not mix up credentials *** FAILED *** (44 milliseconds); [info] java.lang.NoSuchFieldException: credentials; [info] at java.base/java.lang.Class.getDeclaredField(Class.java:2411); [info] at cromwell.filesystems.gcs.GcsPathBuilderSpec.credentialsForPath$1(GcsPathBuilderSpec.scala:326); [info] at cromwell.filesystems.gcs.GcsPathBuilderSpec.$anonfun$new$7(GcsPathBuilderSpec.scala:334); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); [info] at org.scalatest.Transformer.apply(Transformer.scala:22); [info] at org.scalatest.Transformer.apply(Transformer.scala:20); [info] at org.scalatest.flatspec.AnyFlatSpecLike$$anon$5.apply(AnyFlatSpecLike.scala:1832); [info] at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1583190760:18,failure,failure,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7155#issuecomment-1583190760,1,['failure'],['failure']
Availability,"I see, so is the VM tasked through PAPI to pull the docker and then run `gcs_localization.sh` and `script`? Is it the internal PAPI code that failed to properly handle the 403 error when pulling the docker?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545961059:176,error,error,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7132#issuecomment-1545961059,1,['error'],['error']
Availability,"I see. My usual ""workaround"" for such fail (but continue) is like this:. ```; task foo {; 	command {; 		(echo foo; false) || (echo 1>&2 MSG; true); 	}; }. workflow test {; 	call foo; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276419301:105,echo,echo,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276419301,2,['echo'],['echo']
Availability,"I set up a heavily scattered (~1000x) workflow to run overnight on my local machine (Mac OS Catalina, Intel hardware). The machine is set up to never sleep and was connected to AC power. My Cromwell config is set to only run one task at a time, ie, only one shard of a scattered task runs at a time. The workflow stopped processing on shard 885, which was the 233rd shard to start (shards appear to start in a random order, that's not an issue). It looks like the Docker container in question is getting created, but not used. The container is not running according to Docker Desktop and the Docker CLI tools (see output below). ### Workflow; I've seen this happen with a few workflows, but this time around it's this one (failure is occurring on second task): https://github.com/aofarrel/SRANWRP/blob/bioproject_stuff/workflows/is_this_tuberculosis.wdl. ### Ruled out; * Running tasks concurrently/Cromwell config not being respected: The workflow would have either hung Docker or tasks would have returned 137; * Docker application (not the container, the entire application) hanging, like what happens when trying to run tasks concurrently on a local machine: `docker run -it` works in a new terminal window; * IP getting blocked: This would cause error output, and I can still ping SRA from the same IP without issue; * Loss of internet: This would cause error output (`curl command failed`); * Control-S: Control-Q doesn't unfreeze it; * No more disk space: There's about 50 GB free and each instance of the scattered task uses less than a GB. ### Docker container logs; `docker logs cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528` gives no output. ### Entering the container; `docker exec -it cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528 /bin/sh` returns; `Error response from daemon: Container cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528 is not running`. ### Docker inspect; ```; >docker inspect cf6f4828adc61eacf06337ce3caf2c110df6cc0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6946:723,failure,failure,723,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6946,1,['failure'],['failure']
Availability,I should have made this explicit in the previous comment: . I don't currently see a way we can support underscores in bucket names as long as we're using Google's GCS NIO filesystem. But I do think Cromwell can and should fail with useful and timely error messages when presented with bucket names that will not work.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-308538174:250,error,error,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-308538174,1,['error'],['error']
Availability,"I solved it by myself. This issue is caused by region. I use default setting for region in aws.conf.; When I change to region accordingly (In my case, it is set to ap-northeast-2), it works well. I think the error message about this needs improvement a little more precisely. ```; include required(classpath(""application"")). aws {; application-name = ""cromwell""; auths = [{; name = ""default""; scheme = ""default""; }]; region = ""ap-northeast-2""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4294#issuecomment-432934965:208,error,error,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4294#issuecomment-432934965,1,['error'],['error']
Availability,"I specified a zone of us-east1-a, which is not a legitimate zone. The problem is not handled well and the error message is confusing and unhelpful: . See operations/ELXWv-WdKxis4eaT-cfmx7EBINHtgZmgHSoPcHJvZHVjdGlvblF1ZXVl which begins with:. ```; done: true; error:; code: 5; message: no zones available; .; .; .; ```. Cromwell loops forever in this pattern (current develop head, 678712acb303a6cce0d35d6bcb963f19407439f5):. ```; [INFO] [01/26/2017 16:30:48.889] [cromwell-system-akka.dispatchers.backend-dispatcher-31] [akka://cromwell-system/user/cromwell-service/$b/$a] The JES polling actor Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$b#-1628247913] unexpectedly terminated while conducting 1 polls. Making a new one...; [INFO] [01/26/2017 16:30:48.889] [cromwell-system-akka.dispatchers.backend-dispatcher-31] [akka://cromwell-system/user/cromwell-service/$b/$a] watching Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$c#1867219466]; [ERROR] [01/26/2017 16:30:58.241] [cromwell-system-akka.dispatchers.backend-dispatcher-31] [akka://cromwell-system/user/cromwell-service/$b/$a/$c] null; java.lang.NullPointerException; 	at cromwell.backend.impl.jes.Run$.ceInfo$lzycompute$1(Run.scala:122); 	at cromwell.backend.impl.jes.Run$.ceInfo$1(Run.scala:122); 	at cromwell.backend.impl.jes.Run$.machineType$lzycompute$1(Run.scala:123); 	at cromwell.backend.impl.jes.Run$.machineType$1(Run.scala:123); 	at cromwell.backend.impl.jes.Run$.interpretOperationStatus(Run.scala:130); 	at cromwell.backend.impl.jes.statuspolling.JesPollingActor.interpretOperationStatus(JesPollingActor.scala:86); 	at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anon$1.onSuccess(JesPollingActor.scala:72); 	at cromwell.backend.impl.jes.statuspolling.JesPollingActor$$anon$1.onSuccess(JesPollingActor.scala:69); 	at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:197); 	at com.google.api.client.googleapis.batch.BatchUnparsedResponse.pa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915:106,error,error,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915,4,"['ERROR', 'avail', 'error']","['ERROR', 'available', 'error']"
Availability,"I spoke too soon, seeing a lot of `23:47:24.836 [centaur-acting-like-a-system-akka.actor.default-dispatcher-9] ERROR centaur.api.CentaurCromwellClient$ - Submitting invalid_inputs_json_object returned 400 Bad Request`. . Maybe only log as an error (and retry...) if the return code is not 4xx?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4956#issuecomment-491359116:111,ERROR,ERROR,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4956#issuecomment-491359116,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I started a branch [here](https://github.com/broadinstitute/cromwell/tree/aen_arm64_build) but run into this error, which seems to be widespread on the 'net.; ```; docker exporter does not currently support exporting manifest lists; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7107#issuecomment-1496701703:109,error,error,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7107#issuecomment-1496701703,1,['error'],['error']
Availability,"I still see this, somewhat. It might be due to the long time on cromwell; final overhead. In other words, my job finishes, but the overhead takes so; long that an unrelated failure prevents the write to the call-cache; database. On Tue, Apr 4, 2017 at 12:48 PM, Kate Voss <notifications@github.com> wrote:. > If there are no more problems, we'll close this.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1494#issuecomment-291561557>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk0dV9BaGvlpzXleUmgZ3s6-BsN6Lks5rsnRngaJpZM4KJB9H>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1494#issuecomment-291572106:173,failure,failure,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1494#issuecomment-291572106,1,['failure'],['failure']
Availability,"I submitted a workflow with `defaultRuntimeOptions` instead of `default_runtime_attributes` and when I then called `/metadata`, I got this:. ```; {; ""calls"": {; },; ""id"": ""93215d4d-1341-4aba-9ded-ce83beaeedce"",; ""submission"": ""2016-06-23T20:11:26.122Z"",; ""status"": ""Failed"",; ""failures"": [""Workflow contains invalid options JSON: Unsupported key/value pair in WorkflowOptions: defaultRuntimeOptions -> {\""zones\"":\""us-central1-b\""}""],; ""end"": ""2016-06-23T20:11:26.509Z"",; ""start"": ""2016-06-23T20:11:26.137Z""; }; ```. Note that the `""inputs""` field is missing, but I didn't ask to exclude it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1066:277,failure,failures,277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1066,1,['failure'],['failures']
Availability,"I submitted the regular single_sample.wdl with the VIR_1923 .JSON that's; there...I guess I don't have permissions or something to access the files; that are listed. On Thu, Apr 14, 2016 at 9:01 AM, meganshand notifications@github.com; wrote:. > Not sure if this is a separate issue or not, but when @knoblett; > https://github.com/knoblett and I were submitting a workflow yesterday; > we got the exact same error message (submitted with Swagger). The issue for; > her was that there was an input that was specified to be a File type, but; > in reality it was just a String (so I'm guessing the issue was similar in; > that it couldn't find the ""file""). Unfortunately, it validated just fine,; > but we weren't able to submit it.; > ; > I'd be happy to provide the WDL and JSON files (both the broken version; > and the fixed version) but they won't attach in a github comment.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209931581",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209935921:409,error,error,409,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209935921,1,['error'],['error']
Availability,I suspect that making outputs and inputs a toggle-able option will kick this particular can significantly down the road.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/972#issuecomment-224427067:106,down,down,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/972#issuecomment-224427067,1,['down'],['down']
Availability,"I take it all back, I think we need this functionality. The metadata based REST queries will go through the MetadataBuilderActor which in turn pings the MetadataService. Having the MetadataService implies having the summarizer. Pinging @Horneth just to double check me on this as I believe he's the one who put that stuff together.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245821145:143,ping,pings,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1378#issuecomment-245821145,2,"['Ping', 'ping']","['Pinging', 'pings']"
Availability,I tested @TimurIs workaround yesterday. It has the desired effect of throttling API requests. There were no API request errors reported in the logs. I would recommend this as the solution for this issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-442969906:120,error,errors,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-442969906,1,['error'],['errors']
Availability,"I tested it on a local backend. If you use a non-empty array then it works fine with cromwell-36. But with an empty array it fails. workflow:; ```; workflow test_opt_array {; #Array[File] arr1 = [""0.txt"",""1.txt"",""2.txt"",""3.txt"",""4.txt""]; Array[File] arr1 = []. Boolean go = true; if ( go ) {; scatter( i in range(5) ) {; call t1 {input: i=i}; }; }; Array[File] arr2 = select_first([t1.out, arr1]); }. task t1 {; Int i; command {; echo ${i} > out.txt; }; output {; File out = 'out.txt'; }; }; ```. This workflow worked fine with cromwell-34.; ```; $ java -jar ~/cromwell-34.jar run test_opt_array.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2018-10-25 21:20:58,35] [info] Running with database db.url = jdbc:hsqldb:mem:a975ddc6-f298-4393-b1f0-e93250d3cca8;shutdown=false;hsqldb.tx=mvcc; [2018-10-25 21:21:07,82] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-25 21:21:07,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-25 21:21:07,95] [info] Running with database db.url = jdbc:hsqldb:mem:d98689d1-c87b-486c-aa55-626823fb3bb1;shutdown=false;hsqldb.tx=mvcc; [2018-10-25 21:21:08,32] [info] Slf4jLogger started; [2018-10-25 21:21:08,56] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-fcf9c1d"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-25 21:21:08,59] [info] Metadata summary refreshing every 2 seconds.; [2018-10-25 21:21:08,63] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-25 21:21:08,64] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-25 21:21:08,64] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-25 21:21:09,79] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-25 ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:430,echo,echo,430,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['echo'],['echo']
Availability,"I think I am experiencing a bug in cromwell version 37. The problem occurs when I submit a job to SLURM. The job gets submitted but cromwell crashes without waiting for the job to finish. Cromwell works fine when run locally or when I use version 36. . ## my command ; java is v1.8; ```; java -Dconfig.file=cori.conf -jar cromwell-37.jar run test.wdl ; ```. ## wdl ; ```; workflow jgi_dap_leo {. call doSomething { }. }. task doSomething {; runtime {; mem: ""8G""; cpu: 1; time: ""0:60:0""; backend: ""SLURM""; }; command {; free; }; }; ```. ## config ; ```; include required(classpath(""application"")). system {; job-rate-control {; jobs = 1; per = 1 second; }; }. backend {; default=""Local""; providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String time; Int cpu; String mem; """""". submit = """"""; sbatch -J leo_dap -t ${time} -c ${cpu} --mem=${mem} -C haswell -q regular -A m342 --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. ## system I'm running on . NERSC's cori machines:; Cray XC40, comprised of Intel Xeon ""Haswell"" processor nodes. ## cromwell logs; [cromwellError.txt](https://github.com/broadinstitute/cromwell/files/2866540/cromwellError.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4651:1036,alive,alive,1036,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4651,1,['alive'],['alive']
Availability,"I think I broke the tests, I needed to rebase on job_avoidance branch. I'm; working on fixing them now. On Tue, Dec 15, 2015 at 9:36 AM, Chris Llanwarne notifications@github.com; wrote:. > Oh, looks good then. I've restarted the Travis build but assuming the; > failures were just temporary, [image: :+1:]; > ; > —; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164782659; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164786288:262,failure,failures,262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164786288,1,['failure'],['failures']
Availability,"I think I found the one you are talking about. ![screen shot 2018-10-15 at 11 28 47 am](https://user-images.githubusercontent.com/2978948/46960909-92efc580-d06d-11e8-97fe-d81ef63da81a.png). The failure reason is . ```; Task requester_pays_engine_functions.functions:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""ubuntu@sha256:de774a3145f7ca4f0bd144c7d4ffb2931e06634f11529653b23eba85aef8e378""]: exit status 1 (standard error: ""error pulling image configuration: received unexpected HTTP status: 502 Bad Gateway...; ```. which is not related to the requester pays feature but rather yet another dockerhub flaky response that we should ask google to retry IMO.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4233#issuecomment-429901811:194,failure,failure,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4233#issuecomment-429901811,4,"['error', 'failure']","['error', 'failure']"
Availability,"I think in general the number of concurrent jobs is determined by both of client side (cromwell) and server side(aws batch). In cormwell, there should be a rate limit of api call (no matter it is job submission or job status query) to avoid DDoS to the server side. On the server side like aws batch, there is also a config for rate limit of concurrent api call, if the number of concurrent api call exceeds the rate limit of server side, the server side may refuse to server so it is important not to set rate limit on the client side/cromwell over the server side rate limit. While on server side, if the concurrent jobs require more resources than the limit such as cpus and mem (compute env in aws batch) , it is the server side responsibility to put the concurrent jobs to queue and make sure they can be launched later when resource is available rather than throwing errors unless the queue is expired (say, resource is still not available one week later). IMHO, aws batch backend should implement the scatter jobs in array jobs which support multiple jobs submission and status query in one single api call, otherwise, it is too easy to exceed the rate limit of aws batch. jobs submission by user --> cromwell (rate limit config) --> aws batch gateway (rate limit config) --> aws batch compute env (resource limit)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444666395:842,avail,available,842,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444666395,3,"['avail', 'error']","['available', 'errors']"
Availability,I think just an artifact of everything shutting down quickly and some messages ending up floating in space. Not a big deal just polluting the logs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2212#issuecomment-297780855:48,down,down,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2212#issuecomment-297780855,1,['down'],['down']
Availability,"I think some of the test code is redundant with SwaggerServiceSpec but I don't understand swagger well enough to opine. @kshakir - it looks like you did a lot of the swagger work (albeit a long time ago), any thoughts?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289250957:33,redundant,redundant,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289250957,1,['redundant'],['redundant']
Availability,"I think that this is related with https://github.com/docker/machine/issues/2517, but I believe that cromwell can be more robust to a container still running but detached due to timeout.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371154240:121,robust,robust,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371154240,1,['robust'],['robust']
Availability,"I think that's a different request. #2652 is asking to make available the wdltool syntax validation from within cromwell. This one is asking for a new type of validation, analogous in some ways to the GATK Queue ""dry run"" feature.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332257390:60,avail,available,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-332257390,1,['avail'],['available']
Availability,"I think the bug is actually in Cromwell's WDL draft-2 support. Submitting a draft-2 workflow with PCRE pattern `\.gz$` yields an `Unrecognized token` error on the backslash. A work-around is to double-escape with `\\.gz$`, thus becoming non-standard and unexpectedly failing when you moved to 1.0.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-412088680:150,error,error,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-412088680,1,['error'],['error']
Availability,"I think the cache is unrelated, this is purely input localisation. I re-ran the job with caching disabled in the config file. The same error occurs. From this directory: /share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/37e4e046-b256-4f81-95c6-9f0c915810bf/call-SamToFastqAndBwaMem/inputs/-21323395 . There is a file 'cromwell.tmp' which seems to be a partial copy of my cromwell process' CWD. All of the logs are copied in, (cromwell.tmp/cromwell-workflow-logs/) and a single seemingly unrelated job (cromwell.tmp/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/). All of that jobs 'execution' folder, and some of its 'inputs' are copied. It is not clear if more would have been copied in or if the process was ended by the soft link error mentioned above.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803:135,error,error,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825#issuecomment-401217803,2,['error'],['error']
Availability,"I think the effect is fine. We often tell people that once a job is runnable that Cromwell fires it off, but that's always used as a way to help them understand that Cromwell isn't partaking in true scheduling (ie resource based negotiation via SGE, PAPI, etc). IMO it's absolutely ok for jobs which are runnable to have not started if that's the limitation the system imposes. Further, I think it's a good move in the resiliency front. Infinite scalability is a great goal, but from a practical perspective a limit is always going to be reached, so finding ways to make the system manage to keep on ticking ok when that happens is a good thing. That's generally going to involve slowing things down.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370621740:419,resilien,resiliency,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370621740,2,"['down', 'resilien']","['down', 'resiliency']"
Availability,"I think the following workflow (test.wdl) shows some even more insidious issues with localization:; ```; version 1.0. workflow main {; call main {; input:; X = [""/tmp/1"", ""/tmp/2"", ""/tmp/3""],; Y = [""/tmp/1"", ""/tmp/2"", ""/tmp/3""]; }. output {; Array[String] out = main.out; }; }. task main {; input {; Array[File]? X; Array[File]? Y; }. command <<<; echo X=~{if defined(X) then write_lines(select_first([X])) else X}; echo ~{if defined(Y) then ""Y="" + write_lines(select_first([Y])) else Y}; >>>. output {; Array[String] out = read_lines(stdout()); }. runtime {; docker: ""ubuntu:20.04""; }; }; ```. The following run:; ```; $ touch /tmp/{1,2,3}; $ cd /home/freeseek/cromwell; $ java -jar cromwell-51.jar run test.wdl; ```. Generates the following main.out:; ```; [""X=/cromwell-executions/main/bc07dd07-017f-41cf-9ba5-9f6e014a475b/call-main/execution/write_lines_c53d7635054b80e6d4298c99f823d256.tmp"",; ""Y=/home/freeseek/cromwell/cromwell-executions/main/bc07dd07-017f-41cf-9ba5-9f6e014a475b/call-main/execution/write_lines_c53d7635054b80e6d4298c99f823d256.tmp""]; ```. I can guess that in one case write_lines() is run before localization and in one case it is run after localization, generating two at first extremely puzzling different outputs. Notice that the [WDL specification](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#task-input-localization) requires that `Files are localized into the execution directory prior to the task execution commencing.` which does not seem to be the case for Cromwell. This seems to me like a serious bug. Where is it specified when Cromwell performs localization of files?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-657253592:348,echo,echo,348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5540#issuecomment-657253592,2,['echo'],['echo']
Availability,"I think the issue is actually reversed. The call cache diff endpoint should be accessing metadata which means the missing info are the call cache hashes should be in the metadata store. We say that the metadata repository is the collection of every meaningful event that has occurred in the system and that allows downstream clients to shape that information to suit their needs. That's why all user facing ""gie me information about XYZ"" endpoints read from there. This should be the same I think.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648:314,down,downstream,314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306891648,1,['down'],['downstream']
Availability,I think the other one is redundant,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/437#issuecomment-185444571:25,redundant,redundant,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/437#issuecomment-185444571,1,['redundant'],['redundant']
Availability,"I think the real solution would be to make WDLExpression evaluation truly asynchronous, which is not trivial.; The simpler solution to retry evaluations as a whole I would say is not that much work, although it happens in a bunch of places and some might be trickier than others. I would say it's small enough that we could try to squeeze it in 26, which would be nice because we've been seeing an increasingly large number of transient gcs failures lately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288765933:441,failure,failures,441,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288765933,1,['failure'],['failures']
Availability,"I think this change is probably the right way to go. My comment here is about centaur itself. In general, I dislike this kind of test retry. A single retry can hide the fact that a feature has a nearly 50% failure rate. I'd rather the system itself be more resilient to failures, along with having much fewer potentially-flaky full-system checks (though that relies on having a much more robust set of unit tests than we currently have). I also recognize that part of the value of these tests is to protect us from unexpected changes (intended or not) in Google's services. A strategy I've successfully used to decouple from an external service is to have the thinnest possible layer of full-system checks that verify our understanding of how that external system functions, then building a robust set of unit tests of our code based on that understanding.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788:206,failure,failure,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1017490788,5,"['failure', 'resilien', 'robust']","['failure', 'failures', 'resilient', 'robust']"
Availability,"I think this has just bitten me as well. I am reading from a collaborator's bucket of several hundred terabytes that has underscores in the bucket name. I _think_ this is resulting in the input files being unhashable and thus disabling call-caching. The error I see (minus a real bucket path) is:. ```; [2017-05-15 17:08:12,44] [error] a05af6bd:Pre_Merge_SV.Extract_Reads:21:1: Hash error, disabling call caching for this job.; java.lang.Exception: Unable to generate input: File input_cram hash. Caused by java.lang.IllegalArgumentException: Could not find suitable filesystem among Gcs to parse gs://bucket_with_underscores/my.cram.; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-302408146:254,error,error,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-302408146,3,['error'],['error']
Availability,"I think this is actually a separate problem w.r.t. how cromwell constructs its script file. If you look in your GCS folder for the exec.sh, you'll probably see something like:. ```; echo hello && exit 1; cat $? > rc.txt; ```. Although there's no error message returned via the REST api, if you look in the server logs I suspect you'll see something along the lines of ""rc file not found""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-174693109:182,echo,echo,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-174693109,2,"['echo', 'error']","['echo', 'error']"
Availability,"I think this is expected behavior since `array[@]` is not a valid WDL expression and so cannot be used between `${ }` in a command string. Since WDL doesn't have a way to escape the sequence, I suggest inserting the `$` in manually in a way that doesn't trigger WDL string interpolation:; ```; String dollar = ""$""; command <<<; echo ""Hello ${addressee}!""; array=(one two three); for i in ${dollar}{array[@]}; do; echo $i; done; >>>; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1819#issuecomment-271362663:328,echo,echo,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1819#issuecomment-271362663,2,['echo'],['echo']
Availability,"I think this is ready for review, passes my basic workflows okay. Can I request the WDL Biscayne and general WDL labels for this PR?. ### Testing . Alrighty, so I did some digging and looks like similar PRs don't include tests. This doesn't mean I should blindly follow, but I can't see a great way to add a test and check its output, so I've only added validation tests (293256180ea8f6f5398866110ba8b727fd4c148e). I'm not sure if this should make it into the CHANGELOG, but I've added some text here which I'll add into the . > ### New sep function for joining an array of strings; >; > Per [OpenWDL #229](https://github.com/openwdl/wdl/pull/229), we've replaced the `sep=` string interpolator option with a new `sep` engine function, available in the WDL development (Biscayne) specification.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-626405308:736,avail,available,736,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-626405308,1,['avail'],['available']
Availability,"I think this should be fairly high priority. It causes very confusing errors with the PAPI backend, made even more confusing by the fact that the same code will work on the local executor. Are there any workarounds at the moment?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-438104955:70,error,errors,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-438104955,1,['error'],['errors']
Availability,"I think we're good, I got this wdl passing which I think cover all cases we should support for now :. ```; task B {; String B_in; command {; python -c ""print('${B_in}')""; }; output {; String B_out = read_string(stdout()); }; }. task D {; String D_in; command {; python -c ""print(len('${D_in}'))""; }; output {; Int D_out = read_int(stdout()); }; }. task C {; Array[String] C_in; command {; echo ${sep = ' ' C_in}; }; output {; String C_out = read_string(stdout()); }; }. workflow w {; Array[String] an_array = [""a"", ""ab"", ""abc""]. scatter (item in an_array) {; call B {input: B_in = item }; call D {input: D_in = B.B_out }; }. call C {input: C_in = B.B_out }; call C as E {input: C_in = D.D_out }. scatter (item in D.D_out) {; call B as F {input: B_in = item }; }; }; ```. Output:. ```; {; ""w.F.B_out"": [""1"", ""2"", ""3""],; ""w.C.C_out"": ""a ab abc"",; ""w.E.C_out"": ""1 2 3"",; ""w.D.D_out"": [1, 2, 3],; ""w.B.B_out"": [""a"", ""ab"", ""abc""]; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/161#issuecomment-135907461:389,echo,echo,389,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/161#issuecomment-135907461,1,['echo'],['echo']
Availability,"I think we're looking to freeze GCP changes for now due the imminent migration to GCP Batch. We're also not sure if reference disks are staying around, they are maintenance-intensive in Terra.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2107597421:161,mainten,maintenance-intensive,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6762#issuecomment-2107597421,1,['mainten'],['maintenance-intensive']
Availability,"I thought I was going crazy! Thanks @rhpvorderman, I think this is a great PR, and really looking forward to being able to test this on our HPC. I'm still a little concerned with Cromwell requesting the whole file (up to 200GB) and it's impact on our network. But this is definitely a step in the write direction. I was thinking of maybe adding something similar, but would check the file size, and then perform `xxh64` on the first 1MB or some region of the file (eg: `size+xxh64-partial`). I'm happy to produce some documentation (whether a part of this PR, or linked to it), about call-caching on Cromwell and to include this new material once I can test it okay. Thanks for pinging me @rhpvorderman, excited to try this out!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599207733:678,ping,pinging,678,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450#issuecomment-599207733,1,['ping'],['pinging']
Availability,"I thought it might be a good idea to run Centaur against 0.19_hotfix for comparison. It turns out that this particular Centaur test ""passes"" on 0.19_hotfix since the workflow fails consistently. However this failure appears to be due to botched call path construction when pulling down call logs and not for any reason related to write_lines:. ```; java.io.FileNotFoundException: Item not found: cromwell-dev/mlc-executions/write_lines/1b7836f6-841c-4eaa-8d63-8225f67d9507/call-a2f/cromwell-dev/mlc-executions/write_lines/1b7836f6-841c-4eaa-8d63-8225f67d9507/call-a2f/a2f-stdout.log; at com.google.cloud.hadoop.gcsio.GoogleCloudStorageExceptions.getFileNotFoundException(GoogleCloudStorageExceptions.java:42) ~[gcsio-1.4.4.jar:na]; at com.google.cloud.hadoop.gcsio.GoogleCloudStorageReadChannel.getMetadata(GoogleCloudStorageReadChannel.java:579) ~[gcsio-1.4.4.jar:na]; .; .; .; 2016-06-07 10:39:03,194 cromwell-system-akka.actor.default-dispatcher-10 INFO - WorkflowActor [UUID(1b7836f6)]: persisting status of a2f to Failed.; 2016-06-07 10:39:03,194 cromwell-system-akka.actor.default-dispatcher-10 ERROR - WorkflowActor [UUID(1b7836f6)]: Item not found: cromwell-dev/mlc-executions/write_lines/1b7836f6-841c-4eaa-8d63-8225f67d9507/call-a2f/cromwell-dev/mlc-executions/write_lines/1b7836f6-841c-4eaa-8d63-8225f67d9507/call-a2f/a2f-stdout.log; 2016-06-07 10:39:03,207 cromwell-system-akka.actor.default-dispatcher-10 INFO - WorkflowActor [UUID(1b7836f6)]: Beginning transition from Running to Failed.; 2016-06-07 10:39:05,018 cromwell-system-akka.actor.default-dispatcher-12 INFO - WorkflowActor [UUID(1b7836f6)]: transitioning from Running to Failed.; ```. So AFAICT this workflow should be passing on develop.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224306256:208,failure,failure,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224306256,3,"['ERROR', 'down', 'failure']","['ERROR', 'down', 'failure']"
Availability,"I thought it was a feature request (wdlDependencies isn't a documented field for batch endpoint in the README.md) but if you're expecting that it should work, then it's a bug report - because if I submit to `:version/batch` specifying `wdlDependencies`: ; ```; curl http://bionode05/cromwell/api/workflows/V1/batch -FwdlSource=@test.wdl -FworkflowInputs=@test.batch.inputs -FwdlDependencies=@dependency.wdl.zip; ```; I get a failed workflow with the following metadata (no `imports` in the `submittedFiles` block):; ```; {; ""submittedFiles"": {; ""inputs"": ""{\""test.foo.showIt\"":\""that\""}"",; ""workflow"": ""import \""dependency.wdl\"" as dependency\n\nworkflow test {\n\n\tcall dependency.foo\n\n}\n"",; ""options"": ""{\n\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""d97a5124-0933-4243-b542-6467b496ba22"",; ""inputs"": {. },; ""submission"": ""2016-12-08T10:21:10.205+10:00"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Workflow input processing failed.\nUnable to load namespace from workflow: Failed to import workflow, no import sources provided.""; }],; ""end"": ""2016-12-08T10:21:16.957+10:00"",; ""start"": ""2016-12-08T10:21:16.952+10:00""; }; ```. The exact same `curl` command line submission (with suitable inputs file) but to the `:version` endpoint works ok.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532:890,failure,failures,890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-265617532,1,['failure'],['failures']
Availability,"I thought we were going to try @aednichols's idea for autocommitting the heartbeat writes (still batched, just not wrapped in one big transaction) to avoid having to do this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4240#issuecomment-429881369:73,heartbeat,heartbeat,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4240#issuecomment-429881369,1,['heartbeat'],['heartbeat']
Availability,"I tried running a WDL in version 1.1, to use the `as_map(Array[Pair[X,Y]])` function, but it doesn't work. ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [; ; ],; ""message"": ""ERROR: Finished parsing without consuming all tokens.\n\nversion 1.1\n^\n ""; }; ],; ""message"": ""Workflow input processing failed""; }; ],; ```. According to the [LanguageSupport specifications](https://cromwell.readthedocs.io/en/stable/LanguageSupport/), the only supported versions for WDL are `1.0` and `development`. Is this correct?. Would it be possible to add support for `WDL 1.1`?. [Jira issue](https://broadworkbench.atlassian.net/browse/CROM-6710)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6221:113,failure,failures,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6221,2,"['ERROR', 'failure']","['ERROR', 'failures']"
Availability,"I tried running a these two glob output commands; `Array[File] gvcf_list = glob(""split_gvcfs/*.gz"")`; `Array[File] gvcf_index_list = glob(""split_gvcfs/*.tbi"")`; Which should have had 3457 elements in each array. When I go to the output directory bucket of the above task, I see each glob directory does indeed have 3457 elements but when I go to the symbols table in the Cromwell db the arrays only have 1000 elements. This causes tasks further down in the workflow to fail because I am expecting an array of 3457 elements but I am only getting 1000.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/699:445,down,down,445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/699,1,['down'],['down']
Availability,"I tried to ask this on the broad forums but got an error.""You need the Garden.Community.Manage permission to do that."", ""Class"": ""Gdn_UserException"" . I have been using a . backend.default = SGE. Line in my config file. This has been working until now, with my jobs submitted using my defined SGE backend. Today I changed some other parts of the server, unrelated and numerous. Now SOME jobs are being submitted on the SGE backend, and some on the Local backend. (I did not configure a local backend, but I assume one is built in to cromwell. How is the backend to run on chosen by cromwell? (I know of backend.default, and I beleive there is a wdl.task.runtime.backend parameter also (undocumented from what I can tell)). How can I prevent any task ever running on the local backend? I want this to be a hard error, and not overload my sge login node. . Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3533:51,error,error,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3533,2,['error'],['error']
Availability,"I tried to set up AWS Batch with Cromwell 36, and using https://docs.opendata.aws/genomics-workflows/aws-batch/create-custom-ami/ to create the AMI, but when I use /cromwell_root as ScratchMountPoint instead of /scratch, the cloud formation failed to create AMI:. 11:43:03 UTC-0500 | CREATE_FAILED | AWS::EC2::Instance | EC2Instance | Received FAILURE signal with UniqueId i-0428e258c971e355e; -- | -- | -- | -- | --;   | Physical ID:i-0428e258c971e355e;   | Client Request Token:Console-CreateStack-2b86244b-f98c-4c65-9b87-7988dd39a4df. Please advise.; Thanks; Jing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4435:344,FAILURE,FAILURE,344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4435,1,['FAILURE'],['FAILURE']
Availability,"I upgraded to release 0.23 and did a workaround for #1754 ... I had the same issue as described here, however, I did see the exception below, which I do not think was displayed in previous versions of cromwell. Is this at all helpful? @kshakir ? I'd also like to point out that this file does not exist, but I have not done anything to that directory. There are log files from other runs (mostly local backend, though). The permissions are set appropriately for ``/home/lichtens/eval-gatk-protected/cromwell-workflow-logs/``. ```; [ERROR] [12/07/2016 22:51:59.735] [cromwell-system-akka.dispatchers.engine-dispatcher-53] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8968c364-; 3623-4242-b39e-228f43f5d4c3] /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; java.nio.file.NoSuchFileException: /home/lichtens/eval-gatk-protected/cromwell-workflow-logs/workflow.8968c364-3623-4242-b39e-228f43f5d4c3.log; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:244); at sun.nio.fs.AbstractFileSystemProvider.delete(AbstractFileSystemProvider.java:103); at java.nio.file.Files.delete(Files.java:1126); at better.files.File.delete(File.scala:602); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at cromwell.core.logging.WorkflowLogger$$anonfun$deleteLogFile$1.apply(WorkflowLogger.scala:112); at scala.Option.foreach(Option.scala:257); at cromwell.core.logging.WorkflowLogger.deleteLogFile(WorkflowLogger.scala:112); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(WorkflowActor.scala:307); at cromwell.engine.workflow.WorkflowActor$$anonfun$9$$anonfun$applyOrElse$1.apply(Wor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486:532,ERROR,ERROR,532,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-265750486,1,['ERROR'],['ERROR']
Availability,"I use this config in a SGE backend for singularity. ```; Singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; concurrent-job-limit = 100; exit-code-timeout-seconds = 120; runtime-attributes = """"""; String sif; Float? memory_gb; String? bind_path; """""". submit = """"""; qsub \; -terse \; -V \; -b y \; -N ${job_name} \; -wd ${cwd} \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l mem_free="" + memory_gb + ""g""} \; singularity exec -e --bind ${cwd}:${cwd} \; ${""--bind "" + bind_path} \; ${sif} \; ${job_shell} ${script}; """""". job-id-regex = ""(\\d+)""; check-alive = ""qstat -j ${job_id}""; kill = ""qdel ${job_id}""; }; }; ```; Every task should have a `String sif`, point to the path to sif file. You can modify this according to your need.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048:605,alive,alive,605,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6685#issuecomment-1188515048,1,['alive'],['alive']
Availability,"I used cromwell-38.jar for testing. It looks like outputs from tasks with the same task-name and shard index in different scatter blocks share the same scratch directory in a container. Output files are written on this shared directory and globbed at the end of each task and then moved to S3 bucket together with all text/log files. This can result in globbing of wrong output files (from tasks with the same task-name but different alias).; ```; workflow cromwell_aws_glob_test {; scatter(i in range(2)) {; call t1 { input: i=i, alias = ""t1"" }; }. scatter(i in range(2)) {; call t1 as t2 { input: i=i, alias = ""t2"" }; }; }. task t1 {; Int i; String alias; command {; echo ""${alias},${i}"" > ${alias}-${i}.txt; }; output {; Array[File] outs = glob(""*.txt""); }; }; ```; On the execution directory for task t1 on S3. Everything looks okay. We don't have any t2 things.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/; PRE glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:30:54 0; 2019-04-11 20:32:48 30 glob-ef5df339533c1334f081dc8cc75ee4f3.list; 2019-04-11 20:30:54 2238 script; 2019-04-11 20:32:49 2 t1-0-rc.txt; 2019-04-11 20:32:51 0 t1-0-stderr.log; 2019-04-11 20:32:50 0 t1-0-stdout.log. But the glob directory has outputs from task t2.; ubuntu@ip-172-30-0-96:~/test_cromwell$ aws s3 ls s3://encode-pipeline-test-runs/test2/cromwell_aws_glob_test/d3fb65e0-54d0-495c-8bea-503a91d9dff3/call-t1/shard-0/glob-ef5df339533c1334f081dc8cc75ee4f3/; 2019-04-11 20:32:47 277 cromwell_glob_control_file; 2019-04-11 20:32:47 5 t1-0.txt; 2019-04-11 20:32:47 2 t2-0-rc.txt; 2019-04-11 20:32:47 5 t2-0.txt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4834:669,echo,echo,669,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4834,1,['echo'],['echo']
Availability,"I validated `cnv_param_sweep.wdl` and received this error:; ```; ERROR: Missing value or call: Couldn't find value or call with name 'CollectAllelicCountsNormal' in workflow (line 136):. Int model_segments_disk = ceil(size(DenoiseReadCountsTumor.denoised_copy_ratios, ""GB"")) + ceil(size(CollectAllelicCountsTumor.allelic_counts, ""GB"")) + ceil(size(CollectAllelicCountsNormal.allelic_counts, ""GB"")) + disk_pad; ```; `cnv_param_sweep.wdl` imports 3 WDLs, and the error was actually in `cnv_somatic_pair_workflow.wdl`. It would be great to know what WDL the error is in, especially if it's not the primary WDL.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3055:52,error,error,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3055,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I verified this workaround using the following WDL file:; ```; task dollarInterpolation {. 	String dollar = ""$""; 	command <<<; 	 array=(one two three); 	 for i in ${dollar}{array[@]}; do; 	 echo $i; 	 done; 	>>>. 	output {; 		String s = read_string(stdout()); 	}; }. workflow main {; 	call dollarInterpolation; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1819#issuecomment-271366445:190,echo,echo,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1819#issuecomment-271366445,1,['echo'],['echo']
Availability,"I wanted to follow-up on this error: I am now seeing this error after implementing the standard broad institute alignment pipeline on the HPC at my institute: https://portal.firecloud.org/?return=terra#methods/five-dollar-genome-analysis-pipeline-gilad/five-dollar-genome-analysis-pipeline-gilad/1. Specifically my error is: . [INFO] [08/12/2024 19:26:46.031] [cromwell-system-akka.dispatchers.engine-dispatcher-29] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Workflow 8b8c576b-50bc-4a33-b326-0f69be43ece9 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'CreateSequenceGroupingTSV.sequence_grouping': Failed to read_tsv(""sequence_grouping.txt"") (reason 1 of 1): Future timed out after [60 seconds]; Bad output 'CreateSequenceGroupingTSV.sequence_grouping_with_unmapped': Failed to read_tsv(""sequence_grouping_with_unmapped.txt"") (reason 1 of 1): Future timed out after [60 seconds]. Bad output 'GetBwaVersion.bwa_version': Failed to read_string(""/scratch/tpa239/Step123/TN_2036/TN2036_phylogenetics_8_10_testing/slurm/alignment/alignment_TN2036_sample106/cromwell-executions/WholeGenomeGermlineSingleSample/8b8c576b-50bc-4a33-b326-0f69be43ece9/call-UnmappedBamToAlignedBam/UnmappedBamToAlignedBam/207b9946-03a6-4969-bdab-318482635923/call-GetBwaVersion/execution/stdout"") (reason 1 of 1): Future timed out after [60 seconds]. I think it has to do with this read_tsv and function - sometimes an identical job will have this error and sometimes they don't, I think it has to do with how busy the cluster is. . Is there some setting I can change to increase this timeout? Should I increase the number of cpus or memory for these jobs failing?. I am using cromwell version 85. Thank you!. Toby",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2291515502:30,error,error,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2291515502,4,['error'],['error']
Availability,"I was able to see the actual error message from CloudWatch logs. It seems that my Cromwell server instance got the same error as shown here, while on the instance automatically created by AWS Batch compute environment, errors were different in different jobs. So probably the Cromwell server is not the correct place for diagnosis. I'll just close this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-930634231:29,error,error,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6504#issuecomment-930634231,3,['error'],"['error', 'errors']"
Availability,"I was able to track partway back on the Google side of things. In short, Cromwell reported jobs in the ""Running"" state. These were associated with Google Genomics API operations, also listed as ""Running"". However, the instances that were supposed to be associated with those GG operations were not alive. I noticed in my billing statement that there were credits to offset compute instances, so I suspect that something happened at Google that was unexpected. I'll dig a bit further. . In the meantime, it would definitely help to have a way to differentiate exceptions that are ""expected"" from those that are not. In this case, I assumed that a restart of a service after an exception was probably OK, but it was associated with ""dead"" jobs that now appear to have been on the google side of things and not with Cromwell directly. I'll follow up with whatever else I can learn.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260120082:298,alive,alive,298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260120082,1,['alive'],['alive']
Availability,"I was just going to ignore this, but then @abaumann mentioned he was confused by the same thing... so why not. Used to be confusing because it could be referring to a value of type Boolean, rather than literally ""Boolean"":; ```; Expected equal, got ""Boolean"". Boolean conditional = incremented != 0; ^; ```; Used to be confusing because integers could suggest you're using the wrong number of identifiers:; ```; Expected identifier, got ""1"". echo ""~{one.1}"" > ~{one.1}.txt; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3725:442,echo,echo,442,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3725,1,['echo'],['echo']
Availability,"I was running Cromwell with a JES backend. I got through about 300 out of 1000 workflows that were submitted as a batch; the rest are still listed as `Running`. Oddly, the genomics API operations report that they are still running though there are no instances still up. I am not sure if this is a Cromwell issue or a Google one, but here is what I am seeing from Cromwell. Cromwell has been working well for smaller batches. Any thoughts? . Thanks!. ```; [ERROR] [11/12/2016 02:22:12.170] [cromwell-system-akka.actor.default-dispatcher-2686] [akka://cromwell-system/user/cromwell-service/$b/$a] The JES polling actor Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$8#-1275882726] unexpectedly terminated while conducting 11 polls. Making a new one...; [INFO] [11/12/2016 02:22:12.170] [cromwell-system-akka.actor.default-dispatcher-2686] [akka://cromwell-system/user/cromwell-service/$b/$a] watching Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$9#-1852863496]; [WARN] [11/12/2016 02:22:12.171] [cromwell-system-akka.dispatchers.backend-dispatcher-2573] [akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-7f0d7c16-d434-4dd7-a7ba-c7e897701c9a/WorkflowExecutionActor-7f0d7c16-d434-4dd7-a7ba-c7e897701c9a/7f0d7c16-d434-4dd7-a7ba-c7e897701c9a-EngineJobExecutionActor-salmonRun.salmonQuant:NA:1/7f0d7c16-d434-4dd7-a7ba-c7e897701c9a-BackendJobExecutionActor-7f0d7c16:salmonRun.salmonQuant:-1:1/JesAsyncBackendJobExecutionActor] JesAsyncBackendJobExecutionActor [UUID(7f0d7c16)salmonRun.salmonQuant:NA:1]: Caught exception, retrying:; java.lang.RuntimeException: Unexpected actor death!; 	at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:33); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665:457,ERROR,ERROR,457,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665,1,['ERROR'],['ERROR']
Availability,"I was running a bunch of workflows (same wdl, same method) when we had to restart and redeploy cromwell service and I got Errors with no visible message, when we dug into the cromwell metadata we discovered this message that Cris interpreted as a restart issue.; message"": ""Workflow input processing failed:\nWorkflow contains invalid labels JSON: Unexpected end-of-input at input index 0 (line 1, position 1), expected JSON Value:\n\n^\n""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2067:122,Error,Errors,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2067,1,['Error'],['Errors']
Availability,"I was running through the tutorial, https://cromwell.readthedocs.io/en/develop/tutorials/ServerMode/. I typed the inputs.json file with a tab instead of spaces. ```curl -X POST http://localhost:8000/api/workflows/v1 -F workflowSource=@hello.wdl -F workflowInputs=@inputs.json```. ```; {; ""status"": ""fail"",; ""message"": ""Error(s): Input file is not valid yaml nor json: while scanning for the next token\nfound character '\\t(TAB)' that cannot start any token. (Do not use \\t(TAB) for indentation)\n in 'reader', line 2, column 1:\n \t\""test.hello.name\"": \""World\""\n ^\n""; }; ```. However per the JSON spec, tabs, or any whitespace, ""can be inserted between any pair of tokens"" https://www.json.org/. Python:; ```; json.loads(""{\n\t\""valid\"":\""json\""\n}""); ```. Sadly I do not know Java very well or I would just check and or fix whichever parser you are using. Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3487:319,Error,Error,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3487,1,['Error'],['Error']
Availability,"I was trying to label all GCP batch resources using `-o` in the CLI. (according to [this doc](https://github.com/broadinstitute/cromwell/blob/master/docs/wf_options/Google.md) and [this doc](https://github.com/broadinstitute/cromwell/blob/develop/docs/backends/GCPBatch.md) ); My content of options.json is ; ```; {""google_labels"":{""workflow-run-execution-id"":""97fed6c4-6442-4efe-9e73-7b3592a33480""}} ; ```; and my command is `java -Dconfig.file=config -jar /app/cromwell.jar run wf.wdl -i input.json -o options.json`. The error I am getting:; <img width=""1409"" alt=""Screen Shot 2024-01-04 at 9 58 25 AM"" src=""https://github.com/broadinstitute/cromwell/assets/1992953/e559bccf-dd96-4dea-a48a-6649e448a26f"">. After adding string prefix to my own uuid as label value, the error was gone and my workflow ran smoothly. However, I do need to pass in the uuid so downstream analysis pipeline can still work. . Related code reporting error is [here](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L65) and error is produce [here](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L77) and [this is the regex definition](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L21). I am wondering can the [code](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L77) be updated to only check the key of a label so it aligns with GCP which does not have this restriction on label values? Or use another regex `""[a-z0-9]([-a-z0-9]*[a-z0-9])?""` for label value check",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7351:523,error,error,523,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7351,4,"['down', 'error']","['downstream', 'error']"
Availability,I was trying to restart a cromwell instance on server mode after modified config file. but the log was:; `2023-07-05 08:21:07 cromwell-system-akka.actor.default-dispatcher-25 ERROR - Bind failed for TCP channel on endpoint [/10.10.200.221:8000]`; Is there any way to stop an exist instance or restart it by API or something can automate by script.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7170:175,ERROR,ERROR,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7170,1,['ERROR'],['ERROR']
Availability,"I was trying to run the [PublicPairedSingleSampleWf](https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_pipelines/PublicPairedSingleSampleWf_160927.wdl) using Cromwell 0.19.3 and it was consistently failing with the following:. ```; 2016-10-06 21:32:38,239 cromwell-system-akka.actor.default-dispatcher-41 INFO - JES Run [UUID(65d59b8b):SamToFastqAndBwaMem:1]: Status change from Running to Success; 2016-10-06 21:32:38,586 cromwell-system-akka.actor.default-dispatcher-4 ERROR -WorkflowActor [UUID(65d59b8b)]: Completion work failed for call SamToFastqAndBwaMem:1.; java.sql.SQLDataException: data exception: string data, right truncation; table: EXECUTION_EVENT column: DESCRIPTION; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source) ~[cromwell.jar:0.19]; <snip>; ```. I had run the previous version of the PublicPairedSingleSampleWf successfully. I finally realized that it had nothing to do with the workflow, but rather because I changed the GCS path for Cromwell's `baseExecutionBucket` to something much longer. Instead of my previous:. ```; gs://my-bucket/gatk-test/; ```. I had changed to:. ```; gs://my-bucket/broad_pipelines/PublicPairedSingleSampleWf_160927/; ```. When I went back and shortened the path, everything worked again. Along the way, I ran a similar test where I changed the task name for my test [vcf_chr_count.wdl](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/workflows/vcf_chr_count/vcf_chr_count.wdl) from:. ```; task vcf_split; ```. to . ```; task vcf_split_what_if_i_have_a_really_long_task_is_that_the_problem; ```. This also generated the same exception:. ```; java.sql.SQLDataException: data exception: string data, right truncation; table: EXECUTION",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1555:486,ERROR,ERROR,486,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1555,1,['ERROR'],['ERROR']
Availability,"I was trying to use an invalid options file for cromwell 24 methods, through swagger, and the error message returned was: . ```; {; ""status"": ""error"",; ""message"": ""The server was not able to produce a timely response to your request.""; }; ```; The cromwell logs had more information, I was using the wrong variable format for defaultRuntimeOptions.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2041:94,error,error,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2041,2,['error'],['error']
Availability,"I was using 0.19 proper. I compiled the 0.19_hotfix branch, and I got the same error.; I complied the develop branch, and this problem appears resolved... Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1070#issuecomment-228572361:79,error,error,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1070#issuecomment-228572361,1,['error'],['error']
Availability,"I was using 3.4.1 and saw behavior where one of the copies did the download into the cache, and the second saw the partial file in the cache and tried to run it and failed. And yes, it's a local registry that I convinced to run inside singularity instead of docker since my production hosts are centos6 and are not happy with docker currently (and I'd rather have the control singularity gives me).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537244158:67,down,download,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537244158,1,['down'],['download']
Availability,"I was using before the in-memory database, and I use the `--metadata-output` to dump a JSON with the metadata of the workflow. What I am experimenting at the moment is to use the call-caching and the database to run the workflow to account for random failures of docker containers (see https://github.com/broadinstitute/cromwell/issues/3370) . It is unclear for me how the database is setup and which information stores, so I am not sure what can be clean and what is important/usuful for later use. That's why I think that it will be nice to have a documentation page showing how the database can be managed to grab information and clean the unnecessary data from time to time. Thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3415#issuecomment-373394497:251,failure,failures,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3415#issuecomment-373394497,1,['failure'],['failures']
Availability,"I was writing a wdl with this line in the task definition. `Float vaf = .01`. and was getting this error when validating . `Error: Invalid WDL: ERROR: Unexpected symbol (line 28, col 16) when parsing 'e'. Expected identifier, got 01. Float vaf = .01 ^ $e = :identifier <=> :dot :identifier -> MemberAccess( lhs=$0, rhs=$2 )`. if I change it to; ; `Float vaf = 0.01`. it's fine but I think looking at the spec the first iteration should work too - https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants. `$float = (([0-9]+)?\.([0-9]+)|[0-9]+\.|[0-9]+)([eE][-+]?[0-9]+)?`. Easily worked around just a little annoying. This was run on a FireCloud method so I assume its one of the more recent versions? (cromwell 34) but 🤷‍♂️",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4089:99,error,error,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4089,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"I wonder if we could peek at the workflow store in the GET case and if the workflow is just-submitted, return a more helpful error than “not found”",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2671#issuecomment-476351327:125,error,error,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2671#issuecomment-476351327,1,['error'],['error']
Availability,"I would like to reopen this issue. I have been testing the `memory-retry` feature since it should be very useful for my project on GCP. However, Cromwell does not retry any job exited with SIGKILL (`137`) and all jobs killed by an OOM-killer get `137` as an exit-code. So this `memory-retry` feature doesn't work at all. And I found this.; https://github.com/broadinstitute/cromwell/blob/171f12c890373e896b4eab1f9f4ad23660dc80f3/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala#L308. So even though I configure the two `memory-retry` parameters correctly in `backend.conf` and workflow options JSON. it's useless. Cromwell does not retry any job exited with `137`. . I tested with a fake OOM with exit code `1` and `137` and Cromwell retried the task with exit code `1` only.; ```; version 1.0. workflow mem_retry {; call fail_with_fake_oom; call fail_with_true_oom; }. task fail_with_fake_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; >&2 echo ""Killed""; exit 137 # cromwell does not retry the task if it gets 137; #exit 1 # cromwell retries the task if it gets 1 ; fi; >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. task fail_with_true_oom {; command <<<; set -e. TOTAL_MEMORY=$(free -m | awk 'FNR == 2 {print $2}'); echo ""instance memory: $TOTAL_MEMORY""; if [[ ""$TOTAL_MEMORY"" > 2500 ]]; then; echo ""Not killed""; else; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero; fi. >>>; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; maxRetries: 2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372:1027,echo,echo,1027,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-811394372,5,['echo'],['echo']
Availability,"I would like to specify an optional array of strings as a runtime attribute. As such, I tried this in my provider configuration:. runtime-attributes = """"""; Array[String]? mounts = []; """""". Unfortunately, this fails horribly:. ```; [2019-02-27 16:26:09,15] [error] Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; java.lang.RuntimeException: Unsupported config runtime attribute WomMaybeEmptyArrayType(WomStringType) mounts; ```. The error clearly indicates that this type is unsupported. What is the reasoning behind that, or am I just doing it wrong?. (This is with Cromwell 36.1)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4685:257,error,error,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4685,2,['error'],['error']
Availability,"I would suggest the `wdl` tag on Stack Overflow, which is available everywhere, permanent, and easy to search. https://bioinformatics.stackexchange.com/questions/tagged/wdl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1182147447:58,avail,available,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6801#issuecomment-1182147447,1,['avail'],['available']
Availability,"I would support spec-mandated minimums, supplemented by knobs in Cromwell. . For the user experience, a key thing you can do is write really clear error messages. Ie don't make it die with just ""File was too big""; add a note in there about where to get more info/what can be done to get past this. @katevoss can help with this; she has strong feelings about microcopy as I'm sure you know by now ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637:147,error,error,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294874637,1,['error'],['error']
Availability,"I write a new backend. But the hostname verification needs to be canceled due to an internal environment problem. However, after the verification is canceled, the following information is displayed when submitting job:. ```; akka.stream.scaladsl.TcpIdleTimeoutException: TCP idle-timeout encountered on connection to xxxxxxx, no bytes passed in the last 1 minute; java.lang.OutOfMemoryError: GC overhead limit exceeded; ```. If the hostname verification is not canceled, the service is normal. The code is as follows:. ```; val badSslConfig: AkkaSSLConfig = AkkaSSLConfig().mapSettings(s =>; s.withLoose(; s.loose; .withDisableHostnameVerification(true); ); ). val badctx: HttpsConnectionContext = Http().createClientHttpsContext(badSslConfig). private def makeRequest[A](request: HttpRequest)(implicit um: Unmarshaller[ResponseEntity, A]): Future[A] = {. for {; response <- withRetry(() => {. val rsp = if (vkConfiguration.region == ""xxxxxxxxx""){; Await.result({Http().singleRequest(request, badctx)}, Duration.Inf); }else{; Await.result(Http().singleRequest(request), Duration.Inf); }; if (rsp.status.isFailure() && rsp.status.intValue() == 429) {; Future.failed(new RateLimitException(rsp.status.defaultMessage())); } else {; Future.successful(rsp); }; }); data <- if (response.status.isFailure()) {; response.entity.dataBytes.runFold(ByteString(""""))(_ ++ _).map(_.utf8String) flatMap { errorBody =>; Future.failed(new RuntimeException(s""Failed VK request: Code ${response.status.intValue()}, Body = $errorBody"")); }; } else {; Unmarshal(response.entity).to[A]; }; } yield data; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6149:1390,error,errorBody,1390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6149,2,['error'],['errorBody']
Availability,"I'd argue that the arbitrary KV thing was one of the largest mistakes from Original WDL (i.e. what came out of the 2 weeks of us locking ourselves in a room and figuring it all out) as it destroys portability unless one adds *some* structure to it. We've doubled down on it over the years by adding what IMO should be Cromwell workflow options into the runtime block which means that a WDL can now have important control information on it which might ruin the portability of that workflow. The worst example I can think of is `backend` which is a purely Cromwell concept - what happens when that workflow goes to run on DNAnexus? What happens when `backend` is *also* a concept in another engine but it means something else? What happens when one engine interprets `cpu` to mean ""at least this much"" and another ""exactly this much""? What happens when in the former case the user gets charged more money than they thought because more memory than they needed was allocated? What happens when one engine assumes `mem` is just a number representing GB and can't parse a string w/ units?. (admittedly `mem` is a bad example as it's one of the very few things in `runtime` the spec is actually opinionated about, but you get the point). If the goal is to decouple Cromwell from WDL, the most obvious target is the `runtime` section. If people need more control over their Cromwell experience the answer is to a) provide that information in a way which doesn't destroy workflow portability of the WDL and b) expose that via Firecloud if those users need Firecloud. FWIW one of my primary goals for WDL 1.0 is a massive redo of `runtime` including removing the arbitrariness of it. If things are implementation specific they can be passed in to that engine separately, which would also help maintain the portability of the workflow itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2990#issuecomment-349459099:263,down,down,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2990#issuecomment-349459099,1,['down'],['down']
Availability,"I'd be okay with this, except; 1. I don't currently know how to have the test watch messages go from WorkflowActor to CallActor without logging.; 2. The test should not assume the second start message would be sent from the WorkflowActor before the first CallActor picks up its start message and begins running, and possibly even completes. . Log scraping is supposed to be easy with TestKit, something like:. ```; EventFilter.error(message = ""some message"", occurrences = 1) intercept {; // do something which should trigger such a log message; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103311394:427,error,error,427,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103311394,1,['error'],['error']
Availability,"I'd be surprised if a one-line gitignore addition would cause all those errors, which from an n of 1, seem mostly like bad travis weather... 🤞 a restart will fix these",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617315302:72,error,errors,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5489#issuecomment-617315302,1,['error'],['errors']
Availability,"I'd like to bump this, we are running into this issue with cromwell-41 (and I am about to check cromwell-46) that when we have a workflow failure, the failure message appears in the server logs but is never copied to the workflow log. . Eg., ; Workflow Log (empty):; > cat workflow.5a34cc05-9f9a-40a0-8691-2b0eb49cdbc3.log. Server Log:; > grep -A3 5a34cc05-9f9a-40a0-8691-2b0eb49cdbc3 cromwell-2019-09-17.7566.log; 2019-09-25 15:59:21,689 cromwell-system-akka.dispatchers.engine-dispatcher-26816 ERROR - WorkflowManagerActor Workflow 5a34cc05-9f9a-40a0-8691-2b0eb49cdbc3 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Invalid value for File input 'GermlineMasterWF.trimseq.TRIMSEQ_paired.Adapters': empty value; Invalid value for File input 'GermlineMasterWF.trimseq.TRIMSEQ_single.Adapters': empty value; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-535552697:138,failure,failure,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-535552697,3,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,I'd suggest to avoid overlapping `database interaction` arrows with arrows of other types on the diagram. Something like this:; ![image](https://user-images.githubusercontent.com/4853242/68885542-5d3f6500-06e3-11ea-992e-ed9b6d0f3578.png). Looks like something happened with the error between `ServiceRegistryActor` and `HybridMetadataServiceActor`:; ![image](https://user-images.githubusercontent.com/4853242/68885905-0c7c3c00-06e4-11ea-9840-ddb24591df85.png),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554023387:278,error,error,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-554023387,1,['error'],['error']
Availability,"I'll post here the full error message of the failed task, ok? The full Workflow is huge and probably has sensitive information on other tasks. ```python; failures: [{; causedBy: [{; causedBy: [{; causedBy: [],; message: ""Task MakeAnalysisReadyBam.BaseRecalibrator:9:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 19: unexpected exit status 1 was not ignored [Delocalization] Unexpected exit status 1 while running "" / bin / sh - c retry() { for i in `seq 3`; do gsutil - h\ ""Content-Type: text/plain; charset=UTF-8\"" cp /cromwell_root/stdout gs://temporary-files/PET508-001/workspace/SingleSampleGenotyping/b67b285a-1f63-4514-b472-8618f1082470/call-ubam2bam/from_ubam.to_bam_workflow/4306b863-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-BaseRecalibrator/shard-9/ 2> gsutil_output.txt; RC_GSUTIL=$?; if [ \""$RC_GSUTIL\"" = \""1\"" ]; then\n grep \""Bucket is requester pays bucket but no user project provided.\"" gsutil_output.txt && echo \""Retrying with user project\""; gsutil -u bioinfo-prod -h \""Content-Type: text/plain; charset=UTF-8\"" cp /cromwell_root/stdout gs://temporary-files/PET508-001/workspace/SingleSampleGenotyping/b67b285a-1f63-4514-b472-8618f1082470/call-ubam2bam/from_ubam.to_bam_workflow/4306b863-7708-4627-babd-47017753d512/call-MakeAnalysisReadyBam/processing.MakeAnalysisReadyBam/ac5adb53-d888-4b9f-b062-48504e1a4853/call-BaseRecalibrator/shard-9/; fi ; RC=$?; if [ \""$RC\"" = \""0\"" ]; then break; fi; sleep 5; done; return \""$RC\""; }; retry"": Copying file: ///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/stdout [Content-Type=text/plain; charset=UTF-8]... / [0 files][ 0.0 B/ 76.3 KiB] ServiceException: 401 Requester pays bucket access requires authentication. Copying file:///cromwell_root/st",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865:24,error,error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435847865,4,"['error', 'failure']","['error', 'failures']"
Availability,"I'll start working with development, then. We don't have anything in; ""production"" that we cannot break. Thanks for the direction. On Aug 14, 2016 14:05, ""Jeff Gentry"" notifications@github.com wrote:. > Hi @seandavi https://github.com/seandavi - That does seem like it; > should work. Thinking back in my past I've definitely encountered tools; > which expect TMPDIR to exist and aren't smart enough to create it; > themselves. Also in JES there shouldn't be any issues with permissions, etc.; > ; > We'd certainly welcome a PR if you're game for it, either (or both); > against 0.19_hotfix or develop. On that note, I should point out that a; > new release (currently develop) is imminent and for all but one use case; > (call caching) we beliee it to be more robust/stable that 0.19. I'd; > personally recommend people who don't need call caching work with the new; > system, but I understand that some people aren't comfortable working with; > code which isn't yet released.; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687458,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/AAFpE17tmdKE5bY42PWUnqMW6YV6rifAks5qf1jugaJpZM4INzbb; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687994:761,robust,robust,761,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/731#issuecomment-239687994,1,['robust'],['robust']
Availability,"I'm attempting to run a HaplotypeCaller job that requires that the BAM and BAI file are in the same directory. However, for some reason Cromwell is putting the inputs into separate subdirectories. For example:; ```bash; $ tree cromwell-executions/trio/5cb01e4f-98a2-41d8-8946-a84e4e09291f/call-germline_variant_calling/shard-0/germline_variant_calling/5f7e982d-8f5d-4db4-bb08-9e47532da496/call-haplotype_caller/inputs; cromwell-executions/trio/5cb01e4f-98a2-41d8-8946-a84e4e09291f/call-germline_variant_calling/shard-0/germline_variant_calling/5f7e982d-8f5d-4db4-bb08-9e47532da496/call-haplotype_caller/inputs; ├── 1155873852; │   └── recal.bam.bai; ├── -170302265; │   └── recal.bam; └── 379983236; ├── cosmic_test.vcf.gz; ├── cosmic_test.vcf.gz.tbi; ├── exons.bed; ├── GenomeAnalysisTK.jar; ├── ucsc.hg19.dict; ├── ucsc.hg19.fasta; ├── ucsc.hg19.fasta.fai; └── ucsc.hg19.fasta.gz; ```. Thus, I get the error: `##### ERROR MESSAGE: Invalid command line: Cannot process the provided BAM/CRAM file(s) because they were not indexed. `. The relevant parts of my WDL (simplified for this example) are:. ```wdl; task process_bam {; input {; File bam; File bai; File gatk; File reference; Array[File] reference_indices; Array[File] realigner_knowns; Array[File] realigner_known_indices; Array[File] bqsr_knowns; Array[File] bqsr_known_indices; File intervals; }. command {; OUTPUT_DIR=`pwd`; cd /app; /app/process_bam_docker.py \; --bam ""${bam}"" \; --bai ""${bai}"" \; --gatk ""${gatk}"" \; --ref ""${reference}"" \; ${sep="" "" prefix(""--realigner-known "", realigner_knowns)} \; ${sep="" "" prefix(""--bqsr-known "", bqsr_knowns)} \; --intervals ""${intervals}"" \; --indel-realigner \; --output-dir ""$OUTPUT_DIR""; }. runtime {; docker: ""988908462339.dkr.ecr.ap-southeast-2.amazonaws.com/dx_process_bam:latest""; }. output {; File dedup_bam = glob('*dedup.bam')[0]; File dedup_matrix = glob('*dedup.metrics')[0]; File dedup_recal_bam = glob('*recal.bam')[0]; File dedup_recal_bai = glob('*recal.bam.bai')[0]; File dedup_r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4361:904,error,error,904,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4361,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I'm confused, I fixed the compile failure and merged with develop again and now there are more build failures - which do not appear to be consistent with each other?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-764996727:34,failure,failure,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6151#issuecomment-764996727,2,['failure'],"['failure', 'failures']"
Availability,"I'm consistently seeing this error, not the one in #4563 (which could be as simple as ""the files in question are very small in my case""). It appears to be looking for a file called `foo.log` where `foo` is the name of the task. So for instance `heightProduct-stderr.log` exists but `heightProduct.log` does not. I'm not certain where that's coming from",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-479192270:29,error,error,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-479192270,1,['error'],['error']
Availability,"I'm creating a new cromwell server, but running into a migration error with postgres.; ( PostgreSQL v9.2.24 on CentOS7); I can switch to mysql, but just wanted to report the error: . ```; 2019-07-21 23:07:06,634 INFO - Running with database db.url = jdbc:postgresql://localhost:5432/cromwell; 2019-07-21 23:07:13,702 INFO - SELECT COUNT(*) FROM public.databasechangeloglock; 2019-07-21 23:07:13,734 INFO - CREATE TABLE public.databasechangeloglock (ID INTEGER NOT NULL, LOCKED BOOLEAN NOT NULL, LOCKGRANTED TIMESTAMP WITHOUT TIME ZONE, LOCKEDBY VARCHAR(255), CONSTRAINT DATABASECHANGELOGLOCK_PKEY PRIMARY KEY (ID)); 2019-07-21 23:07:13,751 INFO - SELECT COUNT(*) FROM public.databasechangeloglock; 2019-07-21 23:07:13,755 INFO - DELETE FROM public.databasechangeloglock; 2019-07-21 23:07:13,762 INFO - INSERT INTO public.databasechangeloglock (ID, LOCKED) VALUES (1, FALSE); 2019-07-21 23:07:13,767 INFO - SELECT LOCKED FROM public.databasechangeloglock WHERE ID=1; 2019-07-21 23:07:13,778 INFO - Successfully acquired change log lock; 2019-07-21 23:07:17,932 INFO - Creating database history table with name: public.databasechangelog; 2019-07-21 23:07:17,934 INFO - CREATE TABLE public.databasechangelog (ID VARCHAR(255) NOT NULL, AUTHOR VARCHAR(255) NOT NULL, FILENAME VARCHAR(255) NOT NULL, DATEEXECUTED TIMESTAMP WITHOUT TIME ZONE NOT NULL, ORDEREXECUTED INTEGER NOT NULL, EXECTYPE VARCHAR(10) NOT NULL, MD5SUM VARCHAR(35), DESCRIPTION VARCHAR(255), COMMENTS VARCHAR(255), TAG VARCHAR(255), LIQUIBASE VARCHAR(20), CONTEXTS VARCHAR(255), LABELS VARCHAR(255), DEPLOYMENT_ID VARCHAR(10)); 2019-07-21 23:07:17,983 INFO - SELECT COUNT(*) FROM public.databasechangelog; 2019-07-21 23:07:17,985 INFO - Reading from public.databasechangelog; 2019-07-21 23:07:17,986 INFO - SELECT * FROM public.databasechangelog ORDER BY DATEEXECUTED ASC, ORDEREXECUTED ASC; 2019-07-21 23:07:17,987 INFO - SELECT COUNT(*) FROM public.databasechangeloglock; 2019-07-21 23:07:18,146 INFO - CREATE TABLE ""public"".""CALL_CACHIN",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:65,error,error,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,2,['error'],['error']
Availability,"I'm experiencing the issue, running the Broad ""gatk4-data-processing"" pipeline on their sample data, on Google Cloud. Repository with their code: https://github.com/gatk-workflows/gatk4-data-processing. The only change I made to the .wdl was setting Pre-emption to 0, although previous runs with ""3"" resulted in the the same error. I also doubled the size of the ""agg_large_disk"" to 800 GB, because I thought I was running out of space during merging, although the error seems consistent. Relevant log:. `PipelinesApiAsyncBackendJobExecutionActor [UUID(dba9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:11:1]: Status change from Running to Success; 2019-01-18 18:43:32,761 cromwell-system-akka.dispatchers.backend-dispatcher-362 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(dba9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:6:1]: Status change from Running to Success; 2019-01-18 18:43:33,255 cromwell-system-akka.dispatchers.engine-dispatcher-5 ERROR - WorkflowManagerActor Workflow dba9b85f-e9ea-4e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.sca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:325,error,error,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,2,['error'],['error']
Availability,"I'm fine with whatever regarding scalaz, but would appreciate a pull request with `UnionTypes` replacing `Either`, if someone doesn't mind contributing, or a pointer to example code. I understand the theory of right bias for map/flatmap/etc., but am fresh off another exploration down a rabbit hole and feeling unadventurous regarding set theory at this second. Regarding ""Voetize the name"", what's the full definition-- is that just fixing the negative, or is it expanding the abbreviation ""rc""? I changed the flag name to ""continueOnRc"" in the latest push, leaving ""rc"" consistent with the rest of the code. Will also add docs to the README.md once we settle on the name.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/192#issuecomment-142706999:280,down,down,280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/192#issuecomment-142706999,1,['down'],['down']
Availability,"I'm getting an error from Cromwell `Failed to coerce one or more keys or values for creating a Map[String, Int?]` when defining an object as an input to a task that consists of a string, file and int. I would expect the Int to get converted into a string, but the error makes it seem as if Cromwell is trying to convert all of the values into integers. Here is the part of the WDL that's throwing the error: https://github.com/HumanCellAtlas/pipeline-tools/blob/c949cb5ffa2df8f2a7fc7d7a4c34478e8eadbf34/adapter_pipelines/cellranger/adapter.wdl#L222",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4131:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4131,3,['error'],['error']
Availability,"I'm getting an error when trying to run the following WDL, which is using an `Object` type for the output of one of the tasks: https://github.com/HumanCellAtlas/pipeline-tools/blob/master/adapter_pipelines/smart_seq2/adapter.wdl#L46. This WDL previously worked in Cromwell 29. The WDL fails immediately on validation with this error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Object name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```. We're relying on objects in our HCA pipelines so it would be great if this could get fixed soon!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3060:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060,3,"['error', 'failure']","['error', 'failures']"
Availability,"I'm getting the same error when using; ```; output {; Array[File] files = glob(""*.txt""); }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-530285103:21,error,error,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-530285103,1,['error'],['error']
Availability,"I'm getting the same error with this wdl: [mutect2-replicate-validation.zip](https://github.com/broadinstitute/cromwell/files/2242528/mutect2-replicate-validation.zip). And here is the dependency file: [wdl-dependencies.zip](https://github.com/broadinstitute/cromwell/files/2242529/wdl-dependencies.zip). This happens in versions 30 and up. When I run this in v29, cromwell doesn't throw an error but it hangs after completing the first task. . tsato@gsa5:novaseq: java -jar $wom validate mutect2-replicate-validation.wdl; Exception in thread ""main"" java.lang.RuntimeException: This workflow contains a cyclic dependency on m2.Mutect2.filtered_vcf; 	at wdl.draft2.model.Scope.childGraphNodesSorted(Scope.scala:53); 	at wdl.draft2.model.Scope.childGraphNodesSorted$(Scope.scala:44); 	at wdl.draft2.model.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:46); 	at wdl.draft2.model.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:46); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:97); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomGraphMaker$.toWomGraph(WdlDraft2WomGraphMaker.scala:18); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$Ops.toWomGraph$(WomGraphMaker.scala:8); 	at wom.transforms.WomGraphMaker$ops$$anon$1.toWomGraph(WomGraphMaker.scala:8); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomWorkflowDefinitionMaker$.toWomWorkflowDefinition(WdlDraft2WomWorkflowDefinitionMaker.scala:14); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomWorkflowDefinitionMaker$.toWomWorkflowDefinition(WdlDraft2WomWorkflowDefinitionMaker.scala:10); 	at wom.transforms.WomWorkflowDefinitionMaker$Ops.toWomWorkflowDefinition(WomWorkflowDefinitionMaker.scala:8); 	at wom.transforms.WomWorkflowDefinitionMaker$Ops.toWomWorkflowDefinition$(WomWorkflowDefinitionMaker.scala:8); 	at wom.transforms.WomWorkflowDefinitionMaker$ops$$anon$1.toWomWorkflowDefinition(WomWorkflowDefinitionMak",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:21,error,error,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,2,['error'],['error']
Availability,I'm getting this error almost certainly when I run workflows where more samples (e.g. 96) than usual are scattered.; Cromwell version: 60-6048d0e-SNAP. Is there a workaround to this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-1244579032:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-1244579032,1,['error'],['error']
Availability,I'm going to close this since I found out that it's possible to drill down into the exception types and get a sorted frequency list within the Sentry UI itself.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4003#issuecomment-412942971:70,down,down,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4003#issuecomment-412942971,1,['down'],['down']
Availability,"I'm going to take a quick stab at grabbing the WFID, a thought I had last night is it'd be badass to suck down the centaur log and emit it directly. If it is indeed a pain I'll just merge this for now",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1286#issuecomment-239455401:106,down,down,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1286#issuecomment-239455401,1,['down'],['down']
Availability,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:1079,failure,failures,1079,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880,1,['failure'],['failures']
Availability,"I'm having some problems running the AWSBatch examples from the aws opendata page - https://docs.opendata.aws/genomics-workflows/aws-batch/configure-aws-batch-start/ - as far as I can tell I have correctly set the permissions, however when I try running the examples I get the error below. It looks like the output files aren't being written the S3 bucket. . I'm probably missing something obvious, but I can't work out what I'm doing wrong. Would anyone have any suggestions for where might be good to look for errors?. ```[2018-10-31 10:24:09,16] [[38;5;1merror[0m] WorkflowManagerActor Workflow b7e4cdce-ff14-4509-aec3-b226ed31043c failed (during ExecutingWorkflowState): cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt: s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt; Caused by: java.io.IOException: Could not read from s3://concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt: s3://s3.amazonaws.com/concr-genomics-results/cromwell-execution/wf_hello/b7e4cdce-ff14-4509-aec3-b226ed31043c/call-hello/hello-rc.txt; 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:146); 	at cromwell.engine.io.nio.NioFlow$$anonfun$withReader$2.applyOrElse(NioFlow.scala:145); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at cromwell.engine.io.nio.NioFlow.withReader(NioFlow.scala:145); 	at cromwell.engine.io.nio.NioFlow.limitFileContent(NioFlow.scala:154); 	at cromwell.engine.io.nio.NioFlow.$anonfun$readAsString$1(NioFlow.scala:98); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85); 	at cats.effect.internals.IO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4341:277,error,error,277,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4341,2,['error'],"['error', 'errors']"
Availability,"I'm having the same error, but only when I change the location to anything other than us-central1; Even with a bucket located in europe-west4 and zones set to europe-west4, it only works with location set to us-central1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469#issuecomment-921690965:20,error,error,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469#issuecomment-921690965,1,['error'],['error']
Availability,"I'm having trouble getting call caching to work with Singularity and SGE, and I'm wondering if anyone has a working example config or some pointers. My config is below, minus passwords and specific paths/urls, which I've replaced with a label encased in <>. I've tried switching to slower hashing strategies finagling with the command construction to no avail. If there's not an obvious solution, is there an easy way to debug this? There are no network issues preventing connections to dockerhub - pulling images and converting to .sif works fine. It's only call caching that's broken. Even when I see, in the metadata, identical hashes for the docker image and all inputs and outputs, I see a ""Cache Miss"" as the result, every time. . The call caching stanza in my metadata looks like this, for example. Am I missing something? ; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hashes"": {; ""output count"": ""C4CA4238A0B923820DCC509A6F75849B"",; ""runtime attribute"": {; ""docker"": ""4B2AB7B9EA875BF5290210F27BB9654D"",; ""continueOnReturnCode"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""failOnStderr"": ""68934A3E9455FA72420237EB05902327""; },; ""output expression"": {; ""File output_greeting"": ""DFC652723D8EBD4BB25CAC21431BB6C0""; },; ""input count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""backend name"": ""2A2AB400D355AC301859E4ABB5432138"",; ""command template"": ""AFAC58B849BD67585A857F538B8E92F6""; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hit"": false,; ""result"": ""Cache Miss""; },; ```. ```; # simple sge apptainer conf (modified from the slurm one); #; workflow-options; {; workflow-log-dir: ""cromwell-workflow-logs""; workflow-log-temporary: false; workflow-failure-mode: ""ContinueWhilePossible""; default; {; workflow-type: WDL; workflow-type-version: ""draft-2""; }; }. database {; # Store metadata in a file on disk that can grow much larger than RAM limits.; metadata {; profile = ""slick.jdbc.MySQLProfile$""; db {; url = ""jdbc:mysql:<dburl>?rewriteBatchedStatements=true""; driver = ""com.mysql.cj.jdb",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7480:354,avail,avail,354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7480,1,['avail'],['avail']
Availability,"I'm just reading a bit [more](https://linux.die.net/man/1/flock) about `flock` - so with `-x` the idea is that you are creating a write lock - this means that this particular script will be run by many workers, and the first that gets to creating the lock will get it, continue, and do the build. ```; -s, --shared; Obtain a shared lock, sometimes called a read lock.; -x, -e, --exclusive; Obtain an exclusive lock, sometimes called a write lock. This is the default.; ```; I'm wondering - what would be the return code for the second worker that cannot create the lock? What in the above says ""Try to make the lock, if it doesn't work, come back and try again (and do this until the container is available.) . Overall I think this is a really important thing to think about - aside from cluster resources, if you are pulling an image from a remote registry, that might have negative consequences for the registry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509638085:697,avail,available,697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509638085,1,['avail'],['available']
Availability,"I'm leery but not necessarily against it. I'm also generally the one with the most conservative opinion in terms of adding WDL syntax, so view that take as a lower bound of acceptance :). @patmagee what were you thinking in terms of the syntax?. Pinging @vdauwera so she's abreast of this convo.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-325851732:246,Ping,Pinging,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2425#issuecomment-325851732,1,['Ping'],['Pinging']
Availability,"I'm not 100% convinced that scaling is completely wired. Have you confirmed that this increase is also affecting your test timeouts? The linked error from #4223 says:. > Attempted 13 times over 5.126574189 seconds. That's *way* too short for something that should have already been scaled 12x. I'm in the process of [adding some println-debugging](https://github.com/broadinstitute/cromwell/commit/29d2f35662d6a81a4de383ad54df4ee0242611a4) on a similar issue. In my case I suspect one of the many, many timeouts wasn't scaled for an akka `.ask`, but will have to wait until the docker network issues are resolved to find out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430412625:144,error,error,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430412625,1,['error'],['error']
Availability,"I'm not a specialist of GATK but this error doesn't seem like a big deal, it looks like it's defaulting to another logging mode. Maybe @vdauwera would know more ?; If you `ps -elf | grep java` is there anything still running in this docker container ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-367368011:38,error,error,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-367368011,1,['error'],['error']
Availability,"I'm not sure how miniwdl handles this, I'm just testing this one on Cromwell for now. But putting aside the spec, miniwdl, and Cromwell here, when I write some sort of code in any particular language, I always expect at least one of these to work:. * `if defined(variable) then do_something(variable)`; * `if type(variable) is not None then do_something(variable)` or its close cousin `if variable is not None then do_something(variable)`; * `if exists(file) then do_something(file)`. In Cromwell-flavored WDL (perhaps all WDL?), it doesn't seem you can do any of those. The first one will throw an ""Expected X but got X?"" error and the other two don't seem to have equivalents. Compare that to Python, where I can explicitly do the second or third one, and implicitly do the first one. In Python, if I try to do_something() on a variable that isn't defined, Python throws a Name Error, but in Cromwell!WDL trying to do_something() on an optional throws a ""Expected X but got X?"" error that doesn't tell you if X is actually defined or not. The fact that the WDL spec doesn't explicitly say that defined() can coerce a variable doesn't really matter -- I would wager that most people would expect this sort of thing to work. It seems to be a logical conclusion that if a file exists, you can do something to that file, without having to call a totally different function to create a new variable. I did check your workaround, but it throws the same error. So, my understanding is the only way to do this in Cromwell is this:. ```; String basename_tsv = basename(select_first([tsv_file_input, ""bogus fallback value""])); String arg_tsv = if(basename_tsv == ""bogus fallback value"") then """" else basename_tsv; ```. ...which just isn't intuitive. It shouldn't be so complicated to get the basename of an optional file. . Even less intuitive is the fact that if you separate out the select_first() part into a new variable, my workaround doesn't work anymore. ```; String maybe_tsv = select_first([tsv_file_",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354:623,error,error,623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245893354,3,"['Error', 'error']","['Error', 'error']"
Availability,"I'm not sure the exact scenario people would want to use this in, but it seems to replace the previous behavior ""retry a few times for known flakinesses, otherwise fail"" - with a less tuned ""retry a custom number of times for all codes"". * Is there any possibility that users would want to continue retrying on known flakinesses (hint: I really think they do) but not blindly restart on other error codes (which could be expensive for typos!)?; ---; One other thought:; * This seems to only catch JES telling us the task has failed. If the task on JES ""succeeds"" but Cromwell later finds out that it cannot download or evaluate a result, does this still retry?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3490#issuecomment-379274281:393,error,error,393,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3490#issuecomment-379274281,2,"['down', 'error']","['download', 'error']"
Availability,"I'm noticing that in the outputs section of a task, that the basename is automatically applied. For example, running the following workflow:. ```wdl; version development. task basenametest {; input {; 	File inp; }. command <<<; echo '~{basename(inp)}'; >>>; ; output {; String outname = inp; String outbasename = basename(inp); String out = read_string(stdout()); }; }; ```. with:. ```bash; echo '{""inp"": ""inputs.json""}' >> inputs.json; java -jar cromwell-52.jar run basenametest.wdl --inputs ""inputs.json""; ```. I receive the output:. ```json; {; ""outputs"": {; ""escapetest.outname"": ""test.bam"",; ""escapetest.out"": ""test.bam"",; ""escapetest.outbasename"": ""test.bam""; },; ""id"": ""<id>""; }; ```. ---. I think this might be logical, but differs from the spec and MiniWDL. I've put a message in the OpenWDL slack for clarification, but through me off a bit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5785:228,echo,echo,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5785,2,['echo'],['echo']
Availability,"I'm really not a big fan of copy/pasting huge chunks of the swagger doc over and checking it into the repo separately. Is there an option in codegen to target only a subset of the endpoints? If not, is there a way to derive this cut-down swagger from the main one before running codegen over it? (eg programmatically parse the yaml, select only the womtool sections and re-write just that section to a file?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5038#issuecomment-505027777:233,down,down,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5038#issuecomment-505027777,1,['down'],['down']
Availability,"I'm running a modified version of the [paired-fastq-to-unmapped-bam](https://github.com/gatk-workflows/seq-format-conversion/blob/master/paired-fastq-to-unmapped-bam.wdl) workflow on Cromwell 36 running on AWS. However, it seems to fail at `write_lines()`. Any idea why this might be happening? Is there flaw in the AWS implementation of this function? I can post my modified WDL if needed but all that's different from the official workflow that I have removed the `disks` blocks, since they fail on AWS (see #4274); ```; 2018-10-18 06:51:48,151 cromwell-system-akka.dispatchers.backend-dispatcher-3190 ERROR - Failed command instantiation; java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:565); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:500); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.instantiatedCommand$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.instantiatedCommand(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:313); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:312); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.commandScriptContents(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.batchJob$lzycompute(AwsBatchAsyncBackendJobExecutionActor.scala:132); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.batchJob(AwsBatchAsyncBackendJobExecutionActor.scala:131); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeAsync(AwsBatchAsyncBackendJobExecutionActor.scala:342); at cromwell.backend.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275:604,ERROR,ERROR,604,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275,1,['ERROR'],['ERROR']
Availability,"I'm running a pipeline and for some of the tasks, sometime I get the following error:. `[E::hts_open_format] Failed to open file ...`. What I find weird is that if I re-run it, it runs successfully. It looks a ""stochastic"" error. Below you can find the full logs for that task and, as you can see, the file was successfully localized. ```; timestamp,message; 1608596940672,*** LOCALIZING INPUTS ***; 1608596942260,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-47/cacheCopy/SR00c.HG02019.txt.gz.tbi; 1608596944807,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-81/cacheCopy/SR00c.HG03449.txt.gz; 1608596946491,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-94/cacheCopy/SR00c.HG03789.txt.gz.tbi to focal-gwf-core/cromwell-ex",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:79,error,error,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,3,"['down', 'error']","['download', 'error']"
Availability,"I'm running cromwell 46 with AWS, and having problems with call caching ... 2019-09-30 15:37:20,124 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Failed to hash ""s3://bdtx-scratch/Andrei/bdtx_dataset_1.00_anno_column_description.txt"": [Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null); 2019-09-30 15:37:20,125 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - 66419bab:count_lines.countLines:-1:1: Hash error ([Attempted 1 time(s)] - S3Exception: null (Service: S3, Status Code: 301, Request ID: null)), disabling call caching for this job. call caching settings:; call-caching {; enabled = true; invalidate-bad-cache-results = false; }. Thanks. ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5204:154,ERROR,ERROR,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5204,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I'm running gatk-sv workflows with AWS backend and I'm facing some issues on scatter tasks. It seems that for some of the tasks, the reconfigured-script is bad constructed. Below is an excerpt from the script:. ```bash; #!/bin/bash. {; echo '*** LOCALIZING INPUTS ***'; if [ ! -d /tmp/scratch ]; then mkdir /tmp/scratch && chmod 777 /tmp/scratch; fi; cd /tmp/scratch; /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-48/SR00c.NA20802.txt.gz.tbi /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-48/SR00c.NA20802.txt.gz.tbi; /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-31/SR00c.NA19661.txt.gz /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-31/SR00c.NA19661.txt.gz; /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-32/SR00c.NA19678.txt.gz.tbi /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-32/SR00c.NA19678.txt.gz.tbi; /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6106:236,echo,echo,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6106,1,['echo'],['echo']
Availability,"I'm running the ENCODE ATAC SEQ pipeline [https://github.com/ENCODE-DCC/atac-seq-pipeline.git](url) on a SGE cluster.; We don't allow hard-links in my facility (beegfs filesystem). Therefore I've been trying to use the localization parameters in the cromwell configuration file but to no avail. The backend file is being used since I can get errors message by putting non supported keyword in the localization array. I've been trying it with different version of CROMWELL (30.2, 31, 32, 32). Here is the script generated by cromwell based on my WDL file :. ```cd /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution; # make the directory which will keep the matching files; mkdir /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2. # symlink all the files into the glob directory; ( ln -L merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:288,avail,avail,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,2,"['avail', 'error']","['avail', 'errors']"
Availability,"I'm seeing the `detect_sv` tool in the somatic workflow fail with this error (from stderr):. ```; [2018-11-04T19:02:19.372170Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Failed to complete master workflow, error code: 1; [2018-11-04T19:02:19.372320Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] errorMessage:; [2018-11-04T19:02:19.373700Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Unhandled Exception in TaskRunner-Thread-masterWorkflow; [2018-11-04T19:02:19.373750Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Traceback (most recent call last):; [2018-11-04T19:02:19.373786Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1069, in run; [2018-11-04T19:02:19.373812Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] (retval, retmsg) = self._run(); [2018-11-04T19:02:19.373833Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1121, in _run; [2018-11-04T19:02:19.373871Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] self.workflow.workflow(); [2018-11-04T19:02:19.373894Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:71,error,error,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,10,"['ERROR', 'error']","['ERROR', 'error', 'errorMessage']"
Availability,I'm seeing this error on my builds too so I don't think it's related to your changes. I'll keep investigating.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575293472:16,error,error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-575293472,1,['error'],['error']
Availability,"I'm seeing tons of these info logging messages from the checkpointing system (shown below). Is there a way to turn them off or set them to a 'debug' rather than an 'info' logging level?. [2021-05-06 08:27:27,34] [info] Checkpoint start; [2021-05-06 08:27:27,34] [info] checkpointClose start; [2021-05-06 08:27:27,34] [info] checkpointClose synched; [2021-05-06 08:27:27,34] [info] checkpointClose script done; [2021-05-06 08:27:27,34] [info] dataFileCache commit start; [2021-05-06 08:27:27,34] [info] dataFileCache commit end; [2021-05-06 08:27:27,36] [info] checkpointClose end; [2021-05-06 08:27:27,36] [info] Checkpoint end - txts: 6340; [2021-05-06 08:27:27,36] [info] Checkpoint start; [2021-05-06 08:27:27,36] [info] checkpointClose start; [2021-05-06 08:27:27,36] [info] checkpointClose synched; [2021-05-06 08:27:27,37] [info] checkpointClose script done; [2021-05-06 08:27:27,37] [info] dataFileCache commit start; [2021-05-06 08:27:27,37] [info] dataFileCache commit end; [2021-05-06 08:27:27,39] [info] checkpointClose end; [2021-05-06 08:27:27,39] [info] Checkpoint end - txts: 6347; [2021-05-06 08:27:27,39] [info] Checkpoint start; [2021-05-06 08:27:27,39] [info] checkpointClose start; [2021-05-06 08:27:27,39] [info] checkpointClose synched; [2021-05-06 08:27:27,39] [info] checkpointClose script done; [2021-05-06 08:27:27,39] [info] dataFileCache commit start; [2021-05-06 08:27:27,39] [info] dataFileCache commit end; [2021-05-06 08:27:27,41] [info] checkpointClose end; [2021-05-06 08:27:27,41] [info] Checkpoint end - txts: 6349; [2021-05-06 08:27:27,41] [info] Checkpoint start; [2021-05-06 08:27:27,41] [info] checkpointClose start; [2021-05-06 08:27:27,41] [info] checkpointClose synched; [2021-05-06 08:27:27,41] [info] checkpointClose script done; [2021-05-06 08:27:27,41] [info] dataFileCache commit start; [2021-05-06 08:27:27,41] [info] dataFileCache commit end; [2021-05-06 08:27:27,42] [info] checkpointClose end; [2021-05-06 08:27:27,42] [info] Checkpoint end - txts: ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6337:56,checkpoint,checkpointing,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6337,1,['checkpoint'],['checkpointing']
Availability,"I'm still seeing the same error even when I add the `docker.io/` prefix. Confirming the correct command in `gcloud batch job describe`:. ```; ""printf '%s %s\\n' \""$(date -u '+%Y/%m/%d %H:%M:%S')\"" Running\\ user\\ runnable:\\ docker\\ run\\ -v\\ /mnt/disks/cromwell_root:/mnt/disks/cromwell_root\\ --entrypoint\\=/bin/bash\\ docker.io/broadinstitute/cloud-cromwell@sha256:0d51f90e1dd6a449d4587004c945e43f2a7bbf615151308cff40c15998cc3ad4\\ /mnt/disks/cromwell_root/script""; ```. Also it would be good not to require a `docker.io/` prefix as our users would need to edit all of their WDLs referencing Docker Hub images to be able to run on GCP Batch.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2318997035:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2318997035,1,['error'],['error']
Availability,"I'm still slowly investigating, but it looks like the scale factor environment variable isn't `export`ed, thus isn't available to `sbt`. Been playing around with the println's in a433a7f20a74faf70a1ec851545f0e9ec6836ce4 ([jenkins](https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/555/consoleFull)) and 7fc56d3b73ce537b47aaecc6bf9cd0f1c020646f ([jenkins](https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/557/consoleFull)).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430454596:117,avail,available,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4261#issuecomment-430454596,1,['avail'],['available']
Availability,"I'm thinking perhaps this should be address instead through the use of docker User Namespaces and described in the documentation (a critical part of the solution!). When docker runs a container as root, it isolates the user/groups from the host definitions and instead remaps them. This could be important to a user for two reasons. Once, files written as root in the container will instead be written as this remapped user. Second, since IO on the underlying host VM is currently done as root, the container can read files as root. So if you mounted in /etc/passwd you could read/write on top of that. For a good tutorial see:. E.g. http://blog.aquasec.com/docker-1.10-user-namespace. For more details see:. https://docs.docker.com/engine/reference/commandline/dockerd/#daemon-user-namespace-options. Also pinging @davidbernick for more thoughts from a security perspective",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2053#issuecomment-284430367:807,ping,pinging,807,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2053#issuecomment-284430367,1,['ping'],['pinging']
Availability,"I'm trying it with 16g right now. It still slows down abruptly after completing the first task inside of the scatter. The first task is fast, but then the second task (which depends on the first) is slower.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276771289:49,down,down,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-276771289,1,['down'],['down']
Availability,"I'm trying to run Cromwell-48 on [Sherlock](https://www.sherlock.stanford.edu/), a HPC running SLURM. . I have this WDL job:; ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```. When I try to run:; ```; > java -jar ~/cromwell/cromwell-48.jar run echoHello.wdl; [2020-01-28 18:31:30,49] [info] Running with database db.url = jdbc:hsqldb:mem:15405fc3-f9d1-4db3-a492-6b12dfb77913;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:37,96] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2020-01-28 18:31:37,98] [info] [RenameWorkflowOptionsInMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBack",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5395:196,echo,echo,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395,2,['echo'],"['echo', 'echoHello']"
Availability,"I'm trying to run a WDL that examines disk sizes using `df -kh`. Unfortunately, read_object is not able to parse the output which appears to match the spec:; The WDL:. ```; task df_kh {; Int bootDiskSizeGb; command { df -kh / }; runtime {; docker: ""ubuntu:latest""; bootDiskSizeGb: bootDiskSizeGb; }; output {; Int size = read_object(stdout()).Size[0]; }; }. workflow someBootDisks {; call df_kh as smallBootDisk { input: bootDiskSizeGb = 10 }; call df_kh as bigBootDisk { input: bootDiskSizeGb = 50 }; }; ```. The console output (example):. ```; Filesystem Size Used Avail Use% Mounted on; /dev/disk1 465G 70G 396G 15% /; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/615:567,Avail,Avail,567,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/615,1,['Avail'],['Avail']
Availability,"I'm trying to use `quay.io/biocontainers/platypus-variant:0.8.1.1--htslib1.5_0`, but I'm getting the following error:. ```; [ERROR] [08/31/2017 20:01:21.193] [cromwell-system-akka.dispatchers.engine-dispatcher-7] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManage; rActor] WorkflowManagerActor Workflow 73494fe9-ef7b-45e0-b982-b50baf3281d7 failed (during ExecutingWorkflowState): Docker image quay.io/biocontainers/platypus-variant:0.8.1.1--htslib1.5_0 has an invalid syntax.; java.lang.IllegalArgumentException: Docker image quay.io/biocontainers/platypus-variant:0.8.1.1--htslib1.5_0 has an invalid syntax.; at cromwell.docker.DockerImageIdentifier$.fromString(DockerImageIdentifier.scala:60); at cromwell.engine.workflow.lifecycle.execution.preparation.JobPreparationActor.handleDockerValue$1(JobPreparationActor.scala:112); at cromwell.engine.workflow.lifecycle.execution.preparation.JobPreparationActor.cromwell$engine$workflow$lifecycle$execution$preparation$JobPreparatio; nActor$$fetchDockerHashesIfNecessary(JobPreparationActor.scala:129); at cromwell.engine.workflow.lifecycle.execution.preparation.JobPreparationActor$$anonfun$1.applyOrElse(JobPreparationActor.scala:57); at cromwell.engine.workflow.lifecycle.execution.preparation.JobPreparationActor$$anonfun$1.applyOrElse(JobPreparationActor.scala:54); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:663); at akka.actor.FSM.processEvent$(FSM.scala:660); at cromwell.engine.workflow.lifecycle.execution.preparation.JobPreparationActor.processEvent(JobPreparationActor.scala:33); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor.aroundReceive(Actor.scala:513); at akka.actor.Actor.aroundReceive$(Actor.scala:511); at cromwell.engine.workflow.lifecycle.execution.preparation.JobPreparationActor.aroundReceive(JobPreparationActor.scala:33); at akka.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2589:111,error,error,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2589,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I'm uping this as we've seen CloudSQL connection failures recently resulting in a variety of inconsistencies in workflow statuses, metadata and generally DB state.; This is going to be even more important as we move towards CaaS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2016#issuecomment-307434426:49,failure,failures,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2016#issuecomment-307434426,1,['failure'],['failures']
Availability,"I'm using [CaaS-dev](https://cromwell.caas-dev.broadinstitute.org/) when ran into the issue, with version: `34-66c0fa6-SNAP`. When using the `releaseHold` endpoint to release a workflow, if the workflow_id is in a good format but refers to a non-exist workflow (fake uuid), according to the swagger schema, Cromwell should return 404 to the user. However, now it returns a 500 code which seems to be a wrapper around the actual 404 error:; ```; CromIAM unexpected error: cromwell.api.CromwellClient$UnsuccessfulRequestException: Unmarshalling error: HttpResponse(404 Not Found,List(Server: akka-http/10.1.3, Date: Fri, 20 Jul 2018 13:58:13 GMT),HttpEntity.Strict(text/plain; charset=UTF-8,{; ""status"": ""fail"",; ""message"": ""Unrecognized workflow ID: f4272a19-37dd-4d2c-ba48-ff3844107bf8""; }),HttpProtocol(HTTP/1.1)); ```; This is not a big problem but just brings some incovenience to the error handling process to the users.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3911:432,error,error,432,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3911,4,['error'],['error']
Availability,"I'm using `tar` to create `*.zip` like below on a Mac OS; ```; tar -czvf workflow.zip sub_wf.wdl; ```. Also, if you are running a local server, I'm assuming there's a place where zip file is downloaded and unzip to? Do you know where on local machine that is running server does that zip file get unzipped to?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4509#issuecomment-451260898:191,down,downloaded,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4509#issuecomment-451260898,1,['down'],['downloaded']
Availability,I'm using v28. The jar was downloaded from this page; https://github.com/broadinstitute/cromwell/releases/tag/28,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2487#issuecomment-317861750:27,down,downloaded,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2487#issuecomment-317861750,1,['down'],['downloaded']
Availability,"I'm working on mutation calling based on cromwell, the **Failed to summarize metadata** comes out for several shards in the scatter, then the following processes are aborted. How to fixed this error?. ```; [2018-11-17 09:04:45,38] [info] BackgroundConfigAsyncJobExecutionActor [3df56d2bPreProcessingForVariantDiscovery_GATK4.MarkDuplicates:5:1]: job id: 56011; [2018-11-17 09:04:45,48] [info] BackgroundConfigAsyncJobExecutionActor [3df56d2bPreProcessingForVariantDiscovery_GATK4.MarkDuplicates:5:1]: Status change from - to WaitingForReturnCodeFile; [2018-11-17 09:37:07,47] [error] Failed to summarize metadata; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 3785ms.; 	at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145); 	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:14); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:453); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:46); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:249); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:248); 	at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:274); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); [2018-11-17 09:37:14,33] [error] Error summarizing metadata; java.sql.SQLTransientConnectionException: db - Connecti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403:193,error,error,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403,3,"['avail', 'error']","['available', 'error']"
Availability,"I'm working on the [Workbench counterpart](https://broadinstitute.atlassian.net/browse/GAWB-1704) to this. I can reproduce an alternate version of this error on Production FireCloud, using Cromwell 25. This is what seems to be happening:; * write_lines creates two temp files, (a) with unlocalized file paths as described above, and (b) with correctly localized file paths; * file (a) gets localized; * file (b) does not get localized; * the script correctly looks for file (b) but can't find it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-293401783:152,error,error,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-293401783,1,['error'],['error']
Availability,"I'm writing a workflow that I want to run with and without using docker. The workflow has an optional docker input that is passed to each task for the docker entry in the runtime section. When I don't provide a value for docker, I get the error:; java.lang.IllegalArgumentException: Docker image has an invalid syntax. . when what I want is for the task to run without a docker. Thanks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5947:239,error,error,239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5947,1,['error'],['error']
Availability,"I've always known deep down that I was born in the wrong century, wait millennium !",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/471#issuecomment-188452106:23,down,down,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/471#issuecomment-188452106,1,['down'],['down']
Availability,"I've attached two workflows that are essentially the same, one that conforms to draft-2 and the other that conforms to 1.0. The former runs fine (Cromwell v40), the latter one fails. It looks like there's a regression in handling the following situation:. ```wdl; task bar {; input {; Array[Array[String]]? baz; }; command <<<; echo ~{ if defined(baz) then write_tsv(baz) else 'no file' }; >>>; ...; }. Error:. ```java; Failed to process task definition 'bar' (reason 1 of 1): Failed to process expression 'if defined(baz) then write_tsv(baz) else """"' (reason 1 of 1): Invalid parameter 'IdentifierLookup(baz)'. Expected 'Array[Array[String]]' but got 'Array[Array[String]]?'; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:684); at akka.actor.FSM.processEvent$(FSM.scala:681); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.LoggingFSM.processEvent(FSM.scala:820); at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:136); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:672); at akka.actor.A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4981:328,echo,echo,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4981,2,"['Error', 'echo']","['Error', 'echo']"
Availability,I've been getting some failures on an unrelated workflow in the jenkins test. Looking to see if this change somehow caused a regression or if we're just in for a world of hurt once our nightly kicks off tonight,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4972#issuecomment-492804573:23,failure,failures,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4972#issuecomment-492804573,1,['failure'],['failures']
Availability,"I've been getting the following GATK error when running a CWL pipeline on Cromwell:; ```; ##### ERROR MESSAGE: Writing failed because there is no space left on the disk or hard drive. Please make some space or specify a different location for writing output files.; ``` ; To attempt to resolve this, I added the following requirement to the GATK tool in question:; ```yaml; - class: ResourceRequirement; coresMin: 2; ramMin: 8000; tmpdirMin: 100000; outdirMin: 100000; ```. However, this didn't resolve the GATK issue. I looked further into the Cromwell source and I found the following code: https://github.com/broadinstitute/cromwell/blob/44297611175c8a2a92ee71f0fa9c34419b69f0b8/cwl/src/main/scala/cwl/requirement/RequirementToAttributeMap.scala#L46-L57. To me, it looks like these values are being thrown out (I could be wrong). . Should we not convert these into a [`disk` statement](https://cromwell.readthedocs.io/en/develop/RuntimeAttributes/#disks) that Cromwell can understand and implement?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4507:37,error,error,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4507,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"I've been playing around with different settings and testing this out a lot. I like it!. I think I might have found a bug with writing to GCS buckets. I'm using this as my options file:. ```; {; ""workflow_log_dir"": ""gs://sfrazer-dev/foobar"",; ""call_logs_dir"": ""gs://sfrazer-dev/foobar/calls""; }; ```. And I ran a workflow both locally and with JES. In both cases it seemed to write the call logs just fine. However, it seems the workflow log got lost somewhere. It did, however, create a file `gs://sfrazer-dev/foobar` with the contents of the file being `foobar`. I also got this odd error message. Indeed the stderr file that was uploaded was zero bytes. However, it is supposed to be zero bytes! I'll try again and send something else to stderr. Might just be a spurious error when your files happen to be zero bytes... ```; [2016-03-04 10:02:32,132] [info] JesBackend [7beff6f6]: Trying to copy output file gs://cromwell-dev/w/7beff6f6-0b44-4b34-8b20-49868740f235/call-arr/arr-stderr.log to gs://sfrazer-dev/foobar/calls/w/7beff6f6-0b44-4b34-8b20-49868740f235/call-arr/arr-stderr.log; [2016-03-04 10:02:34,490] [info] Got 'range not satisfiable' for reading gs://cromwell-dev/w/7beff6f6-0b44-4b34-8b20-49868740f235/call-arr/arr-stderr.log at position 0; assuming empty.; [2016-03-04 10:02:34,692] [info] JesBackend [7beff6f6]: Trying to copy output file gs://cromwell-dev/w/7beff6f6-0b44-4b34-8b20-49868740f235/call-arr/arr.log to gs://sfrazer-dev/foobar/calls/w/7beff6f6-0b44-4b34-8b20-49868740f235/call-arr/arr.log; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-192314886:585,error,error,585,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/468#issuecomment-192314886,2,['error'],['error']
Availability,"I've been running the 3step workflow on JES. It gets to this point and stops. ```; [2016-07-13 12:52:24,77] [info] WorkflowActor-59abeeeb-eb34-49c6-83d0-c806e6968cb1 [59abeeeb]: transition from FinalizingWorkflowState to WorkflowSucceededState: shutting down; [2016-07-13 12:52:24,77] [info] WorkflowManagerActor Workflow 59abeeeb-eb34-49c6-83d0-c806e6968cb1 succeeded!; [2016-07-13 12:52:24,77] [info] WorkflowManagerActor WorkflowActor-59abeeeb-eb34-49c6-83d0-c806e6968cb1 is in a terminal state: WorkflowSucceededState; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1157:254,down,down,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1157,1,['down'],['down']
Availability,"I've been seeing Travis failures in `MainSpec` ""run reading options"" with maybe 30% consistency while working on this, but that's testing an exception being thrown before the WorkflowActor is even created and I don't believe would have been perturbed by my changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/277#issuecomment-154524208:24,failure,failures,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/277#issuecomment-154524208,1,['failure'],['failures']
Availability,"I've been trying to shepherd these tests through. The previous failure was fixed by CROM-6890 work, and we're now getting a new failure that looks like CROM-6872.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6724#issuecomment-1091722003:63,failure,failure,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6724#issuecomment-1091722003,2,['failure'],['failure']
Availability,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:769,down,download,769,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238,1,['down'],['download']
Availability,"I've downloaded the new jar file, still showing version 30.2, but still seeing the problem in some situations:. 2018-02-21 11:03:39,563 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - Abort requested for workflow f0bff6e2-77a6-46f5-b226-13a64339a286.; 2018-02-21 11:03:39,564 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - WorkflowExecutionActor-f0bff6e2-77a6-46f5-b226-13a64339a286 [UUID(f0bff6e2)]: Aborting workflow; 2018-02-21 11:03:39,567 cromwell-system-akka.dispatchers.engine-dispatcher-50 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Several SGE queue jobs continue to run/stay in the queue waiting state. Terminating the server with a ctrl-C does not affect the queued jobs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3325:5,down,downloaded,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3325,1,['down'],['downloaded']
Availability,"I've downloaded the new jar file, still showing version 30.2, but still seeing the problem in some situations:. 2018-02-21 11:03:39,563 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - Abort requested for workflow f0bff6e2-77a6-46f5-b226-13a64339a286.; 2018-02-21 11:03:39,564 cromwell-system-akka.dispatchers.engine-dispatcher-95 INFO - WorkflowExecutionActor-f0bff6e2-77a6-46f5-b226-13a64339a286 [UUID(f0bff6e2)]: Aborting workflow; 2018-02-21 11:03:39,567 cromwell-system-akka.dispatchers.engine-dispatcher-50 WARN - unhandled event EngineLifecycleActorAbortCommand in state SubWorkflowRunningState. Several SGE queue jobs continue to run/stay in the queue waiting state. Terminating the server with a ctrl-C does not affect the queued jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3259#issuecomment-367435337:5,down,downloaded,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3259#issuecomment-367435337,1,['down'],['downloaded']
Availability,"I've found at least three kinds of brokenness here:; 1. Cromwell never sends a `WorkflowManagerAbortSuccess` from the `WorkflowManagerActor` to the API handler, so the response to the abort POST is never completed on a successful abort.; 2. If the `WorkflowManagerActor` is asked to abort a workflow ID it doesn't recognize it throws a `WorkflowNotFoundException` which eventually completes the abort request with a 404. Unfortunately the decoupling between the `WorkflowStore` and the `WorkflowManagerActor` introduces a gaping race condition where a legitimate workflow ID may not be known to the `WorkflowManagerActor` for a long time after that ID has been returned to the submitter.; 3. There's a third failure mode where the abort request seems to be ignored with various error and warning messages and jobs just keep running:. ```; 2016-09-09 15:50:44,628 cromwell-system-akka.actor.default-dispatcher-7 INFO - Workflow aed1aad8-588d-4f84-aa09-da0f663d68c0 submitted.; 2016-09-09 15:50:56,111 cromwell-system-akka.actor.default-dispatcher-15 INFO - 1 new workflows fetched; 2016-09-09 15:50:56,112 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Starting workflow UUID(aed1aad8-588d-4f84-aa09-da0f663d68c0); 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - WorkflowManagerActor Successfully started WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0; 2016-09-09 15:50:56,116 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2016-09-09 15:50:56,175 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - WorkflowActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; 2016-09-09 15:50:56,261 cromwell-system-akka.dispatchers.engine-dispatcher-22 INFO - MaterializeWorkflowDescriptorActor-aed1aad8-588d-4f84-aa09-da0f663d68c0 [UUID(aed1aad8)]: Call-to-Backend assignmen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733:708,failure,failure,708,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1253#issuecomment-246048733,2,"['error', 'failure']","['error', 'failure']"
Availability,"I've found that this issue happens only when I use my custom reference files for assembly in BWA (mouse reference downloaded from Sanger ftp). I don't know if this is an issue with Cromwell or just the files I'm using. When I use the Broad's reference files downloaded from the google cloud storage, everything behaves as normal.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-540813650:114,down,downloaded,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5221#issuecomment-540813650,2,['down'],['downloaded']
Availability,"I've gone through and updated the dependency graph of updating sttp, you can view diff at https://github.com/delagoya/cromwell/tree/update-depversions . Still one more set of errors to fix: ; ```; root(update-depversions)> | 31>; [info] Compiling 4 Scala sources to $HOME/src/cromwell/womtool/target/scala-2.12/classes...; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:46: value foldMap is not a member of Set[wdl.draft2.model.WdlGraphNode]; [error] val thisLevelNodesAndLinks: NodesAndLinks = callsAndDeclarations foldMap { graphNode =>; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:56: value foldMap is not a member of Set[wdl.draft2.model.WdlGraphNode]; [error] val subGraphNodesAndLinks: NodesAndLinks = subGraphs foldMap { wdlGraphNode =>; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:26: private val clusterCount in object GraphPrint is never used; [error] private val clusterCount: AtomicInteger = new AtomicInteger(0); [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/GraphPrint.scala:40: local default argument in method listAllGraphNodes is never used; [error] def upstreamLinks(wdlGraphNode: WdlGraphNode, graphNodeName: String, suffix: String = """"): Set[String] = wdlGraphNode.upstream collect {; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/WomGraph.scala:54: value foldMap is not a member of Set[wom.graph.GraphNode]; [error] graph.nodes foldMap nodesAndLinks _; [error] ^; [error] $HOME/src/cromwell/womtool/src/main/scala/womtool/graph/WomGraph.scala:89: private method nodesAndLinks in class WomGraph is never used; [error] private def nodesAndLinks(graphNode: GraphNode): NodesAndLinks = {; [error] ^; [error] 6 errors found; [error] (womtool/compile:compileIncremental) Compilation failed; [error] Total time: 4 s, completed Apr 16, 2018 9:00:54 AM; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-381592626:175,error,errors,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-381592626,23,['error'],"['error', 'errors']"
Availability,"I've got a server up and running and I tried out my first WDL. I get the error below. I've pasted my WDL and JSON below the error: . ```; 2016-06-27 08:01:07,766 cromwell-system-akka.actor.default-dispatcher-9 ERROR - Failures during localization; java.lang.UnsupportedOperationException: Could not localize /cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R1_001.fastq -> /cil/shed/apps/internal/cromwell/cromwell-executions/smartseq_amr/21612af1-2c5f-400a-a53f-6ac66ec47674/call-RunSTARAlignment/cil/shed/apps/internal/RNA_utilities/SmartSeq_StarBasedPipeline/scripts/version0.2/SampleData/Mouse-A2-single_S2_L001_R1_001.fastq ; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$localize$1$3.apply(SharedFileSystem.scala:243) ~[cromwell-0.19.jar:0.19]; at scala.Option.getOrElse(Option.scala:121) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localize$1(SharedFileSystem.scala:242) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.adjustFile$1(SharedFileSystem.scala:264) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$class.localizeWdlValue(SharedFileSystem.scala:271) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.sge.SgeBackend.localizeWdlValue(SgeBackend.scala:60) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(SharedFileSystem.scala:223) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$16.apply(SharedFileSystem.scala:223) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(SharedFileSystem.scala:225) ~[cromwell-0.19.jar:0.19]; at cromwell.engine.backend.local.SharedFileSystem$$anonfun$17.apply(SharedFileSystem.scala:224) ~[cromwell-0.19.j",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1071:73,error,error,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1071,4,"['ERROR', 'Failure', 'error']","['ERROR', 'Failures', 'error']"
Availability,"I've noticed the return code is only being recorded for the final command in a sequence (LocalBackend line 136). I think if I were a WDL writer and I specified failOnRc, I'd want any command failure anywhere in the sequence to trigger a failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/200#issuecomment-143250619:191,failure,failure,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/200#issuecomment-143250619,2,['failure'],['failure']
Availability,"I've seen this again on Cromwell 25-f80282a, after I aborted a workflow. Rebooting does NOT seem to have cleared it up this time. The workflow bucket doesn't exist. As far as I can tell, there is no mention of the workflow in Cromwell logs (weirdly).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509:73,Reboot,Rebooting,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-289962509,1,['Reboot'],['Rebooting']
Availability,"I've started a new PR #4635 for some documentation, and @TMiguelT will hopefully be able to contribute to that. If I've got questions, I'll ping you on there to keep it a little more isolated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-462535223:140,ping,ping,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-462535223,1,['ping'],['ping']
Availability,"ID: CB48F5CFE95BBD50); > at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); > at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:563); > ... 31 common frames omitted; > [2019-01-09 05:21:48,83] [error] WorkflowManagerActor Workflow fb387147-f98a-4397-92b3-700d8c607a45 f; > ailed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionAc; > tor failed and didn't catch its exception.; > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:183); > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:180); > at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); > at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); > at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); > at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); > at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); > at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); > at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); > at akka.dispatch.Mailbox.run(Mailbox.scala:224); > at akka.dispatch.Mailbox.exec(Mailbox.scala:235); > at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); > Caused by: java.lang.Exception: Failed command instantiation; > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:565); > at cromwel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365:1253,Fault,FaultHandling,1253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365,1,['Fault'],['FaultHandling']
Availability,IIRC the error in question was passing the 403 Forbidden back from Google,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231715169:9,error,error,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231715169,1,['error'],['error']
Availability,"INFO] [06/02/2016 19:49:46.378] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user/$$ib] $$ib transition from InitializingWorkflowState to WorkflowFailedState: shutting down; [INFO] [06/02/2016 19:49:46.378] [test-system-akka.actor.default-dispatcher-5] [akka://test-system/user/$$ib/WorkflowInitializationActor-fd304efe-2bba-4859-9ffd-6ef7d4ae29d5] State is now terminal. Shutting down.; [WARN] [06/02/2016 19:49:46.381] [test-system-akka.actor.default-dispatcher-3] [akka://test-system/user] unhandled message from TestActor[akka://test-system/user/$$ib]: cromwell.engine.workflow.WorkflowActor$WorkflowFailedResponse; [INFO] [06/02/2016 19:49:46.494] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$a] $a: Call-to-Backend assignments: three_step.ps -> local, three_step.cgrep -> local, three_step.wc -> local; [INFO] [06/02/2016 19:49:46.494] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$a] $a transition from ReadyToMaterializeState to MaterializationSuccessfulState: shutting down; [INFO] [06/02/2016 19:49:46.495] [pool-7-thread-2-ScalaTest-running-WorkflowActorSpec] [akka://test-system/user/$$kb] $$kb transitioning from WorkflowUnstartedState to MaterializingWorkflowDescriptorState; [WARN] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-6] [akka://test-system/user] unhandled message from TestActor[akka://test-system/user/$$kb]: cromwell.engine.workflow.WorkflowActor$WorkflowFailedResponse; [INFO] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$$kb] $$kb transitioning from InitializingWorkflowState to WorkflowFailedState; [INFO] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$$kb] $$kb transition from InitializingWorkflowState to WorkflowFailedState: shutting down; [INFO] [06/02/2016 19:49:46.498] [test-system-akka.actor.default-dispatcher-7] [akka://test-system/user/$$kb/WorkflowInitial",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/933:1822,down,down,1822,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/933,1,['down'],['down']
Availability,"INTO cromwell.SQLMETADATADATABASECHANGELOG (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, `DESCRIPTION`, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('metadata_index_removals', 'mcovarr', 'metadata_changesets/metadata_index_removals.xml', NOW(), 8, '8:b32b63103dfbe3664806be3eccf78b09', 'dropIndex indexName=METADATA_JOB_AND_KEY_IDX, tableName=METADATA_ENTRY; dropIndex indexName=METADATA_JOB_IDX, tableName=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:37,918 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - Metadata summary refreshing every 1 second.; 2019-07-21 23:34:38,046 WARN - 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.service-dispatcher-13 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2019-07-21 23:34:38,160 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2019-07-21 23:34:38,594 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:1623,heartbeat,heartbeat,1623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,3,"['failure', 'heartbeat']","['failureShutdownDuration', 'heartbeat', 'heartbeatInterval']"
Availability,IO logging: print error message,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5989:18,error,error,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989,1,['error'],['error']
Availability,IOException kills workflow on first failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1890:36,failure,failure,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1890,1,['failure'],['failure']
Availability,"Ideally size would be checked prior to doing anything. . For instance: If there's a 1T file sitting on GCS and we're trying to `read_bool()` on it, it'd be nice to know that we shouldn't even bother downloading the file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294354889:199,down,downloading,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294354889,1,['down'],['downloading']
Availability,Identify error codes other than 13 (if any) to retry,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1909:9,error,error,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1909,1,['error'],['error']
Availability,"If I run a WDL like the following in JES, the echo'ed path is relative and I would like it to be absolute. This will allow me to reference it from other locations than the current working directory more easily. ```; task echo {; File in_file; command {; echo ${in_file}; }; runtime {; docker: ""ubuntu:14.04""; }; }. workflow abs {; call echo ; }; ```. with this input json:. `{; ""abs.echo.in_file"": ""gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict""; }`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1332:46,echo,echo,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1332,5,['echo'],['echo']
Availability,"If I understand this correctly, this will allow all scalatest tests to try twice - with the first failure being reported to a triage system in case it works the second time and the tests failing as usual if the same test fails twice in a row?. That sounds awesome!. The comments on adding this to the scalacheck tests seem like they could be part of a second pass since (a) there's not many of them and (b) they don't fail very often, so leaving them in the current ""must pass first time"" seems fine to me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658#issuecomment-398533395:98,failure,failure,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658#issuecomment-398533395,1,['failure'],['failure']
Availability,"If I'm understanding the concern correctly, you're worried about about Cromwell retrying a task based on the return code, even when the problem was not memory related. Cromwell requires a member of `system.memory-retry-error-keys` to be present, so it does not just use the return code. Note that memory retry was marked as an experimental feature and has experienced a breaking change since this issue was filed: https://github.com/broadinstitute/cromwell/releases/tag/56. Since I _think_ your concern is already addressed, I'm going to close the issue. Feel free to open if otherwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-794320092:219,error,error-keys,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815#issuecomment-794320092,1,['error'],['error-keys']
Availability,"If a CWL workflow has `DockerRequirement` specified as a hint, there's no way to run that workflow without Docker enabled currently. Provide a workflow option such that in these circumstances the workflow will be run *without* Docker. This workflow option should only be recognized by the SFS backends (local + HPC). @ruchim @cjllanwarne we might want to consider expanding this more generally to also include cases where `DockerRequirement` is a requirement and for WDL (where there is no hint), IOW a general docker on/off switch for people who know the consequences of their actions. If we did want to go down that path I'd make it a separate option from this one",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3542:608,down,down,608,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3542,1,['down'],['down']
Availability,"If a WDL task generates a file with a space in its name, and that file is an output, Cromwell fumbles the outputs and throws an error (at least on GCP-Terra!Cromwell). Additionally, this doesn't seem to be logged clearly. This workflow takes in a bunch of BioSample accessions, downloads their associated run FASTQs, and processes them. https://dockstore.org/workflows/github.com/aofarrel/myco/myco_sra:4.1.2?tab=files. During one run, I accidentally passed in a file of BioSample accessions which had two spaces before each accession, eg; ```; SAMEA104027315; SAMEA104027345; SAMEA104027406; SAMEA104164787; SAMEA104172469; SAMEA104172474; SAMEA104172508; SAMEA104221066; SAMEA104362398; SAMEA104394395; SAMEA104394505; SAMEA104414628; SAMEA104446901; ```. The workflow is scattered per BioSample, so one instance of the scattered task takes in ` SAMEA104027315` as the input `biosample_accession` (type String). The task writes a file like this:. ```; echo ""~{biosample_accession}"" >> ~{biosample_accession}_pull_results.txt; ```; eg ` SAMEA104027315_pull_results.txt`. The workflow output section contains:. ```; String results = read_string(""~{biosample_accession}_pull_results.txt""); ```. eg ` SAMEA104027315_pull_results.txt`, same as what's in the command section. . In the task level logs, I see . ```; 2023/04/18 21:54:34 Starting delocalization.; 2023/04/18 21:54:35 Delocalization script execution started...; 2023/04/18 21:54:35 Delocalizing output /cromwell_root/memory_retry_rc -> gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submissions/93bf6971-bfa1-4cb8-bb22-c8a753f58c49/myco/10fa31a8-acbe-4ab7-a96a-6550ec08df12/call-pull/shard-0/memory_retry_rc; 2023/04/18 21:54:37 Delocalizing output /cromwell_root/rc -> gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submissions/93bf6971-bfa1-4cb8-bb22-c8a753f58c49/myco/10fa31a8-acbe-4ab7-a96a-6550ec08df12/call-pull/shard-0/rc; 2023/04/18 21:54:39 Delocalizing output /cromwell_root/stdout -> gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submis",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:128,error,error,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,3,"['down', 'echo', 'error']","['downloads', 'echo', 'error']"
Availability,"If a workflow fails, I would like cromwell to tell me somewhere really obvious, like perhaps the last thing it prints to stdout:. * The first thing that failed.; * Whether it was a failure that occurred while cromwell was running the wdl, or whether it occurred during the execution of a task.; * If a cromwell error, which line of which wdl it happened on.; * If a task, which task and the stderr file. I think a lot of this already exists, but I'm suggesting for it to be super-simple and in one place. . Here's an example of the desired output:. ```; Lots of output. . .; . . .; Your workflow failed while executing task HelloWorld, See cromwell-executions/Hello/c44566iifgg57/call-HelloWorld/execution/stderr for details.; ```. Or. ```; Cromwell failed while executing line 346 of HelloWorld.wdl. The index 6 is out of bounds for the array popular_salutations.; ```. This would be extremely helpful whenever a non-expert runs a workflow, for example our mutect2 wdl. Currently I debug with a mix of home-brewed greps through the cromwell metadata, fishing through cromwell-executions, and running the darn thing myself. It would be really great just to tell them to look at the last line of stdout. A few points:; * The first error is all you need because you can iterate and fix bugs one at a time.; * It's crucial to let the user know very explicitly if this is a cromwell error or a within-task error.; * Stack traces from running the tool could be useful and acceptable, but cromwell stack traces with all that stuff about akka and spark would be overwhelming.; * Rigor isn't important here. For example, the order of execution is not prescribed to the point that the first task to fail will be the same every time, but for this it doesn't matter. Just reporting the first error on the wall clock is sufficient.; * It doesn't matter which workflows, subworkflows etc fails. Just the task.; * Putting this on the last line of stdout has the added virtue of avoiding hassle in a screen session.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3226:181,failure,failure,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3226,6,"['error', 'failure']","['error', 'failure']"
Availability,If at first you fail try try check-failure-retry-count times again,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3334:35,failure,failure-retry-count,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3334,1,['failure'],['failure-retry-count']
Availability,"If cromwell can't bind to it's tcp port, it should shut down, not continue running in zombie mode.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2645:56,down,down,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2645,1,['down'],['down']
Availability,"If i run cromwell with `-Dworkflow-options.workflow-failure-mode=""ContinueWhilePossible""` it does not work. Also using `-Dconfig.file=application.conf` does not work. Only **workflow_failure_mode** option in JSON config works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1005#issuecomment-245632780:52,failure,failure-mode,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1005#issuecomment-245632780,1,['failure'],['failure-mode']
Availability,"If my understanding is correct, this should run call c. ```; task sleeper {; Int seconds; Int output_seconds; Int rc; command {; sleep ${seconds}; echo ${output_seconds} > done. echo ""exit ${rc}"" >> script.sh; chmod +x script.sh; ./script.sh; }; runtime {docker: ""ubuntu:latest""}; output {; Int o = read_int(""done""); }; }. workflow w {; call sleeper as a {input: seconds=1, output_seconds=0, rc=1}; call sleeper as b {input: seconds=300, output_seconds=30, rc=0}; call sleeper as c {input: seconds=b.o, output_seconds=0, rc=0}; }; ```. Options. ```; {; ""workflow_failure_mode"": ""ContinueWhilePossible""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1168:147,echo,echo,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1168,2,['echo'],['echo']
Availability,"If some error occurs during release, it'd be nice to kick off the whole job again and have the process adjust accordingly. ## Should we release this project?; Would need to consult that these are correct and current:. * git tags; * release on github; * jar in JFrog; * brew Pr exists (for cromwell). This decision will determine version number for downstream projects.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2573:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2573,2,"['down', 'error']","['downstream', 'error']"
Availability,If the AWS backend uses ioActor this may already be covered in configuration?; ```; system {; io {; # Global Throttling - This is mostly useful for GCS and can be adjusted to match; # the quota availble on the GCS API; #number-of-requests = 100000; #per = 100 seconds. # Number of times an I/O operation should be attempted before giving up and failing it.; #number-of-attempts = 5; }; }; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436885778:194,avail,availble,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436885778,1,['avail'],['availble']
Availability,"If the service registry fails to get instantiated (wrong service classpath in the conf...), an exception is thrown but Cromwell still starts and an ActorRef is available in the `serviceRegistryActor` field, although the actor itself doesn't exist. All messages to the service registry are then lost.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/896:160,avail,available,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/896,1,['avail'],['available']
Availability,"If there is a failure in the `WorkflowMaterialiserActor` phase, there is no external way to debug the error. Allow the `WorkflowMaterialiserActor` to send failure messages to the metadata service in this instance, so that it's possible to debug errors!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/966:14,failure,failure,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/966,4,"['error', 'failure']","['error', 'errors', 'failure']"
Availability,"If this error happened in production, the Cromwell process would terminate quickly and... presumably restarted by k8s or something. With these changes, the Cromwell process will sleep instead of terminating. Will this negatively impact startup time? I guess it would depend on how long a cold start takes to start initializing backends; I don't know how long that takes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6487#issuecomment-919073756:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6487#issuecomment-919073756,1,['error'],['error']
Availability,"If this ever does get fixed - the last version that didn't throw `No getWomBundle method implemented in CWL v1` (`31.1`) threw an error for me when I ran on https://github.com/NCI-GDC/gdc-dnaseq-cwl/blob/master/workflows/dnaseq/transform.cwl:. ```; $ java -jar ~/bin/womtool-31.1.jar womgraph transform.cwl; Exception in thread ""main"" scala.MatchError: WomMaybePopulatedFileType (of class wom.types.WomMaybePopulatedFileType$); 	at womtool.graph.WomGraph$.fakeInput(WomGraph.scala:222); 	at womtool.graph.WomGraph$.$anonfun$womExecutableFromCwl$2(WomGraph.scala:205); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:231); 	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:462); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at womtool.graph.WomGraph$.$anonfun$womExecutableFromCwl$1(WomGraph.scala:205); 	at scala.util.Either.map(Either.scala:350); 	at womtool.graph.WomGraph$.womExecutableFromCwl(WomGraph.scala:201); 	at womtool.graph.WomGraph$.fromFiles(WomGraph.scala:172); 	at womtool.Main$.$anonfun$womGraph$2(Main.scala:98); 	at womtool.Main$.continueIf(Main.scala:102); 	at womtool.Main$.womGraph(Main.scala:96); 	at womtool.Main$.dispatchCommand(Main.scala:38); 	at womtool.Main$.delayedEndpoint$womtool$Main$1(Main.scala:167); 	at womtool.Main$delayedInit$body.apply(Main.scala:12); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at womtool.Main$.main(Main.scala:12); 	at womtool",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4119#issuecomment-584388032:130,error,error,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4119#issuecomment-584388032,1,['error'],['error']
Availability,"If this is alignment to the human genome reference, each aligning job will require ~8GB of RAM. If you have 10 jobs running concurrently you would want to make sure there are ~80Gb of RAM available. Alternatively, with CallCaching active, you can just re-run the workflow until all tasks have succeeded.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6188#issuecomment-796893864:188,avail,available,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6188#issuecomment-796893864,1,['avail'],['available']
Availability,"If this ticket is about what I think it is (picking up jobs that were running when Cromwell is restarted), I think this ticket should be ""Recover Support"". I believe ""retry"" describes Cromwell's resilience to transient failures with Google Cloud services.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/749#issuecomment-217901703:138,Recover,Recover,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/749#issuecomment-217901703,3,"['Recover', 'failure', 'resilien']","['Recover', 'failures', 'resilience']"
Availability,"If this works and removes the inconsistent test failures then I fully support turning test tragedies towards tip-top triumphs through taming the trusty tahr. In other words, :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/319#issuecomment-164066862:48,failure,failures,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/319#issuecomment-164066862,1,['failure'],['failures']
Availability,"If you don't want to open the zip:. WDLTesting/src/wdl/Workflow.wdl:; ```; version development. import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write. workflow TestingWF; {; 	call Write.WriteTask as Writer; 	{; 		input:; 			input1 = ""Foo""; }. }. ```; WDLTesting/src/wdl/WriteTask.wdl:; ```; version development. #################################################################################################; ## 				This WDL script writes its inputs to stdout				 ##; #################################################################################################. task WriteTask {. 	String	input1	# Variable with no default value; 	String	input2 = ""Default""; 	; 	command <<<; 		echo ""input1 = ${input1}""; 		echo ""input2 = ${input2}""; 	>>>; 	; output {; String	isDone = input2; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881119314:680,echo,echo,680,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881119314,2,['echo'],['echo']
Availability,"If you have an error in your json file, say a trailing comma on the last line, the error message is:. `Unexpected character '}' at input index 1681 (line 23, position 1), expected '\""':\n}\n^\n`. However it doesn't tell you that the problem is in the json and not the wdl. Changing line 23 in the wdl obviously won't help, so it would be nice to know which line 23 it's referring to.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1933:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1933,2,['error'],['error']
Availability,"If you have spaces or braces or other symbols which should be escaped in input file names, bash will be sad about this:. > /bin/bash: line 6: syntax error near unexpected token `('. Example input:; {; ""elc.input"": ""/home/vagrant/example (1).file""; }. because in resulting bash file it will be transformed into:. > /home/vagrant/cromwell/cromwell-executions/elc/5646fba9-3bdc-4c63-aeba-16adf80ae7d2/call-tsk_ELC_/home/vagrant/example (1).file. I think Cromwell should just put paths between single quotation marks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1306:149,error,error,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1306,1,['error'],['error']
Availability,If you haven't already it's worth pinging @vdauwera to let her know of the incoming change,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311425862:34,ping,pinging,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-311425862,1,['ping'],['pinging']
Availability,If you rebase onto the tip of `develop` you should be able to fix that test failure. We had to do some fixes to those docker tests thanks to some changes in Travis,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4938#issuecomment-490123112:76,failure,failure,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4938#issuecomment-490123112,1,['failure'],['failure']
Availability,"If you use a java or scala client (e.g. CromIAM...) to connect to Cromwell over HTTPS, you'll get an error like:; ```; javax.net.ssl.SSLProtocolException: handshake alert: unrecognized_name; ```; cf. Google, this can be ignored client-side by adding:; ```; System.setProperty(""jsse.enableSNIExtension"", ""false""); ```; However, it would probably be nicer if Cromwell addressed this by adding an appropriate name when running in HTTPS mode. Since CromIAM is happy talking to Sam over HTTPS, they probably know how to do this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2513:101,error,error,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2513,1,['error'],['error']
Availability,"Ignore my redundant self-approval of this PR. I intended to click ""merge"" but apparently the green ""Approve"" button was too tempting for me not to click on first",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6374#issuecomment-868768959:10,redundant,redundant,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6374#issuecomment-868768959,1,['redundant'],['redundant']
Availability,Ignoring failures of `should successfully run drs_usa_hca`:; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770806; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770807; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770814; * https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/560770815. because it is currently a known failure (due to the test data being deleted) and because this is an urgent fix.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6689#issuecomment-1048227851:9,failure,failures,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6689#issuecomment-1048227851,2,['failure'],"['failure', 'failures']"
Availability,Ignoring the PAPI2 failures resulting from their API change,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3772#issuecomment-397102286:19,failure,failures,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3772#issuecomment-397102286,1,['failure'],['failures']
Availability,"Im testing moving LSAPI runs to Batch with v86, and I keep getting the following error:. textPayload: ""docker: invalid spec: /mnt/disks/cromwell_root:/mnt/disks/cromwell_root:: empty section between colons."". for this command:; Executing runnable container:{image_uri:""gcr.io/google.com/cloudsdktool/cloud-sdk:434.0.0-alpine"" commands:""-c"" commands:""printf '%s %s\\n' \""$(date -u '+%Y/%m/%d %H:%M:%S')\"" Starting\\ container\\ setup."" entrypoint:""/bin/sh"" volumes:""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root:""} timeout:{seconds:300} labels:{key:""logging"" value:""ContainerSetup""} for Task task/job-49cc8a88-722b-43067ba4-ab34-48bc00-group0-0/0/0 in TaskGroup group0 of Job job-49cc8a88-722b-43067ba4-ab34-48bc00. The docker volumes are defined as:; volumes:""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root:""}. Shouldn't there be a rw permissions entry after the last colon? As far as I know, there is no way for users to modify the docker launch config to fix this. Is there something I have malformed or missing in my conf file?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7408:81,error,error,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7408,1,['error'],['error']
Availability,"Immutable Docker hash request keys to guard against credential mutations breaking lookups, more graceful handling of actual failure cases.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5230:124,failure,failure,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5230,1,['failure'],['failure']
Availability,"Implement a JES PBE, this ticket covers the basics but should NOT include:; - retry support; - call caching support; - recovery support; - metadata support; - abort support. as these are covered in other tickets with the ""JES PBE""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/657:119,recover,recovery,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/657,1,['recover'],['recovery']
Availability,Implement recover in TES and improve abort,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4197:10,recover,recover,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4197,1,['recover'],['recover']
Availability,Implement recoverAsync for AWS backend [BA-4857],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216:10,recover,recoverAsync,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216,1,['recover'],['recoverAsync']
Availability,"Implement the [paging protocol](https://github.com/ga4gh/workflow-execution-service-schemas/pull/30) specified by Cromwell. If it makes sense to do so this could be a candidate to update Cromwell itself. To the extent that **that** could require a Cromwell API change, contact @ruchim if going down that path",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3843:294,down,down,294,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3843,1,['down'],['down']
Availability,"Imports of local files seem to be broken in the development version of cromwell. The paths to the imported wdl files seem to be expected to be absolute paths. Any relative path is treated as relative to the root directory of the system. . For example:; In the following workflow, test.wdl is imported. If test.wdl is located in the same directory as this workflow it can't be found (`/home/<various directories>/test/test.wdl`), however, if test.wdl is located in the root directory it will be found (`/test.wdl`).; ```; version 1.0. import ""test.wdl"" as test. workflow test2 {; call test.sayHello as blah {; input:; name=""Grog""; }. output {; String out = blah.blah; }; }; ```; test.wdl looks like this:; ```; version 1.0. task sayHello {; input {; String name; }. command {; echo Hello, ~{name}; }. output {; String blah = read_string(stdout()); }; }; ```; The following is mentioned in the printed output:; ```; Failed to import 'test.wdl' (reason 1 of 2): Failed to resolve 'test.wdl' using resolver: 'relative to directory / (without escaping None)' (reason 1 of 1): Import file not found: test.wdl; Failed to import 'test.wdl' (reason 2 of 2): Failed to resolve 'test.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'test.wdl' relative to nothing; ```; Looking at the cromwell source code I suspect the problem lies with the directory path being given to `DirectoryResolver` in `localFilesystemResolvers` ([this line](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/lifecycle/materialization/MaterializeWorkflowDescriptorActor.scala#L271)). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-fi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3986:776,echo,echo,776,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3986,1,['echo'],['echo']
Availability,Improve AWS Integration:. - Docker Hub Authentication; - awsBatchRetryAttempts; - ulimits; - Call Caching with ECR private ([geertvandeweyer](https://github.com/geertvandeweyer) and [markjschreiber](https://github.com/markjschreiber/cromwell/)); - revised localization functions to improve stability ([geertvandeweyer](https://github.com/geertvandeweyer)); - Extra failure handling for Batch ([geertvandeweyer](https://github.com/geertvandeweyer)); - AWS/Batch error handling improvements ([geertvandeweyer](https://github.com/geertvandeweyer)); - Correct retry logic for spot kills ([geertvandeweyer](https://github.com/geertvandeweyer)); - handling of very rare early/late job killing ([geertvandeweyer](https://github.com/geertvandeweyer)); - Sychronize multipart uploads between callcache and jobscripts ([geertvandeweyer](https://github.com/geertvandeweyer)),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6835:365,failure,failure,365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6835,2,"['error', 'failure']","['error', 'failure']"
Availability,Improve DRS Localizer error logging [WA-373],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5908:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5908,1,['error'],['error']
Availability,Improve Error messaging surrounding Cwl parsing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3022:8,Error,Error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3022,1,['Error'],['Error']
Availability,Improve checkpointing doc [BW-518],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6169:8,checkpoint,checkpointing,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6169,1,['checkpoint'],['checkpointing']
Availability,Improve checkpointing documentation,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6165:8,checkpoint,checkpointing,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6165,1,['checkpoint'],['checkpointing']
Availability,"Improve error message for Type error, was ""invalid""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1963:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1963,2,['error'],['error']
Availability,Improve error message for unrecognized filesystem,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3268:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3268,1,['error'],['error']
Availability,Improve error output when parsing of CWL Fails,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2804:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2804,1,['error'],['error']
Availability,Improve parsing error messages,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3180:16,error,error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3180,1,['error'],['error']
Availability,Improve parsing errors for CWL,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3174:16,error,errors,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3174,1,['error'],['errors']
Availability,"Improved ""error 10"" tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5101:10,error,error,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5101,1,['error'],['error']
Availability,Improvements to reliability of S3 multipart copies and s3 localization into containers,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5839:16,reliab,reliability,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5839,1,['reliab'],['reliability']
Availability,"In 0.22 you could have something like this in config backend:; ```; runtime_attributes = """"""; String? pbs_email; """"""; ```; And then reference `pbs_email` inside an expression in the `submit` definition like so:; ```; submit = """"""; qsub ${""-me ea -M"" + pbs_email}; """"""; ```; so that if `pbs_email` isn't supplied, neither are the preceding option flags (as documented in `Optional Parameters and Type Constraints -> Prepending a String to an Optional Parameter` at https://software.broadinstitute.org/wdl/devzone.php). This doesn't work anymore in 23:; ```; [ERROR] [12/09/2016 11:09:29.763] [cromwell-system-akka.dispatchers.backend-dispatcher-19] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-71ccb54a-f88c-49b2-aeee-1af92749e337/WorkflowExecutionActor-71ccb54a-f88c-49b2-aeee-1af92749e337/71ccb54a-f88c-49b2-aeee-1af92749e337-EngineJobExecutionActor-testMe.worker:NA:1/71ccb54a-f88c-49b2-aeee-1af92749e337-BackendJobExecutionActor-71ccb54a:testMe.worker:-1:1/SharedFileSystemAsyncJobExecutionActor] SharedFileSystemAsyncJobExecutionActor [UUID(71ccb54a)testMe.worker:NA:1]: Error attempting to Execute the script; java.lang.UnsupportedOperationException: Could not evaluate expression: ""-m ea -M "" + pbs_email; 	at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:49); 	at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:107); 	at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:107); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(T",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1765:558,ERROR,ERROR,558,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1765,1,['ERROR'],['ERROR']
Availability,"In Cromwell 29, having a ':' character present in the filename of a non-glob task output results in a workflow failure. It does not matter whether this is hard-coded or simply in the filename. . `WorkflowManagerActor Workflow 1356670a-cea2-45b7-a9c4-e379dc601f9f failed (during ExecutingWorkflowState): Could not evaluate A.out = ""A:out""; java.lang.RuntimeException: Could not evaluate A.out = ""A:out""`. This is reproducible with the following wdl:; ```; workflow colon_filename{; 	call A {}; }. task A{; 	command{; 		echo ""testing"" > A:out; 	}; 	output{; 		File out = ""A:out""; 	}; }. ```; Task A completes normally with return code 0 and creates the file A:out. . I expected this workflow to complete normally without error. . Workaround; -; I have an acceptable workaround where I simply replace all ':' characters with '-'.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2919:111,failure,failure,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2919,3,"['echo', 'error', 'failure']","['echo', 'error', 'failure']"
Availability,"In DSDE methods, there are four backend configurations we would like to support.; - local w/ docker; - local w/o docker; - SGE (req: no docker); - JES (req: docker). But we would like to support these without having to change the WDL file itself. Currently, this is done with complex scripts that choose default options and application configurations. But perhaps there is an easier way... by parameterizing runtime attributes. I hear cromwell already supports that, except for the case where we do not want to run in docker at all (e.g. local w/o docker backends). Assuming that you can specify runtime parameters as part of a workflow. In other words, assuming that the following is valid:. ```wdl. workflow yo {; String msg; String docker_image. call task1 {; input:; msg=msg,; docker_image=docker_image; }; }. # Run a message in an arbitrary docker container (e.g. ""broadinstitute/eval-gatk-protected:crsp_validation_latest""); task task1 {; String msg; String docker_image; ; command {; echo ${msg}; } ; ; runtime {; docker: ""${docker_image}""; memory: ""1GB""; }; }; ```; ```; {; ""yo.msg"": ""foo""; ""yo.docker_image"": ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; }; ```; The above WDL+json should work for JES and ""local w/ docker"" backends. However, to support local w/o docker, we need to be able to specify a ""null"" value, which cromwell will interpret as, ""do not use docker"" or ""docker was never specified"". For example:; ```; {; ""yo.msg"": ""foo""; ""yo.docker_image"": """"; }; ```; My understanding of cromwell is that this json + WDL above will cause a failure in cromwell. If this functionality already exists, please close this issue. This would make our WDLs more complicated, but it would increase flexibility and move runtime specification into the json file (which is easier than juggling default options).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1804:991,echo,echo,991,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1804,2,"['echo', 'failure']","['echo', 'failure']"
Availability,"In Google Compute Engine, one can create custom networks and even delete the default network. ; Docs: https://cloud.google.com/vpc/docs/using-vpc; List of networks: https://console.cloud.google.com/networking/networks/list. This is commonly done for projects that have high security requirements to enforce firewalls etc. The ability to specify a network where operations are created is supported in v2alpha1, but there is no place to specify it in Cromwell (which always uses the ""default"" network). AC: Add an option to Cromwell's global config where a user can specify the VPC network name, for the PAPI v2 backend. This would override the current ""default"" network used by Cromwell. Testing Criteria:; - Confirm that Cromwell honors using a non-default network when specified via the config.; - If the network name specified doesn't exist, the error returned to the user contains information about 1) a link to documentation on how to create a network and 2) how to confirm a network exists through the cloud console.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4017:848,error,error,848,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4017,1,['error'],['error']
Availability,"In Google Compute Engine, one can create custom networks and even delete the default network.; Docs: https://cloud.google.com/vpc/docs/using-vpc; List of networks: https://console.cloud.google.com/networking/networks/list. The ability to specify a network where operations are created is supported in v2alpha1, but there is no place to specify it in Cromwell (which always uses the ""default"" network). AC: Add an option to Cromwell's global config where a user can specify the VPC network name, for the PAPI v2 backend. This would override the current ""default"" network used by Cromwell. Testing Criteria:; Confirm that Cromwell honors using a non-default network when specified via the config.; If the network name specified doesn't exist, the error returned to the user contains information about 1) a link to documentation on how to create a network and 2) how to confirm a network exists through the cloud console.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4005#issuecomment-414163462:745,error,error,745,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4005#issuecomment-414163462,1,['error'],['error']
Availability,"In JES jobs, retry Error 13 up to 2 times prior to failing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1888:19,Error,Error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1888,1,['Error'],['Error']
Availability,"In JES, the use case where a task exits without producing 1 or more expected output files will end up with the JES job in a failed status (because of non-existing output files failed to be delocalized), and the error message ""Failed to delocalize files"".; This looks like an error from JES where it's actually most likely an error of the task failing to produce some outputs.; If the job fails Cromwell never tries to read the RC file which makes it difficult for a user to know if it's a JES error or a task error. 1) Make sure that JES will delocalize the RC file first before trying to delocalize outputs - so it's available in the bucket.; 2) Have cromwell try to read the file even if the job failed and make sure it's present in the metadata.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1848:211,error,error,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1848,6,"['avail', 'error']","['available', 'error']"
Availability,"In PR #2925, the `no_new_calls` test sometimes generates a cromwell log [message](https://github.com/broadinstitute/cromwell/blob/c6ed64617c51c572863b87d324fa8e68fa085b1a/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L118-L121):. > Cromwell server was restarted while this workflow was running. As part of the restart process, Cromwell attempted to reconnect to this job, however it was never started in the first place. This is a benign failure and not the cause of failure for this workflow, it can be safely ignored. This occurs when cromwell is restarted while `shouldSucceed` is still running. `shouldSucceed` finishes, and then a `Restarting calls: no_new_calls.delayedTask:NA:1` is generated, even though `boundToFail` has already failed and NoNewCalls should be started. The easiest way to reproduce this locally and see the delay is to increase the sleep in the wdl from 100 to something like 300 (five minutes). FYI if cromwell is not restarted, `delayedTask` does not start, does not fail, and does not have a metadata stanza for the call.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2926:488,failure,failure,488,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2926,2,['failure'],['failure']
Availability,"In WDL 1.0 onwards task inputs must be in an `input` block: https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#task-inputs. The following modification validates as expected:. `Workflow.wdl`; ```; version development. import ""WDLTesting/src/wdl/WriteTask.wdl"" as Write. workflow TestingWF; {; call Write.WriteTask as Writer; {; input:; input1 = ""Foo""; }. }; ```. `WriteTask.wdl`; ```; version development. #################################################################################################; ## 				This WDL script writes its inputs to stdout				 ##; #################################################################################################. task WriteTask {. input {; String input1	# Variable with no default value; String input2 = ""Default""; }; 	; command <<<; echo ""input1 = ${input1}""; echo ""input2 = ${input2}""; >>>; 	; output {; String	isDone = input2; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881126827:794,echo,echo,794,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881126827,2,['echo'],['echo']
Availability,"In WDL 1.0, variables defined in nested if-statements are ""doubly optional"" (`Int??`) and break `select_first()`. Note that the degree of optionality maxes out at two, regardless of how deep the nesting is. This workflow is fine; ```; workflow Test {. Int a = 5. if (true) {; if (true) {; if (true) {; Int b = 5; }; }; }. Int c = select_first([a, b]); }; ```; but this one fails; ```; version 1.0. workflow Test {. Int a = 5. if (true) {; if (true) {; if (true) {; Int b = 5; }; }; }. Int c = select_first([a, b]); }; ```; with error; ```; Failed to process workflow definition 'Test' (reason 1 of 1):; Failed to process declaration 'Int c = select_first([a, b])' (reason 1 of 1):; Cannot coerce expression of type 'Int??' to 'Int'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3766:528,error,error,528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3766,1,['error'],['error']
Availability,"In [CROM-6338](https://broadworkbench.atlassian.net/browse/CROM-6338) Denis reports that Cromwell is unexpectedly failing to retry 503s and provides the following sample error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""503 Service Unavailable\nBackend Error""; }; ],; ""message"": ""Could not read from gs://broad-epi-cromwell/workflows/ChipSeq/ce6a5671-baf6-4734-a32b-abf3d9138e9b/call-epitope_classifier/memory_retry_rc: 503 Service Unavailable\nBackend Error""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://broad-epi-cromwell/workflows/ChipSeq/ce6a5671-baf6-4734-a32b-abf3d9138e9b/call-epitope_classifier/memory_retry_rc: 503 Service Unavailable\nBackend Error""; }; ]; ```. In https://github.com/broadinstitute/cromwell/issues/6154 @freeseek reports that Cromwell is unexpectedly failing to retry 504s and provides the following sample error:; ```; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=me",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6155:170,error,error,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6155,6,"['Error', 'error', 'failure']","['Error', 'error', 'failures']"
Availability,"In `CwlCodecs.decodeCwl` , our entry point for parsing CWL, we have poor error messaging. Currently if `Cwl` types fail to parse, the only output is a single `CNil` as it fails to parse either a `Workflow` or a `CommandLineTool` (the inhabitants of the coproduct type `Cwl`). I think we should parse the two types manually (ie. `decode[Workflow]` and `decode[CommandLineTool]`) and think of a nice way to report that neither one was successful. It'd be a nice bonus to figure out which one ""got farther"" before it failed so that it may be more relevant to the caller.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2804:73,error,error,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2804,1,['error'],['error']
Availability,"In a Cromwell configured with multiple backends, be able to dynamically determine which backend to send a job based on where the input files live. . 1. If there are no files, use the default backend; 2. If all of the files are on the same filesystem, use the backend associated w/ that filesystem (see below); 3. If the files are not all on the same filesystem, error. This will require moving backend determination (at least for many tasks) out of materialization and closer to the runtime dispatch. Note that the 2nd stage really only works right now because there's (mostly) a 1-1 mapping of backend to filesystem. If you can find a way to do this slickly in a world where a backend might support multiple filesystems, so much the better. Note also that this relates to #1312 (ideally this is done after that, but shouldn't matter too much either way). The workflow option should always override dynamic dispatch determination.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2584:362,error,error,362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2584,1,['error'],['error']
Availability,"In a conversation with @hjfbynara he referenced seeing an issue a few more times which had come up in the past. I remembered that @kshakir posted the following in slack and wanted to capture it for posterity here:. the firecloud thingy is “worked around”. cromwell 1 and 2 were restarted, and both started running jobs again. ruchi and I glanced at the supervision in the CromwellRootActor of the WorkflowStoreActor, and didn’t see anything special that would catch 100% of errors*. However— it was only a hypothesis that the WorkflowStoreActor was dead, as doug and henry reported little to no cpu activity from cromwell if I recall correctly. * The WorkflowStoreActor appears to be supervised for initialization exceptions, but the rest are handled with a “default”. It looks like default might be equal restart, but as of this second I don’t know what exact state a new instance of a WSA actor/fsm restarts based on the akka spec. Also there was some error logging on db futures within the WSA, but it’s possible an exception was throws before the `Future` got a chance to wrap the exception in a failure? All guesses at this point that could be way off target.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1874:474,error,errors,474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1874,3,"['error', 'failure']","['error', 'errors', 'failure']"
Availability,"In a more recent scale test we observed this error on 37 out of 4000 workflows submitted. @geoffjentry @ruchim @danbills . ETA: Sorry, just saw that Rex already commented that (hadn't reloaded the page)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-437068102:45,error,error,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-437068102,1,['error'],['error']
Availability,"In a scatter of 20500 shards, we ran a task that basically took in one file input and output a glob of files. We first tried this with a glob where we expected ~900 files to be output and no memory issues were found and everything went relatively smoothly. Because of some outside factors we decided to change this task to instead output ~3000 files in the glob. After about 13000 tasks were processed(Sucess -> Done) we started seeing some slow down that coincided with errors in the logs like the following:. ```; 2016-08-03 03:34:04,971 cromwell-system-akka.actor.default-dispatcher-51 WARN - Caught exception, retrying: Remote host closed connection during handshake; javax.net.ssl.SSLHandshakeException: Remote host closed connection during handshake; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:992) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[na:1.8.0_72]; at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:153) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:93) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201:446,down,down,446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248#issuecomment-237583201,2,"['down', 'error']","['down', 'errors']"
Availability,"In addition to the akka migration docs, and the commented out implicit compilation error, there a several new compilation warnings / recommendations that appeared with this change. They each look straightforward enough to fix and quiet, in a following PR or this PR. While we're in there, perhaps we can also use [`""com.timushev.sbt"" % ""sbt-updates""`](https://github.com/rtimush/sbt-updates) to update our other dependencies in cromwell, lenthall, and wdl4s, too. Btw, the explicit form that compiles your implict error:. ``` scala; val futureAny = actorRef.?(message)(timeout = timeout, sender = actorRef); ```. Switch `timeout` and `actorRef` with whatever `implicit val` you intended, but those were the closest in scope from `trait DefaultTimeout` and the enclosing method parameters, respectively.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823:83,error,error,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1370#issuecomment-244788823,2,['error'],['error']
Availability,"In addition to the commit comments. Specifically for ""the docker"":; - InputParameters weren't wired in for secondary files. Now they are, with Polys and a bit of borrowing from the OutputParameters. The common code was refactored into FileParameter.; - CWL spec says InputArraySchema's can't have secondary files, and that they should be specified on the InputBinding. The CWL via ""the docker"" wants them on the IAS anyway. The IAS secondary files are now wired in through a Poly.; - ""The docker"" was using strings like `$(""foo"")/$(""bar"")` that was tripping up our various expression regex patterns. This PR replaces them with the (latest as of Friday) copy-port of cwltool's state machine. The state machine works on _any_ string, even those without `""$(""`. But the spec for secondary files says that only expressions should be returned-as-rendered, while plain strings should be instead appended instead of used as literals. It turns out this discrimination is done in cwltool `""$("" in expr` (see numerous links in the scala comments). So the presence of `""$(""` is now used for our `ECMAScriptExpression` to similarly guess if a string is a interpolated string or a regular string. `ECMAScriptExpression` and `InterpolatedString` were pretty much the same thing and were merged back together. There was also a bit of code where if a malicious (or errant) CWL was submitted a `java.lang.Error` would be thrown possibly exiting the JVM. The code around this section was refactored to use a compile-time-checked Poly instead of hope-we-got-them-all pattern matching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3350#issuecomment-370296979:1388,Error,Error,1388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3350#issuecomment-370296979,1,['Error'],['Error']
Availability,"In an issue [BA-5929](https://broadworkbench.atlassian.net/browse/BA-5929) a problem with big numbers, exceeding Int capacity, was introduced. Therefore, I decided to add BigDecimal type support in WDL language.; Changes:; - BigDecimal type introduced in WDL; - Basic binary and unary operations for number types are implemented for BigDecimal. ---; I referred to this list during operations implementation:; https://software.broadinstitute.org/wdl/documentation/spec#types. Manually tested on:; ### inputs.json:; ```json; {; ""my_wf.largenumber"": 7000000000; }; ```. ### workflow; ```; workflow my_wf {; BigDecimal largenumber. call print_number {; input: largenumber=largenumber; }; }; task print_number {; BigDecimal largenumber; BigDecimal largeNumIncr = largenumber + 42; command {; echo SOME_LARGE_NUMBER ${largeNumIncr}; }; output {; String lnum=read_string(stdout()); }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5135:787,echo,echo,787,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5135,1,['echo'],['echo']
Availability,"In case it helps anyone else: I had the same error message, but in a different context (I wasn't using docker compose. Instead, I was trying to set up a local backend run using a limited number of CPUs via the `concurrent-job-limit` configuration value, as described on these pages: [1](https://cromwell.readthedocs.io/en/stable/Configuring/), [2](https://cromwell.readthedocs.io/en/stable/backends/Backends/#backend-job-limits), [3](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/README.md)). I ended up fixing the error by changing the value for the `backend.providers.LocalExample.config.submit-docker` option in my configuration file. I.e. initially, I was using the value from the [example config file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/cromwell.example.backends/LocalExample.conf), but for some reason this was giving me an error. When I replaced it with an updated version obtained from [this internal cromwell file](https://github.com/broadinstitute/cromwell/blob/b9b1adef95bea3c74db8534736b61625b6c66ebe/core/src/main/resources/reference_local_provider_config.inc.conf), it started working",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288:45,error,error,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6353#issuecomment-1451569288,3,['error'],['error']
Availability,"In case someone is looking to acheive this: The ~wdl task that runs as part of configuration has access to 'job_name'. I have passed this through as an environment variable. . ```; submit-docker = """"""; docker run \; --entrypoint ${job_shell} \; -e CROMWELL_JOB_NAME=${job_name} \; ```. Side note it would be nice if the variables available to that conf script were documented - I am reverse engineering from the example configs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721:330,avail,available,330,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1575#issuecomment-426880721,1,['avail'],['available']
Availability,"In case you don't want to fire up a server on this branch just to see the rendered example in the swagger UI...:. ```json; {; ""valid"": true,; ""errors"": [; ""The 'errors' field will be filled if 'valid' is false"",; ""We might also provide warnings to a 'valid' workflow here"",; ""Otherwise, 'errors' will be empty or unspecified""; ],; ""name"": ""my_wf"",; ""inputs"": [; {; ""name"": ""my_wf.string_input"",; ""valueType"": {; ""typeName"": ""String""; },; ""optional"": false; },; {; ""name"": ""my_wf.array_input"",; ""valueType"": {; ""typeName"": ""Array"",; ""arrayType"": {; ""typeName"": ""String""; }; },; ""optional"": false; },; {; ""name"": ""my_wf.optional_input"",; ""valueType"": {; ""typeName"": ""Optional"",; ""optionalType"": {; ""typeName"": ""String""; }; },; ""optional"": true,; ""default"": ""hello""; },; {; ""name"": ""my_wf.map_input"",; ""valueType"": {; ""typeName"": ""Map"",; ""mapType"": {; ""keyType"": {; ""typeName"": ""String""; },; ""valueType"": {; ""typeName"": ""Int""; }; }; },; ""optional"": false; },; {; ""name"": ""my_wf.object_input"",; ""valueType"": {; ""typeName"": ""Object"",; ""objectFieldTypes"": [; {; ""fieldName"": ""int_field"",; ""fieldType"": {; ""typeName"": ""Int""; }; },; {; ""fieldName"": ""int_array_field"",; ""fieldType"": {; ""typeName"": ""Array"",; ""arrayType"": {; ""typeName"": ""Int""; }; }; }; ]; },; ""optional"": false; },; {; ""name"": ""my_wf.int_string_pair_input"",; ""valueType"": {; ""typeName"": ""Tuple"",; ""tupleTypes"": [; {; ""typeName"": ""Int""; },; {; ""typeName"": ""String""; }; ]; },; ""optional"": false; }; ],; ""images"": [; [; ""quay.io/seqware/seqware_full/1.1"",; ""ubuntu:latest"",; ""EXPRESSION_BASED_IMAGE""; ]; ],; ""submitted_descriptor_type"": {; ""descriptorType"": ""WDL"",; ""descriptorTypeVersion"": ""1.0""; },; ""imported_descriptor_types"": [; {; ""descriptorType"": ""WDL"",; ""descriptorTypeVersion"": ""1.0""; }; ],; ""meta"": {; ""author"": ""Batman"",; ""copyright"": ""BSD 3-clause""; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4432:143,error,errors,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4432,3,['error'],['errors']
Availability,"In cases where call caching is not being used (as is the case for Genomes On the Cloud for the time being), the ability to have workflows ""fail fast"" is desired. Fail fast in this case mean, once a task has failed no new tasks should be started (currently running tasks can finish) and then the workflow should be set to failed. This is important because when running a workflow, and you can't reuse the non-failed work, you want to know the workflow will fail as soon as possible. With a long running workflow, an early failure with few downstream dependencies you have to wait a long time (24 hours for GOTC) to realize it's actually failed. This should be specifyable as a workflow option, e.g. failure-strategy = fast where the current behavior is 'slow'. If it's easy to also have this as a server config setting for the default that would be a nice to have but not critical.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/587:521,failure,failure,521,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/587,3,"['down', 'failure']","['downstream', 'failure', 'failure-strategy']"
Availability,"In certain cases, cromwell/wdltool is unable to resolve call statements in subworkflows (cromwell v29, wdltool v0.14). This may be related to #2753. A simple case to demonstrate this:. main.wdl:; ```; import ""sub.wdl"" as sub; workflow main {; call sub.wf; }; ```. sub.wdl:; ```; workflow wf {; call a; String b = a.out; }. task a {; command {; echo ""hello""; }; output {; String out = read_string(stdout()); }; }; ```; Running the validate command against sub.wdl returns no error messages. Running validate against main.wdl returns:; ```; ERROR: Missing value or call: Couldn't find value or call with name 'a' in workflow (line 4):. String b = a.out; ^; ```; Interestingly, if I add to sub.wdl:; ```; task b {; String msg; command {; echo ${msg}; }; output {; String out = read_string(stdout()); }; }; ```; and replace String b=a.out with:; ```; call b { input: msg=a.out }; ```; it validates ok. Am I doing something wrong with the WDL? Or is there a workaround for this?. My actual use case is a bit more complex - I have a series of mutually exclusive optional cases, so I want to put the File? output from each into an Array, and then use the select_first. But when I try to create the array from the outputs:; ```; if (a_condition) {; call task_a; }; if (b_condition) {; call task_b; }; ...; Array[File?] files = [task_a.out, task_b.out ...]; File file = select_first(files); ```; I see the above manifestation/error message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2756:344,echo,echo,344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2756,5,"['ERROR', 'echo', 'error']","['ERROR', 'echo', 'error']"
Availability,"In https://github.com/broadinstitute/cromwell/issues/4772 we learned that Google reserves the right to omit keys from the operation metadata map instead of supplying the default values. In https://github.com/broadinstitute/cromwell/pull/4773 we added robustness for when a particular key, `preemptible`, is absent. Look at the `interpretOperationStatus` functions for PAPI v1 and v2, research which keys Google might chose to omit in the future, and make sure they don't break.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4774:251,robust,robustness,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4774,1,['robust'],['robustness']
Availability,"In my FireCloud workflow, call caching is turned on by default. ; <img width=""1153"" alt=""screen shot 2018-06-07 at 8 57 27 am"" src=""https://user-images.githubusercontent.com/10040800/41101026-f1183d66-6a30-11e8-9fc0-75486f87e18f.png"">. and I also just realized that within the same workflow, some early tasks have the cache results recognized (Hit), for example:; <img width=""509"" alt=""screen shot 2018-06-07 at 9 03 24 am"" src=""https://user-images.githubusercontent.com/10040800/41101331-debd6938-6a31-11e8-903a-c6bf9c6e85e6.png"">; However, once it reaches to one task (M2), which splits the job based on scatter count (in my case, it is 50, basically, each subjob will only take care of a fraction of genome), I think the fraction of genome each job takes care of in different runs should be the same because no parameter has changed. But quite unexpectedly, the subjob cant recognize previous run (Miss). for example:; <img width=""905"" alt=""screen shot 2018-06-07 at 9 19 38 am"" src=""https://user-images.githubusercontent.com/10040800/41102077-f3d8fde4-6a33-11e8-8e24-0c13ada5865a.png"">. If I can't copy whatever successfully finished in previous subjobs, i have to start the whole 50 subjobs every time, it will dramatically increase my cost and time and there is no guarantee that new job will finish successfully because of those transient error. . Maybe there is something I am missing to set up call caching correctly, but as a newbie, I can't figure out myself. . Thanks all in advance",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3740#issuecomment-395418531:1346,error,error,1346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3740#issuecomment-395418531,1,['error'],['error']
Availability,"In order to allow the Private IP flag, it needs to be set both at pipeline creation time and run time. Run time resources override create time resources. However mount point must be set at create time but cannot be re-set at runtime... . ```; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""disks must not have mount points at run time"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""disks must not have mount points at run time"",; ""status"" : ""INVALID_ARGUMENT""; }; ```. This sets mount points to null for run time only. The rest is strictly identical to create time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1507:262,error,errors,262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1507,1,['error'],['errors']
Availability,"In order to discern if a workflow is **really** running instead of having been picked up to be run but abandoned, running workflows should send a heartbeat ping to the workflow store. After some configurable time limit, if a workflow has not sent in a heartbeat, move it to a state (see below) such that it'll again be pick-upable. I think it'd be good to have a new state (e.g. `LostContact`) vs resetting it to `Submitted`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3341:146,heartbeat,heartbeat,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3341,3,"['heartbeat', 'ping']","['heartbeat', 'ping']"
Availability,"In order to improve our scalability, performance, and resilience, we would like to be able to demonstrate measurable improvements. This epic lists the performance tests we would like to run as part of this effort, as well as what to measure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4796:54,resilien,resilience,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4796,1,['resilien'],['resilience']
Availability,"In other PC I got another error "" docker: command not found"". Probably it is because it tries to run docker inside cromwell docker container.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2284#issuecomment-302877669:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2284#issuecomment-302877669,1,['error'],['error']
Availability,"In response to:; 1. the difficulty of debugging https://github.com/broadinstitute/cromwell/issues/4555 and https://github.com/broadinstitute/cromwell/issues/4512 due to excessive, unformatted output; 2. the lack of tests on the error message generation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4661:228,error,error,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4661,1,['error'],['error']
Availability,In that case I’d side with more and dialing down instead of less and dialing up,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352072779:44,down,down,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352072779,1,['down'],['down']
Availability,"In the `AsyncBackendJobExecutionActor`, the `isTransient` method is defined as `!isFatal(throwable)` which means any non fatal error will trigger infinite retries which is not great.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2813:127,error,error,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2813,1,['error'],['error']
Availability,"In the cromwell README...; ```wdltool 0.4```. ```; import ""sub_wdl.wdl"" as sub. workflow main_workflow {. call sub.hello_and_goodbye { input: hello_and_goodbye_input = ""sub world"" }. # call myTask { input: hello_and_goodbye.hello_output }. output {; # I believe this will cause a validation failure. This is what is currently in the README...; String main_output = hello_and_goodbye.hello_output. # I believe this will NOT cause a validation failure. ; hello_and_goodbye.hello_output; }; }; ```. My real-world example:. ```; workflow dl_ob_training {. File bam_file; File bam_file_index; File gatk_jar; File ref_fasta; File oncotated_m1; String entity_id; File createOxoGIntervalList. call CollectSequencingArtifactMetrics {; input:; entity_id=entity_id,; bam_file=bam_file,; output_location_prepend=entity_id,; gatk_jar=gatk_jar,; ref_fasta=ref_fasta; }. call CreateObIntervalList {; input:; oncotated_m1=oncotated_m1,; entity_id=entity_id,; createOxoGIntervalList=createOxoGIntervalList; }. call ExtractReadInfo {; input:; bam_file=bam_file,; bam_file_index=bam_file_index,; gatk_jar=gatk_jar,; ref_fasta=ref_fasta,; pre_adapter_file=CollectSequencingArtifactMetrics.pre_adapter_detail_metrics,; interval_list=CreateObIntervalList.interval_list,; }. output {; # VALIDATES; ExtractReadInfo.read_infos. # DOES NOT VALIDATE; Array[File] read_infos = ExtractReadInfo.read_infos; }; }; .....snip.... task ExtractReadInfo {; File gatk_jar; File pre_adapter_file; File interval_list; File bam_file; File bam_file_index; File ref_fasta. command {; java -jar ${gatk_jar} ExtractReadInfo \; -P ${pre_adapter_file} \; -L ${interval_list} \; -I ${bam_file} \; -R ${ref_fasta} \; -OP out/; }. output {; Array[File] read_infos = glob(""out/*""); }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1837:291,failure,failure,291,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1837,2,['failure'],['failure']
Availability,"In the documentations to struct the syntax that is mentioned is:; ```; Person a = {""name"": ""John"",""age"": 30}; ```; So I expect that ; ```; QuantifiedRun quantified_run = {""run"": srr, ""folder"": quant_folder, ""quant"": quant, ""lib"": quant_lib}; ```; should be treated in a similar way. If in a development version you changed the syntax the error should be thrown at compiletime and it should be explained in the documentation",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-464251330:338,error,error,338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-464251330,1,['error'],['error']
Availability,"In the spirit of ""everything will be different in a month"" I'm less concerned about some of the weird stuff I pointed out above and I realized it'd be good to get this functionality in 0.21 as some downstream constituents would like this behavior",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248762101:198,down,downstream,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248762101,1,['down'],['downstream']
Availability,"In this case `df` produces space-separated columns, but `read_object` is looking for tab separators. So this particular failure is to be expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/615#issuecomment-270210508:120,failure,failure,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/615#issuecomment-270210508,1,['failure'],['failure']
Availability,"In this situation, metadata requests (via, eg., `curl` or similar) either hang (seemingly) indefinitely or time out with a message like these:. ```; status"": ""error"",; ""message"": ""Communications link failure\n\nThe last packet successfully received from the server was 3 milliseconds ago. The last packet sent successfully to the server was 173,470 milliseconds ago.""; }; ```. ```; The server was not able to produce a timely response to your request.; Please try again in a short while!; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-320284502:159,error,error,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-320284502,2,"['error', 'failure']","['error', 'failure']"
Availability,"In version 31 I had in my configuration: ; ```temporary-directory = ""$(mkdir -p $TMPDIR && echo $TMPDIR)""```. Which resulted in:; ```; tmpDir=$(; set -e; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy_Workflow/f7b9ac1e-994c-46bd-bad5-e11ac6696165/call-happy/execution; tmpDir=""$(mkdir -p $TMPDIR && echo $TMPDIR)""; echo ""$tmpDir""; ); chmod 777 ""$tmpDir""; ```. In version 32 the same config results in:; ```; cd /share/ScratchGeneral/evaben/cromwell/cromwell-executions/Happy/356aa17f-6276-44e0-9859-391c6c58cf49/call-happy/execution; tmpDir=`$(mkdir -p $TMPDIR && echo $TMPDIR)`; chmod 777 ""$tmpDir""; ```. which executes using my $() as well as the cromwell provided ``, causing an error (which is no longer caught by set -e). Then many subsequent errors as the script tries to write to / (lucky I did not rm -r!). I thought there was documentation on readthedocs but I cannot find it with the inbuilt search or google. If I just remove my $(), it should work, but as the change was not announced and there does not seem to be documentation, I wanted to check what the actual contract is. I would also prefer cromwell did not chmod my tmpdir, but that is a separate issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3784:91,echo,echo,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3784,6,"['echo', 'error']","['echo', 'error', 'errors']"
Availability,"InMetadata] 100%; [2020-01-28 18:31:38,06] [info] Running with database db.url = jdbc:hsqldb:mem:804bf0c2-e198-491b-8dce-708650038640;shutdown=false;hsqldb.tx=mvcc; [2020-01-28 18:31:38,48] [info] Slf4jLogger started; [2020-01-28 18:31:38,67] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-4defb12"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this i",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5395:1691,error,error,1691,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395,1,['error'],['error']
Availability,"Incidental finding, no immediate action required:. > The Performance Schema tables are intended to replace the INFORMATION_SCHEMA tables, which are deprecated as of MySQL 5.7.6 and are removed in MySQL 8.0. . https://dev.mysql.com/doc/refman/5.7/en/upgrading-from-previous-series.html. It looks like the new performance schema is available on 5.6 but is not enabled on our prod config (requires flag)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-870944153:330,avail,available,330,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6382#issuecomment-870944153,1,['avail'],['available']
Availability,Inconsistent failure messages for workflows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:13,failure,failure,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,1,['failure'],['failure']
Availability,Increase # of connections made available by CloudSQL-proxy in firecloud-develop,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4897:31,avail,available,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4897,1,['avail'],['available']
Availability,Increase the number of characters shown in case of error [NOJIRA],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6132:51,error,error,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6132,1,['error'],['error']
Availability,Indecipherable error message in cromwell (subworkflow WDL issue?),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3039:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3039,1,['error'],['error']
Availability,"Indeed, the following workflow:; ```; $ echo 'version development. workflow main {; input {; Directory d = ""/etc""; }; }' > main.wdl; ```; Will fail the womtool parser:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process input declaration 'Directory d = ""/etc""' (reason 1 of 1): Cannot coerce expression of type 'String' to 'Directory'; ```; Despite [coercion](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#type-coercion) from `String` to `Directory` being allowed by the WDL specification and this being among the examples (see [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#task-inputs) and [here](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#primitive-types)). Surprisingly, you can coerce a `String` into a `Directory` if it comes from an input file:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; }' > main.wdl. $ echo '{; ""main.d"": ""/etc""; }' > main.json; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl -i main.json; Success!; ```. Also puzzling is the following:; ```; $ echo 'version development. workflow main {; input {; Directory d; }; String s = sub(d, ""x"", ""y""); }' > main.wdl; ```; And then:; ```; $ java -jar womtool-67.jar validate main.wdl; Failed to process workflow definition 'main' (reason 1 of 1): Failed to process declaration 'String s = sub(d, ""x"", ""y"")' (reason 1 of 1): Failed to process expression 'sub(d, ""x"", ""y"")' (reason 1 of 1): Invalid parameter 'IdentifierLookup(d)'. Expected 'File' but got 'Directory'; ```; First of all, it is unclear why womtool claims sub expects a `File`, as the definition of [sub](https://github.com/openwdl/wdl/blob/main/versions/development/SPEC.md#string-substring-string-string) is `String sub(String, String, String)` so `File` is not something that should be expected. Here it should be allowed to coerce `Directory` to `String`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228:40,echo,echo,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6501#issuecomment-925057228,2,['echo'],['echo']
Availability,"Indeed. but the problem was that we couldn't tell that a file was missing; and indeed which file it was. On Mon, Jul 11, 2016 at 8:03 AM, Jeff Gentry notifications@github.com; wrote:. > IIRC the error in question was passing the 403 Forbidden back from Google; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231715169,; > or mute the thread; > https://github.com/notifications/unsubscribe/ACnk0ps25RuScsJmjoD9M1qUPteP2aLqks5qUjD_gaJpZM4JHehH; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231720082:195,error,error,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1137#issuecomment-231720082,1,['error'],['error']
Availability,"Instead of failing fast on database `Exception`s (NOTE: not `Error`s), the actors/fsms should schedule a retry. Example from [`WriteMetadataActor`](https://github.com/broadinstitute/cromwell/blob/9a734c4f36ae122153736004e8d54fc44af8fbde/services/src/main/scala/cromwell/services/metadata/impl/WriteMetadataActor.scala#L45-L52):; ```scala; case Event(FlushBatchToDb, HasEvents(e)) =>; log.debug(""Flushing {} metadata events to the DB"", e.length); addMetadataEvents(e.toVector) onComplete {; case Success(_) => self ! DbWriteComplete; case Failure(regerts) =>; log.error(""Failed to properly flush metadata to database"", regerts); self ! DbWriteComplete; }; stay using NoEvents; ```. On an exception, the `e` events could be rescheduled.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2016:61,Error,Error,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2016,3,"['Error', 'Failure', 'error']","['Error', 'Failure', 'error']"
Availability,"Instead of the regular `${script}` variable from, this would close #5768. . Pinging @rhpvorderman and @TMiguelT as container / batch system users I know of.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5784:76,Ping,Pinging,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5784,1,['Ping'],['Pinging']
Availability,"Interesting point!. I like the ~~idea~~ philosophy of CC tables being engine only, and queries being completely and solely calculable from metadata. It would probably mean ~~piping~~ forwarding all CC hashes, toggles of ""allowResultReuse"", failures to copy results, etc to the metadata.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306897781:240,failure,failures,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2338#issuecomment-306897781,1,['failure'],['failures']
Availability,"Interesting! I may have found another - deterministic - way, based on how it's done in [gopsutil](https://github.com/shirou/gopsutil/blob/e4ec7b275ada47ca32799106c2dba142d96aaf93/disk/disk_linux.go#L271):. 1. Find the `st_dev` device attribute for the mount point in `/proc/self/mountinfo` file,; which is ""the most authoritative source to check your mounts"" [1],; and is always present in modern Linux kernels [2]. Per [3], `st_dev`; > Identifies the device containing the file. The st_ino and st_dev, taken together, uniquely identify the file. The st_dev value is not necessarily consistent across reboots or system crashes, however. The format of `mountinfo`, according to [2]:. 3.5	/proc/<pid>/mountinfo - Information about mounts; --------------------------------------------------------. This file contains lines of the form:. 36 35 98:0 /mnt1 /mnt2 rw,noatime master:1 - ext3 /dev/root rw,errors=continue; (1)(2)(3) (4) (5) (6) (7) (8) (9) (10) (11). (1) mount ID: unique identifier of the mount (may be reused after umount); (2) parent ID: ID of parent (or of self for the top of the mount tree); (3) major:minor: value of st_dev for files on filesystem; (4) root: root of the mount within the filesystem; (5) mount point: mount point relative to the process's root; (6) mount options: per mount options; (7) optional fields: zero or more fields of the form ""tag[:value]""; (8) separator: marks the end of the optional fields; (9) filesystem type: name of filesystem of the form ""type[.subtype]""; (10) mount source: filesystem specific information or ""none""; (11) super options: per super block options. So for example, inside my task; ```; grep cromwell_root /proc/self/mountinfo. 904 885 8:16 / /cromwell_root rw,relatime master:325 - ext4 /dev/disk/by-id/google-local-disk rw; ```; `8:16` here is `st_dev`, with; > (3) major:minor: value of st_dev for files on filesystem. 2. Now we look up `major minor` in `/proc/diskstats` [4]:. The /proc/diskstats file displays the I/O statistics; of b",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-529613084:601,reboot,reboots,601,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-529613084,2,"['error', 'reboot']","['errors', 'reboots']"
Availability,"Interesting,. The above command gives me ; ```; $ java -jar cromwell-36.jar run checker_workflow_wrapping_workflow.cwl --inputs md5sum.json --imports .; [2018-11-13 10:26:04,67] [info] Running with database db.url = jdbc:hsqldb:mem:5089344a-b618-4a55-bf07-de1c4d6a9a1e;shutdown=false;hsqldb.tx=mvcc; [2018-11-13 10:26:09,77] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-11-13 10:26:09,78] [info] [RenameWorkflowOptionsInMetadata] 100% ; [2018-11-13 10:26:09,89] [info] Running with database db.url = jdbc:hsqldb:mem:740605a3-c63f-4491-b41a-58b162626bcb;shutdown=false;hsqldb.tx=mvcc; [2018-11-13 10:26:10,18] [info] Slf4jLogger started ; Exception in thread ""main"" java.io.IOException: Is a directory ; at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ; at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46) ; at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) ; at sun.nio.ch.IOUtil.read(IOUtil.java:197) ; ```. The same error occurs with a full path like ; ```; java -jar cromwell-36.jar run checker_workflow_wrapping_workflow.cwl --inputs md5sum.json --imports `pwd`; ```. It does give me a workaround since the following does lead to success!; ```; $ zip -r foo.zip .; $ java -jar cromwell-36.jar run checker_workflow_wrapping_workflow.cwl --inputs md5sum.json --imports foo.zip; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-438308547:1032,error,error,1032,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366#issuecomment-438308547,1,['error'],['error']
Availability,"Interestingly, I can get it to build for a single arch at once just fine [0] and the error is caused by lack of support for multiarch images somewhere in the local toolchain. [0] `platforms = List(""linux/arm64"")`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7107#issuecomment-1496729775:85,error,error,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7107#issuecomment-1496729775,1,['error'],['error']
Availability,"Intermittent Workflow Failure: ""Google credentials are invalid: connect timed out""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886:22,Failure,Failure,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886,1,['Failure'],['Failure']
Availability,Intermittent failure in test StandardFileHashingActorSpec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2816:13,failure,failure,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2816,1,['failure'],['failure']
Availability,"IntervalList to Done.; 2016-05-16 23:42:51,977 cromwell-system-akka.actor.default-dispatcher-10 INFO - WorkflowActor [UUID(303ad2dd)]: persisting status of $scatter_2 to Starting.; 2016-05-16 23:42:55,593 cromwell-system-akka.actor.default-dispatcher-10 INFO - WorkflowActor [UUID(303ad2dd)]: persisting status of $scatter_2 to Done.; 2016-05-16 23:43:15,790 cromwell-system-akka.actor.default-dispatcher-17 INFO - JES Run [UUID(303ad2dd):CollectQualityYieldMetrics:0:2]: Status change from Initializing to Running; 2016-05-16 23:43:18,436 cromwell-system-akka.actor.default-dispatcher-4 INFO - JES Run [UUID(7bbc0491):HaplotypeCaller:35]: Status change from Running to Success; 2016-05-16 23:43:19,178 cromwell-system-akka.actor.default-dispatcher-17 INFO - WorkflowActor [UUID(7bbc0491)]: persisting status of HaplotypeCaller:35 to Done.; 2016-05-16 23:43:29,519 cromwell-system-akka.actor.default-dispatcher-21 ERROR - Error during processing of request HttpRequest(GET,http://app:8000/api/workflows/v1/9c68fe34-7a9e-434a-b958-aa4d91339da9/status,List(Connection: Keep-Alive, X-Forwarded-Server: cromwell.gotc-prod.broadinstitute.org, X-Forwarded-Host: cromwell.gotc-prod.broadinstitute.org, X-Forwarded-For: 69.173.127.107, User-Agent: Java/1.8.0, Host: app:8000),Empty,HTTP/1.1); com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet successfully received from the server was 0 milliseconds ago. The last packet sent successfully to the server was 0 milliseconds ago.; at sun.reflect.GeneratedConstructorAccessor101.newInstance(Unknown Source) ~[na:na]; at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_72]; at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[na:1.8.0_72]; at com.mysql.jdbc.Util.handleNewInstance(Util.java:400) ~[cromwell.jar:0.19]; at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1038) ~[cromwell.jar:0.19]; at com.mysql.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/742#issuecomment-221909650:1743,ERROR,ERROR,1743,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/742#issuecomment-221909650,3,"['Alive', 'ERROR', 'Error']","['Alive', 'ERROR', 'Error']"
Availability,"Introduces a `RuntimeEnvironment` type in order to provide CWL expressions the proper values for the `runtime` ECMAscript variable. . See spec for explicit detail: http://www.commonwl.org/v1.0/CommandLineTool.html#Runtime_environment. Also introduce `MinimumRuntimeSettings` as the spec says ; > ""if an implementation can't provide the actual number of reserved cores during the expression evaluation time, it should report back the minimal requested amount."". I've made a few tradeoffs which I intend to document as tickets unless there are objections. To be clear the tradeoff is these compromises for speed, as I'm trying to ""spike"" on 1st-workflow and get it working :. * MinimumRuntimeSettings should come from the config. I see a major dependency tree coming all the way down from `RootCromwellActor` and I'm trying to think of a better way. In this PR I've taken the shortcut of instantiating MinimumRuntimeSettings with default values hardcoded.; * The values of `outdirSize` and `tmpdirSize` are specified in CWL but I haven't yet figured out how to provide those values accurately. I will likely create an issue to do this effectively as I doubt this is regularly used.; * I think we could constrain the types of `RuntimeEnvironment` better than the `String` and `Int` we are using currently, e.g. using `Path` and `MemorySize`. This requires moving these types up to the `wom` package.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2810:777,down,down,777,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2810,1,['down'],['down']
Availability,Investigate failure to disconnect FTP client properly,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4173:12,failure,failure,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4173,1,['failure'],['failure']
Availability,Investigate workflow failures in FireCloud,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1821:21,failure,failures,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1821,1,['failure'],['failures']
Availability,Is that not a real failure?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164844157:19,failure,failure,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164844157,1,['failure'],['failure']
Availability,"Is the problem that the error message is unhelpful, or that there is an error in the migration that is blocking?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2108#issuecomment-290504640:24,error,error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2108#issuecomment-290504640,2,['error'],['error']
Availability,Is this issue related to problem with deadlocks in `ServicesStoreSpec`?; If this task is still actual I'll appreciate more information about it (if any available).; @gemmalam @danbills,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4762#issuecomment-518659520:152,avail,available,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4762#issuecomment-518659520,1,['avail'],['available']
Availability,Is this kind of redundant to the token system?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370562042:16,redundant,redundant,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370562042,1,['redundant'],['redundant']
Availability,Is this scheme robust to large files?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/272#issuecomment-154187746:15,robust,robust,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/272#issuecomment-154187746,1,['robust'],['robust']
Availability,"Is this to account for things that are ""provider-dependent"" as described in the [FileSystems JavaDoc](https://docs.oracle.com/javase/8/docs/api/java/nio/file/FileSystems.html)? I'm curious exactly what behavior you saw, such as `getFileSystem` returning a reference to a closed file system or throwing `FileSystemNotFoundException`. I think it's worth having a link to any documentation that describes any provider-dependent behavior or documenting our observations ourselves. I'm also still contemplating that potential gap between `getFileSystem` and `newFileSystem`. I'd love to avoid adding our own synchronization around this, but I guess that really depends on how the blob filesystem behaves and what failure modes look like and how we want to handle them.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457:708,failure,failure,708,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6816#issuecomment-1204475457,1,['failure'],['failure']
Availability,"Issue was resolved and issue was due to launch template of EC2 and It; should be launched with ssm agent installed on it .; Better option is to use Amazon genomics cli . When you deploy a Context agc; will create batch queues and s3 for you . It's better to use that queue and; s3 in cromwell cofig .; It works better. Regards,; Divya. On Fri, Apr 8, 2022 at 3:13 AM thousand-petalled ***@***.***>; wrote:. > Hey @DivyaThottappilly <https://github.com/DivyaThottappilly> do you; > still have this issue? I'm trying to get up and running a basic Hello World; > but keeps getting an S3Exception null error (301).; >; > It seems like you've already past that stage and if you don't mind, could; > you help me setup this?; >; > —; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1092523379>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AGA4NTQVWHYKZC7RLKGT4OTVD7MCNANCNFSM5N3DE7QQ>; > .; > You are receiving this because you were mentioned.Message ID:; > ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127:598,error,error,598,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6671#issuecomment-1274196127,1,['error'],['error']
Availability,"Issues addressed:. - Custom-named stdout or stderr files were not being delocalized.; - The wrong stdout and stderr files were being delocalized (user action instead of user command).; - Optional output files that were not produced were being published to metadata as if they had been produced.; - Existence check for optional files was a `-a` test not actually supported by Alpine's `/bin/sh`. This failed with a zero error code and only minor noise to stderr.; - The fallback non-link chasing symlink that fails noisily for `cwl.output.json` and other no-match globs should now fail silently.; - The output manipulator didn't handle cases of > 1 type (optionals, coproducts) correctly in most cases. This is only a partial fix since it turned out not to be required for this test.; - Several non-WDL classes were found in a `wdl` package.; - The `read_tsv` Centaur test was falsely claiming success when the files referenced in the generated TSV were never delocalized from the VM to their stated cloud locations.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3668:419,error,error,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3668,1,['error'],['error']
Availability,"Issues addressed:. - ~~Custom-named stdout or stderr files were not being delocalized.~~ Handled in #3695 instead; - ~~The wrong stdout and stderr files were being delocalized (user action instead of user command).~~ Handled in #3695 instead; - ~~Optional output files that were not produced were being published to metadata as if they had been produced.~~ Revert fixes for this due to existence check performance issues; - Existence check for optional files was a `-a` test not actually supported by the delocalization Docker's `/bin/sh`. This failed with a zero error code and only minor noise to stderr.; - ~~The fallback non-link chasing symlink that fails noisily for `cwl.output.json` and other no-match globs should now fail silently.~~ Reverted by reviewer request.; - The output manipulator didn't handle cases of > 1 type (optionals, coproducts) correctly in most cases. This is only a partial fix since it turned out not to be required for this test.; - Several non-WDL classes were found in a `wdl` package. Now correctly moved thanks to reviewer input. 🙂 ; - The `read_tsv` Centaur test was falsely claiming success when the files referenced in the generated TSV were never delocalized from the VM to their stated cloud locations. With the existence checks removed from Cromwell proper this has reverted back to ""failing to fail"" which is mistaken for success.; - File literals now actually create a file, needed because the OutputManipulator does existence checks on outputs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3682:564,error,error,564,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3682,1,['error'],['error']
Availability,"It allows a user to understand that they're requesting to abort something which is already terminal. Whether or not that's useful is a matter of debate. Clearly someone does because it was put in by someone. I say that it's ""useful"" but am using quotes because I doubt anyone is ever going to act on knowledge. I don't recall if it returns 200 or just a generic error (I could dig it up if it matters, but not now as I don't have the energy and I don't think we should keep it)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379:362,error,error,362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379,1,['error'],['error']
Availability,"It appears that the `evaluateFiles` method on `WomExpression` first tries to evaluate the expression, basically calling `evaluateValue`.; In the case of globs, this means trying to read the file containing the list of files that have been globbed, which is; 1) (Almost - see 2)) bound to fail since `evaluateFiles` is called before the task is run to determine the output files that _will_ be generated by an expression, therefore trying to evaluate the glob is pointless and generates unnecessary I/O.; 2) If for some reason the list file _is_ there but is invalid, the result of `evaluateFiles` will be invalid. Why 2) would happen is unknown at the moment, but some of our centaur integration test (dontglobinputs) present very odd failure mode consistent with 2)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4202:735,failure,failure,735,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4202,1,['failure'],['failure']
Availability,It appears that the better file implementation of `nameWithoutExtension` can check for the existence of the file under some circumstances. For GCS files this means an http request which seems unnecessary and and prone to failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3602:221,failure,failures,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3602,1,['failure'],['failures']
Availability,"It appears that this is the same error being referenced in #1782, closing that one but leaving the breadcrumb in case someone wants a stacktrace",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1846#issuecomment-274598829:33,error,error,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1846#issuecomment-274598829,1,['error'],['error']
Availability,"It is not about cromwell subscribing its own events. As u already said, cromwell has exposed restful api for external integration, so it is your job to monitor workflow status as u want such as maintaining an event system like influxdb. Say, you can setup a telegraf exec plugin for polling cromwell server periodically and streaming status into infuxdb, then use influxdb as an event system and trigger all downstream actions once status is changed, you can even setup a grafana as dashboard of workflows monitor system. Or if your crowmwell server can be accessed via internet, the easier way is to poll it from AWS lambda and put workflow status to aws SQS or SNS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833:408,down,downstream,408,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833,1,['down'],['downstream']
Availability,"It is very common to provide folders as inputs to different bioinformatic tools. For instance, STAR index is usually computed once per reference genome and then provided to each STAR-based RNA-Seq task as an input. However, when this is done a common caching failure is reported (because it is a folder):; ```; ""hashFailures"": [; {; ""causedBy"": [],; ""message"": ""Is a directory""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2735:259,failure,failure,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2735,1,['failure'],['failure']
Availability,"It looks like cromwell doesn't properly handle optional variables in tasks. If they are undefined this error occurs: ""Optional value was not set and no 'default' attribute was provided"". Setting a default (either in the input block or like so: `~{default="""" optionalValue}`) will cause the workflow to run, but this seems to me to go entirely against the reason for having an optional datatype to begin with. It also looks like setting an optional with default to `null` in the input JSON is ignored, which is in conflict with the SPECs [here](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#optional-inputs-with-defaults). I'm getting this error when using WDL version 1.0 (draft-3). The error occurs in both cromwell version 33 and 34. Full stacktrace:; ```; 2018-07-24 09:54:27,543 cromwell-system-akka.dispatchers.backend-dispatcher-53 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(514f031f)AlignStar.star:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:536); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:471); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:265); at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:264); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:208); at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(Sha",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927:103,error,error,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927,5,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"It looks like it does now shut down, but the exit code is 0, which is potentially deceiving to people or programs that interact with it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2645#issuecomment-394727569:31,down,down,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2645#issuecomment-394727569,1,['down'],['down']
Availability,"It looks like the current CI failure (below) is spurious and unrelated to the changes in this PR. The [CI clone of this PR](https://github.com/broadinstitute/cromwell/pull/5314) is successfully passing CI checks. . $ src/ci/bin/test.sh; src/ci/bin/test.inc.sh: line 645: /home/travis/build/broadinstitute/cromwell/src/ci/bin/testCentaurHoricromtalPapiV2.sh: No such file or directory; The command ""src/ci/bin/test.sh"" exited with 1.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5270#issuecomment-583121715:29,failure,failure,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5270#issuecomment-583121715,1,['failure'],['failure']
Availability,"It looks like the link OP posted is now a 404, but I'd also like to voice an interest in this option. I have written a few workflows by now and have found that the output of Cromwell is very difficult to navigate through. It's tricky to quickly find errors, especially if the output is piped through a file with no color coding or any situation involving automated tests.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3919#issuecomment-1092107938:250,error,errors,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3919#issuecomment-1092107938,1,['error'],['errors']
Availability,"It looks like the parent `PipelinesApiRequestManager` may be handling this okay but that should be confirmed. ```; ERROR akka.actor.OneForOneStrategy - 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server Error)!!1</title>; <style>; *{margin:0;padding:0}html,code{font:15px/22px arial,sans-serif}html{background:#fff;color:#222;padding:15px}body{margin:7% auto 0;max-width:390px;min-height:180px;padding:30px 0 15px}* > body{background:url(//www.google.com/images/errors/robot.png) 100% 5px no-repeat;padding-right:205px}p{margin:11px 0 22px;overflow:hidden}ins{color:#777;text-decoration:none}a img{border:0}@media screen and (max-width:772px){body{background:none;margin-top:0;max-width:none;padding-right:0}}#logo{background:url(//www.google.com/images/branding/googlelogo/1x/googlelogo_color_150x54dp.png) no-repeat;margin-left:-5px}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>That’s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>That’s all we know.</ins>. com.google.api.client.http.HttpResponseException: 502 Bad Gateway; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 502 (Server E",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917:115,ERROR,ERROR,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'errors']"
Availability,"It looks like upgrading from `Constructor` to `SafeConstructor` does not make much difference, Cromwell errors out and refuses to proceed with a similar message in both cases. But it seems like a good move anyway. With `SafeConstructor`:. `java -jar /Users/anichols/Projects/cromwell/server/target/scala-2.12/cromwell-70-1a6c161-SNAP.jar run test3.cwl`; ```; could not determine a constructor for the tag tag:yaml.org,2002:javax.script.ScriptEngineManager; ```. With `Constructor`:. `java -jar cromwell-69.jar run test3.cwl`:; ```; could not determine a constructor for the tag '!!javax.script.ScriptEngineManager'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6510:104,error,errors,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510,1,['error'],['errors']
Availability,"It seemed to go like this in FC:. - Cromwell received some workflows in a batch; - [...] some unknown amount of time passed; - `abort` was run on the remaining workflows; - Some workflows aborted fine (but always took at least 2 `abort` calls to abort); - Others returned 404s; - It's unknown whether these also took 2 `abort` calls to abort, and only the second returned 404. Because rawls is retrying on any error, these `abort` calls are now chewing up a lot of Cromwell resource unnecessarily",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4497:410,error,error,410,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497,1,['error'],['error']
Availability,"It seems like the root cause of the bug is an error by the [story](https://broadworkbench.atlassian.net/browse/BW-568) author 😄 . > Behavior is undefined when groups are tied, whatever happens by default is fine",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1067386292:46,error,error,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1067386292,1,['error'],['error']
Availability,"It seems possible to use the --metadata-output option with the cromwell server command, but when I send the command it returns a syntax error:. ```; java -jar /home/shared/cromwell/cromwell-54.jar server --metadata-output meta.json; Error: Unknown option --metadata-output; Error: Unknown argument 'meta.json'; ```. Notably, it seems to work on ""run"":; ```; java -jar /home/shared/cromwell/cromwell-54.jar run main.wdl --metadata-output meta.json; ```. Ideally, I'd like to send a command like such:; ```; java -Dconfig.file=cromwell.conf -jar /home/shared/cromwell/cromwell-54.jar server --metadata-output meta.json &; ```. I'm basing my usage here on this documentation:; https://cromwell.readthedocs.io/en/stable/CommandLine/. Any help here would be much appreciated.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6076:136,error,error,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6076,3,"['Error', 'error']","['Error', 'error']"
Availability,It seems that @TMiguelT has already addressed part of [this issue](https://github.com/hpcng/singularity/issues/2597). . The singularity cache is not meant as a replacement for storing your images. It is merely meant as a way to avoid downloading too much. . It seems we have to hack something together in bash or contribute something upstream. I think bash hacking can be fairly succesful but it will be very ugly. The requirements for the cache:; * Make use of singularity's layer cache to avoid too much downloading.; * Should be thread safe. Singularity 3.6.0 commands will use a threadsafe cache. Older version will have to use a universal file lock.; * Will put all `.sif` images in a single shared location.; * Will check if a file exists before pulling. This will mean a cache-first approach. Internet outages should not affect workflows that have already run several times.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844:234,down,downloading,234,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-631402844,3,"['down', 'outage']","['downloading', 'outages']"
Availability,"It seems that the input to the missing-in-action scatter, the indexed array that it scatters on, is empty and that is the root cause of the failure....; IMO it would help to see in the log/report that the scatter has actually been observed but it simply had 0 elements which quite often may indicate that something went wrong (perhaps it should have a warning icon next to it in the report (e.g. a yellow ! triangle) .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4136#issuecomment-423669377:140,failure,failure,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136#issuecomment-423669377,1,['failure'],['failure']
Availability,"It seems that the protocol for setting runtime attributes is to do so within the task, thus allowing expressions based on their values. Say,. ```wdl; task foo {; Int cpus. runtime {; cpus = cpus; }. command {; ./my_binary --threads ${cpus * 2}; }; }; ```. However, a lot of the time, it's not appropriate (in a ""separation of concerns"" sense) to thread the value through the task invocation. For example, you may be setting a Unix group under which all data should be accessed, defining credentials, etc. We're doing this by using the `default_runtime_attributes`, passed in as workflow options. However, these are not visible to the task. This is what we'd like to be able to do, for example:. Workflow Options:; ```json; {; ""default_runtime_attributes"": {; ""AUTH_USER"": ""foo"",; ""AUTH_TOKEN"": ""bar""; }; }; ```. Workflow:; ```wdl; task {; command {; export AUTH_USER=""${AUTH_USER}"" # Taken from default_runtime_attributes; export AUTH_TOKEN=""${AUTH_TOKEN}"" # Taken from default_runtime_attributes; ./my_authenticated_command; }; }; ```. At the moment, this will fail, as `AUTH_USER` and `AUTH_TOKEN` are not defined within the task. Even if you explicitly define it in the task (`String AUTH_USER`, etc.), Cromwell won't automatically seed this from the default options. I can see why it would be useful to define the inputs in the task, for clarity's sake. I'm just thinking out aloud -- so this is very much half-baked -- but perhaps an option would therefore be to have an additional keyword that made it explicit that the task value was to be taken from options:. ```wdl; task {; Int something; runtime String AUTH_USER # ""runtime"" implies this is from the runtime attributes (default or otherwise); runtime String AUTH_TOKEN # Raise an error if undefined or modified within the task. command {; export AUTH_USER=""${AUTH_USER}""; export AUTH_TOKEN=""${AUTH_TOKEN}""; ./my_authenticated_command -n ${something}; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4741:1741,error,error,1741,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4741,1,['error'],['error']
Availability,It seems the sentry configuration is generating too much log from log.error due to log level set to WARN or above. So removing this configuration altogether from logback.xml.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5065:70,error,error,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5065,1,['error'],['error']
Availability,"It seems to me that callCaching is not working when a task takes a Directory as input. Take the following example WDL:; ```; version development. workflow main {; call task1 { input: s = ""file"" }; call task2 { input: d = task1.d }; output { String s = task2.s }; }. task task1 {; input {; String s; }. command <<<; set -euo pipefail; mkdir dir; touch ""dir/~{s}""; >>>. output {; Directory d = ""dir""; }. runtime {; docker: ""debian:stable-slim""; }; }. task task2 {; input {; Directory d; }. command <<<; set -euo pipefail; ls ""~{d}""; >>>. output {; String s = read_string(stdout()); }. runtime {; docker: ""debian:stable-slim""; }; }; ```. On a first `141477ef-e8e6-4fb9-ae58-5c2e8a646088` run, callCaching for `task2` is negative, as it should, with this error:; ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [; {; ""message"": ""gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir"",; ""causedBy"": []; }; ],; ""message"": ""[Attempted 1 time(s)] - FileNotFoundException: gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir""; }; ],; ""allowResultReuse"": false,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ```. Now though, the directory has been created as a result of the WDL succeeding:; ```; $ gsutil ls gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir; gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir/file; ```. On a second `2690f8a5-4cd4-45e2-a93a-55125a1107f8` run, callCaching for `task2` is negative again though, with this error:; ```; ""callCaching"": {; ""hashFailures"": [; {; ""causedBy"": [; {; ""message"": ""gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir"",; ""causedBy"": []; }; ],; ""message"": ""[Attempted 1 time(s)] - FileNotFoundException: gs://my-bucket/cromwell-executions/main/141477ef-e8e6-4fb9-ae58-5c2e8a646088/call-task1/dir""; }; ]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6509:751,error,error,751,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6509,1,['error'],['error']
Availability,"It should be fairly easy to add yes, we can also drill down deep in the caused by chain or not depending on how wide/specific we want to be",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183#issuecomment-295243139:55,down,down,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183#issuecomment-295243139,1,['down'],['down']
Availability,"It still exists and no one ever determined if it was appropriate, so it is still potentially relevant. It's just not how we typically handle errors that occur in API calls so it looked weird. May or may not actually be an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2418#issuecomment-336300094:141,error,errors,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2418#issuecomment-336300094,1,['error'],['errors']
Availability,"It turns out that ""Complete Success"" just wasn't quite right for Cromwell. EDIT: the actual motivation for this is that this was the cause for some of our `dead letter` errors appearing at the end of `cromwell run` commands.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2218:169,error,errors,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2218,1,['error'],['errors']
Availability,"It turns out the original task above didn't work when `combined_gvcf=""gs://dsde-palantir/SnapshotExperiment2015/CEUTrio/gvcfs/NA12878.d691bf66-37af-4375-9c09-6a6607f322e8.g.vcf.gz""` even in V24. I updated the task to:. ```; task IndexVCF {; File combined_gvcf; Int disk_size. command {; /usr/gitc/tabix ${combined_gvcf}; }; runtime {; docker: ""broadinstitute/genomes-in-the-cloud@sha256:d7aa37fc8351074a2d6fb949932d3283cdcefdc8e53729dcf7202bee16ab660a""; memory: ""13 GB""; cpu: ""1""; disks: ""local-disk "" + disk_size + "" HDD""; }; output {; File gvcf_index = ""${combined_gvcf}.tbi""; }; }; ```. The thing is I still don't know why the first task in the comment above doesn't work. It would be nice to have a better error message.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-298415178:710,error,error,710,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229#issuecomment-298415178,1,['error'],['error']
Availability,"It turns out the syntax change; https://github.com/openwdl/wdl/blob/main/versions/draft-2/SPEC.md#scatter--gather. I wonder if the documentation could be improved?. all the examples I found by googling things like 'wdl array iteration' found links to version 1 example. eventually, I stumbled on the idea of searching for wdl gather. Gather is not a standard term in computer science. Iterating over arrays does not require a scatter task. . I understand it is hard to write front ends with good error messages. I wonder if there is a way to write a something that checks for wdl version incompatibilities. I belive my womtool reported my wdl was valid. . Kind regards. Andy. Also there is a type in the code example https://github.com/openwdl/wdl/blob/main/versions/draft-2/SPEC.md#scatter--gather . ; ```; call sum {input: ints = inc.increment}; ```. should be; ```; call sum {input: ints = inc.incremented}; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7093#issuecomment-1478250519:496,error,error,496,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7093#issuecomment-1478250519,1,['error'],['error']
Availability,"It will take me a while to dig up an online reference (googling java thread safety returns a ton of results to sort through). But creating one's own private lock is an extra level of paranoia, kind of like marking all java variables as `final`, or reducing the scope of classes to `private`. If one uses `this` as a mutex, then others can actually steal your lock, by locking **you**. ```scala; object LiquibaseUtils {; def echoQuick = {; this.synchronized {; println(""hello""); }; }; }. object ForeignImplementation {; LiquibaseUtils.synchronized {; // I have your lock!; Thread.sleep(1.day.toMillis); }; }; ```. If however the synchronization is done on a private variable, it can never be shared by outside participants. ```scala; object LiquibaseUtils {; private val cantTouchThis = new Object; def echoQuick = {; cantTouchThis.synchronized {; println(""hello""); }; }; }. object ForeignImplementation {; LiquibaseUtils.synchronized {; // Doesn't affect echoQuick; Thread.sleep(1.day.toMillis); }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4390#issuecomment-439099381:424,echo,echoQuick,424,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4390#issuecomment-439099381,3,['echo'],['echoQuick']
Availability,"It would be easier to consume errors if they were wrapped in JSON. I believe 400 errors are already represented this way already, but a 500 response timeout returns with Content-Type:`text/plain`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/947:30,error,errors,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/947,2,['error'],['errors']
Availability,"It would be great if the error was more descriptive, including what the error is and what the user can do about it (if anything).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-290504375:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-290504375,2,['error'],['error']
Availability,"It would be great to be able to declare variables based on output from tasks. In this example WDL I want to use an array of all the shards from TaskA as input to TaskB even though TaskB is in the same scatter. ```; task TaskA {; File single_input_file. command {; #Something happens that takes a single file and returns a single file; }; output {; File output_single_file = ""some.file""; }; }. task TaskB {; Array[File] array_of_files; File single_input_file. command {; #Something happens that takes an array of files and a single file; }; }. workflow inputsFromScatter {; Array[File] list_of_files = [""a.file"", ""b.file"", ""c.file""]. scatter(file in list_of_files) {; call TaskA {; input:; single_input_file = file; }. call TaskB {; input:; single_input_file = file,; array_of_files = variable_declared_outside_of_scatter; }; }. Array[File] variable_declared_outside_of_scatter = TaskA.output_single_file; }; ```. This workflow currently results in this error: `Workflow input processing failed.; Workflow has invalid declarations: Could not find a value for TaskA`. I may have oversimplified this example because TaskB could be in a separate scatter. If you'd like a real example with actual tasks I'd be happy to provide it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1513:953,error,error,953,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1513,1,['error'],['error']
Availability,"It would be great to have at least some unit tests targeted towards key request flows that CromIAM is expected to perform.; This should include correct order of the requests between SAM <-> CromIAM <-> Cromwell and validity of the responses.; Mocks of SAM and Cromwell could be useful to not depend on real implementation. A/C: This ticket is a good example of something that it would be nice for those unit tests to catch: https://github.com/broadinstitute/cromwell/issues/4284; Other example would be:; - A submit requests first goes to sam to check against the whitelist before going to Cromwell (if whitelisted), or returning an error (if not)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4286:633,error,error,633,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4286,1,['error'],['error']
Availability,"It would be nice to have conda recipes for cromwell and its dependencies as part of the [conda-forge channel](https://conda-forge.github.io/). The package would be roughly analogous to the one available via Homebrew, except multi-platform.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1446:193,avail,available,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1446,1,['avail'],['available']
Availability,"It would be nice to have some more documentation about this. When I first logged in this morning, I couldn't access the board, so I tried creating an account and that also failed initially for an _unexpected error, please try again later_ sort of thing. . Also, what board do we create Cromwell issues under? My best guess is `Jira Support` and that's where I created my issue: [Cromwell (server) loses ability to poll some workflows](https://broadworkbench.atlassian.net/browse/JS-34), but all of the other issues aren't really Cromwell related. A ""query"" field might also be useful. . These are the boards currently on Jira:; - `Batch Analysis`; - `Cloud Accounts`; - `Data-repo`; - `DevOps`; - `DSP-ELT Backlog`; - `Interactive Analysis`; - `Jira Support`; - `New Project`; - `PERF`; - `PRODUCTION`; - `QA`; - `SAND-NG`; - `SANDBOX`; - `SUPPORT`; - `TERRA ROADMAP`; - `TerraUI`; - `User Metrics`; - `UX`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502892778:208,error,error,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-502892778,1,['error'],['error']
Availability,"It would be nice to have this information available from the command line too, i.e. a -version flag that dumps the version/hash and exits.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1694#issuecomment-262027405:42,avail,available,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1694#issuecomment-262027405,1,['avail'],['available']
Availability,It would be really handy to have a `cromwell.server` script that would make starting and stopping a server cromwell easy. ```; $ cromwell.server start; ... Started!. $ cromwell.server status; running. $ echo $?; 0. $ cromwell.server stop; ... Stopped!. $ cromwell.server status; stopped. $ echo $?; 1; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2623:203,echo,echo,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2623,2,['echo'],['echo']
Availability,"It's a bit confusing, you've posted the error log from a stage called `CollectSequencingArtifactMetrics`, but posted the WDL for a task called `FilterByOrientationBias`. Could you please send the WDL for the same task that's failing?. A case where this bug can happen is when you do doing string manipulation with `File` type variables. For example, if you `sub()` a `File` object to generate a new filename, it will use the S3 URL, as the input to `sub`. However, by the S3 URL, when interpreted by Bash as a path, doesn't exist on the right disk. I wrote an issue about it [here](https://github.com/openwdl/wdl/issues/260). Tell me if it's this same issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436311106:40,error,error,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436311106,1,['error'],['error']
Availability,"It's a bit of a pet peeve of mine when error messages don't put quotes around raw values. This leads to unnecessarily confusing scenarios like; ```; Your input did not meet our requirements.; ```; for an input of `""""`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4262:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4262,1,['error'],['error']
Availability,"It's a dependency resolution error, which seems surprising. I cleared Travis cache and restarted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505586193:29,error,error,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-505586193,1,['error'],['error']
Availability,"It's a real error. I know what it is, how to resolve might end up being tricky. Will see when I'm back",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164846710:12,error,error,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/323#issuecomment-164846710,1,['error'],['error']
Availability,It's also possible that by; >all 500 errors. @cjllanwarne means 5xx,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521654448:37,error,errors,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521654448,1,['error'],['errors']
Availability,It's happening again. Last time this was the result of our LB blocking new pingdom servers. @hjfbynara could you please run update script?. I don't see any added in december here: https://help.pingdom.com/hc/en-us/community/posts/208953545-Pingdom-Probe-Servers-Pingdom-IPs.; But it seems too coincidental that the symptoms are exactly the same as they were last time. ![image](https://user-images.githubusercontent.com/165320/50653143-8f762700-0f56-11e9-9b2d-76da416c47f2.png),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-451223835:75,ping,pingdom,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-451223835,3,"['Ping', 'ping']","['Pingdom-Probe-Servers-Pingdom-IPs', 'pingdom']"
Availability,"It's possible for Cromwell to not write a `start` time against a call in metadata. ### Impact ; If this happens, Cromwell's metadata ends up with a call with no `start` time but with an `end` time. This causes problems amongst many clients, not least of which is Job Manager which returns a `500` error page and no further details. . ### Cause. #### Why is the start time not set?; This appears to be happening if the `input` section of a call has an expression which fails to evaluate:; Here's a potential example which might cause this:; ```wdl; Array[String] strings = [""0"", ""1""]; call foo { input:; x = strings[2]; }; ```. #### Why is Job Manager Unhappy?; From the logs, the error is being caused by: ; ```; May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: File ""/home/vmagent/app/jobs/controllers/jobs_controller.py"", line 126, in get_job; May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: sorted_tasks = sorted(tasks, key=lambda t: t.start); May 16 14:41:51 gce-job-manager-prod101 job-manager-api[2834]: TypeError: can't compare datetime.datetime to NoneType; ```. ### What to do about it. This ticket is to address this problem in two ways:; * Cromwell should not be writing calls to metadata with no `start` time (especially if they *do* have an end time. It's hard to prove this 100%, but at least for the above case we should be resilient.; * Job Manager should not return a 500 error code if a `start` time is missing. It should be resilient enough to render the tasks (even if the sorting is awkward). Bonus points for indicating bad metadata somehow so that the user can forward on the problematic metadata to us.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4978:297,error,error,297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4978,5,"['error', 'resilien']","['error', 'resilient']"
Availability,It's very cool that we get the timing breakdown for the preempted ones. As far as I know all of the other failures don't get any events...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/458#issuecomment-185924247:106,failure,failures,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/458#issuecomment-185924247,1,['failure'],['failures']
Availability,"Iterate at whatever pace makes sense towards running a 10K JG w/ the mock WDL. If there issues (errors and/or things which seem unreasonably pokey) follow this decision tree:. - If the problem seems small, fix it and continue; - If the problem seems large but there's a clear hack/workaround, file a ticket and do the hack/workaround then continue; - If the problem seems large but there is no clear hack/workaround, bring this up with the team to figure out how to proceed",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2145:96,error,errors,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2145,1,['error'],['errors']
Availability,JABJEA failure catcher. Closes #1435,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1588:7,failure,failure,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1588,1,['failure'],['failure']
Availability,"JDOM was removed in https://github.com/broadinstitute/cromwell/pull/6785 and this branch is updated from `develop`:. ```; root(aen_bw_1228)> | 81> whatDependsOn org.jdom jdom2; [ ... ]; [error] whatDependsOn org.jdom jdom2; [error] ^; ```. ( This is the normal output when `whatDependsOn` does not find something, see https://github.com/broadinstitute/cromwell/pull/6775 https://github.com/broadinstitute/cromwell/pull/6776 ). For Protobuf, the new MySQL pulls in a safe version ≥ 3.16.1:. ```; +-mysql:mysql-connector-java:8.0.29; | +-com.google.protobuf:protobuf-java:3.19.4; ```. which evicts older versions used by other dependencies. ```; +-io.opencensus:opencensus-proto:0.2.0; | +-com.google.protobuf:protobuf-java:3.19.4; | +-com.google.protobuf:protobuf-java:3.5.1 (evicted by: 3.19.4); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6793:187,error,error,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6793,2,['error'],['error']
Availability,"JDX0dTVVRJTD0kPwppZiBbICIkUkNfR1NVVElMIiAhPSAiMCIgXTsgdGhlbgogIHByaW50ZiAnJXMgJXNcbicgIiQoZGF0ZSAtdSAnKyVZLyVtLyVkICVIOiVNOiVTJykiIGdzdXRpbFwgXCBcIGNwXCAvY3JvbXdlbGxfcm9vdC9yY1wgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvY3JvbXdlbGwtZXhlY3V0aW9uL2V4Y2VlZF9kaXNrX3NpemUvYThlMzJjZDItZTMwNi00OGI2LTlkNWMtODQ3YjFlZDg1NzU0L2NhbGwtc2ltcGxlX2xvY2FsaXplX2FuZF9mZXRjaF9zaXplL1wgZmFpbGVkCiAgIyBQcmludCB0aGUgcmVhc29uIG9mIHRoZSBmYWlsdXJlCiAgY2F0IGdzdXRpbF9vdXRwdXQudHh0CiAgCiAgIyBDaGVjayBpZiBpdCBtYXRjaGVzIHRoZSBCdWNrZXRJc1JlcXVlc3RlclBheXNFcnJvck1lc3NhZ2UKICBpZiBncmVwIC1xICJCdWNrZXQgaXMgcmVxdWVzdGVyIHBheXMgYnVja2V0IGJ1dCBubyB1c2VyIHByb2plY3QgcHJvdmlkZWQuIiBnc3V0aWxfb3V0cHV0LnR4dDsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgUmV0cnlpbmdcIHdpdGhcIHVzZXJcIHByb2plY3QKICAgIGdzdXRpbCAtdSBicm9hZC1kc2RlLWNyb213ZWxsLWRldiAgY3AgL2Nyb213ZWxsX3Jvb3QvcmMgZ3M6Ly9zc19jcm9td2VsbF9idWNrZXQvY3JvbXdlbGwtZXhlY3V0aW9uL2V4Y2VlZF9kaXNrX3NpemUvYThlMzJjZDItZTMwNi00OGI2LTlkNWMtODQ3YjFlZDg1NzU0L2NhbGwtc2ltcGxlX2xvY2FsaXplX2FuZF9mZXRjaF9zaXplLwogIGVsc2UKICAgIGV4aXQgIiRSQ19HU1VUSUwiCiAgZmkKZWxzZQogIGV4aXQgMApmaQogICkKICBSQz0kPwogIGlmIFsgIiRSQyIgPSAiMCIgXTsgdGhlbgogICAgYnJlYWsKICBmaQogIGlmIFsgJGkgLWx0IDMgXTsgdGhlbgogICAgcHJpbnRmICclcyAlc1xuJyAiJChkYXRlIC11ICcrJVkvJW0vJWQgJUg6JU06JVMnKSIgV2FpdGluZ1wgNVwgc2Vjb25kc1wgYW5kXCByZXRyeWluZwogICAgc2xlZXAgNQogIGZpCmRvbmUKZXhpdCAiJFJDIg==\\\""));' > /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh && chmod u+x /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh && sh /tmp/c7f23139-58e4-466e-b109-c06db3209f35.sh\"": ""; }; ]; ```. AC:; Improve this error message by...; 1. Removing all of the message starting from ` Unexpected exit status 1 while running ....`; 2. Replace:; `Execution failed: action 9: unexpected exit status 1 was not ignored\n[Localization] Input name: input_file `; with something similar to:; `Execution failed: action 9: Failed to copy input <input_name> <input_path ie, gs://.....>. Please check the log file for more details: <link to call log>`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4603:8889,error,error,8889,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4603,1,['error'],['error']
Availability,JES Backend - Add retries to certain JES VM failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1910:44,failure,failures,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1910,1,['failure'],['failures']
Availability,JES Recover Closes #751,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1197:4,Recover,Recover,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1197,1,['Recover'],['Recover']
Availability,JES WF hangs when Disk Full error encountered,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2054:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2054,1,['error'],['error']
Availability,JES backend giving permission error during localization that seems incorrect.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1960:30,error,error,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1960,1,['error'],['error']
Availability,JES error causes cromwell to hang and spew useless (for a user) error messages,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:4,error,error,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,2,['error'],['error']
Availability,JES errors are hard to debug and better reporting is helpful,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1637:4,error,errors,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1637,1,['error'],['errors']
Availability,JES fails with mysterious error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2270:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2270,1,['error'],['error']
Availability,JES gave an intermittent 404 error when trying to copy outputs from the cached call directory to the current call directory (discovered by Vivek when using Call Caching for GoTC) . AC: Have Cromwell retry Call Caching when upon receipt of the 404 error.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/517:29,error,error,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/517,2,['error'],['error']
Availability,"JES has added an additional ""event"" to their metadata. For example:; events: ; - description: copied 1 file(s) to ""gs://some/path/sample.vcf"" ; startTime: '2016-08-15T19:39:49.261235611Z'. gcloud alpha genomics operations describe EJ7K1P3oKhjukpy15ueGiy0gw7vetLsXKg9wcm9kdWN0aW9uUXVldWU (for more details). Cromwell tries to record these execution events to the db, but sometimes the new events exceed the 255 char limit on the 'DESCRIPTION' column. . AC: Filter out these file copying events in 0.19_hotfix so that they don't clog up the execution events table, which can lead to downstream status update failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1299:581,down,downstream,581,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1299,2,"['down', 'failure']","['downstream', 'failures']"
Availability,"JES job recovery attempted despite ""abort-jobs-on-terminate"" enabled",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050:8,recover,recovery,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050,1,['recover'],['recovery']
Availability,"JES now allows for labels to be applied to pipeline runs. Labels are important because they allow us to tag pipeline tasks with metadata which is exposed in other Google APIs, specifically in billing exports that are visible in BigQuery. This will allow us to, for example, calculate the exact cost of a pipeline run which is immensely important for FireCloud and @abaumann . The changes to the pipelines API are described here:. https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines/run#RunPipelineArgs.FIELDS.labels. Cromwell should propagate the workflow level labels to the JES calls for downstream use. According to Google, we should be able to see these labels right away in the operations metadata",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1624:606,down,downstream,606,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1624,1,['down'],['downstream']
Availability,"JES/PAPI only has [two](https://cloud.google.com/genomics/reference/rest/v1alpha2/pipelines#DockerExecutor) current settings for running a docker container:; - `imageName`: Image name from either Docker Hub or Google Container Registry.; - `cmd`: The command or newline delimited script to run. The command string will be executed within a bash shell. This particular ticket may need to be escalated. The entrypoints present in the docker image do seem to cause failures with JES/PAPI. This could be mitigated by PAPI using [`docker run --entrypoint="""" …`](https://docs.docker.com/engine/reference/run/#entrypoint-default-command-to-execute-at-runtime). Also note: PAPI using `bash` effectively makes #1384 moot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-316418026:462,failure,failures,462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-316418026,1,['failure'],['failures']
Availability,"JIRA Issue for ref: [BA-6364](https://broadworkbench.atlassian.net/browse/BA-6364). Hi, I have a simple workflow that import subworkflow. After the subworkflow is run I would like to access an output from particular task from it. The output is not declared as an output of the subworkflow, and subworkflow is not maintained by us so I do not have a (easy) possibility to modify it. Here is the workflow:. ```; import ""subworkflow.wdl"" as sub; workflow hello {; call sub.hello; call show {; input: data = hello.say_hello.out; }; }; task show {; String data; command {; cat ${data}; }; }; ```. And subworkflow.; ```; workflow hello {; String name = ""John""; call say_hello {input: name = name}; }; task say_hello {; String name; command {; which python; python --version; echo ""Hello ${name}!""; }; output {; String out = stdout(); }; }; ```; Of course I have an error: Call ‘hello’ doesn’t have an output ‘say_hello’ (line 6, col 29). But according to specs: `If the output {...} section is omitted, then the workflow includes all outputs from all calls in its final output.`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5479:769,echo,echo,769,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5479,2,"['echo', 'error']","['echo', 'error']"
Availability,"JIRA Ticket: https://broadworkbench.atlassian.net/browse/CROM-6867. Hi. . I've been running the GATK-SV pipeline with AWS backend and, sometimes, due to some intermittent errors the tasks are aborted half-way trough. Then Cromwell re-launches the task but some files, generated by the previous run, are already there what makes the pipeline to fail. With this in mind, I'm preparing do a change in cromwell that remove all the files (except for the script which gets created for each run/job) from the task folder before it starts and I would like to ask:; 1. if this makes sense?; 2. if there is any problem on doing this. can the same folder be used twice? or does each task has its own “workspace”? or Will this change impact any other downstream jobs as we will remove everything except “script” file?. Thanks in advance",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6651:171,error,errors,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6651,2,"['down', 'error']","['downstream', 'errors']"
Availability,JIRA: [DSDEEPB-2455](https://broadinstitute.atlassian.net/browse/DSDEEPB-2455); wdl4s PR: https://github.com/broadinstitute/wdl4s/pull/7. This WDL file now runs as expected. ```; task golden_pie {; Float pi = 3.1415926; Float tau = pi + pi. command {; echo 1.6180339887; echo ${tau} 1>&2; }. output {; Float Au = read_float(stdout()); Float tauValue = read_float(stderr()); }; }. workflow wf {; call golden_pie; output {; golden_pie.pi; golden_pie.Au; }; }; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/395:252,echo,echo,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/395,2,['echo'],['echo']
Availability,Jackson-databind: vulnerable version 2.13.2 -> safe version 2.13.2.1+ (used 2.13.3); Nimbus JOSE+JWT: vulnerable version 9.9.3 => safe version 9.22 (used 9.23). ```; root(aen_bw_1227_2)> | 80> whatDependsOn com.fasterxml.jackson.core jackson-databind 2.13.2; [...]; [error] Expected '2.13.3'; [error] whatDependsOn com.fasterxml.jackson.core jackson-databind 2.13.2; [error] ^; ```; ```; root(aen_bw_1227_2)> | 80> whatDependsOn com.nimbusds nimbus-jose-jwt 9.9.3; [...]; [error] Expected '9.23'; [error] whatDependsOn com.nimbusds nimbus-jose-jwt 9.9.3; [error] ^; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6776:267,error,error,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6776,6,['error'],['error']
Availability,Jeff and all;; This is bcbio CWL triggering the problem. We use 0 for disk when we have operations that don't need to download/manage the inputs and are just manipulating things in memory. If changing turns out to be a major issue I'm happy to re-explore how we handle it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4150#issuecomment-424730864:118,down,download,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4150#issuecomment-424730864,1,['down'],['download']
Availability,"Jeff;; Thanks so much for looking at this. I went back through my runs and realized it is only happening on a single filesystem, the `/n/groups` filesystem on orchestra2 at Harvard Medical School. The folks there were nice enough to let me know the details about the setup:. > /n/groups is NFSv3 being served from an EMC/Isilon fileserver:; > ; > alb15@login03:~$ mount | grep groups; > orchestra2-p05.med.harvard.edu:/ifs/systems/Orchestra/groups on /n/groups type; > nfs (rw,relatime,vers=3,rsize=131072,wsize=524288,namlen=255,hard,proto=; > tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.120.41.64,mountvers=3,mountport=; > 300,mountproto=udp,local_lock=none,addr=10.120.41.64). Does anything in there help provide some clues to dig into it? Apologies, I'm fishing around in the dark with trying to provide useful information since I'm ignorant of both the magic of shared filesystem peculiarities and how that would impact Hsqldb here. All of my searches on this error have to do with database setups and older version of hsqldb so my Google debugging is not helping. Thanks again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3607#issuecomment-388827013:966,error,error,966,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607#issuecomment-388827013,1,['error'],['error']
Availability,Jes flavored abort failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1405:19,failure,failure,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1405,1,['failure'],['failure']
Availability,JesApi run creation failures were being caught by the JesApi status handler and not getting handled correctly.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2251:20,failure,failures,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2251,1,['failure'],['failures']
Availability,"Jira reference: https://broadworkbench.atlassian.net/jira/software/c/projects/BA/issues/BA-6638; Cromwell 51. When using local method in look-up Docker hash together with Docker digest (instead of tag) causes an error because ""@"" symbol is substituted with colon. I have to use tags to be able to get the digest. In ""submit-docker"" section the image is correctly inserted i.e. with ""@"". ```; 2020-10-08 16:08:57,342 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Assigned new job execution tokens to the following groups: 45d03417: 1; 2020-10-08 16:08:57,443 INFO - Attempting to pull python:sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4; 2020-10-08 16:08:57,503 ERROR - Docker pull failed; java.lang.RuntimeException: Error running: docker pull python:sha256:d03b690584424b88488e555e26820e458cc624e9d004e3fa0fe3ff99aa81b2b4; Exit code: 1; invalid reference format. 	at cromwell.docker.local.DockerCliClient.$anonfun$forRun$1(DockerCliClient.scala:58); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.docker.local.DockerCliClient.forRun(DockerCliClient.scala:50); 	at cromwell.docker.local.DockerCliClient.pull(DockerCliClient.scala:37); 	at cromwell.docker.local.DockerCliClient.pull$(DockerCliClient.scala:36); 	at cromwell.docker.local.DockerCliClient$.pull(DockerCliClient.scala:94); 	at cromwell.docker.local.DockerCliFlow$.pull(DockerCliFlow.scala:101); 	at cromwell.docker.local.DockerCliFlow.$anonfun$run$1(DockerCliFlow.scala:35); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:139); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:355); 	at cats.effect.internals.IORunLoop$RestartCallback.run(IORunLoop.scala:366); 	at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); 	at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:212,error,error,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,3,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"Job Manager would like to distinguish between ""normal"" and ""exceptional"" failures, e.g. preemption versus something truly wrong. One implementation idea is to use the `causedBy` field but we have not vetted this at all. A/C distinguish between preemption and all other failures in the metadata response for an attempt - i.e. why did this particular attempt for a call fail?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4894:73,failure,failures,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4894,2,['failure'],['failures']
Availability,JobDescriptor merging checkpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/694:22,checkpoint,checkpoint,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/694,1,['checkpoint'],['checkpoint']
Availability,"JobExecutionActor.scala:211); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more; ```; This is our configuration for PBS:; ```; PBSPRO {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; ; runtime-attributes = """"""; Int cpu = 1; Int memory_mb = 2048; String queue = ""normal""; String account = """"; String walltime = ""48:00:00""; ; Int? cpuMin; Int? cpuMax; Int? memoryMin; Int? memoryMax; String? outDirMin; String? outDirMax; String? tmpDirMin; String? tmpDirMax; """"""; submit = """"""; qsub -V -l wd -N ${job_name} -o ${out} -e ${err} -q ${queue} -l walltime=${walltime} -l ncpus=${cpu} -l mem=${memory_mb}mb -- /usr/bin/env bash ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+).*""; filesystems {. local {; localization: [""soft-link""]; caching {; duplication-strategy: [""soft-link""]; hashing-strategy: ""path""; }; }; }. }; }; ```; Thanks for any tips or pointers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345:4933,alive,alive,4933,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-455621345,1,['alive'],['alive']
Availability,JobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:2223,robust,robustExecuteOrRecover,2223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,2,['robust'],['robustExecuteOrRecover']
Availability,JobStore write failure error message,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2219:15,failure,failure,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219,2,"['error', 'failure']","['error', 'failure']"
Availability,Jobs failed with Unable to complete PAPI request due to system or connection error (PipelinesApiRequestHandler actor termination caught by manager),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6203:77,error,error,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6203,1,['error'],['error']
Availability,Jobs which fail for user error logged with error severity,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3962:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3962,2,['error'],['error']
Availability,"JoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); 	at akka.dispatch.Mailbox.run(Mailbox.scala:223); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: java.util.NoSuchElementException: key not found: ps-stdOut; 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor$$anonfun$4.applyOrElse(EngineJobExecutionActor.scala:14",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012:6441,Fault,FaultHandling,6441,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012,1,['Fault'],['FaultHandling']
Availability,JoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.UnsupportedOperationException: Neither execute() nor executeAsync() implemented by class cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor; 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:805); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:804); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.execute(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:821); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recover(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:829); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:829); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecutionActor.recoverAsync(GcpBatchAsyncBackendJobExecutionActor.scala:132); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:1253); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:1248); 	at cromwell.backend.google.batch.actors.GcpBatchAsyncBackendJobExecuti,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:7941,recover,recover,7941,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['recover'],['recover']
Availability,"Just a though --what if we added a ""retryOn"" attribute that took a boolean expression. Then we add a function like grep(pattern, var/file, mode) <see R for example function> which would probably be similar work and much more reusable. Then Jose could write something like. retryOn = grep(""(foo|bar"", $stderr, ""any""). But could also check any other element. Syntax is lousy typing from tiny keyboard. > On Feb 16, 2017, at 11:23 AM, Kate Voss <notifications@github.com> wrote:; > ; > Adding @vdauwera's comment about adding error codes to GATK from DSDE-docs #1742:; > ; > We may be able to put in error codes for things like this in GATK4. Should ask David Roazen or Louis Bergelson.; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-280384314:523,error,error,523,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-280384314,2,['error'],['error']
Availability,"Just adding confirmation that I'm receiving the same error for localizing WDL directories (in the development spec). Notably, if you use the cached-copy strategy, it will result in copying your directories twice - as I understand, Cromwell won't be able to hard-link the original directory so copies it to `cached directory) and hence copies it again. Not a big deal that would get solved by fixes mentioned earlier, but in case anyone finds themselves where I am.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-686181004:53,error,error,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-686181004,1,['error'],['error']
Availability,"Just as a safety measure, maybe we should consider add a config option called something like ""allowCrossFSLocalisation"" - just to avoid accidentally downloading a 500GB file from GCS and the costs involved?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161010494:149,down,downloading,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-161010494,1,['down'],['downloading']
Availability,"Just curious, why are you giving the qsub command to singularity? You might consider launching a job that runs singularity, not the other way around. If the above works for you then ok, but it seems likely to lead to error to me. If you use this method you will likely find you don't need all of those binds. The need for multiprocessing with python was also a requirement that was pre-3.0 - after 3.0 python is totally removed. So if you are adding extra binds for the benefit of the Singularity python, you definitely don't need to do this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438681841:217,error,error,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-438681841,1,['error'],['error']
Availability,"Just in time for thumbs to come in, I'm realizing that I rather blindly added `55` to expected failures when in fact there is a chance I broke it. Investigating...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4823#issuecomment-482727793:95,failure,failures,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4823#issuecomment-482727793,1,['failure'],['failures']
Availability,"Just noticed, this PR uses different hashes for conformance tests for Local / PapiV1 / PapiV2. I'm assuming that was not intentional. I have an incoming PR (as soon as PRs quiet down + I get travis to pass for once) that refactors this into reusable includes. That will hopefully help making CI changes in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3627#issuecomment-389284660:178,down,down,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3627#issuecomment-389284660,1,['down'],['down']
Availability,"Just to be clear the ""tasks"" referred to here are Slick tasks and not Cromwell / WDL tasks (that error message is produced by the Slick library). I'm speculating a bit but it may be that the unrestricted query was tying up the database for so long that too many tasks backed up behind it and overflowed the Slick task queue of size 1000. More restrictive server-side filtering like you're doing now definitely seems like a good idea. 🙂",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516:97,error,error,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-394784516,1,['error'],['error']
Availability,"Just to clarify, I think the motivation here was to make sure the workflow that went terminal does not get recovered and a job that already ran once gets re-run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1247#issuecomment-304958771:107,recover,recovered,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1247#issuecomment-304958771,1,['recover'],['recovered']
Availability,"Just to comment that this feature would be really useful for AWS/Cfncluster based work, since cfncluster has a known issue where compute nodes could be scaled down even with jobs still running, leaving no rc file and no way for cromwell to know that the job is already dead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-371303358:159,down,down,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-371303358,1,['down'],['down']
Availability,"Just wanted to report a behavior I saw while trying to scale Cromwell horizontally. I haven't had time to take a look to it but is related to metadata and the way that is written in the database. How to reproduce:; 1. Deploy:; 1 VM with Nginx to act as the load balancer; 2 VMs with Cromwell 29; 1 VM with MySQL 5.6.36. 2. Submit Hello World workflow 10000 times. 3. Try to get status for some/all workflows while workflows are being processed. 4. These issues are manifested:. Duplicated entry exception (this one happens repeatedly) =>; ``` ; 2017-07-13 22:07:39,149 cromwell-system-akka.dispatchers.service-dispatcher-243 ERROR - Failed to summarize metadata; com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry 'cromwell-workflow-id-cromwell-6d021019-ac3f-4a28-b034-d58fb92022' for key 'UC_CUSTOM_LABEL_ENTRY_CLK_CLV_WEU'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.Util.getInstance(Util.java:408); 	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:935); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3973); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3909); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2527); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2680); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2490); 	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1858); 	at com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2079); 	at com.mysql.jdbc.PreparedStatement.executeUpdateInternal(PreparedStatement.java:2013); 	at com.mysql.jdbc.PreparedSta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2452:625,ERROR,ERROR,625,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2452,1,['ERROR'],['ERROR']
Availability,Keep the transient google communication errors in the poll queue Closes #1665,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1686:40,error,errors,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1686,1,['error'],['errors']
Availability,"Killed"" >&2; 		tail /dev/zero; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; 	; }. task TestBadCommandRetry {; 	command <<<; free -h; df -h; cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		bedtools intersect nothing with nothing; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; }. My.conf:. include required(classpath(""application"")). system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"". system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; config {; project = ""$my_project""; root = ""$my_bucket""; name-for-call-caching-purposes: PAPI; slow-job-warning-time: 24 hours; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600. # Setup GCP to give more memory with each retry; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; ; # Number of workers to assign to PAPI requests; request-workers = 3. virtual-private-cloud {; network-label-key = ""network-key""; network-name = ""network-name""; subnetwork-name = ""subnetwork-name""; auth = ""auth""; }; pipeline-timeout = 7 days; genomics {; auth = ""auth""; compute-service-account = ""$my_account""; endpoint-url = ""https://lifesciences.googleapis.com/""; location = ""us-central1""; restrict-metadata-access = false; localization-attempts = 3; parallel-composite-upload-threshold=""150M""; }; filesystems {; gcs {; auth = ""auth""; project = ""$my_project""; caching {; duplication-strategy = ""copy""; }; }; }; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; runtime {; cpuPlatform: ""Intel Cascade Lake""; }; default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7451:1574,error,error-keys,1574,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451,2,"['Error', 'error']","['Error', 'error-keys']"
Availability,Kills workflow if HPC Scheduler kills job due to out of memory error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5107:63,error,error,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107,1,['error'],['error']
Availability,Known issues: . - A `WomMaybeListedDirectory` used as input to an IWDR expression does not have its `listing` attribute populated as expected.; - Even if this directory's `listing` attribute is force-populated the files are not copied to the IWDR.; - Even if the files are force-copied to the IWDR the test runs a `find` command that does not tolerate the presence of detritus.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3575:343,toler,tolerate,343,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3575,1,['toler'],['tolerate']
Availability,"Known issues:. ---. 1. The `Centaur GCP Batch` [test](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221029126?pr=7177) is unexpectedly trying to use Application Default Credentials when it should be using the service account `cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com`. ```; The Application Default Credentials are not available. They are available if running in Google Compute Engine.; ```. The SA should be available as evidenced by the following output near the top of the log, so it looks like an issue with selecting the right credentials. In other words, I think this is an application logic issue in GCP Batch rather than an environment problem. (Cromwell uses service account auth for everything but local development.); ```; Activated service account credentials for: [cromwell@broad-dsde-cromwell-dev.iam.gserviceaccount.com]; ```. Plausibly responsible party to fix: Burwood. ---. 2. DRS-related failures in [Centaur Horicromtal PapiV2 Beta](https://github.com/broadinstitute/cromwell/actions/runs/5590808626/jobs/10221030693?pr=7177#logs) seem to be the downstream of not being able to build/push the `cromwell-drs-localizer` image. Example error below; images should appear [in the GCR for `broad-dsde-cromwell-dev`](https://console.cloud.google.com/gcr/images/broad-dsde-cromwell-dev/global/cromwell-drs-localizer?project=broad-dsde-cromwell-dev) and the named one does not exist. ```; Error response from daemon:; manifest for gcr.io/broad-dsde-cromwell-dev/cromwell-drs-localizer:github-5590808626 not found; ```. I've replicated the inability to build locally, including on `develop`, and am iterating in this PR: https://github.com/broadinstitute/cromwell/pull/7179. Plausibly responsible party to fix: Broad. ---. 3. Unit tests are [failing](https://github.com/broadinstitute/cromwell/actions/runs/5590808615/jobs/10221028981?pr=7177) because an assertion is looking for different paths in some cases. Examples:. ```; GcpBatchFileInput(""wf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966:362,avail,available,362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7177#issuecomment-1641142966,4,"['avail', 'failure']","['available', 'failures']"
Availability,Known test failures:; - [x] cromwell.SimpleWorkflowActorSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorWithBadMetadataSpec; - [x] cromwell.ArrayWorkflowSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorWithMetadataSpec; - [x] cromwell.ArrayOfArrayCoercionSpec; - [x] cromwell.WdlFunctionsAtWorkflowLevelSpec; - [x] cromwell.MapWorkflowSpec; - [x] cromwell.WorkflowOutputsSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorWithMetadataOnFailureSpec; - [x] cromwell.WorkflowFailSlowSpec; - [x] cromwell.FilePassingWorkflowSpec; - [x] cromwell.engine.workflow.SingleWorkflowRunnerActorNormalSpec; - [x] cromwell.engine.WorkflowManagerActorSpec; - [x] cromwell.MultipleFilesWithSameNameWorkflowSpec; - [x] cromwell.CopyWorkflowOutputsSpec; - [x] cromwell.PostfixQuantifierWorkflowSpec; - [x] cromwell.ScatterWorkflowSpec. Ready for review!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1371:11,failure,failures,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1371,1,['failure'],['failures']
Availability,"L doesn't really have a proper understanding of mutual exclusivity, so it doesn't realize that anything under a ""is optional variable X defined?"" block can only happen if optional variable X is defined. In other words, if variant_caller.errorcode has type Array[String?], the following code block is invalid, and womtool correctly flags it as such:. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = variant_caller.errorcode; }; ```. > Failed to process declaration 'Array[String] varcall_error_if_earlyQC_filtered = variant_call_after_earlyQC_filtering.errorcode' (reason 1 of 1): Cannot coerce expression of type 'Array[String?]' to 'Array[String]'. The normal workaround for this is to use select_first() with a bogus fallback value, since the `defined` check means that fallback value will never be selected. ```; if(defined(variant_caller.errorcode)) { ; 	Array[String] not_optional_error_code = select_first([variant_caller.errorcode, [""according to all known laws of aviation""]]); }; ```. The same holds true if I only care about the first (index 0) variable in the array. That's the case for me, since the actual workflow I'm working on will be run on Terra data tables, eg each instance of the workflow only gets one sample but dozens of instances of the workflow will be created. For compatibility reasons I cannot convert the variant caller into a non-scattered task, so its error code will still have type Array[String]? even though that array will only have one value. ```; if(defined(variant_caller.errorcode)) { ; 	String not_optional_error_code = select_first([variant_caller.errorcode[0], ""according to all known laws of aviation""]); }; ```. ## the womtool bug; I only care about variant_caller.errorcode[0] if it does not equal the word ""PASS"", so I wrote this:. ```; String pass = ""PASS""; if(defined(variant_caller.errorcode)) {; 	if(!variant_caller.errorcode[0] == pass)) {; 		String not_optional_error_code = select_first([variant_caller.erro",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7194:1416,error,errorcode,1416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7194,1,['error'],['errorcode']
Availability,"Last [CRON errors](https://travis-ci.org/broadinstitute/cromwell/jobs/340487274):. ```scala; 2018-02-12 13:50:46,018 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 4c983def-4f9c-4e62-ae38-4543da9922de failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta_fai' not specified; Required workflow input 'CNVSomaticPanelWorkflow.PreprocessIntervals.ref_fasta' not specified; Required workflow input 'CNVSomaticPanelWorkflow.PreprocessIntervals.ref_fasta_fai' not specified; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta' not specified; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta_dict' not specified; cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta_fai' not specified; Required workflow input 'CNVSomaticPanelWorkflow.PreprocessIntervals.ref_fasta' not specified; Required workflow input 'CNVSomaticPanelWorkflow.PreprocessIntervals.ref_fasta_fai' not specified; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta' not specified; Required workflow input 'CNVSomaticPanelWorkflow.CollectCounts.ref_fasta_dict' not specified; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:203); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3270:11,error,errors,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3270,2,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,"Last [CRON errors](https://travis-ci.org/broadinstitute/cromwell/jobs/340487274):. ```scala; java.lang.Exception: Task Mutect2.oncotate_m2:NA:1 failed. Job exit code 1. Check gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/oncotate_m2-stderr.log for more information. PAPI error code 5. Message: 10: Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/HCC1143.maf.annotated -> gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/HCC1143.maf.annotated (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/HCC1143.maf.annotated gs://cloud-cromwell-dev/cromwell_execution/travis/Mutect2/a5d2b104-647b-45fa-8149-7bddf6bc2094/call-oncotate_m2/HCC1143.maf.annotated, command failed: CommandException: No URLs matched: /mnt/local-disk/HCC1143.maf.annotated\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3271:11,error,errors,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3271,2,['error'],"['error', 'errors']"
Availability,"Lastly,; ```; 2019-01-31 20:30:56,569 INFO - changelog.xml: changesets/failure_metadata.xml::deduplicate_failure_messages::cjllanwarne: ChangeSet changesets/failure_metadata.xml::deduplicate_failure_messages::cjllanwarne ran successfully in 8ms; 2019-01-31 20:30:56,593 ERROR - changelog.xml: changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne: Change Set changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne failed. Error: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 2019-01-31 20:30:56,617 INFO - changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne: Successfully released change log lock; 2019-01-31 20:30:56,631 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSER",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:270,ERROR,ERROR,270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,3,"['ERROR', 'Error', 'failure']","['ERROR', 'Error', 'failures']"
Availability,"Latest ""develop"" branch, simple workflow with single parameter in the inputs (will post here if deemed important), and get this error once I run it the second, third, etc. times:. ```; 2018-06-11 16:10:16,080 cromwell-system-akka.dispatchers.api-dispatcher-630 INFO - Unspecified type (Unspecified version) workflow ab42cf3c-726f-4148-a30f-0f907c843361 submitted; 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - 1 new workflows fetched; 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowManagerActor Starting workflow UUID(ab42cf3c-726f-4148-a30f-0f907c843361); 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - WorkflowManagerActor Successfully started WorkflowActor-ab42cf3c-726f-4148-a30f-0f907c843361; 2018-06-11 16:10:35,955 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-11 16:10:35,959 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Parsing workflow as WDL draft-2; 2018-06-11 16:10:35,970 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - MaterializeWorkflowDescriptorActor [UUID(ab42cf3c)]: Call-to-Backend assignments: wf_hello.hello -> AWSBATCH; 2018-06-11 16:10:36,997 cromwell-system-akka.dispatchers.engine-dispatcher-36 INFO - WorkflowExecutionActor-ab42cf3c-726f-4148-a30f-0f907c843361 [UUID(ab42cf3c)]: Starting wf_hello.hello; 2018-06-11 16:10:37,958 cromwell-system-akka.dispatchers.engine-dispatcher-47 ERROR - Failed copying cache results for job BackendJobDescriptorKey_CommandCallNode_wf_hello.hello:-1:1, invalidating cache entry.; cromwell.core.CromwellFatalException: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at cromwell.core.CromwellFatalException$.apply(core.scala:18); 	at cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:128,error,error,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,1,['error'],['error']
Availability,"Latest on aws_backend branch. ________________________________; From: mcovarr <notifications@github.com>; Sent: Thursday, June 7, 2018 6:47:04 AM; To: broadinstitute/cromwell; Cc: Thomas Dyar (EXTERNAL); Author; Subject: Re: [broadinstitute/cromwell] Strange ""Boxed Error"", probably authorization / config (#3736). Also what version of Cromwell is this?. —; You are receiving this because you authored the thread.; Reply to this email directly, view it on GitHub<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_broadinstitute_cromwell_issues_3736-23issuecomment-2D395377185&d=DwMCaQ&c=n7UHtw8cUfEZZQ61ciL2BA&r=Wpxr3yZIgJUDc4CsPhUpuiAtTKDn_lya4DWla3Q21iI&m=vxqjxBUg7eYY0Pzk0lUj-fru5Fu_Xj93aim9v5CyjEk&s=U0Ofhj4NWKhfebpsRfeTCvMxBZRUhJ44bevIpm6SR-E&e=>, or mute the thread<https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_notifications_unsubscribe-2Dauth_AIfLudmaJuhICx-5FxkNqusXZWh8pJ14zvks5t6QSogaJpZM4UdPeJ&d=DwMCaQ&c=n7UHtw8cUfEZZQ61ciL2BA&r=Wpxr3yZIgJUDc4CsPhUpuiAtTKDn_lya4DWla3Q21iI&m=vxqjxBUg7eYY0Pzk0lUj-fru5Fu_Xj93aim9v5CyjEk&s=qPDfKyTsVifxuNzZbVjE9HCwrHl6ANQrTo9wh-9YTJE&e=>.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395392027:266,Error,Error,266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395392027,1,['Error'],['Error']
Availability,"Launch any workflow, let it run long enough to start a job in GCP Batch. Shut down Cromwell, restart Cromwell:. ```; 2024-08-19 14:47:51 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Cromwell 88-e59a6aa-SNAP service started on 0:0:0:0:0:0:0:0:8000...; 2024-08-19 14:47:51 cromwell-system-akka.dispatchers.engine-dispatcher-55 INFO - MaterializeWorkflowDescriptorActor [UUID(119e11a5)]: Call-to-Backend assignments: wf_hello.hello -> GCPBATCH; 2024-08-19 14:47:52 cromwell-system-akka.dispatchers.engine-dispatcher-54 INFO - WorkflowExecutionActor-119e11a5-b981-4510-a6d9-b5c26dfbb4e3 [UUID(119e11a5)]: Restarting wf_hello.hello; 2024-08-19 14:47:53 cromwell-system-akka.dispatchers.engine-dispatcher-53 INFO - Assigned new job restart checking tokens to the following groups: 119e11a5: 1; 2024-08-19 14:47:55 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - Not triggering log of restart checking token queue status. Effective log interval = None; 2024-08-19 14:47:55 cromwell-system-akka.dispatchers.engine-dispatcher-41 INFO - Triggering log of execution token queue status. Effective log interval = 300 seconds; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - Assigned new job execution tokens to the following groups: 119e11a5: 1; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-47 INFO - BT-322 119e11a5:wf_hello.hello:-1:1 is eligible for call caching with read = true and write = true; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-43 INFO - BT-322 119e11a5:wf_hello.hello:-1:1 cache hit copying nomatch: could not find a suitable cache hit.; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.engine-dispatcher-43 INFO - 119e11a5-b981-4510-a6d9-b5c26dfbb4e3-EngineJobExecutionActor-wf_hello.hello:NA:1 [UUID(119e11a5)]: Could not copy a suitable cache hit for 119e11a5:wf_hello.hello:-1:1. No copy attempts were made.; 2024-08-19 14:48:00 cromwell-system-akka.dispatchers.backend-disp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7495:78,down,down,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7495,1,['down'],['down']
Availability,"Learned that a change in Pipelines API omitted the ""preemptible"" key from the operations metadata, and that change introduced a null pointer in the Cromwell code. . AC: As a way to address this, it would be great if we could modify the Cromwell code so that when its parsing operation metadata, that if certain keys are missing (such as Preemptible) -- we use the defaults where possible, else fail gracefully with an error that states which information couldn't be parsed, and that caused the workflow to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4772#issuecomment-477169804:418,error,error,418,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772#issuecomment-477169804,1,['error'],['error']
Availability,"Let's talk after standup. I think we're going to need it - IIRC there's already something incorrectly reading/writing from metadata, and recovery was the use case which informed it in the first place & that's incoming",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1031#issuecomment-227760284:137,recover,recovery,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1031#issuecomment-227760284,1,['recover'],['recovery']
Availability,Lets us trickle declarations and call outputs down into nested workflows. Eg this wdl:; ```wdl; workflow nested_lookups {; Int i = 27; if(true) {; if(true) {; if(true) {; call mirror as m1 { input: i = i}; }; }; }; }; ```. Gives us this womgraph:; ![test](https://user-images.githubusercontent.com/13006282/32580790-0eb26dc8-c4b5-11e7-8852-99d941823a2d.png),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2841:46,down,down,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2841,1,['down'],['down']
Availability,Like.$anonfun$run$1(WordSpecLike.scala:1192); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.WordSpecLike.run(WordSpecLike.scala:1192); at org.scalatest.WordSpecLike.run$(WordSpecLike.scala:1190); at cromwell.CromwellTestKitWordSpec.run(CromwellTestKitSpec.scala:250); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4457:10054,Error,ErrorHandling,10054,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4457,2,['Error'],['ErrorHandling']
Availability,Like.$anonfun$run$1(WordSpecLike.scala:1192); at org.scalatest.SuperEngine.runImpl(Engine.scala:521); at org.scalatest.WordSpecLike.run(WordSpecLike.scala:1192); at org.scalatest.WordSpecLike.run$(WordSpecLike.scala:1190); at cromwell.CromwellTestKitWordSpec.run(CromwellTestKitSpec.scala:250); at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:314); at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:507); at sbt.TestRunner.runTest$1(TestFramework.scala:113); at sbt.TestRunner.run(TestFramework.scala:124); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.$anonfun$apply$1(TestFramework.scala:282); at sbt.TestFramework$.sbt$TestFramework$$withContextLoader(TestFramework.scala:246); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFramework$$anon$2$$anonfun$$lessinit$greater$1.apply(TestFramework.scala:282); at sbt.TestFunction.apply(TestFramework.scala:294); at sbt.Tests$.processRunnable$1(Tests.scala:347); at sbt.Tests$.$anonfun$makeSerial$1(Tests.scala:353); at sbt.std.Transform$$anon$3.$anonfun$apply$2(System.scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:9872,Error,ErrorHandling,9872,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,4,['Error'],['ErrorHandling']
Availability,"Listing a few things I noticed were out of data or underspecified in the Cromwell Swagger and/or README:; - `GET .../metadata` {include,exclude}Key applies to all keys in the response including nested objects; - `PATCH .../labels` returns only the updated labels; - `GET .../metadata` failures and calls.failures are underspecified, actual definition is:. ```; FAILURES := [; {; message: """"; causedBy: FAILURES; },; ]; ```. - Subworkflows are not represented in the response; - Some fields I noticed were missing from Swagger yaml (not exhaustive):; - workflow.labels; - workflow.workflowName; - workflow.submittedFiles; - workflow.calls.shardIndex; - workflow.calls.subWorkflowMetadata (correspondingly, missing parentWorkflowId and workflowRoot from WorkflowMetadata); - workflow.calls.outputs; - workflow.calls.callCaching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228:285,failure,failures,285,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2533#issuecomment-322565228,4,"['FAILURE', 'failure']","['FAILURES', 'failures']"
Availability,"Local Cromwell & CromIAM, pointed at Sam dev,. `/describe` endpoint:. Disabled user `anichols@broadinstitute.org`; ```; 403 Forbidden; The supplied authentication is not authorized to access this resource; ```; Enabled user `oednichols@gmail.com`; ```; 200 OK; {; 	""valid"": true,; 	""errors"": [],; 	""validWorkflow"": true,; 	""name"": ""HelloWorld"",; 	""inputs"": [],; 	""outputs"": [],; 	""images"": [],; 	""submittedDescriptorType"": {; 		""descriptorType"": ""WDL"",; 		""descriptorTypeVersion"": ""1.0""; 	},; 	""importedDescriptorTypes"": [],; 	""meta"": {},; 	""parameterMeta"": {},; 	""isRunnableWorkflow"": true; }; ```. `/backends` endpoint:. Disabled user `anichols@broadinstitute.org`; ```; 403 Forbidden; The supplied authentication is not authorized to access this resource; ```; Enabled user `oednichols@gmail.com`; ```; 200 OK; {; 	""defaultBackend"": ""Local"",; 	""supportedBackends"": [; 		""Local"",; 		""LocalBourneShell"",; 		""LocalCacheableRuntimeAttribute"",; 		""LocalDockerSecure"",; 		""LocalNoDocker""; 	]; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6826:283,error,errors,283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6826,1,['error'],['errors']
Availability,Local PBE should return retryable on failures (which should always be false in this PBE),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/756:37,failure,failures,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/756,1,['failure'],['failures']
Availability,Local backend can use more resources than available on a machine,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1354:42,avail,available,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1354,1,['avail'],['available']
Availability,Log access URLs with sensitive parts masked BT-235,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6333:37,mask,masked,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6333,1,['mask'],['masked']
Availability,"Log from broad-dsde-dev here: https://gist.githubusercontent.com/scottfrazer/a0838aa2180e7972da84c4730975b9f5/raw/d0cebdd317489298d6553e5602bfd4b775ab9ea3/gistfile1.txt. Most specifically:. ```; WorkflowActor [UUID(0790bc7e)]: Beginning transition from Running to Aborting.; WorkflowActor [UUID(0790bc7e)]: transitioning from Running to Aborting.; JES Run [UUID(0790bc7e):hello]: Status change from Running to Success; ERROR - 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Precondition check failed."",; ""reason"" : ""failedPrecondition""; } ],; ""message"" : ""Precondition check failed."",; ""status"" : ""FAILED_PRECONDITION""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Precondition check failed."",; ""reason"" : ""failedPrecondition""; } ],; ""message"" : ""Precondition check failed."",; ""status"" : ""FAILED_PRECONDITION""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/700:419,ERROR,ERROR,419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/700,3,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,"Log message that gets repeated over and over:. 2020-01-08 15:15:57,852 cromwell-system-akka.actor.default-dispatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	a",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:120,ERROR,ERROR,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033,2,"['ERROR', 'Failure']","['ERROR', 'Failure']"
Availability,Log on Failures during retry.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/256:7,Failure,Failures,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/256,1,['Failure'],['Failures']
Availability,"Logs captured from alpha environment:; ```; November 2nd 2018, 20:16:15.000 | 2018-11-03 00:16:15 [cromwell-system-akka.actor.default-dispatcher-57321] ERROR c.e.w.w.WorkflowStoreSubmitActor - Workflow com.mysql.jdbc.exceptions.jdbc4.MySQLTransactionRollbackException: Lock wait timeout exceeded; try restarting transaction submit failed.; -- | --.   | November 2nd 2018, 20:16:15.000 | 2018-11-03 00:16:15 [cromwell-system-akka.actor.default-dispatcher-57321] ERROR c.e.w.w.WorkflowStoreSubmitActor - Workflow com.mysql.jdbc.exceptions.jdbc4.MySQLTransactionRollbackException: Lock wait timeout exceeded; try restarting transaction submit failed.   | November 2nd 2018, 10:16:21.000 | 2018-11-02 14:16:21 [cromwell-system-akka.actor.default-dispatcher-42970] ERROR c.e.w.w.WorkflowStoreEngineActor - Error trying to fetch new workflows; com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet successfully received from the server was 1 milliseconds ago. The last packet sent successfully to the server was 1 milliseconds ago.; 	at sun.reflect.GeneratedConstructorAccessor75.newInstance(Unknown Source); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:990); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3562); 	at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3462); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3905); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2530); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2683); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2491); 	at com.mysql.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:4807); 	at com.zaxxer.hikari.pool.ProxyConnection.setAutoCommit(ProxyConnection.java:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4360:152,ERROR,ERROR,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4360,5,"['ERROR', 'Error', 'failure']","['ERROR', 'Error', 'failure']"
Availability,"Logs:; 2019-10-09 14:05:52,263 cromwell-system-akka.actor.default-dispatcher-2 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.ht",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:79,ERROR,ERROR,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,"['ERROR', 'Failure']","['ERROR', 'Failure']"
Availability,Look for failure mode option in the right place in the config. Closes #1380,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1381:9,failure,failure,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1381,1,['failure'],['failure']
Availability,"Looked at this quickly w/ @salonishah11 and I suspect the problem is due to our ""artisanal"" friend, `cromwell.api.CromwellClient`, which as we discovered earlier this week just gleefully eats errors from Cromwell and returns them in an insane way",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3911#issuecomment-406643905:192,error,errors,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3911#issuecomment-406643905,1,['error'],['errors']
Availability,"Looking at status message in the swagger GUI, I see that my workflow failed. However, the job it specifies is scattered and I do not know which shard is the issue.... If there is an easy way to find this, that I missed, then apologies. Example:. ```; ....; ""status"": ""Failed"",; ""failures"": [; {; ""message"": ""Call case_gatk_acnv_workflow.TumorCalculateTargetCoverage: return code was -1""; }; ],; ....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1479:279,failure,failures,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1479,1,['failure'],['failures']
Availability,"Looking at the existing `JobPreparationActor` code I saw that the Docker hash credentials are being created by calling this method in `BackendLifecycleActorFactory`:; ```; def dockerHashCredentials(initializationDataOption: Option[BackendInitializationData]): List[Any] = List.empty; ```; The JES backend overrides this to return a non-empty `List`. Since we don't yet have the required `BackendInitializationData` during workflow materialization, @Horneth suggested the Docker hash calculation be performed slightly downstream in workflow initialization instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289156621:517,down,downstream,517,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289156621,1,['down'],['downstream']
Availability,"Looking at the gitter convo, the problem looks more like a case of overzealous logging - could we just drop it from `info` down to `debug`? I'm curious why it was `info` in the first place tbh",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242800302:123,down,down,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242800302,1,['down'],['down']
Availability,Looking forward to this feature. When enabled the `check-alive` command call via `exit-code-timeout-seconds` currently polls on average once every 10 seconds per running job (under minimum load).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-431870949:57,alive,alive,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220#issuecomment-431870949,1,['alive'],['alive']
Availability,"Looking into a real test failure here, works for me in IntelliJ but failing in GHA.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7221#issuecomment-1758452459:25,failure,failure,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7221#issuecomment-1758452459,1,['failure'],['failure']
Availability,Looks good to me once those Failures are added. Happy for the array expression outputs/preevaluation stuff to be put off until later.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/118#issuecomment-125236175:28,Failure,Failures,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/118#issuecomment-125236175,1,['Failure'],['Failures']
Availability,"Looks good to me. I tried to make some sense of this compiler error this morning. One thing to note is that `sbt compile` does work, but it's the assembly that seems to be creating the issues. Judging from the output of `sbt assembly`, I think perhaps it could be a conflict with another library, because it seems to have the error immediately after importing a bunch of JARs:. ```; ...; [info] Including: jackson-jaxrs-json-provider-2.4.1.jar; [info] Including: jackson-module-jsonSchema-2.4.1.jar; [info] Including: jackson-jaxrs-base-2.4.1.jar; [error] missing or invalid dependency detected while loading class file 'WorkflowStatusResponse.class'.; [error] Could not access type AnyRef in package scala,; [error] because it (or its dependencies) are missing. Check your build definition for; [error] missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); [error] A full rebuild may help if 'WorkflowStatusResponse.class' was compiled against an incompatible version of scala.; [error] missing or invalid dependency detected while loading class file 'WorkflowSubmitResponse.class'.; [error] Could not access type AnyRef in package scala,; [error] because it (or its dependencies) are missing. Check your build definition for; [error] missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.); [error] A full rebuild may help if 'WorkflowSubmitResponse.class' was compiled against an incompatible version of scala.; [error] two errors found; [error] (test:compileIncremental) Compilation failed; [error] Total time: 32 s, completed Jun 2, 2015 8:39:34 AM; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107940395:62,error,error,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/30#issuecomment-107940395,16,['error'],"['error', 'errors']"
Availability,"Looks like Travis caches' deletion doesn't solve the problem with centaurHoricromtalEngineUpgradePapiV2 builds (BA-6164). There're still errors like `ERROR: for cromwell-summarizer-plus-backend Container ""dcdaaee217fb"" is unhealthy.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5340#issuecomment-571829738:137,error,errors,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5340#issuecomment-571829738,2,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,"Looks like one test is legit failing. ```; - should successfully run drs_wf_level_read_size *** FAILED *** (44 seconds, 659 milliseconds); centaur.test.CentaurTestException: Unexpected terminal status Failed but was waiting for Succeeded (workflow ID: 0f9eb46c-44ce-4c92-99f6-0184196298eb). Metadata 'failures' content: [; {; ""causedBy"" : [; {; ""causedBy"" : [; ],; ""message"" : ""Failed to evaluate 'wf_level_read_and_size.fileSize1' (reason 1 of 1): Evaluating size(input1) failed: java.lang.IllegalArgumentException: Could not build the path \""dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0\"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: HTTP, Google Cloud Storage, LinuxFileSystem. Failures: \nHTTP: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 does not have an http or https scheme (IllegalArgumentException)\nGoogle Cloud Storage: Cloud Storage URIs must have 'gs' scheme: dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (IllegalArgumentException)\nLinuxFileSystem: Cannot build a local path from dos://wb-mock-drs-dev.storage.googleapis.com/4a3908ad-1f0b-4e2a-8a92-611f2123e8b0 (RuntimeException)\n Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems""; },. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279:301,failure,failures,301,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5039#issuecomment-505086279,2,"['Failure', 'failure']","['Failures', 'failures']"
Availability,"Looks like read_json in current trunk still has some issues.; When I give a json like this:; ```json; {; ""Homo sapiens"": {; ""transcriptome"" : ""/pipelines/indexes/HUMAN/27/gencode.v27.transcripts.fa"",; ""gtf"": ""/pipelines/indexes/HUMAN/27/"",; ""salmon"": ""/pipelines/indexes/HUMAN/27/salmon""; },; ""Mus musculus"": {; ""transcriptome"" : ""/pipelines/indexes/MOUSE/M16/gencode.vM16.transcripts.fa"",; ""gtf"": ""/pipelines/indexes/MOUSE/M16/gencode.vM16.annotation.gtf"",; ""salmon"": ""/pipelines/indexes/MOUSE/M16/salmon""; }; }; ```; with wdl like this; ```; Map[String, Map[String, String]] indexes = read_json(references) ; ```; I get:; ```; Workflow input processing failed; WorkflowFailure(ERROR: indexes is declared as a Map[String, Map[String, String]] but the expression evaluates to a Object: Map[String, Map[String, String]] indexes = read_json(references) ^ ,List()); ```; I do not get what is wrong there, I've tried different type combinations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-370112127:679,ERROR,ERROR,679,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-370112127,1,['ERROR'],['ERROR']
Availability,"Looks like the data in mysql will be lost when the docker compose is shut 'down'. Is that true? If so, it would be good to document (or make it so) to mount in a volume in the mysql docker compose so the database survives a restart",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1878#issuecomment-273869187:75,down,down,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1878#issuecomment-273869187,1,['down'],['down']
Availability,"Looks like the format is (in some awful pseudo-CFG):; ```; FAILURE := (message: STRING [, causedBy: FAILURE ]); FAILURES := FAILURE*; ```. @ansingh7115 - Is it the format you don't like (i.e. you don't like the causedBy) or the message contents?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282800499:59,FAILURE,FAILURE,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282800499,4,['FAILURE'],"['FAILURE', 'FAILURES']"
Availability,"Looks like there is a test that's guarding against my change. I believe this test is incorrect, but I'll leave that confirmation to you (maybe @cjllanwarne as it looks like you wrote the original test):. [`ValueEvaluatorSpec.scala#L538-L542`](https://github.com/broadinstitute/cromwell/blob/50f28da6a665526c1bdb1d5528400ee9deeaa5d4/wdl/model/draft2/src/test/scala/wdl/expression/ValueEvaluatorSpec.scala#L538-L542); ```scala; ""Optional values"" should ""fail to perform addition with the + operator if the argument is None"" in {; val hello = WomString(""hello ""); val noneWorld = WomOptionalValue.none(WomStringType); hello.add(noneWorld) should be(Failure(OptionalNotSuppliedException(""+""))); }; ```. Relevant text from original PR:. The [WDL Spec: _Interpolating and concatenating optional strings_](https://github.com/openwdl/wdl/blob/master/versions/development/SPEC.md#interpolating-and-concatenating-optional-strings). > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971:646,Failure,Failure,646,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5583#issuecomment-661974971,1,['Failure'],['Failure']
Availability,Looks like this error was specific to my runtime+configuration.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4186#issuecomment-425939282:16,error,error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4186#issuecomment-425939282,1,['error'],['error']
Availability,"Looks like this logic assumes that the array has at least one element:; https://github.com/broadinstitute/cromwell/blob/develop/wom/src/main/scala/wom/values/WomObject.scala#L83. So we just need to add a case for when it's empty. EDIT: The error is thrown earlier, and tells us that it can't even recognize `WomCompositeType` as `WomObjectType` in this case for some reason.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4595#issuecomment-458252514:240,error,error,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4595#issuecomment-458252514,1,['error'],['error']
Availability,"Looks like this was a bad error message, but the task works when I change my runtime+configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4185#issuecomment-425939958:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4185#issuecomment-425939958,1,['error'],['error']
Availability,Looks like we now have `504 Gateway Timeout` errors too! Should we perhaps add a new case for that?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-572676517:45,error,errors,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-572676517,1,['error'],['errors']
Availability,"Looks ok as a patch-- but that's me not being an expert at FSMs. Pinging @salonishah11 to see if I can get her expert opinion? . When things calm down, I'll also want to know if we want to patch now and add tests (even unit tests?) later, or we want to wait and patch -and- test at the same time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078:65,Ping,Pinging,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5591#issuecomment-663723078,2,"['Ping', 'down']","['Pinging', 'down']"
Availability,Loosen criteria for retrying PAPI initialization errors [CROM-6808],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6572:49,error,errors,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6572,1,['error'],['errors']
Availability,"Lots of changes to `SingleWorkflowRunnerActor`:; - Clients communicate with SWRA via an ask, giving callers like `Main` a `Future` whose success status can be used to generate an exit code.; - SWRA no longer manages shutting down the actor system, that becomes the responsibility of the caller. This allows tests that want to see certain error messages to shut down the system only after the error messages are seen by an event filter. The previous structure allowed for a race condition where messages that filters wanted to see were produced, but the actor system was torn down before the messages were delivered to the filters.; - SWRA is now an FSM. Also increased the default patience in `CromwellTestkitSpec` for `InputLocalizationWorkflowSpec` and friends. Does _not_ include any changes to address MySQL connection issues sporadically seen in SlickDataAccessSpec; per discussion with Jeff I'll ticket that separately.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/311:225,down,down,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/311,5,"['down', 'error']","['down', 'error']"
Availability,"M. Example pipeline:. ```; version 1.0. # WORKFLOW DEFINITION; workflow WholeGenomeGermlineSingleSample {; call SumFloats; output {; Float out = SumFloats.total_size; }; }. task SumFloats {; input {; Array[Float] sizes = [1,2,3,4,5.0]; Int preemptible_tries=3; }. command <<<; python -c ""print ~{sep=""+"" sizes}""; >>>; output {; Float total_size = read_float(stdout()); }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/python:2.7""; preemptible: preemptible_tries; }; }; ```. The error raised with cromwell-53 is:; Failed to read_float(""/data/og/ted/cromwell-executions/WholeGenomeGermlineSingleSample/00090ef9-5211-4f18-9de9-daf3de791408/call-SumFloats/execution/stdout"") (reason 1 of 1): For input string: ""15.0; 15.0""; The stdout file truly contains this. Running with local backend returns no error.; Contents of conf file:. ```; backend {; default = ""SLURM""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); concurrent-job-limit = 30; }; }; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 1; Int requested_memory_mb_per_core = 8000; Int memory_mb = 4000; String queue = ""short""; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpu} \; --mem ${memory_mb} \; --wrap ""/bin/bash ${script}""; """"""; submit-docker = """"""; docker pull ${docker}. sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpu} \; --mem ${memory_mb} \; --wrap ""docker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_cwd}/execution/script""; """""". kill = ""scancel ${job_id}""; check-alive = ""scontrol show job ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; ```. Any thoughts?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5932:1990,alive,alive,1990,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5932,1,['alive'],['alive']
Availability,"Made a new ticket for this since it's a new issue. basename() does not work on optional values, but sometimes it seems to think things that aren't optional are optional. This passes miniwdl and Cromwell, ie, is expected behavior:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input, ""/path/to/file.txt""])) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```. This passes miniwdl, but fails Cromwell, even though it really ought to pass both:. ```; version 1.0. task T {; 	input {; 		File? tsv_file_input; 		String foo = select_first([tsv_file_input, ""/path/to/file.txt""]); 		String tsv_arg = if defined(tsv_file_input) then basename(foo) else """"; 	}. 	command <<<; 		echo ~{tsv_arg}; 	>>>. }. workflow W {; 	input {; 		File? tsv_file_input; 	}. 	call T {; 		input:; 			tsv_file_input = tsv_file_input; 	}; }; ```; Cromwell's error is:; > 14:27:13.383 [main] ERROR io.dockstore.client.cli.ArgumentUtility - Problem parsing WDL file: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'; > 14:27:13.385 [main] ERROR io.dockstore.client.cli.ArgumentUtility - wdl.draft3.parser.WdlParser$SyntaxError: Failed to process task definition 'T' (reason 1 of 1): Failed to process expression 'select_first([tsv_arg, if defined(tsv_file_input) then basename(foo) else """"])' (reason 1 of 1): Invalid parameter 'IdentifierLookup(foo)'. Expected 'File' but got 'String?'. _Originally posted by @aofarrel in https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245982086_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6910:435,echo,echo,435,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6910,5,"['ERROR', 'echo', 'error']","['ERROR', 'echo', 'error']"
Availability,"Made redundant by a recent change to the test expectations. Closing this ""now just whitespace"" PR",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5996#issuecomment-729791692:5,redundant,redundant,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5996#issuecomment-729791692,1,['redundant'],['redundant']
Availability,"Made some changes to our Github Actions that should prevent certain slack messages from getting lost. Notes:; - Slack messages are sent to `#cromwell-integration-action`; - Slight change in behavior: We will get a message for any integration test that fails against `develop`, even if that failure didn't happen during a nightly run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7159:290,failure,failure,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7159,1,['failure'],['failure']
Availability,"Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathParsingException.<init>(PathParsingException.scala:5); 	... 35 common frames omitted; 2018-06-13 14:29:48,009 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - a67833cb:demux_only.illumina_demux:-1:1: Hash error, disabling call caching for this job.; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: /Volumes/nextseq_ngs/180405_NB501680_0019_AHKGNJBGX5.tar.lz4 exists on a filesystem not supported by this instance of Cromwell. Supported filesystems are: s3. Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:47); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:42); 	at cromwell.core.path.PathFactory.buildPath(PathFactory.scala:29); 	at cromwell.core.path.PathFactory.buildPath$(PathFactory.scala:29); 	at cromwell.backend.impl.aws.AwsBatchWorkflowPaths.buildPath(AwsBatchWorkflowPaths.scala:51); 	at cromwell.backend.io.WorkflowPaths.$anonfun$getPath$1(WorkflowPaths.scala:43); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:8848,error,error,8848,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['error'],['error']
Availability,Make Cromwell more resilient to GCS IOException,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113:19,resilien,resilient,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113,1,['resilien'],['resilient']
Availability,Make Cromwell more resilient to GCS IOException [BA-5881/#5113 resubmission],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5297:19,resilien,resilient,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297,1,['resilien'],['resilient']
Availability,Make Services library available for publishing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1144:22,avail,available,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1144,1,['avail'],['available']
Availability,Make checkpoint example last long enough to be checkpointed [BW-495],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6164:5,checkpoint,checkpoint,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6164,2,['checkpoint'],"['checkpoint', 'checkpointed']"
Availability,Make inputs available to each other,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3249:12,avail,available,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3249,1,['avail'],['available']
Availability,Make interpretOperationStatus robust to nulls from PAPI,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4774:30,robust,robust,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4774,1,['robust'],['robust']
Availability,Make parsing of runtime attributes far more robust via ValidationNELs…,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/163:44,robust,robust,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/163,1,['robust'],['robust']
Availability,Make services lib available for publishing. Closes #1144.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1143:18,avail,available,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1143,1,['avail'],['available']
Availability,Makes the IoActor available to the carbonite worker and readers by allowing the service registry to receive and store its reference.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5194:18,avail,available,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5194,1,['avail'],['available']
Availability,"Makes use of the helper traits created in the I/O actor PR to abstract managing of backpressure messages etc.. . Creates a `DockerClientHelper` trait to isolate the timeout management logic (if the docker actor never responds, ensures that we don't hang forever). The changes in `HttpFlowWithRetry` add exponential backoff retries (previously they were simple immediate retries) to HTTP responses, and list explicitly the HTTP codes that are retryable.; More specifically, Http ""failures"", as in ""the http request itself failed"", are not retried, since akka already does that by default under the hood. Only Http responses with a retryable error code are retried asynchronously, following the same model as the I/O actor.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2077:479,failure,failures,479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2077,2,"['error', 'failure']","['error', 'failures']"
Availability,Making errors in tryStartingRunnableCalls() bubble up correctly and a…,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/243:7,error,errors,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/243,1,['error'],['errors']
Availability,"Managed to fix the problem. Cromwell 32 errorred and explained the problem. The filesystem section was moved to the SGE section. Are config file now looks like this:; ```HOCON; backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-attributes= """"""; Int? cpu=1; Int? memory=4; """"""; submit = """"""; qsub \; -terse \; -V \; -b n \; -wd ${cwd} \; -N ${job_name} \; ${'-pe BWA ' + cpu} \; ${'-l h_vmem=' + memory + ""G""} \; -o ${out} \; -e ${err} \; ${script}; """"""; kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)"". filesystems {; local {; localization: [; ""soft-link"", ""copy"", ""hard-link""; ]; caching {; duplication-strategy: [ ""soft-link"", ""copy"", ""hard-link"" ]; hashing-strategy: ""file""; }; }; }; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-402984197:40,error,errorred,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-402984197,2,"['alive', 'error']","['alive', 'errorred']"
Availability,"Many types of common errors could be checked faster, but instead fail when they get to a task. Move as much checking as possible up-front so that submission fails (or allow user to request a dummy run with a no-op backend wherein all these checks can be made). Things that aren't checked immediately: ; 1. input miss-specification; 2. docker image non-availability; 3. missing dependencies",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3503:21,error,errors,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3503,2,"['avail', 'error']","['availability', 'errors']"
Availability,MaterializeWorkflowDescriptorActor: coercion failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1067:45,failure,failures,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1067,1,['failure'],['failures']
Availability,"May fix weird test failures. I began getting test failures on my BackendConfiguration branch after I rebased on develop unrelated to anything I had changed. It looks like the workflow from the KV store test is visible to the test that runs after it, which led to more success messages than the assertions expected. Making this a separate PR in case the problem is waiting to bite anyone else.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/706:19,failure,failures,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/706,2,['failure'],['failures']
Availability,"Maybe I'm being melodramatic. Probably if something needs 128GB of memory then something has gone wrong or we need a new method. It's worth a failure to go take a look. Yeah, I could be on board with two parameters like the Java xmx/xms.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499246858:142,failure,failure,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499246858,1,['failure'],['failure']
Availability,"MemoryRetryTest {; 	String message = ""Killed""; 	; 	call TestOutOfMemoryRetry {}; 	call TestBadCommandRetry {}; }. task TestOutOfMemoryRetry {; 	command <<<; 		free -h; 		df -h; 		cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		tail /dev/zero; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; 	; }. task TestBadCommandRetry {; 	command <<<; free -h; df -h; cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		bedtools intersect nothing with nothing; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; }. My.conf:. include required(classpath(""application"")). system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"". system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; config {; project = ""$my_project""; root = ""$my_bucket""; name-for-call-caching-purposes: PAPI; slow-job-warning-time: 24 hours; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600. # Setup GCP to give more memory with each retry; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; ; # Number of workers to assign to PAPI requests; request-workers = 3. virtual-private-cloud {; network-label-key = ""network-key""; network-name = ""network-name""; subnetwork-name = ""subnetwork-name""; auth = ""auth""; }; pipeline-timeout = 7 days; genomics {; auth = ""auth""; compute-service-account = ""$my_account""; endpoint-url = ""https://lifesciences.googleapis.com/""; location = ""us-central1""; restrict-metadata-access = false; localization-attempts = 3; parallel-composite-upload-threshold=""150M""; }; filesystems {; gcs {; auth = ""auth""; project = ""$my_project""; caching {; duplication-strategy = ""copy""; }; }; }; system {; memory-retr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7451:1245,error,error-keys,1245,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451,2,"['Error', 'error']","['Error', 'error-keys']"
Availability,Merging despite AWS test errors because:; * The AWS backend test suite has known issues relating to dockerhub pull limits; * We are not running AWS in Terra production; * The changes here are **extremely** unlikely to have had any impact on the AWS backend,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6123#issuecomment-740827511:25,error,errors,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6123#issuecomment-740827511,1,['error'],['errors']
Availability,Merging despite known sbt failure.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3390#issuecomment-372422393:26,failure,failure,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3390#issuecomment-372422393,1,['failure'],['failure']
Availability,Merging despite the slurm test failure because:. * this is for a hotfix which is needed urgently; * the error is in a slurm test. Slurm is currently unavailable in Terra.; * the error is in a CWL test. CWL is currently unavailable in Terra.; * the test failure does not appear to be new in this PR (the same failure affects the otherwise unrelated build: https://travis-ci.com/broadinstitute/cromwell/builds/152505665),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120:31,failure,failure,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5445#issuecomment-597139120,5,"['error', 'failure']","['error', 'failure']"
Availability,"Merging for expediency despite two flaky test failures because (1) the 53_hotfix branch will be retested and added to before being deployed, and (2) the hotfix is not being used in situations relevant to the test failures. The tests which failed were:; * CWL conformance on PAPIv2 beta; * PAPIv2 beta on MariaDB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141:46,failure,failures,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5997#issuecomment-719658141,2,['failure'],['failures']
Availability,"Message, not throw, failures in WEA.startRunnableScopes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2030:20,failure,failures,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2030,1,['failure'],['failures']
Availability,Metadata entries for early failures. Closes #966,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/976:27,failure,failures,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/976,1,['failure'],['failures']
Availability,"Might not eliminate the ""programmer errors"" just yet, but does improve logging on (unexpected) restarts and eliminates the weird and unintentional parenting of the RootWorkflowFileHashCacheActor by the WorkflowManagerActor.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5358:36,error,errors,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5358,1,['error'],['errors']
Availability,"Might this eventually expand to be a more general purpose ""allow certain error codes to be retried""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1849#issuecomment-272218648:73,error,error,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1849#issuecomment-272218648,1,['error'],['error']
Availability,Migrate Unflat Failure Metadata,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039:15,Failure,Failure,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039,1,['Failure'],['Failure']
Availability,"Migrate existing failures in the metadata database from the former un-flat failure metadata style:; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. to the new flat style that new workflow metadata will be created with:; ```; ""failures"": [{; ""message"": ""connect timed out""; }, {; ""message"": ""Failed to upload authentication file""; }, {; ""message"": ""Error getting access token for service account: ""; }]; ```. Also note that aggregated exceptions were previously being input using the same random integer for every message part (i.e. every part of the aggregate was overwriting the previous one), so migration could... fix that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039:17,failure,failures,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039,6,"['Error', 'failure']","['Error', 'failure', 'failures']"
Availability,Minor change in order to make services library available for publishing. Formal reviewer: @kshakir.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1143:47,avail,available,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1143,1,['avail'],['available']
Availability,Missing *-version.conf files will no longer cause errors when running from IntelliJ.; Fixed projectName (ex: 'cromwell-engine') vs artifactName (ex: 'cromwell-engine.jar').; Also snuck in sbt 1.x syntax fix missed in a prior commit.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2842:50,error,errors,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2842,1,['error'],['errors']
Availability,"More details about recreating the error here: https://gatkforums.broadinstitute.org/firecloud/discussion/10740/error-the-local-copy-message-must-have-path-set. Essentially, if a task looks like ; ```; task t {; 	File x = """"; 	; 	command {; ...; 	}; 	runtime {; ...; 	}; }; ```; The job fails with: ; ```; BackendJobDescriptorKey_CommandCallNode_w.t:-1:1/CCHashingJobActor-b12fef61-w.t:NA:1] Failed to hash ; cromwell.core.path.PathParsingException: java.lang.IllegalArgumentException: Either exists on a filesystem not supported by this instance of Cromwell, or a failure occurred while building an actionable path from it. Supported filesystems are: Google Cloud Storage. Failures: Google Cloud Storage: does not have a gcs scheme (IllegalArgumentException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; 	at cromwell.core.path.PathFactory$.$anonfun$buildPath$4(PathFactory.scala:64); 	at scala.Option.getOrElse(Option.scala:121); 	at cromwell.core.path.PathFactory$.buildPath(PathFactory.scala:58); 	at cromwell.core.path.PathFactory.buildPath(PathFactory.scala:30); ```. AC: ; 1. In case that a user has set the value of a required File as an empty string --this error message should instead accommodate for this special case and point out that an empty string isn't valid input for a File object. ; 1b. If easy, it would also be nice to remove the link to the HPC config docs and the rest of the info about supported filesystems. ; [Optional] 2. If this doesn't hurt performance somehow, it would be nice if the error message also included the name of the input that failed to hash, not just the name of the call/value of the file.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4158:34,error,error,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4158,6,"['Failure', 'error', 'failure']","['Failures', 'error', 'error-the-local-copy-message-must-have-path-set', 'failure']"
Availability,"More info on the current issue:; - The local backend, running on the Config backend, supports running commands with or without docker.; - However the Config backend only allows `run-in-background` to be set for _both_ with and without docker.; - When `run-in-background` is set to true, the process id of the backgrounded process is stored for later killing.; - One cannot halt a docker container using [just `kill`](https://www.fpcomplete.com/blog/2016/10/docker-demons-pid1-orphans-zombies-signals). This gets ignored by the linux kernal and leaves the container running:; > If you must send a SIGKILL to your process, use the docker kill command instead; - `docker kill` takes in the container id, not a pid.; - `docker run -d` outputs the container id, detaching the process.; - For this detached docker, one should set `run-in-background = false`, the opposite of Local without docker. Workaround-- make the runtime attribute ""docker"" mandatory and run docker in detached mode:. ```; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; run-in-background = false. runtime-attributes = """"""; String docker; String? docker_user; """""". submit-docker = """"""; docker run \; -d --rm \; ${""--user "" + docker_user} \; -v ${cwd}:${docker_cwd} \; ${docker} \; /bin/bash -c '/bin/bash ${script} > ${out} 2> ${err} < /dev/null'; """""". kill = ""docker kill ${job_id}""; check-alive = ""docker ps ${job_id}""; job-id-regex = ""([0-9a-f]+)""; }; }; ```. Actual fix-- Refactor the config backend, better separating the docker vs. non-docker config. Only `submit` vs. `submit-docker` are differentiated currently, but `run-in-background`, `job-id-regex`, `kill`, etc. are all different for docker vs. non-docker. **TL;DR: The local backend should run the docker container in the background, capture the docker container id from stdout, and later issue a `docker kill <container id>`.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453:1415,alive,alive,1415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1126#issuecomment-282569453,1,['alive'],['alive']
Availability,"More info:; - Again `WdlFile` is allowing a directory to slip in, while this isn't technically supported. For another example see #1935, and others I cannot locate at the moment.; - With an input file of `""""`, this directory equates to the present working directory.; - Cromwell is attempting to localize the whole directory.; - hard-link to a directory always fails, and since this is docker, soft-link isn't available.; - Cromwell is then copying everything in the pwd. Note, this includes everything under `cromwell-executions`, so it could be potentially large.; - Issuing a Control-C at this point doesn't interrupt the copy localization. I'm not sure how we'll implement aborting copying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-282575488:410,avail,available,410,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-282575488,1,['avail'],['available']
Availability,More informative error message from Cromwell when input files are missing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1691:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1691,1,['error'],['error']
Availability,More informative error messages at `/metadata` endpoint,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4856:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4856,1,['error'],['error']
Availability,"More precisely, come up with a mechanism that ; 1) Gets all ""workflow-related"" actors to stop doing more work and make sure all DB write operations have been sent to ensure consistency.; 2) Waits for all DB write actors to empty their queue; 3) Shuts down the JVM",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2373#issuecomment-316154937:251,down,down,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2373#issuecomment-316154937,1,['down'],['down']
Availability,Most of this code is just propagating the IOFunction ErrorOr to the edges. `GlobFunctions` used some existing logic to re-create globbing function.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2656:53,Error,ErrorOr,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2656,1,['Error'],['ErrorOr']
Availability,"Most of this is just wiring so start by looking at the changes in CallCacheReadActor. I've updated it and made it accessed via a global, routed actor in CromwellRoot. Tests will inject their own dummy version. Mainly this makes testing easier as it's easier to inject the actor rather than relying on it being created to aim at an empty database. Potential downside: actor hierarchy is a little bit more top heavy now :(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1326:357,down,downside,357,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1326,1,['down'],['downside']
Availability,Moved supervision from JES down into Standard trait.; Fixed some vals in the Standard trait.; Fixed non-Travised integration test `JesAttributesSpec`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1781:27,down,down,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1781,1,['down'],['down']
Availability,"Much sad:. ```; 2016-05-24 14:11:46,501 cromwell-system-akka.actor.default-dispatcher-23 ERROR - Workflow d539eabc-3e74-4f06-85ca-0f52ac1f8a2b failed (during ExecutingWorkflowState): java.lang.Throwable: Failed post processing of outputs: ; Workflow d539eabc-3e74-4f06-85ca-0f52ac1f8a2b post processing failedglob function is not supported by this implementation; ```. ```; glob function is not supported by this implementation; ```. ```; 2016-05-24 14:11:46,501 cromwell-system-akka.actor.default-dispatcher-23 ERROR - Workflow d539eabc-3e74-4f06-85ca-0f52ac1f8a2b failed (during ExecutingWorkflowState): ; ```. ```; 2016-05-24 14:11:46,501 cromwell-system-akka.actor.default-dispatcher-23 INFO - WorkflowActor-d539eabc-3e74-4f06-85ca-0f52ac1f8a2b has gone terminal; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/882:89,ERROR,ERROR,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/882,2,['ERROR'],['ERROR']
Availability,Multiple tickets have a root cause of CromwellTestKitSpec and are leading to frequent enough failures in QA Triage to warrant some eyeballs. These tickets include (but are not limited to):. #4457 ; #4454 ; #4453 ; #4418 ; #4469; #4470 ; #4521. --. https://fc-jenkins.dsp-techops.broadinstitute.org/view/Testing/view/Test%20Runners/job/cromwell-test-runner/2438,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4458:93,failure,failures,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4458,1,['failure'],['failures']
Availability,"Must have forgotten this info... 1) develop (cromwell-23-79f6e12-SNAPSHOT.jar); 2) No, cannot believe I forgot again...; 3) No. On Fri, Nov 4, 2016 at 9:43 AM, Chris Llanwarne notifications@github.com; wrote:. > @LeeTL1220 https://github.com/LeeTL1220; > - Is this .21, .22 or develop?; > - Did you capture the 'CMD \' thread dump from the JVM?; > - Was there an error message from Cromwell before it got stuck?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258434307,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk5vBVr6UFscUsg3sazpo1H9pyVgMks5q6zaXgaJpZM4Ko1_r; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258435488:363,error,error,363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258435488,1,['error'],['error']
Availability,"My ""test"" right now is the following code in a Scala worksheet:; ```; import scala.concurrent.{ExecutionContext, Future}. implicit val ec = ExecutionContext.global. val x = Future(throw new Exception(""hello world"")). val y = x.map(_ => println(""wasd""))(ec); .recover { case a: Throwable => println(""Exception was: "" + a.getMessage) }(ec); ```; which prints; ```; Exception was: hello world; ```; I'm working on figuring out how construct this in situ in a way that meaningfully tests something.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5022#issuecomment-501411862:259,recover,recover,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5022#issuecomment-501411862,1,['recover'],['recover']
Availability,"My JSON file:. {; ""CNVSomaticPairWorkflow.common_sites"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/common_snps_sample-chr20.interval_list"",; ""CNVSomaticPairWorkflow.gatk_docker"": ""8f0ef5140437"",; ""CNVSomaticPairWorkflow.intervals"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/chr20.interval_list"",; ""CNVSomaticPairWorkflow.normal_bam"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143_BL-n1-chr20-downsampled.deduplicated.bam"",; ""CNVSomaticPairWorkflow.normal_bam_idx"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143_BL-n1-chr20-downsampled.deduplicated.bam.bai"",; ""CNVSomaticPairWorkflow.bin_length"": ""10000"",; ""CNVSomaticPairWorkflow.read_count_pon"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/wgs-no-gc.pon.hdf5"",; ""CNVSomaticPairWorkflow.ref_fasta_dict"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/human_g1k_v37.chr-20.truncated.dict"",; ""CNVSomaticPairWorkflow.ref_fasta_fai"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/human_g1k_v37.chr-20.truncated.fasta.fai"",; ""CNVSomaticPairWorkflow.ref_fasta"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/human_g1k_v37.chr-20.truncated.fasta"",; ""CNVSomaticPairWorkflow.tumor_bam"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsampled.deduplicated.bam"",; ""CNVSomaticPairWorkflow.tumor_bam_idx"": ""/Users/mkanaszn/Broad_Institute/Code/gatk_ssh/gatk/src/test/resources/large/cnv_somatic_workflows_test_files/HCC1143-t1-chr20-downsample",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388871590:600,down,downsampled,600,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388871590,2,['down'],['downsampled']
Availability,"My WDL pipeline failed to run with Cromwell 55 configured with the `cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory` Google API with a long list of errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; I was under the expectation that this had been handled in issue #5344 and that Cromwell would retry to access the files until available (the files do indeed exist at the time of this writing).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154:175,error,errors,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154,5,"['avail', 'down', 'error']","['available', 'download', 'errors']"
Availability,"My admin has added the policy serviceusage.services.use to my Service Account, whatever that means (I have no idea). Now I get this error:; ```; [2020-07-27 19:13:48,68] [error] PipelinesApiAsyncBackendJobExecutionActor [bf8fa2c2wf_hello.hello:NA:1]: Error attempting to Execute; java.io.IOException: Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Futur",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:132,error,error,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,3,"['Error', 'error']","['Error', 'error']"
Availability,"My group has been running Cromwell with AWS Batch as part of our pipeline development process and we've observed several cases of workflows ""silently"" failing where no Batch jobs have failed but the workflow log points to missing RC files. From our testing, this issue seems to affect ~10% of the samples that we try to process, although the issue appears ""randomly"" as there is no single set of samples that reproduces the error time after time. After digging through several logs, I believe I've traced the error to an issue where a batch job is being submitted, but it the service finds a previously run job that uses a completely different set of input files and runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:424,error,error,424,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,4,"['error', 'failure']","['error', 'failure']"
Availability,"My subworkflow wdl seems to ignore the task-call order. It skips M2 task entirely, which is a subworkflow in `hapmap_sensitivity_no_python.wdl`. I get this error with the latest cromwell master branch. cromwell jar: `/home/unix/tsato/cromwell-jars/cromwell-25-31ae549-SNAP.jar`. wdl: `/humgen/gsa-hpprojects/dev/tsato/mutect/strand-artifact/hapmap_sensitivity_no_python.wdl`; json: `/humgen/gsa-hpprojects/dev/tsato/mutect/strand-artifact/hapmap_sensitivity_10plex.json`; wdl-dependencies: `/humgen/gsa-hpprojects/dev/tsato/mutect/strand-artifact/mutect2.zip`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2117:156,error,error,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2117,1,['error'],['error']
Availability,"My task are not using docker. Also I see no attempts at all to copy or softlink the files. Not in the log, and not in the cromwell-executions folder.; Also hard-linking seems to persist using the `SGE` backend. Even though the localization has the same configuration as above. So the error is not backend specific. Fortunately, all the other values in the config are used. Which makes me think that either my configuration file has some error (keys in wrong place). But I have checked this over and over again already with the example files and it seems to be correct (though I am not infallible of course).; Or the backend just ignores the values due to a bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-356221440:284,error,error,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3109#issuecomment-356221440,2,['error'],['error']
Availability,"My view of difficulty has only increased. You could narrow it down to a subset of things - . - Nothing in the backend; - Nothing in the service registry; - In the future, nothing involving filesystems. And it'd be easy to do. But most of the stuff people will be fiddling with are in those blocks. One could set up maybe some service (not necessarily ServiceRegistry service)where things which care about config register themselves and then process htings that way but that seems like a giant pain in the ass for not enough gain.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325490308:62,down,down,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1598#issuecomment-325490308,1,['down'],['down']
Availability,My workflows are running without errors now.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4772#issuecomment-477128999:33,error,errors,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772#issuecomment-477128999,1,['error'],['errors']
Availability,NB also check how Cromwell reports errors in imported WDL files,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1933#issuecomment-276481524:35,error,errors,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1933#issuecomment-276481524,1,['error'],['errors']
Availability,"NB the failures in `travis/pr` `centaurJes` were caused by the `customLabels` change merging into develop before I rebased. . I didn't want to wait 30 minutes to rerun the tests, since they were already passing on the other backends, and in `travis/push`...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2941#issuecomment-347637020:7,failure,failures,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2941#issuecomment-347637020,1,['failure'],['failures']
Availability,NOT MERGED closing temporarily for repairs as there are at least a couple of legit broken PAPI Centaur tests,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3668#issuecomment-391168787:35,repair,repairs,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3668#issuecomment-391168787,1,['repair'],['repairs']
Availability,"NOTE FOR REVIEWERS: I was hoping to get feedback for how to best implement `successHandler` located in `CromwellApiService.scala`. Passing it in the `askSubmit` function gives a variety of errors on github and in code. Put it before `case Success(w)` and I get an error in Github because of mismatched states. Put the handler anywhere else in the function and it's unreachable. Currently, only the `errorHandler` is being used in the `askSubmit` function. Do I need to implement the `successHandler` in the function? Or can I leave it as is?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030:189,error,errors,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6779#issuecomment-1181771030,3,['error'],"['error', 'errorHandler', 'errors']"
Availability,"NOTE: Not a request for immediate work. I'm not even sure that the suggestions are technically sound. This is something I've been mulling over for a while and wanted to get it down as a place holder and also in case anyone else is interested in experimenting. In particular the source and sink I'm not sure about. Currently when a workflow is submitted a record is dropped into the workflow store. At a regular interval the WMA will pull up to N submitted workflows and for each one will create a WorkflowActor which will both create its MaterializeWorkflowDescriptorActor and start running. Instead of a now and then batch system convert this to a streaming system controlled by backpressure. My thinking here is to have the source be the database table of submitted workflows (can a source be a perpetual query?) and then the stream could materialize the descriptor, create a workflow actor, the WA could register itself w/ the WMA and then start. I was picturing using Sink.actorRefWithAck on the WMA to provide back pressure (again, not sure it works like this). . In theory this would allow us to handle submitted workflows as rapidly as the WMA can handle w/o being overwhelmed. . Beyond the two key ""I don't know if it works like that"" points another is if there's another chokepoint which would better serve as the key backpressure signal and if so if it'd still make sense to wire this up as a stream - e.g. would the backpressure signaling point be too deep into the system to be practical?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1705:176,down,down,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1705,1,['down'],['down']
Availability,"NOTE: The publishing-contract tests are failing due to active maintenance on the pact broker. That said, the updated query doesn't change any behavior or payloads from Cromwell so there shouldn't be any changes to the contracts to begin with.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867:62,mainten,maintenance,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7228#issuecomment-1747137867,1,['mainten'],['maintenance']
Availability,Need to investigate the Travis failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/220#issuecomment-145907163:31,failure,failures,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/220#issuecomment-145907163,1,['failure'],['failures']
Availability,"Net improvement to tests 👍 will merge. Failures are known entities, `Application Default Credentials are not available` and `cromwell-drs-localizer`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7186#issuecomment-1650661033:39,Failure,Failures,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7186#issuecomment-1650661033,2,"['Failure', 'avail']","['Failures', 'available']"
Availability,"Nevermind again, testing now but I don't think we need to change anything here. `test.inc.sh` is already set up to use `VAULT_ROLE_ID` and `VAULT_SECRET_ID` env vars for auth if they're available, so all that's needed is to add those and remove `VAULT_TOKEN`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031922728:186,avail,available,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031922728,1,['avail'],['available']
Availability,"Nevermind, just noticed some failures due to Docker rate-limiting, which I would love to completely avoid. I'm going to try installing Vault directly rather than using Docker.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031903603:29,failure,failures,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6669#issuecomment-1031903603,1,['failure'],['failures']
Availability,"New info since our meeting earlier today:. I was able to confirm that Batch actually *can* pull and run Docker Image Format v1 images (PR to explicitly assert this [here](https://github.com/broadinstitute/cromwell/pull/7522)). So that does not appear to be the source of my private Docker woes. I also pushed a new image that is just a re-tag of `ubuntu:latest` to `broadinstitute/cloud-cromwell:2024-08-30`. Trying to run with that, with or without the `docker.io/` prefix results in the error:. ```; docker: Error response from daemon: pull access denied for broadinstitute/cloud-cromwell, repository does not exist or may require 'docker login': denied: requested access to the resource is denied. ```. which is a complaint about being able to access the repository, not the format of a particular image within the repository. Not sure what's going on here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095:489,error,error,489,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515#issuecomment-2322317095,2,"['Error', 'error']","['Error', 'error']"
Availability,New known failure list,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3118:10,failure,failure,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3118,1,['failure'],['failure']
Availability,"New theory: I believe this is seen just before every ""deadlock"" failure. ```java; Exception in thread ""db-2"" java.lang.IllegalArgumentException: requirement failed: count cannot be increased; at scala.Predef$.require(Predef.scala:277); at slick.util.ManagedArrayBlockingQueue.$anonfun$increaseInUseCount$1(ManagedArrayBlockingQueue.scala:43); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at slick.util.ManagedArrayBlockingQueue.locked(ManagedArrayBlockingQueue.scala:201); at slick.util.ManagedArrayBlockingQueue.increaseInUseCount(ManagedArrayBlockingQueue.scala:42); at slick.util.AsyncExecutor$$anon$2$$anon$1.beforeExecute(AsyncExecutor.scala:117); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-439187592:64,failure,failure,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-439187592,1,['failure'],['failure']
Availability,"New thoughts/additional info. . I just tried a workflow that evaluates all the way to the end successfully (and had been run before on the same data and no, call caching didn't happen, as is expected with AWS backend), with the exception of adding workflow options to specify the output and logs directories for the final results. . Interestingly enough the new prefixes were generated but files were not transferred over EXCEPT for the log. The workflow log transferred just fine. In the log there appears to be no errors or indication that the intended outputs were not successfully transferred over. . I'm looking at the workflow status, and while all the files were made correctly (so all tasks completed successfully), but the workflow as a whole failed b/c it knows it failed to transfer over the output data. However again, there are no errors indicated in the metadata indicating why no files were copied. . I'm wondering if this too would be expected to be a hashing failure? Are the identities of the files created that are intended as outputs defined by the hashing? Would this behavior be expected given the current issues with call caching? Or is this a new issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-462857084:516,error,errors,516,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-462857084,3,"['error', 'failure']","['errors', 'failure']"
Availability,"Nice, OK, so the test is `should abort a workflow mid run abort.scheduled_abort` and the failure is `expected: ""Aborted"" but got: ""Failed""`. So it seems like `true` makes an actual abort happen as expected, as opposed to an abort that somehow looks like a failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466:89,failure,failure,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2111156466,2,['failure'],['failure']
Availability,"Nice, handy to have this written down for future Google searchers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245922919:33,down,down,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245922919,1,['down'],['down']
Availability,Nicer error message for not-enabled LanguageFactories,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3923:6,error,error,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3923,1,['error'],['error']
Availability,Nix FailureEvent,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/964:4,Failure,FailureEvent,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/964,1,['Failure'],['FailureEvent']
Availability,"No effect on final status. Just exits with non-zero. On Mon, Mar 27, 2017 at 9:58 AM, Thib <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Does the error have the effect; > on the actual final status of the workflow ? Or does it cause cromwell to; > exit with a non 0 exit code ? I wasn't able to reproduce the exact same; > error but I've had similar ones and I've got a branch that should fix it,; > if you want to try it out.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289461429>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk2yfUNo1FrM2Y7ftpxQSHOsOwCc3ks5rp8BpgaJpZM4Mi6Mp>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289471493:179,error,error,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079#issuecomment-289471493,2,['error'],['error']
Availability,No error message when the input JSON has incorrect workflow names,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3120:3,error,error,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3120,1,['error'],['error']
Availability,"No fatal error. so, close.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-449074013:9,error,error,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3269#issuecomment-449074013,1,['error'],['error']
Availability,"No issues found with the production code, but a recent CaaS scare led to one additional unit and Centaur test each. The Centaur test would definitely benefit from some ""flock programming"" (h/t @aednichols) for more robust metadata assertions. Right now the only assertion is workflow success, which seems pretty weak except that it's better than what CaaS reported. Ideally I'd like to assert on something like the following: . From ; `; curl --silent -X GET ""http://localhost:8000/api/workflows/v1/<<UUID>>/metadata?expandSubWorkflows=false"" -H ""accept: application/json"" | jq -M '[.calls[""error_10_preemptible.delete_self_if_preemptible""] | .[] | {attempt,preemptible,retryableFailure,executionStatus,backendStatus} | del(.[]| nulls)]'; `. ```; [; {; ""attempt"": 1,; ""preemptible"": true,; ""retryableFailure"": true,; ""executionStatus"": ""RetryableFailure"",; ""backendStatus"": ""Preempted""; },; {; ""attempt"": 2,; ""preemptible"": false,; ""executionStatus"": ""Done"",; ""backendStatus"": ""Success""; }; ]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5101:215,robust,robust,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5101,1,['robust'],['robust']
Availability,"No worries, that's what PRs are for. 😉 . Regarding the text:. > Fixed a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large. This PR doesn't address the _initial_ read timeouts. But it does mitigate them piling up to create bigger batches that might cause more timeouts. What I'm not sure of is with this change of a new-batch-per-request will the ""Attempted 10 times"" be able to weather whatever is causing the timeout hiccups or not. I don't want to overpromise in the message, so maybe something like?. > Better handling for a bug that could cause workflows to unexpectedly fail with errors related to Google Cloud Storage. The errors reference GcsBatchFlow.BatchFailedException and Read timed out or 413 Request Entity Too Large.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991:129,error,errors,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800636991,4,['error'],['errors']
Availability,"No, not yet. A bunch of batching code was put into `JesPollingActor`, but that logic is not available in a generic way. If anyone with cycles does come across this ticket, that might be a good first place to start. The logic would need to be slightly different. ~JES~ PAPI allows batching _any_ jobs together, while a batch of GridEngine jobs have to use the same resource requirements, aka runtime attributes. Still partial-grouping-for-similar-jobs would still be an improvement over the current single-submit-per-job.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-371909917:92,avail,available,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-371909917,1,['avail'],['available']
Availability,"Not 100% sure what wasn't working at what point. I suspect that based on the order of the original commits<sup>1</sup>, the `RunMysql` and server should have both worked at ""4."". At that point I believe the config `url` still contained `useSSL=true`, the application config was being passed on the command line, and the mysql jdbc code should have been in the main assembly. By the time I was running ""11."" earlier today, the configuration `url` no longer contained `useSSL=true`, and connections within `SlickDataAccess` were returning the error combo:. ```; java.sql.SQLTimeoutException: Timeout after 1000ms of waiting for a connection.; ...; Caused by: java.sql.SQLException: Access denied for user '…'@'…' (using password: YES); ```. I did add another variable in ""11."" by always testing with `useSSL=true&requireSSL=true`, but according to the [logs](http://pastebin/209) of the latest 'RunMysql', `jdbcMain` and `jdbcRequireSsl` passed. So that _shouldn't_ have changed the results. Meanwhile, all test combinations of setting ssl worked for both slick and raw datasource connections, in tests via the url (*Ssl*), or via the dataSource properties (*Prop). So I think just setting back the `useSSL=true` is the minimum required fix, but I'd prefer to see `requiredSSL=true` added as well, as was successfully run in `slickSslDriver`. <sup>1</sup> What I believe is the previous order of the commits:; 1. Updated run.sh to pass in the mysql key & trust stores.; 2. log database config; 3. make mysql not test-only; 4. Add config file option in run.sh to make container use custom configuration; 5. debugging ""script""; 6. log actual uniquified config; 7. Test at JDBC level.; 8. hardcode use of SSL; 9. count rows in WORKFLOW_EXECUTION; 10. Logging the just the URL in SlickDataAccess, not the entire config.; 11. Added a suite of mysql ssl test.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-123520815:541,error,error,541,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/85#issuecomment-123520815,1,['error'],['error']
Availability,"Not entirely sure I understand the gcsFilesystem in WorkflowDescriptor, but if everyone agrees that it's fine I'll pipe down. So, :+1: from me!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170681519:120,down,down,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170681519,1,['down'],['down']
Availability,"Not expecting the AWS build to pass, that failure is addressed in #6547.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6546#issuecomment-947148974:42,failure,failure,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6546#issuecomment-947148974,1,['failure'],['failure']
Availability,"Not expecting the PAPI builds to pass, those failures are addressed in #6546.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6547#issuecomment-947148731:45,failure,failures,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6547#issuecomment-947148731,1,['failure'],['failures']
Availability,"Not for this PR, but I noticed that the WorkflowActor is more than 1000 lines now, we might need to think of a way to break things down a little bit more in the future, because I have the feeling it's going to keep growing as we add more features..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/310#issuecomment-161990464:131,down,down,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/310#issuecomment-161990464,1,['down'],['down']
Availability,"Not necessarily the problem, but even non-obvious GCP quotas can limit how many workers are scheduled. Specifically, some things to look out for:. - Preemtible specific resources, like CPUs and Memory (if you're running preemtibles); - If you are using preemtibles, there may not be enough available instances; - Local SSD (GB); - Internal IP addresses; - In-use IP addresses; - List requests per 100 seconds. I thought mine were high enough, but from this page (replace `$region` with your region) you can click the ""Current Usage"" to sort by in-demand resources:. - https://console.cloud.google.com/iam-admin/quotas?project=portable-pipeline-project&location=$region. ![image](https://user-images.githubusercontent.com/22381693/72295508-d2132900-36ab-11ea-8380-256c2ad381b4.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305:290,avail,available,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305,1,['avail'],['available']
Availability,"Not really for this PR I think, but looking at the `validateInputs` method, it only validates the inputs if a file was passed (makes sense), but if no input file is specified and the WDL does need an input, validation will return success, which is weird IMO.; @geoffjentry My 2 cents are I personally prefer having small short-lived actors instead of a singleton actor that handle all the requests. I think it's more robust, faster, and less leading towards godlike actors.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195393365:417,robust,robust,417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195393365,1,['robust'],['robust']
Availability,"Not really, I have a work-around (if /sys/block/sdb/ is a directory and /dev/sdb is mounted in mtab, use /sys/block/sdb/); ```; function findBlockDevice() {; MOUNT_POINT=$1; FILESYSTEM=$(grep -E ""$MOUNT_POINT\s"" /proc/self/mounts \; | awk '{print $1}'); DEVICE_NAME=$(basename ""$FILESYSTEM""); FS_IN_BLOCK=$(find -L /sys/block/ -mindepth 2 -maxdepth 2 -type d \; -name ""$DEVICE_NAME""); if [ -n ""$FS_IN_BLOCK"" ]; then; # found path to the filesystem in the block devices. get the; # block device as the parent dir; dirname ""$FS_IN_BLOCK""; elif [ -d ""/sys/block/$DEVICE_NAME"" ]; then; # the device is itself a block device; echo ""/sys/block/$DEVICE_NAME""; else; # couldn't find, possibly mounted by mapper.; # look for block device that is just the name of the symlinked; # original file. if not found, echo empty string (no device found); BLOCK_DEVICE=$(ls -l ""$FILESYSTEM"" 2>/dev/null \; | cut -d'>' -f2 \; | xargs basename 2>/dev/null \; || echo); if [[ -z ""$BLOCK_DEVICE"" ]]; then; 1>&2 echo ""Unable to find block device for filesystem $FILESYSTEM.""; if [[ -d /sys/block/sdb ]] && ! grep -qE ""^/dev/sdb"" /etc/mtab; then; 1>&2 echo ""Guessing present but unused sdb is the correct block device.""; echo ""/sys/block/sdb""; else ; 1>&2 echo ""Disk IO will not be monitored.""; fi; fi; fi; }; ```. I am not sure if this is a google VM problem, a docker problem, or a problem with how cromwell specifies volumes to docker; but I took their response to be ""we don't care and won't fix it"". Fortunately for me the work-around nearly always works for cromwell jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-529486322:621,echo,echo,621,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-529486322,7,['echo'],['echo']
Availability,"Not really. The problem is that then the command has to be written very; carefully so that errors in intermediate steps propagate...we already have; this problem, but this will make it worse. On Fri, Jan 27, 2017 at 4:54 PM, Chris Llanwarne <notifications@github.com>; wrote:. > @yfarjoun <https://github.com/yfarjoun> Is this good enough as an interim?; >; > task foo {; > command {; > # do some stuff...; > # verify and write results to verification.txt; > }; > output {; > # normal outputs; > # Boolean verification = read_bool(verification.txt); > }; > }; >; > task fail {; > command {; > # something guaranteed to fail; > }; > }; >; > workflow bar {; > call foo; > if (!foo.verification) { call fail }; > }; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-275785501>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0qC2URNFfJfnNoDOm6_RCQtKt4p_ks5rWmejgaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-275787429:91,error,errors,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-275787429,1,['error'],['errors']
Availability,"Not sure how much UI we do within the cromwell code base. Agree with Jeff that perhaps this might be a better contribution to another UI team, but I'm not the ticket master. Also, I don't really have the ramp up to review HTML/JS for code style conventions / quality myself. Tried it out locally. As long as the jobs don't run too fast, the endpoint seems to work pretty well!. Btw, when _all_ calls run with start/stop _inside_ a particular second, the page renders with a cryptic red box that says:. > Cannot read property 'v' of undefined. Probably not a big deal unless we expect lots of folks running `ThreeStep.wdl` like I did. ``` javascript; // At least one row must span a second 00. // Ok; var dataRow = ['call_name','call_name', null,; new Date(""2015-11-02T18:00:00.000-05:00""),; new Date(""2015-11-02T18:00:00.001-05:00""),; null , 100, null];. // or also ok; var dataRow = ['call_name','call_name', null,; new Date(""2015-11-02T18:00:00.999-05:00""),; new Date(""2015-11-02T18:00:01.000-05:00""),; null , 100, null];. // But if all rows within a second, then error!. // Red box of death; var dataRow = ['call_name','call_name', null,; new Date(""2015-11-02T18:00:00.001-05:00""),; new Date(""2015-11-02T18:00:00.999-05:00""),; null , 100, null];. // Also an error too; var dataRow = ['call_name','call_name', null,; new Date(""2015-11-02T18:00:00.001-05:00""),; new Date(""2015-11-02T18:00:00.002-05:00""),; null , 100, null];; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/266#issuecomment-153182822:1066,error,error,1066,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/266#issuecomment-153182822,2,['error'],['error']
Availability,"Not sure if this is a separate issue or not, but when @knoblett and I were submitting a workflow yesterday we got the exact same error message (submitted with Swagger). The issue for her was that there was an input that was specified to be a File type, but in reality it was just a String (so I'm guessing the issue was similar in that it couldn't find the ""file""). Unfortunately, it validated just fine, but we weren't able to submit it. . I'd be happy to provide the WDL and JSON files (both the broken version and the fixed version) but they won't attach in a github comment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209931581:129,error,error,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/703#issuecomment-209931581,1,['error'],['error']
Availability,"Not sure what I should be doing. I have tried the following command:; ```; gcloud logging read 'timestamp>=""2020-09-01T00:00:00Z""' > logs; ```; And then:; ```; $ cat logs | grep 30148356615-compute@developer.gserviceaccount.com -A10 | grep -i permission | cut -d: -f2 | sort | uniq -c; 14 lifesciences.operations.cancel; 425 lifesciences.workflows.run; 12 storage.buckets.get; 30629 storage.objects.create; 30985 storage.objects.delete; 12819 storage.objects.get; 157 storage.objects.getIamPolicy; 6859 storage.objects.list; ```; It does seem to be the case that `storage.objects.delete` is requested many times, so that is definitely an issue when you only have roles `storage.objectCreator` and `storage.objectViewer` but not `storage.objectAdmin`. I did not observe any permission from role `iam.serviceAccountUser` but that role is indeed needed. And I observe some requests for permission `storage.buckets.get` that do end in ERROR, but it does not seem to affect the pipeline.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685986970:931,ERROR,ERROR,931,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685986970,1,['ERROR'],['ERROR']
Availability,"Not sure what was going on there but that seems to have been a transient problem. I created a branch in our repo with your changes and it failed the same way for me once, but I restarted it and got past the error. You can see the progress of the builds here: https://travis-ci.com/broadinstitute/cromwell/builds/113681945",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5000#issuecomment-497167230:207,error,error,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5000#issuecomment-497167230,1,['error'],['error']
Availability,"Note how the wdl below attempts to use `output_dir` in its output for `reproducibility_files`. This causes a failure. If I remove the `String output_dir` from the task and update the output to a hardcoded path, `Array[File] reproducibility_files = glob(""reproducibility_output/*.*"")`, it works fine. . ``` wdl; task run_plot_reproducibility {; File file1; File file2; String output_dir; command {; run_plot_reproducibility ${file1} ${file2} ${output_dir} 3.7; }; output {; File reproducibility_table = ""${output_dir}/reproducibility.tsv""; File reproducibility_final_results = ""${output_dir}/final_results.tsv""; File reproducibility_plot = ""${output_dir}/reproducibility_Reproducibility.png""; #### HERE; Array[File] reproducibility_files = glob(""${output_dir}/*.*""); }; runtime {; docker: ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; memory: ""2GB""; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1302:109,failure,failure,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302,1,['failure'],['failure']
Availability,"Note that in order to aggregate files after a scatter step, I must include the python code (see `run_plot_purity_series` task) in the next task. The next task takes in a file of files. ; The python inside the wdl is very counter-intuitive, prone to error, and unnecessary in other execution managers. See my real example below... ``` wdl; workflow crsp_validation_workflow {. ....snip....; Array[Array[File]] triplet_file_array = read_tsv(input_triplet_file_list); Float ploidy=""2"". scatter (triplet in triplet_file_array) {; ....snip.... call run_sensitivity_precision {; input:; entity_id=triplet[0],; oncotated_target_seg_gt_file = oncotate.oncotated_target_seg_gt_file,; ploidy=ploidy; }; }. call run_plot_purity_series {; input:; output_dir=""plots/"",; amp_sens_prec=run_sensitivity_precision.amp_sens_prec_file,; del_sens_prec=run_sensitivity_precision.del_sens_prec_file,; small_sens=run_sensitivity_precision.small_sens_file; }; }; ....snip....; task run_sensitivity_precision {; File oncotated_target_seg_gt_file; Float ploidy; String entity_id. command {; # Ignore chromosome 2, since the normal has this event and HCC1143T does not, so ground truth may be off, since; # detection of deletions could be reduced. Chromosome 6 may have a similar issue.; run_sensitivity_precision -i ""[2]"" ${oncotated_target_seg_gt_file} ${ploidy} ${entity_id}.sens_prec; }. output {; File amp_sens_prec_file = ""${entity_id}.sens_prec.amp.tsv""; File del_sens_prec_file = ""${entity_id}.sens_prec.del.tsv""; File small_sens_file = ""${entity_id}.sens_prec.small_segs.tsv""; File gene_segs_sens_prec_file = ""${entity_id}.sens_prec.gene_seg""; }. runtime {; docker: ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; memory: ""2GB""; }; }. task run_plot_purity_series {; String output_dir; Array[File] amp_sens_prec; Array[File] del_sens_prec; Array[File] small_sens. command {; ################# HERE; python <<CODE; files = ""${sep="","" amp_sens_prec}"".split("",""); files.extend(""${sep="","" del_sens_prec}"".split(",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1263:249,error,error,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1263,1,['error'],['error']
Availability,"Note that the travis failure is a good thing (in that the change is working, not that it's an awesome circumstance), as it was due to not all of the centaur tests passing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/987#issuecomment-225329843:21,failure,failure,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/987#issuecomment-225329843,1,['failure'],['failure']
Availability,"Note to reviewers: this is currently packaged as 4 commits. The ""Upgrade to ScalaTest 3.2.1"" commit was my shot in the dark at fixing a runtime error that turned out not to fix the runtime error but did produce an upgrade to ScalaTest 3.2.1. It's a huge and tedious commit that's not really related to what's happening in the other commits. You'll probably want to ignore the ScalaTest 3.2.1 upgrade commit and focus on the other 3 instead.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5751:144,error,error,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5751,2,['error'],['error']
Availability,"Note to self, the test `drs_usa_jdr_preresolve` failed with log output; ```; 2023/07/18 20:58:44 Starting container setup.; 2023/07/18 20:58:46 Done container setup.; 2023/07/18 20:58:49 Starting localization.; 2023/07/18 20:59:06 Localization script execution started...; 2023/07/18 20:59:06 Localizing input gs://cloud-cromwell-dev-self-cleaning/cromwell_execution/ci/drs_usa_jdr/4f16ae4f-05d4-4bea-9f13-831c2d1ac006/call-skip_localize_jdr_drs_with_usa/script -> /cromwell_root/script; 2023/07/18 20:59:12 Localization script execution complete.; Error response from daemon: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: ""-m"": executable file not found in $PATH: unknown; ```. `gs://cloud-cromwell-dev-self-cleaning/cromwell_execution/ci/drs_usa_jdr/4f16ae4f-05d4-4bea-9f13-831c2d1ac006/call-skip_localize_jdr_drs_with_usa/skip_localize_jdr_drs_with_usa.log`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7179#issuecomment-1641077440:549,Error,Error,549,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7179#issuecomment-1641077440,1,['Error'],['Error']
Availability,Note to self: Metadata field ; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: blah blah blah; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```. Should be:; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]:message; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: blah blah blah; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```; And: ; ```; *************************** 5. row ***************************; METADATA_JOURNAL_ID: 5; WORKFLOW_EXECUTION_UUID: ...; METADATA_KEY: failures[0]:causedBy[]; CALL_FQN: NULL; JOB_SCATTER_INDEX: NULL; JOB_RETRY_ATTEMPT: NULL; METADATA_VALUE: NULL; METADATA_TIMESTAMP: 2016-06-23 20:11:26.508000; METADATA_VALUE_TYPE: string; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2201#issuecomment-297123346:168,failure,failures,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2201#issuecomment-297123346,3,['failure'],['failures']
Availability,"Note to those trying to reproduce-- the wdl is reported to run fine on the combination of:; 1. Local; 2. Small inputs. It should reproducibly fail on the combination of:; 1. Current JES / Firecloud (cromwell 0.18); 2. Normal (aka large) input files. The JES logs showed that it started localizing, and then four minutes later began delocalizing files. The STAR stderr printed that the program was starting, but there was no other error other message in the stderr nor jes log. AFAIK, there did not seem to be any other current indication via the Firecloud interface as to why the job was exiting prematurely.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/698#issuecomment-209079497:430,error,error,430,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/698#issuecomment-209079497,1,['error'],['error']
Availability,Note: This PR breaks the cromwell-as-a-git-submodle functionality. But I've got verbal confirmation from @hjfbynara that Green is no longer using cromwell this way. This is the error that one sees with the `sbt-git` used in this PR plus a git submodule:. ```java; fatal: Invalid gitfile format: /Users/kshakir/src/cromwell/.git; [error] java.util.NoSuchElementException: head of empty stream; [error] 	at scala.collection.immutable.Stream$Empty$.head(Stream.scala:1104); [error] 	at scala.collection.immutable.Stream$Empty$.head(Stream.scala:1102); [error] 	at com.typesafe.sbt.SbtGit$.$anonfun$buildSettings$21(SbtGit.scala:138); [error] 	at sbt.internal.util.Init$Value.$anonfun$apply$3(Settings.scala:804); [error] 	at sbt.internal.util.EvaluateSettings.$anonfun$constant$1(INode.scala:197); [error] 	at sbt.internal.util.EvaluateSettings$MixedNode.evaluate0(INode.scala:214); [error] 	at sbt.internal.util.EvaluateSettings$INode.evaluate(INode.scala:159); [error] 	at sbt.internal.util.EvaluateSettings.$anonfun$submitEvaluate$1(INode.scala:82); [error] 	at sbt.internal.util.EvaluateSettings.sbt$internal$util$EvaluateSettings$$run0(INode.scala:93); [error] 	at sbt.internal.util.EvaluateSettings$$anon$3.run(INode.scala:89); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); [error] 	at java.lang.Thread.run(Thread.java:745); [error] java.util.NoSuchElementException: head of empty stream; ```. cc https://github.com/broadinstitute/cromwell/issues/644,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2785#issuecomment-339382680:177,error,error,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2785#issuecomment-339382680,16,['error'],['error']
Availability,"Note: Treat this as a draft/early look/WIP rather than a request to merge immediately. I forgot to push the ""draft"" button. A bunch of bash scripts to be run by Jenkins and executed on VMs in order to get overnight centaur testing of our performance scripts. * Lets the overnight perf tests work again; * Splits the logic into stages so that we aren't doing everything on a single VM (unblocking the transition to horicromtal); * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-deploy-and-centaur and the sub-jobs which it calls (especially https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-run-centaur), which make use of these scripts.; * See https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-perf-composite-all-centaur-tests for a jenkins job which runs *all* of our centaur tests against ad-hoc perf Cromwells. TODOs:; - [ ] Document these changes in the google doc; - [ ] Better error reporting(?); - [x] Compile into a suite in jenkins(?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5075:949,error,error,949,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5075,1,['error'],['error']
Availability,Note: We currently can't test CWLs on AWS with v37 Cromwell because of the input/output staging issue I mentioned over with the call caching problems. We just get the input/output error and the jobs won't move forward enough to even see what other issues might arise. . Related to:; https://github.com/broadinstitute/cromwell/issues/4563. That's a deal breaker for our institution. @wleepang These are all related.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4587#issuecomment-467545872:180,error,error,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4587#issuecomment-467545872,1,['error'],['error']
Availability,Note: this is a tactical fix to allow the cache-fetch post-processing to fail and the workflow to continue.; It does nothing to prevent the errors from happening in the first place - for that issue see #3979,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3978:140,error,errors,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3978,1,['error'],['errors']
Availability,"Noticed that cached-copy localization strategy is broken in Cromwell-51. This test works on a cluster where `execution-dir` is some place to do execution and copy the inputs to, and `/different/file/system/` is on a different disk. Small task (`catsmallfile.wdl`):. ```wdl; version development. task CatSmallFile {; input {; File inp; }; command {; cat ${inp}; }; output {; String out = read_string(stdout()); }; }; ```. Config (`cromwell.conf`):; ```hocon; include required(classpath(""application"")). backend: {; ""default"": ""Local"",; ""providers"": {; ""Local"": {; ""actor-factory"": ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"",; ""config"": {; ""root"": ""<execution-dir>"",; ""filesystems.local.duplication-strategy"": [; ""cached-copy""; ]; }; }; }; }; ```. Command:. ```bash; echo ""Goodbye, call-caching"" >> /different/file/system/inp.txt; echo '{""inp"": ""/different/file/system/inp.txt""}' >> inputs.json. java -Dconfig.file=cromwell.conf -jar cromwell-50.jar run catsmallfile.wdl -i inputs.json; # <execution-dir>/cached-inputs/ is empty. java -Dconfig.file=cromwell.conf -jar cromwell-50.jar run catsmallfile.wdl -i inputs.json; # <execution-dir>/cached-inputs/ is populated; ```. ----. Anecdotally, I've noticed some of the permissions of localised files have changed, I wonder if this is related to that?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5533:792,echo,echo,792,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5533,2,['echo'],['echo']
Availability,"Now I am having trouble running the stack again (I've edited it to change the default of ScratchMountPoint to /cromwell_root). I deleted the old stacks. Getting a failure when trying to create the ec2 instance, but there is no helpful error as to why. . Is it possible to change the value of AWS_CROMWELL_LOCAL_DISK? Where do I change that? In the config file somewhere? If I could change that from /cromwell_root to /scratch then things ought to work with my existing AMI....",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468828705:163,failure,failure,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468828705,2,"['error', 'failure']","['error', 'failure']"
Availability,"Now and then we see jobs get wedged in a state where the following exception kicks in infinitely (as far as we've seen - at least several hours). Restarting cromwell and reattaching those jobs will immediately resolve it - implying that the error wasn't ""real"" at least not any more and that something was bonkers on our side. Standard logging doesn't reveal anything interesting that I've seen. It's not something I've found a way to reproduce reliably - at least not consistently. ```2016-12-09 16:45:21,896 cromwell-system-akka.dispatchers.backend-dispatcher-28 WARN - JesAsyncBackendJobExecutionActor [UUID(50f9b932)JointGenotyping.SplitGlob:72:1]: could not download return code file, retrying:; com.google.cloud.storage.StorageException: Remote host closed connection during handshake; at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); at com.google.cloud.storage.spi.DefaultStorageRpc.get(DefaultStorageRpc.java:322); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:182); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:179); at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:179); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:193); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.fetchSize(CloudStorageReadChannel.java:144); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.create(CloudStorageReadChannel.java:53); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newReadChannel(CloudStorageFileSystemProvider.java:258); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.newByteChannel(CloudStorageFileSystemProvider.java:222); at java.nio.file.Files.newByteChannel(Files.java:361); at java.nio.file.Files.newByteChannel(Files.java:407); at java.nio.file.Files.readAllBytes(Files.java:315",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1782:241,error,error,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782,3,"['down', 'error', 'reliab']","['download', 'error', 'reliably']"
Availability,"Now that we've updated our WDLs to 1.0, we've found that `womtool graph` no longer works. It looks like it only supports draft2 and earlier WDL. `test.wdl`:; ```; version 1.0. workflow Test { }; ```. ```; $ java -jar womtool-35.jar graph /tmp/test.wdl ; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: ERROR: Finished parsing without consuming all tokens. version 1.0; ^; ; 	at wdl.draft2.parser.WdlParser.parse(WdlParser.java:2330); 	at wdl.draft2.parser.WdlParser.parse(WdlParser.java:2335); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:266); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:160); 	at scala.util.Try$.apply(Try.scala:209); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:160); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:156); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:571); 	at womtool.graph.GraphPrint$.generateWorkflowDigraph(GraphPrint.scala:19); 	at womtool.WomtoolMain$.graph(WomtoolMain.scala:94); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:48); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:125); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:130); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:18); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:18); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```; ; The `womgraph` command still works, but the output from that command is so verbose it's unusable for viewing our workflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4234:322,ERROR,ERROR,322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234,1,['ERROR'],['ERROR']
Availability,"O OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; The [cromwell.examples.conf](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/cromwell.examples.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies the default type version.; #workflow-type-version: ""draft-2"". # To set a default hog group rather than defaulting to workflow ID:; #hogGroup: ""static""; }; }; ```; However, most are set with the equals sign:; ```; # Google configuration; google {. #application-name = ""cromwell"". # Default: just application default; #auths = [. # Application default; #{; # name = ""application-default""; # scheme = ""application_default""; #},. # Use a refresh token; #{; # name = ""user-via-refresh""; # scheme = ""refresh_token""; # client-id = ""secret_id""; # client-secret = ""secret_secret""; #},; (etc); ```. Do both of these work? If so, is one format preferred to minimize confusion?. What prompted me to write this issue is that setting some defaults isn't yet worki",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4913:1745,failure,failure-mode,1745,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913,1,['failure'],['failure-mode']
Availability,"OK thanks. So it sounds like this test is doing exactly what it was meant to do, but we have some work to do in making Cromwell resilient to this scenario.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638326392:128,resilien,resilient,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638326392,1,['resilien'],['resilient']
Availability,"OK, I won't notify you again about this release, but will get in touch when a new version is available. If you'd rather skip all updates until the next major or minor version, let me know by commenting `@dependabot ignore this major version` or `@dependabot ignore this minor version`. If you change your mind, just re-open this PR and I'll resolve any conflicts on it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172:93,avail,available,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6591#issuecomment-995908172,5,['avail'],['available']
Availability,"OK. Let's hard code a `version ""28.1""` in the formula even though it won't match the URL, since that will prevent the checksum error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316067289:127,error,error,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316067289,1,['error'],['error']
Availability,"OM)\n# https://askubuntu.com/a/823798\ntail /dev/zero"",; ""shardIndex"": -1,; ""jes"": {; ""endpointUrl"": ""https://lifesciences.googleapis.com/"",; ""machineType"": ""custom-1-2048"",; ""googleProject"": ""encode-dcc-1016"",; ""monitoringScript"": ""gs://caper-data/scripts/resource_monitor/resource_monitor.sh"",; ""executionBucket"": ""gs://encode-pipeline-test-runs/caper_out_10"",; ""zone"": ""us-central1-b"",; ""instanceName"": ""google-pipelines-worker-ead27fbad8aa73b157bfc126cd63331f""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""[0,137]"",; ""docker"": ""ubuntu:latest"",; ""maxRetries"": ""1"",; ""cpu"": ""1"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-b"",; ""memoryMin"": ""2 GB"",; ""memory"": ""2 GB""; },; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""hashes"": {; ""output count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""runtime attribute"": {; ""failOnStderr"": ""68934A3E9455FA72420237EB05902327"",; ""docker"": ""A84529F7A095541F1249576699F24AA1"",; ""continueOnReturnCode"": ""614DAABB2D7AAB5D41921614A49E4F92""; },; ""input count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""backend name"": ""50F66ECBC45488EE5826941BFBC50411"",; ""command template"": ""F41FEBA57D556A16A5F6C4EEF68ED1E0""; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache""; },; ""inputs"": {},; ""backendLabels"": {; ""wdl-task-name"": ""fail-oom"",; ""cromwell-workflow-id"": ""cromwell-87492280-9828-4afa-b53e-bec675103c42""; },; ""labels"": {; ""wdl-task-name"": ""fail_oom"",; ""cromwell-workflow-id"": ""cromwell-87492280-9828-4afa-b53e-bec675103c42""; },; ""failures"": [; {; ""causedBy"": [],; ""message"": ""The compute backend terminated the job. If this termination is unexpected, examine likely causes such as preemption, running out of disk or memory on the compute instance, or exceeding the backend's maximum job duration.""; }; ],; ""jobId"": ""projects/99884963860/locations/us-central1/operations/1374639517116411519"",; ""monitoringLo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815:4956,failure,failures,4956,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815,1,['failure'],['failures']
Availability,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:2528,echo,echo,2528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027,14,['echo'],['echo']
Availability,"OOI, what's the use-case for this? I'm not saying this code change is bad (as a code change it looks totally fine) but the description is flagging up warning signs in my head. The WDL spec claims that workflows are robust to calls being specified in **any order** since the DAG is 100% implied by inter-call dependencies rather than list-order. Eg this should be fine:. ```; workflow x {; call b { input: i = a.i }; call a; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235933879:215,robust,robust,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235933879,1,['robust'],['robust']
Availability,Objectives. confirm:. - deadlocks are not observed; - work is distributed correctly; - abandoned workflows are recovered; - abort workflow is functional,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4241:111,recover,recovered,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4241,1,['recover'],['recovered']
Availability,"Observed in a run from Firecloud: an input File was specified as a string ""gs://....."" where a stray space was carelessly inserted into the name. Three days later (don't ask) when the workflow got to its end, a JES error was thrown on failure to localize the file. I believe we should check for valid file names at the outset of a workflow, and that would include looking for an illegal space character.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2823:215,error,error,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2823,2,"['error', 'failure']","['error', 'failure']"
Availability,Occasionally run `check-alive` in config backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2315:24,alive,alive,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2315,1,['alive'],['alive']
Availability,"Oh also; ```; blah:blah:failures[123]:failure; ```; into ; ```; blah:blah:failures[123]:message; ```. And since I suspect we have collisions from aggregates, we probably want to regenerate the IDs, i.e. turning:; ```; blah:blah:failures[123]:message; ```; into:; ```; blah:blah:failures[456]:message; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236:24,failure,failures,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282845236,5,['failure'],"['failure', 'failures']"
Availability,"Oh go on, I'll ping @geoffjentry too",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282839997:15,ping,ping,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282839997,1,['ping'],['ping']
Availability,"Oh nm, it looks like #751 is the Recover ticket. Is this the preemptibility ticket?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/749#issuecomment-217903096:33,Recover,Recover,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/749#issuecomment-217903096,1,['Recover'],['Recover']
Availability,"Oh right, I forgot about this comment... sorry :sweat_smile: The issue turned out to be with the call-caching strategy we were using. Because there were a lot of files being created, cromwell needed to do a large amount of hashing, which used up all of the available CPUs eventually leading to the timeouts. We changed the call-caching strategy and are no now longer running into this error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-432255713:257,avail,available,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-432255713,2,"['avail', 'error']","['available', 'error']"
Availability,Oh yeah. IIRC the irritation I had here was that it'd not give me all the errors at once so i'd think i was done only to get hit by yet another block :). I mentioned the unused vars in slack. There are some *definite* unused ones in there. Most of them were due to our (over?)reliance on overloaded function signatures where we may or may not use all of the args. Some of them were due to our pattern (particularly in the backend) of supplying a default no-op implementation of things. . There were a handful though that were really bizarre. As in I couldn't figure out why the compiler thought it was unused.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2433#issuecomment-314243598:74,error,errors,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2433#issuecomment-314243598,1,['error'],['errors']
Availability,"Oh, apart from the old failure/timestamp that doesn't seem to be used anywhere any more",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282843011:23,failure,failure,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282843011,1,['failure'],['failure']
Availability,"Oh, looks good then. I've restarted the Travis build but assuming the failures were just temporary, :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164782659:70,failure,failures,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/325#issuecomment-164782659,1,['failure'],['failures']
Availability,"Ok - I have made it work via Cromwell using a specific version of deFuse / gmap / the reference dataset.; The reference dataset is built after downloading deFuse and gmap and I think it uses those tools to build the reference indexes. Here is what I've found:. ## Works locally and on Cromwell:; - deFuse 0.6.2; - gmap 2014-08-20; - Reference set ensembl v 75 (built using deFuse and gmap of the above versions). ## Works locally, fails on Cromwell:; - deFuse 0.8.1; - gmap 2018-07-04; - Reference set ensembl v 69 (built using deFuse and gmap of the above versions). ## Works locally, fails on Cromwell:; - deFuse 0.6.2; - gmap 2018-07-04; - Reference set ensembl v 69 (built using deFuse 0.8.1 and gmap 2018-07-04). ## Fails locally and on Cromwell:; - deFuse 0.6.2; - gmap 2018-07-04; - Reference set ensembl v 75 (built using deFuse 0.6.2 and gmap 2014-08-20). It seems like only when using an old version of deFuse, old gmap, and a newer reference set will deFuse run successfully via Cromwell. I am still in the dark about why some of these combinations succeed locally and don't work on Cromwell, but it looks like the heart of the problem is in newer versions of gmap.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-447169423:143,down,downloading,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465#issuecomment-447169423,1,['down'],['downloading']
Availability,"Ok, I got singularity working, although I'm new to cromwell so please let me know if there's a better way!. hello.wdl:; ```; task hello {; command {; echo 'Hello world!'; }. runtime {; image: ""~/test.sif""; }. output {; File response = stdout(); }; }. workflow test {; call hello; }; ```. local.conf:; ```; include required(classpath(""application"")); backend {; default = singularity; providers {; singularity {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; job-shell=""/bin/sh""; run-in-background = true ; runtime-attributes = """"""; String? image; """"""; submit = """"""; singularity exec ${image} ${job_shell} ${script}; """"""; }; }; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-580114191:150,echo,echo,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-580114191,1,['echo'],['echo']
Availability,"Ok, so validation fails because I had a typo in my input variable name, fine. But TELL ME WHICH VARIABLE IT WAS!!!. The rest of this description is an example:. ```; workflow foo {; 	call bar; 	# Oops, bar didn't have an output called b:; 	call baz as bad_output_name { input: b = bar.b }; 	# Oops, baz doesn't have an input called a:; 	call baz as bad_input_name { input: a = bar.a }; }. task bar {; 	command {; 		# noop; 	}; 	output {; 		String a = ""a""; 	}; 	runtime {; 		docker: ""ubuntu:latest""; 	}; }. task baz {; 	String b; 	command {; 		# noop; 	}; 	runtime {; 		docker: ""ubuntu:latest""; 	}; }; ```. The messages I actually get:; ```; Unable to load namespace from workflow: ERROR: Expression references input on call that doesn't exist (line 4, col 47):. 	call baz as bad_output_name { input: b = bar.b }; ^; Unable to load namespace from workflow: ERROR: Call references an input on task 'baz' that doesn't exist (line 6, col 38). 	call baz as bad_input_name { input: a = bar.a }; ^; ```. The message I want:; ```; Unable to load namespace from workflow: ERROR: Cannot use 'bar.b' as an input. That variable was never created. (line 4, col 47):. 	call baz as bad_output_name { input: b = bar.b }; ^; Unable to load namespace from workflow: ERROR: Call supplies an input 'a' that isn't declared in the 'baz' task (line 6, col 38). 	call baz as bad_input_name { input: a = bar.a }; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2211:681,ERROR,ERROR,681,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211,4,['ERROR'],['ERROR']
Availability,"Okay preliminary testing. On a workflow that creates multiple jobs at once I get these errors: `Failure writing to call cache: [SQLITE_BUSY] the database file is locked (database is locked)`. This has to do probably with SQLITE not supporting multiple threads or something similar. It is probably reproducible by creating a mock scatter workflow that spawns a 1000 jobs at once. To reproduce; ```WDL; version 1.0. workflow thousand_scatters {; input {}; scatter (i in range(200)) {; call hello_world {input: hello=i}; }; output {; Array[String] hellos = hello_world.out; }; }. task hello_world {; input {; String hello = ""world"" ; }; command {; echo ""Hello ~{hello}!""; }; output {; String out = stdout(); }; runtime {; docker: ""quay.io/biocontainers/samtools:1.11--h6270b1f_0""; }; }; ```. With config; ```hocon; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text""; numThreads=1; }; }; call-caching {; enabled=true; }; ```. I already tried if running only one akka thread would solbe it with:; ```hocon; akka {; actor.default-dispatcher.fork-join-executor {; parallelism-max = 1; }; }. ```; But this had no effect. I still get:; ```[ERROR] [11/27/2020 13:47:06.907] [cromwell-system-akka.dispatchers.engine-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/WorkflowExecutionActor-9f19cf8b-7b86-4c9d-9c90-0aa23817636c/9f19cf8b-7b86-4c9d-9c90-0aa23817636c-EngineJobExecutionActor-thousand_scatters.hello_world:85:1] 9f19cf8b:thousand_scatters.hello_world:85:1: Failure writing to call cache: [SQLITE_BUSY] The database file is locked (database is locked)```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140:87,error,errors,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734786140,5,"['ERROR', 'Failure', 'echo', 'error']","['ERROR', 'Failure', 'echo', 'errors']"
Availability,"Okay, I've made more progress. But more issues are popping up. @cjllanwarne . You cannot ask for the filesize of an sra file to configure the disk space you'd like at runtime thats what causes the ; `[2020-08-24 15:28:47,48] [error] 'nioPath' not implemented for SraPath` issue mentioned above. When I remove that line from my wdl things get better, but I'm running into two new separate issues. 1. Cromwell tries to chmod the mounted sra directory which is not allowed.; code:; https://github.com/broadinstitute/cromwell/blob/5c8f932b6e1a5706286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:226,error,error,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,3,['error'],['error']
Availability,"Okay, I've make a very minimal test case that fails on AWS: https://github.com/TMiguelT/WdlFileNameTooLong. Hopefully it's faster than the centaur one so this ends up being useful. For me I get a failure in around 12 seconds, which is a good turnaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4279#issuecomment-474714816:196,failure,failure,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4279#issuecomment-474714816,1,['failure'],['failure']
Availability,"Okay. So initially, I was passing the `ServiceRegistryActor` reference via props down the chain of actors, and each of the actors needed to follow a pattern of steps to insert into the metadata (e.g. create a `MetadataEvent` with a `MetadataPutAction` within which the `Metadata[Key, Value]` resided, send this message over to the the service registry, and handle failures, if any. It did not look good by any stretch IMO. All the metadata generating entities needed to be aware of the `MetadataService` (the erstwhile dataAcess). **Edit:** Using the above design in-fact now. ~~Currently, what I've done below is create a single instance of the `ServiceRegistryActor` in the `WorkflowManagerActor`and then create a `WorkflowProfilerActor` (one per workflow) which is supposed to handle all the metadata information coming from the engine side for a particular workflow. The way it happens is based on the presumption that almost all the information that we needed was present in the `StateName` and `StateData` of our FSMs. Unfortunately, with Akka's `SubscribeTransitionalCallback` we can only monitor the FSM states, and not the data. So I've created a trait (which the Engine's FSMs can extend from) which provide the semantics of wrapping up the state and data of the FSM in a message, and publish it into Akka's event stream. The ProfilerActor is the listener of these events and handles them appropriately. With this, I was able to make the FSMs unaware of the MetadataServices, and simply publish it's state and data in the event stream while performing any transitions.~~. ~~Let me know if you guys have any (other?) ideas / suggestions.~~. Contents added to metadata with this PR:; - [x] workflowName; - [ ] calls (To come from the backends); - [x] outputs ; - [x] id; - [x] inputs; - [x] submission; - [x] status; - [x] end; - [x] start",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829:81,down,down,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829,2,"['down', 'failure']","['down', 'failures']"
Availability,"Okay. So initially, I was passing the `ServiceRegistryActor` reference via props down the chain of actors, and each of the actors needed to follow a pattern of steps to insert into the metadata (e.g. create a `MetadataEvent` with a `MetadataPutAction` within which the `Metadata[Key, Value]` resided, send this message over to the the service registry, and handle failures, if any. It did not look good by any stretch IMO. All the metadata generating entities needed to be aware of the `MetadataService` (the erstwhile dataAcess). Currently, what I've done below is create a single instance of the `ServiceRegistryActor` in the `WorkflowManagerActor`and then create a `WorkflowProfilerActor` (one per workflow) which is supposed to handle all the metadata information coming from the engine side for a particular workflow. The way it happens is based on the presumption that almost all the information that we needed was present in the `StateName` and `StateData` of our FSMs. Unfortunately, with Akka's `SubscribeTransitionalCallback` we can only monitor the FSM states, and not the data. So I've created a trait (which the Engine's FSMs can extend from) which provide the semantics of wrapping up the state and data of the FSM in a message, and publish it into Akka's event stream. The ProfilerActor is the listener of these events and handles them appropriately. With this, I was able to make the FSMs unaware of the MetadataServices, and simply publish it's state and data in the event stream while performing any transitions. . Let me know if you guys have any (other?) ideas / suggestions. Edit: Names of new actors might be pretty bad IMO. Please suggest better ones if you have any. _P.S. : Currently based out of the Chris's branch since his metadata changes were needed._; _P.P.S. : Still a WIP for improving some stuff._",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/822:81,down,down,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/822,2,"['down', 'failure']","['down', 'failures']"
Availability,"On Cromwell version, ""34-5156b78-SNAP"", when making a request to the query endpoint that includes the `""includeSubworkflows"": ""false""` parameter, the database will throw an error if the query matches ~1000 results. . Example POST request:; ```; curl -X POST ""https://cromwell.caas-dev.broadinstitute.org/api/workflows/v1/query"" -H ""accept: application/json"" -H ""authorization: Bearer XXXXX"" -H ""Content-Type: application/json"" -d ""[ { \""status\"": \""Failed\"", \""includeSubworkflows\"": \""false\"" }]""; ```; Error message:; ```; {; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@27386688 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@2e52a800[Running, pool size = 200, active threads = 200, queued tasks = 999, completed tasks = 16375]""; }; ```; This error is very similar to the other issue: https://github.com/broadinstitute/cromwell/issues/3115; but `includeSubworkflows` shouldn't limit the number of results the `/query` endpoint can handle.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3873:173,error,error,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3873,3,"['Error', 'error']","['Error', 'error']"
Availability,"On Cromwells which are not our own production instance, people seem to be more frustrated than delighted by having the initial read limits be arbitrarily restrictive. - [x] Will probably need to update `firecloud-develop` to set some values which are still relying on the previous low defaults back down to 128k",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5034:299,down,down,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5034,1,['down'],['down']
Availability,"On Wed, Jun 13, 2018 at 4:08 PM Dan Billings <danb@broadinstitute.org> wrote:; I think we can patch it on our end, doesn't sound too bad. OK, thanks and sorry again.; ; On Wed, Jun 13, 2018 at 3:54 PM Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Ok good to know, doesn't sound too hard to patch but we'd also have to patch the previous release if we don't want it to be broken there too. I'm adding Dan and Ruchi who can decide on what to do. Thibault. On Wed, Jun 13, 2018 at 3:43 PM Aaron Kemp <kemp@google.com> wrote:; On Wed, Jun 13, 2018 at 3:38 PM, Thibault Jeandet <tjeandet@broadinstitute.org> wrote:; Thank you,. I just saw a bunch of those today:. PAPI error code 3. Execution failed: creating instance: inserting instance: Invalid value for field 'resource.disks[1].initializeParams.diskType': 'zones/us-central1-b/diskTypes/PERSISTENT_SSD'. The referenced diskType resource cannot be found. I don't think I've touched anything related to disk types. Is this a quota issue ?. No this was a change on our side. There was a bug in the v2alph1 backend where it was ignoring the disk type requested. I fixed it and pushed it out. The diskType in v2alpha1 is the standard GCE names (so in this case, pd-ssd) - in v1 they were a custom enumeration. This will probably require a cromwell change depending on where that value is coming from (eg, maybe it's in the WDL file itself?); I think we're translating it from WDL-speak. Where can I find the ""standard GCE names"" (e.g. ""pd-ssd""). ""gcloud compute disk-types list"" shows the full per-zone list, but in practice it's really just one of ""pd-ssd, pd-standard or local-ssd"".; ; and would it become ""zones/us-central1-b/diskTypes/pd-ssd"" or just ""pd-ssd"" ?. Pipelines adds the zonal resource chunk based on the zone we choose from the set you give us. So you just say 'pd-ssd' and we staple on the appropriate zone to the front. Aaron",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3776:673,error,error,673,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3776,1,['error'],['error']
Availability,"On behalf of @leepc12, opening this as a new issue (previously a comment in #2992). Another if-scatter bug.; i built a new `cromwell-31-d716fd2-SNAP.jar` from your `develop` branch.; ```wdl; workflow test {; 	Boolean b0 = true; 	Boolean b1 = true; 	Boolean b2 = true; 	scatter( i in range(3) ) {; 		if ( b0 ) {; 			call t0 as t1 { input: i=i }; 		}; 	}; 	if ( b1 ) {; 		scatter( i in range(3) ) {; 			call t0 as t2 { input: i=t1.out[i] }; 		}; 	}; 	if ( b1 && b2 ) {; 		scatter( i in range(3) ) {; 			call t0 as t3 { input: i=t2.out[i] }; 		}; 	}; }. task t0 {; 	Int? i; 	command {; 		echo ${i}; 	}; 	output {; 		Int out = read_int(stdout()); 	}; }; ```; error log; ```; $ java -jar /users/leepc12/code/cromwell/./target/scala-2.12/cromwell-31-d716fd2-SNAP.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 20:11:15,13] [info] Running with database db.url = jdbc:hsqldb:mem:7e58cfd2-b9b6-47f9-bda1-6fe045e7a665;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:21,83] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 20:11:21,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 20:11:22,02] [info] Running with database db.url = jdbc:hsqldb:mem:e02f9206-cb15-468a-929a-82676a83a9b8;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:22,47] [info] Slf4jLogger started; [2017-12-05 20:11:22,67] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 20:11:22,68] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3007:585,echo,echo,585,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007,2,"['echo', 'error']","['echo', 'error']"
Availability,"On cromwell version 30.1, when making a request to the query endpoint that includes an `additionalQueryResultFields` parameter, the database will throw an error if the query matches ~1000 results. This seems to be because cromwell is querying the db to get that field's value for each workflow in the result set. Example GET request:; ""https://cromwell.mint-dev.broadinstitute.org/api/workflows/v1/query?additionalQueryResultFields=parentWorkflowId"". Error message: ; ```{; ""status"": ""fail"",; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@349b84c3 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@37b87f04[Running, pool size = 200, active threads = 200, queued tasks = 1000, completed tasks = 3928972]""; }; ```. This does not occur when paginating the results (as long as the page size is not 1000+)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3115:155,error,error,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3115,2,"['Error', 'error']","['Error', 'error']"
Availability,"On my OS; ```; java -version; openjdk version ""11.0.8"" 2020-07-14; OpenJDK Runtime Environment (build 11.0.8+10-post-Debian-1deb10u1); OpenJDK 64-Bit Server VM (build 11.0.8+10-post-Debian-1deb10u1, mixed mode, sharing); ```; Using conda-forge compiled java (default for conda package of cromwell); ```; java -version; openjdk version ""1.8.0_192""; OpenJDK Runtime Environment (Zulu 8.33.0.1-linux64) (build 1.8.0_192-b01); OpenJDK 64-Bit Server VM (Zulu 8.33.0.1-linux64) (build 25.192-b01, mixed mode); ```. Both give the same error. Maybe it is Oracle vs OpenJDK related errors?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-684392711:528,error,error,528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5808#issuecomment-684392711,2,['error'],"['error', 'errors']"
Availability,"On the servers where this will be used, . > Did you install the Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files?; > http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/377#issuecomment-171666835:193,down,downloads,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/377#issuecomment-171666835,2,['down'],"['download-', 'downloads']"
Availability,"Once #1909 and #1925 are complete, populate the set of retryable JES non-preemption errors with the codes from #1909.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1926:84,error,errors,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1926,1,['error'],['errors']
Availability,"Once I implemented a cache locally the workflow runs to Succeeded, but then the Centaur test fails because it's configured to expect workflowfailure. Judging by the fact that this test was part of [Tyburn](https://github.com/broadinstitute/tyburn/pull/27/files) which I believe lacked support for asserting failures, I don't think this is the correct expectation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224117892:307,failure,failures,307,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224117892,1,['failure'],['failures']
Availability,"Once we decide on a style guide, go through the code and resolve all the issues / errors cropping up because of the check style.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/537:82,error,errors,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/537,1,['error'],['errors']
Availability,"One downside of `grep` approach - result of `grep` is collapsed by default, so one would need to expand it to see actual errors. I think this is tolerable.; ![image](https://user-images.githubusercontent.com/4853242/85418371-b5263000-b53e-11ea-95f3-7bff39b08cff.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5553:4,down,downside,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5553,3,"['down', 'error', 'toler']","['downside', 'errors', 'tolerable']"
Availability,One key question is what should happen if the file is too large? Just silently continue? Error? Provide some form of feedback to the user? Emitting to the cromwell server log seems useless for most of our user personas.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294300861:89,Error,Error,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-294300861,1,['Error'],['Error']
Availability,"One of Morgan's input files was missing an md5 in its object metadata. Cromwell was dutifully falling back to our backup option, which is to read every byte of the file into memory and calculate the hash itself. This resulted in extraordinary network and CPU usage that destabilized the instance and caused a continual crash/reboot cycle. We think this is also what Lori ran into with the featured workspaces. Now, we detect & avoid this condition, print a warning, and carry on without call caching:; ```; 41183c60:ImputationBeagle.SubsetVcfToRegion:3:1:; Hash error ([Attempted 1 time(s)] - Exception:; File of type BlobPath requires hash in object metadata, not present for; https://lz8b0d07a4d28c13150a1a12.blob.core.windows.net/sc-94fd136b-4231-4e80-ab0c-76d8a2811066/hg38/inputs/palantir_merged_input_samples.liftedover.vcf.gz),; disabling call caching for this job.; ```. Obviously, we'd like to enhance this in the future so that call caching is still possible for these jobs, but we have to walk before we can run. ---. Visualization eye candy section!. Swiftly downloading a file on the datacenter multi-gigabit LAN:. ![Screenshot 2024-05-02 at 19 24 04](https://github.com/broadinstitute/cromwell/assets/1087943/46484bbd-30e0-4f88-8f6c-05b50649c557). Telltale CPU curve as we chew through one file after another:. ![Screenshot 2024-05-03 at 11 32 13](https://github.com/broadinstitute/cromwell/assets/1087943/7916ce63-8d4c-46f7-a86a-b3313edf0d77). Flame graph showing the smoking gun, `generateMd5FileHashForPath`:. ![Screenshot 2024-05-02 at 14 02 25](https://github.com/broadinstitute/cromwell/assets/1087943/0d06f3ad-8155-4b43-bef7-6d9ccce35132)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7419:325,reboot,reboot,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7419,3,"['down', 'error', 'reboot']","['downloading', 'error', 'reboot']"
Availability,One quick thing that could be done to see if the NPE is related to other undesired behavior. In JesApiQueryManager.handleTerminated I put in some logic a while back to automatically retry any polling actor which failed under the belief that it was going to be some goofy google error. That'd also be picking up actors which died via NPE. Changing it we're either specially handling NPE or being specific about what *is* being handled (which should be part of #1914 anyways) would make this better,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1965#issuecomment-279012967:278,error,error,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1965#issuecomment-279012967,1,['error'],['error']
Availability,"One representative pair of timings on a stone cold prod clone (restarted between queries) for a single submission ID label with two ORed collection labels:. develop: 31.20 sec; this branch: 0.08 sec. No index changes required. MySQL will use a k/v index on `CUSTOM_LABEL_ENTRY` if it's available but its presence doesn't seem to actually make much difference in execution time. Exclude labels have been folded into the new system, I still need to measure the performance impact of those changes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4610:286,avail,available,286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4610,1,['avail'],['available']
Availability,"One thing I'd like to see before trying stuff is to have a setup that can reliably reproduce the problem and allows us to at least sorta kinda measure how effective our stopgaps & solution attempts are. . For instance, is it possible that cranking the queue size causes something else to fall over? It'd be good to know that before diving in. (Probably not, but just trying to make the point)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2381#issuecomment-311532510:74,reliab,reliably,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2381#issuecomment-311532510,1,['reliab'],['reliably']
Availability,One up. I have similar error,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-517178996:23,error,error,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-517178996,1,['error'],['error']
Availability,Ooooh none of us could remember the exact reason but I think you’re right. And iirc there was some performance problem which was eventually tracked down to be something silly we hadn’t accounted for. This makes sense,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2948#issuecomment-347748418:148,down,down,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2948#issuecomment-347748418,1,['down'],['down']
Availability,Oops. Raised in error. This actually does work,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1988#issuecomment-280177392:16,error,error,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1988#issuecomment-280177392,1,['error'],['error']
Availability,Option to skip (but don't invalidate) call cache hits that fail for permission errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1587:79,error,errors,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1587,1,['error'],['errors']
Availability,"Or maybe provide a flag to switch the default?. I get this error when running the hello world example from docs on nixos:. ```; cromwell.core.CromwellFatalException: java.io.IOException: Cannot run program ""/bin/bash"": error=2, No such file or directory; ```; Is it just to change this line?. https://github.com/broadinstitute/cromwell/blob/1b1a56372659b9cb7a168bb1fa2a2296103e1256/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/BackgroundAsyncJobExecutionActor.scala#L22. I could possibly just make my own version building from source then .. It is due to the way nixos is built. A similar case is referenced here :. https://github.com/RcppCore/RcppArmadillo/issues/15",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3201:59,error,error,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3201,2,['error'],['error']
Availability,"Or, now that I think I understand the original scheme a little better:. - The job enters the `Running` state; - We immediately schedule an `CheckAlive` message to the actor at the configurable time; - If the cadence is not set, we never send that message (this would be the default); - When that CheckAlive arrives we can run `isAlive` and remember the result (and if we're still alive, schedule another `CheckAlive` again after the same delay); - To decide the new status:; - If we see a return code file, we use it; - Otherwise if `isAlive` is false and we've waited too long (could be configurable but I think we can set a sensible default) then we fail. I think that way the behavior is identical to today by default, but if we schedule `isAlive`s, then they behave as your scheme would imply. Do you think that would work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423571735:380,alive,alive,380,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423571735,1,['alive'],['alive']
Availability,"Order of events:. Start with #3344 and #3342 as those are the requirements for having multiple writer-Cromwell nodes share a database. Next, start #4239 to re-create a deadlocking issue, and address it with a solution:; #4249; #4240 . Generate test cases to make sure Cromwell is able to recover appropriately in case of shutdown:; #4242 ; And test cases to ensure that Cromwell is running/aborting workflows as expected across multiple nodes:; #4241",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4369:288,recover,recover,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4369,1,['recover'],['recover']
Availability,"Order of events:. Start with #3344 and #3342 as those are the requirements for having multiple writer-Cromwell nodes to safely share a database. Next, start #4239 de-serialize workflow heartbeats, and build a test case #4414 to re-create the deadlocking issue we've seen before in production. Follow up that work with a solution that addresses the deadlock. Below are two ideas brainstormed in the past:; #4249; #4240. Generate test cases to make sure Cromwell is able to recover appropriately in case of shutdown:; #4242. And test cases to ensure that Cromwell is running/aborting workflows as expected across multiple nodes:; #4241 . GDoc of plan as of March '19; https://docs.google.com/document/d/10AGE3foZsKOHlgUpq3BE4mkUYphcjYyxMt0miQz4FGk/edit?usp=sharing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4370:185,heartbeat,heartbeats,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4370,2,"['heartbeat', 'recover']","['heartbeats', 'recover']"
Availability,Order of heartbeats database updates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4240:9,heartbeat,heartbeats,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4240,1,['heartbeat'],['heartbeats']
Availability,Original report here: https://gatkforums.broadinstitute.org/firecloud/discussion/10273/error-messages-should-include-the-problematic-input-whenever-possible-disk-strings. AC: ; The failure message returned today looks like `Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE'`. but it should be ; `Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE'. Found 'foo'`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4161:87,error,error-messages-should-include-the-problematic-input-whenever-possible-disk-strings,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4161,2,"['error', 'failure']","['error-messages-should-include-the-problematic-input-whenever-possible-disk-strings', 'failure']"
Availability,"Original report here: https://gatkforums.broadinstitute.org/firecloud/discussion/10319/incorrect-error-message-when-task-fails-to-complete. Today, with PAPI v1 & v2, when a command fails and doesn't produce all expected output files, the failure reported is ""File `x` failed to delocalize..` while the real underlying cause is something about the command that failed and led to a failing return code. AC: ; While this isn't possible with PAPI v1, there may be scope in PAPI v2 to switch the order of checks that occur when a job hits a terminal status to determine success/failure:; 1. first check error code of a command then try to delocalize files. Reason for error should be ""Invalid Error Code"" first, possibly ""Missing File"" second.; 2. show the error code in the failure message, along with links to the stdout/stderr as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4160:97,error,error-message-when-task-fails-to-complete,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4160,8,"['Error', 'error', 'failure']","['Error', 'error', 'error-message-when-task-fails-to-complete', 'failure']"
Availability,"Original report:; https://gatkforums.broadinstitute.org/firecloud/discussion/10434/weird-error-message-encountered-www-googleapis-com. ![screen shot 2018-09-27 at 2 10 26 am](https://user-images.githubusercontent.com/14941133/46126308-bbb53580-c1fa-11e8-9001-2a662b9374dd.png). Here is an example of a workflow that fails with not being able to access www.googleapis.com. Its not implicitly obvious to a user what the consequences of this are and whether it makes sense to re-run their workflow. . In the case above -- it seems like the job that failed with this error had an rc of 0 and the expected outputs were present -- so likely the output evaluation failed, or some other file operation never completed as this is a StorageException. AC: Supplement the existing top-level message with something like ```Please consult this https://status.cloud.google.com/ to ensure that there are no reported outages with the Google Cloud Platform. Otherwise, this is likely a transient error and the workflow should be re-run.```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4159:89,error,error-message-encountered-www-googleapis-com,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4159,4,"['error', 'outage']","['error', 'error-message-encountered-www-googleapis-com', 'outages']"
Availability,"Originally posted this two in the JIRA issue tracker back in August. Reposting here since it didn't get a response over there: https://broadworkbench.atlassian.net/browse/BA-6548. > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl script which runs several subjobs in parallel. I believe the correct parlance is a scatter. I noticed that in some of the jobs of the scatter, some reference files failed to download from S3 even though they existed (Connection Reset by Peer). This failure caused the overall job to fail after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but recently, in June 2020, it appears the AWS Batch backend was majorly overhauled (by @markjschreiber, thanks! Also, tagging you because I suspect you might be the resident expert here :) ), and the previous fix (using the ecs proxy image) was supposedly obsoleted.; > ; > I also see that the s3fs library appears to be vendored into cromwell, and after digging around, it appears that one might be able to set retries via an environment variable(?). But even then, I feel like if that were to work, it would be much nicer if it was configurable through cromwell's config file somehow.; >; > So that brings me to my final question. Is there some configuration that allows me to retry failed downloads some number of times before failing the whole job? Or, perhap there is some alternative configuraiton which I've overlooked and someone could point me to it? Thanks!. In addition, just wondering if perhaps there is a service limit I might be running into?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946:449,down,download,449,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946,3,"['down', 'failure']","['download', 'downloads', 'failure']"
Availability,"Originally reported here:; https://gatkforums.broadinstitute.org/gatk/discussion/comment/54964#Comment_54964. Example workflow as the acceptance criteria:; ```; version 1.0; workflow example{; call print{; }; }. task print{; input{; String s = ""\""""; }; command{; echo ""${s}""; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4500:263,echo,echo,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4500,1,['echo'],['echo']
Availability,Otherwise we get errors when a subworkflow presents no outputs: ; - [x] `sub_workflow_var_refs`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2937:17,error,errors,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2937,1,['error'],['errors']
Availability,"Out of curiosity, did you observe a behavior that makes you think that tasks are not being run as early as they could be ?; Cromwell periodically traverses the ""unstarted"" nodes to determine if all their dependencies are satisfied, and if so starts the task, which should effectively run all tasks as soon as possible. *There is an exception to this for sub-workflows, tasks depending on a sub workflow output will only be run once the whole sub-workflow completes, even if this specific output is available before that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2736#issuecomment-336140019:498,avail,available,498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2736#issuecomment-336140019,1,['avail'],['available']
Availability,Out of memory error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5347:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347,1,['error'],['error']
Availability,Outage April 11-12 Post Mortem,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3511:0,Outage,Outage,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3511,1,['Outage'],['Outage']
Availability,"Overall I'm liking the `Validation` angle to these changes, this seems like a nice system which could be used in other spots in Cromwell. I think the attribute parsing could be made more tolerant so that Kristian's examples of `8G` and `8GB` actually would parse, but that's orthogonal to getting helpful error messages when something is unparseable. I'm happy to address more tolerant parsing in a separate PR. Also, it feels like the case classes might have been overused in these changes; they aren't replacing type aliases but are wrapping what used to be raw types. This does buy some added type safety in . ``` scala; (failOnStderr |@| cpu |@| preemptible |@| disks |@| memory){ RuntimeAttributes(docker, zones, _, _, _, _, _) }; ```. But then everywhere else there's noise for boxing and unboxing the raw types to and from these case classes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/163#issuecomment-135972385:187,toler,tolerant,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/163#issuecomment-135972385,3,"['error', 'toler']","['error', 'tolerant']"
Availability,"P/blob/bioproject_stuff/workflows/is_this_tuberculosis.wdl. ### Ruled out; * Running tasks concurrently/Cromwell config not being respected: The workflow would have either hung Docker or tasks would have returned 137; * Docker application (not the container, the entire application) hanging, like what happens when trying to run tasks concurrently on a local machine: `docker run -it` works in a new terminal window; * IP getting blocked: This would cause error output, and I can still ping SRA from the same IP without issue; * Loss of internet: This would cause error output (`curl command failed`); * Control-S: Control-Q doesn't unfreeze it; * No more disk space: There's about 50 GB free and each instance of the scattered task uses less than a GB. ### Docker container logs; `docker logs cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528` gives no output. ### Entering the container; `docker exec -it cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528 /bin/sh` returns; `Error response from daemon: Container cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528 is not running`. ### Docker inspect; ```; >docker inspect cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528; [; {; ""Id"": ""cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528"",; ""Created"": ""2022-11-09T05:37:16.872195116Z"",; ""Path"": ""/bin/bash"",; ""Args"": [; ""/bark-bark/IS_THIS_TUBERCULOSIS/7570b5dc-4714-48a7-96b2-9c62245e3618/call-get_organism_names/shard-885/execution/script""; ],; ""State"": {; ""Status"": ""created"",; ""Running"": false,; ""Paused"": false,; ""Restarting"": false,; ""OOMKilled"": false,; ""Dead"": false,; ""Pid"": 0,; ""ExitCode"": 0,; ""Error"": """",; ""StartedAt"": ""0001-01-01T00:00:00Z"",; ""FinishedAt"": ""0001-01-01T00:00:00Z""; },; ""Image"": ""sha256:1c9072d6415cff6481014f64cf7486482dc61620bf09806bafffb1697bf344b1"",; ""ResolvConfPath"": """",; ""HostnamePath"": """",; ""HostsPath"": """",; ""LogPath"": ""/var/lib/docker/containers/cf6f4828adc61eacf06337ce3caf2c110df6cc049375",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6946:1801,Error,Error,1801,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6946,1,['Error'],['Error']
Availability,"P2 - to optimize the case of restarting a single cromwell, upon shutdown the heartbeat related information (server, timestamp) should be deleted so the workflow is immediately picked up on restart rather than after the timeout period",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4242:77,heartbeat,heartbeat,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4242,1,['heartbeat'],['heartbeat']
Availability,PAPI better error message when localization/delocalization fails,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4718:12,error,error,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4718,1,['error'],['error']
Availability,PAPI error code 14,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6306:5,error,error,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6306,1,['error'],['error']
Availability,"PAPI error code 2. Execution failed: pulling image: docker login: generic::unknown: retry budget exhausted (10 attempts): running docker login: exit status 1 (standard error: ""WARNING! Using --password via the CLI is insecure. Use --password-stdin.\nError response from daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers). Seen [here](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-cron-papiv2/170)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4438:5,error,error,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4438,2,['error'],['error']
Availability,PAPI v2 Error logging again,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4445:8,Error,Error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4445,1,['Error'],['Error']
Availability,PAPI v2 private docker failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4553:23,failure,failures,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553,1,['failure'],['failures']
Availability,PAPI v2 vague error message for localization failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4603:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4603,2,"['error', 'failure']","['error', 'failure']"
Availability,PAPI2 error reporting,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3722:6,error,error,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3722,1,['error'],['error']
Availability,"PR 1 of 3:; 1. Remove cromwell-core dependency from cloud-support; 2. Run jes centaur on travis; 3. Generate coverage for integration tests. ---. Instead of credentials requiring WorkflowOptions, any String => String will do, including Map[String, String].; Retrieving credentials only requires actorSystem/executionContext when retrying.; Moved logback dependencies from common library over to testing.; Added mockito to all artifact tests.; Fixed akka-stream-testkit dependency appearing in core's main instead of test.; Split confusingly named baseDependencies into configDependencies ++ catsDependencies.; Other dependency cleanup to reduce duplicates and extra transitive dependencies.; Log stderr from centaur'ed cromwell failures.; The total attempt time to connect to cromwell for a test is now longer than the timeout of a cromwell restart.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2938:728,failure,failures,728,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2938,1,['failure'],['failures']
Availability,"Pair accessing was never added properly to the docs.; When a pair is accessed incorrectly, it'd be nice to have an error along the lines of `use pair.left and pair.right to access Pair members`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2300:115,error,error,115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2300,1,['error'],['error']
Availability,Papi V2: Transient(?) error '[Errno 111] Connection refused.',MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,1,['error'],['error']
Availability,Papi V2: Transient(?) error 'repository ... not found',MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861,1,['error'],['error']
Availability,Papi v2 disk error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3776:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3776,1,['error'],['error']
Availability,Parse Error when using variables prepended by if*,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6956:6,Error,Error,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6956,1,['Error'],['Error']
Availability,"Part 2 of implementing the ""Remove Object"" [PR in OpenWDL](https://github.com/openwdl/wdl/pull/228). Allow us to treat calls as structs of their outputs, for purposes of evaluating downstream expressions, eg:; ```wdl; workflow blah { ; call foo; FooOutputStruct foo_outputs = foo; }. task foo {; # ... output {; Int i = ...; String s = ...; }; }. struct FooOutputStruct {; Int i; String s; }; ```. TODOs:; - [x] Simple use case (eg the example above); - [x] Within `scatter`s and `if`s; - [x] Don't disrupt the `after` logic; - [ ] Make sure WDL draft-2 and 1.0 still can't do this",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4535:181,down,downstream,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4535,1,['down'],['downstream']
Availability,"Part 2 of implementing the ""Remove Object"" [PR in OpenWDL](https://github.com/openwdl/wdl/pull/228). Allows us to treat calls as structs of their outputs, for purposes of evaluating downstream expressions, eg:; ```wdl; workflow blah { ; call foo; FooOutputStruct foo_outputs = foo; }. task foo {; # ... output {; Int i = ...; String s = ...; }; }. struct FooOutputStruct {; Int i; String s; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4534:182,down,downstream,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4534,1,['down'],['downstream']
Availability,"Part of the #2942 work involved converting the config backend to use the WOM API. The config backend uses WDL to define its commands but it does not have a workflow (it's a WdlNamespace with a bunch of tasks). This conversion mostly went smoothly with the exception of uninitialized task optionals like `docker_user` found in the Local backend's `submit-docker`:. ```; runtime-attributes = """"""; String? docker; String? docker_user; """"""; submit = ""/bin/bash ${script}""; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; --rm -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """"""; ```; Evaluating that `${""--user "" + docker_user}` expression currently blows up with no useful diagnostics in the absence of an explicit `docker_user` input. To hack around this I changed the config backend to force in `none` inputs for all optional declarations in a task, but this would have the effect of clobbering any initialized optionals:. ```; String? docker_user = ""mobydock""; ```. With the #2942 changes `docker_user` would be forced to `none` and `""mobydock""` would be lost (at sea). It's not clear why this is happening when using the WOM API at a task level and not at the workflow level. There may be some `none`-initialization done at the workflow level that should get pushed down to tasks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2946:1343,down,down,1343,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2946,1,['down'],['down']
Availability,"PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:514); at akka.actor.Actor.aroundReceive$(Actor.scala:512); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right plac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927:5177,Error,Error,5177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927,1,['Error'],['Error']
Availability,"Parting thoughts:. * The detritus portion of this was not handled at all so conformance test 87 is being skipped for the time being. Conformance 87 runs a `find` in the execution directory which will not tolerate detritus and defeats our current detritus filtering hack.; * It could also turn out the prepopulated-`listing` fix for the IWDR is too narrow and real dynamicism may be required for other non-`listing` IWDR operations.; * Somewhat aligned with the above, the `listing` runs before inputs have been localized so it is pointed to a filesystem location relative to where the Cromwell server is currently running rather than the call's inputs directory. Input localization should possibly happen before we try to do anything related to IWDR listing determination, but that's not the order of operations now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3575#issuecomment-389168565:204,toler,tolerate,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3575#issuecomment-389168565,1,['toler'],['tolerate']
Availability,Pass through Cromwell & Sam HTTP errors to Cromiam.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4624:33,error,errors,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4624,1,['error'],['errors']
Availability,"Per @jmthibault79 -. ```; failures: [ ""single error string"" ]; ```; Should be:; ```; failures: [{; message: ""single error string"" ; causedBy: []; }]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2201:26,failure,failures,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2201,4,"['error', 'failure']","['error', 'failures']"
Availability,"Per discussion with @ruchim and @cjllanwarne, the coercion from `File` to `String` in task declarations can't possibly yield a localized path since wdl4s coercions simply don't have the information required to do this. We're beginning to think File to String coercions probably shouldn't be allowed at all, but I'll keep the issue of disallowing them out of this ticket. There's still an issue with the optional ""filling"" not being transferred between the optional File and optional String. Here's a WDL that reproduces the issue. `coerced_int` and `coerced_string` stringify as empty:. ```; task strings {; Int? int; String? string. Int? coerced_int = string; String? coerced_string = int. command {; echo int: ${int} string: ${string} coerced_int: ${coerced_int} coerced_string: ${coerced_string}; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow w {; call strings { input: int = 1, string = ""2"" }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1945#issuecomment-277806238:702,echo,echo,702,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1945#issuecomment-277806238,1,['echo'],['echo']
Availability,"Per https://cloud.google.com/container-registry/docs/pushing-and-pulling:. >The four options are:; >; > gcr.io hosts the images in the United States, but the location may change in the future; > us.gcr.io hosts the image in the United States, in a separate storage bucket from images hosted by gcr.io; > eu.gcr.io hosts the images in the European Union; > asia.gcr.io hosts the images in Asia. Cromwell today would appear to support the first three but not the last, `asia.gcr.io`. The place in code that seems to be at fault is: https://github.com/broadinstitute/cromwell/blob/develop/dockerHashing/src/main/scala/cromwell/docker/registryv2/flows/gcr/GoogleFlow.scala#L20",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4466:520,fault,fault,520,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4466,1,['fault'],['fault']
Availability,"Per my investigation of #1740 I can confirm the validation is running synchronous to submission, which it should not be. But the API is returning an error for malformed input and not just timing out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328286341:149,error,error,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1882#issuecomment-328286341,1,['error'],['error']
Availability,"Per our test review meeting, we see this is one of the more commonly failing tests. . ![image](https://user-images.githubusercontent.com/165320/46823419-236f9280-cd5c-11e8-8f32-0401c7bdee71.png). (working on a BQ sample of an error for some more details)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4232:226,error,error,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4232,1,['error'],['error']
Availability,"Perhaps a config error... I am using the TES backend, which is configured to utilize my funnel server, that in turn is passing tasks to AWS Batch (nice, right?). This seems to work OK with the simplest workflow possible, but now that I have added some inputs, I am getting an error. Here is my setup and error trace from the server, and I am running on the latest from the develop branch:. [hello.inputs.txt](https://github.com/broadinstitute/cromwell/files/2081428/hello.inputs.txt); [my-cromwell.conf.txt](https://github.com/broadinstitute/cromwell/files/2081429/my-cromwell.conf.txt); [myWorkflow_awsbatch.wdl.txt](https://github.com/broadinstitute/cromwell/files/2081843/myWorkflow_awsbatch.wdl.txt). ```; 2018-06-07 13:09:05,646 cromwell-system-akka.dispatchers.api-dispatcher-119 INFO - Unspecified type (Unspecified version) workflow af282f7a-1e95-4390-8cf7-c3bbd93b10b2 submitted; 2018-06-07 13:09:15,813 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - 1 new workflows fetched; 2018-06-07 13:09:15,813 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(af282f7a-1e95-4390-8cf7-c3bbd93b10b2); 2018-06-07 13:09:15,814 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2; 2018-06-07 13:09:15,814 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-07 13:09:15,815 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Parsing workflow as WDL draft-2; 2018-06-07 13:09:15,826 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(af282f7a)]: Call-to-Backend assignments: wf_hello.hello -> TES; 2018-06-07 13:09:16,844 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowExecutionActor-af282f7a-1e95-4390-8cf7-c3bbd93b10b2 [UUID(af282f7a)]:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3743:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3743,3,['error'],['error']
Availability,Ping @grsterin for review,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5711#issuecomment-671535576:0,Ping,Ping,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5711#issuecomment-671535576,2,['Ping'],['Ping']
Availability,Ping me when it's ready for review.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-287444838:0,Ping,Ping,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2075#issuecomment-287444838,1,['Ping'],['Ping']
Availability,Ping! We still would like this. Is it on the roadmap?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-357252477:0,Ping,Ping,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-357252477,1,['Ping'],['Ping']
Availability,"Pinging @Horneth and @kshakir on thoughts on triviality of this. I think we'd need a query to convert everything that looks like: ; ```; blah:blah:failures[123]:causedBy:causedBy:message; ```; into something like:; ```; blah:blah:failures[123456]:message; ```. AFAIK there's not a simple SQL query that'd do that so we'd need to look more like the scala-hooky metadata migration script? How trivial was that? Also, would it take another O(migration) to complete it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282844631,3,"['Ping', 'failure']","['Pinging', 'failures']"
Availability,Pinging @ansingh7115 @katevoss @abaumann for thoughts on a priority for this...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282839921:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282839921,1,['Ping'],['Pinging']
Availability,Pinging @chapmanb in case he has thoughts on what we could do here,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4613#issuecomment-460404199:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4613#issuecomment-460404199,1,['Ping'],['Pinging']
Availability,Pinging @cjllanwarne and @mcovarr for a hopefully quick review. Should fix `sbt test` when mysql is not setup.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/961#issuecomment-224062615:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/961#issuecomment-224062615,1,['Ping'],['Pinging']
Availability,Pinging @cjllanwarne as I believe he has opinions on this :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-366981017:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3241#issuecomment-366981017,1,['Ping'],['Pinging']
Availability,Pinging @cjllanwarne as he was the prometheus of `operations`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324401972:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2572#issuecomment-324401972,1,['Ping'],['Pinging']
Availability,Pinging @danbills to ask whether the above sample output is sufficient for your use case or whether we need another ticket for a part II,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4567#issuecomment-457355868:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4567#issuecomment-457355868,1,['Ping'],['Pinging']
Availability,"Pinging @gbggrant on this one, when this is merged it's another tool in your all's arsenal for stability under load",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/442#issuecomment-182545324:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/442#issuecomment-182545324,1,['Ping'],['Pinging']
Availability,Pinging @kcibul for priority,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-256067052:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-256067052,1,['Ping'],['Pinging']
Availability,"Pinging @kshakir for an optional review since (1) you apparently ""recently edited these files"" and (2) faced the pain of this in UL so might have opinions on whether this counts as a fix",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665232325:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5595#issuecomment-665232325,1,['Ping'],['Pinging']
Availability,Pinging @kshakir for direction... can you think of any reason for us not to merge this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6475#issuecomment-914365601:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6475#issuecomment-914365601,1,['Ping'],['Pinging']
Availability,Pinging @natechols and @kshakir in case they have thoughts,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083#issuecomment-513595663:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083#issuecomment-513595663,1,['Ping'],['Pinging']
Availability,"Pinging @ruchim and @danbills, the current PO and TL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-417701911:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2450#issuecomment-417701911,1,['Ping'],['Pinging']
Availability,Pinging for re-review since there were a few non-trivial fixups required,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5701#issuecomment-673225025:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5701#issuecomment-673225025,1,['Ping'],['Pinging']
Availability,Pinging this ticket with another requester from the forums: http://gatkforums.broadinstitute.org/wdl/discussion/9936/is-there-a-way-to-tell-cromwell-to-ignore-the-runtime-section-of-the-tasks-in-a-wdl-script#latest,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-314394676:0,Ping,Pinging,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-314394676,1,['Ping'],['Pinging']
Availability,Pins the docker image used by the client build script to one which does not error out when we try the build.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6945:76,error,error,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6945,1,['error'],['error']
Availability,"PipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz.tbi to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-84/cacheCopy/SR00c.HG03556.txt.gz.tbi; 1608597482359,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/script to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-MergeSRFilesByContig/shard-5/script; 1608597484342,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz to focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-49ef-bd67-4e88965c2e01/call-SetSampleIdSR/shard-15/cacheCopy/SR00c.HG00599.txt.gz; 1608597486185,download: s3://focal-gwf-core/cromwell-execution/GATKSVPipelineSingleSample/41bca0c7-e664-486c-8c40-52fd6cfb0636/call-Module00c/Module00c/7d4766c9-ce90-47ed-b14e-7c68055d8b1c/call-EvidenceMerging/EvidenceMerging/4eaf013d-9a23-4",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6141:149441,down,download,149441,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6141,2,['down'],['download']
Availability,"Please check this issue.; I think this issue is another problem . > Caused by: common.exception.AggregatedMessageException: Error(s):; > Could not evaluate expression: write_lines(array_of_files): Access Denied (Service: S3Client; Status; > Code: 403; Request ID: CB48F5CFE95BBD50); > at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:68); > at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:64); > at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExe; > cutionActor.scala:563); > ... 31 common frames omitted; > [2019-01-09 05:21:48,83] [error] WorkflowManagerActor Workflow fb387147-f98a-4397-92b3-700d8c607a45 f; > ailed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionAc; > tor failed and didn't catch its exception.; > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:183); > at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrE; > lse(StandardSyncExecutionActor.scala:180); > at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); > at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); > at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); > at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); > at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); > at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); > at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); > at akka.dispatch.Mailbox.run(Mailbox.scala:224); > at akka.dispatch.Mailbox.exec(Mailbox.scala:235); > at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); > at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); > at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); > at akka.dispatch.forkjo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365:124,Error,Error,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4275#issuecomment-452577365,2,"['Error', 'error']","['Error', 'error']"
Availability,"Please check this issue.; When I configure the aws, I followed up this page. (https://docs.opendata.aws/genomics-workflows/; ); I did not use the all-in-one template. With 3 step configure, I setup the cromwell server (Custom AMI -> VPC..and etc -> cromwell server instance). <!-- Which backend are you running? -->; aws. <!-- Paste/Attach your workflow if possible: -->; ```; task echoHello{; command {; echo ""Hello AWS!""; }; runtime {; docker: ""ubuntu:latest""; }. }. workflow printHelloAndGoodbye {; call echoHello; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; [cromwell-server.log](https://github.com/broadinstitute/cromwell/files/2897001/cromwell-server.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4677:382,echo,echoHello,382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4677,3,['echo'],"['echo', 'echoHello']"
Availability,"Please file a new bug report in the format ""expected result"" / ""actual result"", taking into account that Cromwell has no control over whether Google servers return 504 errors.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760314632:168,error,errors,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760314632,1,['error'],['errors']
Availability,"Please ping @geoffjentry before picking up this ticket, he has some ideas",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2313#issuecomment-305527188:7,ping,ping,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2313#issuecomment-305527188,1,['ping'],['ping']
Availability,Plumb through submission time and use to sort heartbeat writes.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4442:46,heartbeat,heartbeat,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4442,1,['heartbeat'],['heartbeat']
Availability,"Pool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); 	at akka.dispatch.Mailbox.run(Mailbox.scala:223); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.lang.RuntimeException: java.util.NoSuchElementException: key not found: ps-stdOut; 	at cromwell.engine.workflow.lifecycle.execution.job.EngineJobExecutionActor$$anonfun$4.applyOrElse(EngineJobExecutionActor.scala:143); 	at cromwell.engine.workflow.lifecycle.execution.job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012:6490,Fault,FaultHandling,6490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012,1,['Fault'],['FaultHandling']
Availability,PoolTime Out error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7200:13,error,error,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7200,1,['error'],['error']
Availability,Poor womtool validate output (error formatter),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4041:30,error,error,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4041,1,['error'],['error']
Availability,Possibly the first in a series as I trace down two other classes of failures.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3223:42,down,down,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3223,2,"['down', 'failure']","['down', 'failures']"
Availability,Post Mortem of degraded performance May 17-20 2018,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3666:15,degraded,degraded,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3666,1,['degraded'],['degraded']
Availability,"Post task success processing failed with a Communications link failure. ```; 2016-04-26 14:06:36,930 cromwell-system-akka.actor.default-dispatcher-8 INFO - JES Run [UUID(143681e1):GatherBqsrReports]: Status change from Running to Success; 2016-04-26 14:06:37,506 cromwell-system-akka.actor.default-dispatcher-15 ERROR - WorkflowActor [UUID(143681e1)]: Completion work failed for call GatherBqsrReports.; com.mysql.jdbc.exceptions.jdbc4.CommunicationsException: Communications link failure. The last packet successfully received from the server was 324 milliseconds ago. The last packet sent successfully to the server was 0 milliseconds ago.; at sun.reflect.GeneratedConstructorAccessor102.newInstance(Unknown Source) ~[na:na]; at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_72]; at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[na:1.8.0_72]; at com.mysql.jdbc.Util.handleNewInstance(Util.java:400) ~[cromwell.jar:0.19]; at com.mysql.jdbc.SQLError.createCommunicationsException(SQLError.java:1038) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3434) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.reuseAndReadPacket(MysqlIO.java:3334) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3774) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2447) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2594) ~[cromwell.jar:0.19]; at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2541) ~[cromwell.jar:0.19]; at com.mysql.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:4882) ~[cromwell.jar:0.19]; at com.zaxxer.hikari.proxy.ConnectionProxy.setAutoCommit(ConnectionProxy.java:334) ~[cromwell.jar:0.19]; at com.zaxxer.hikari.proxy.ConnectionJavassistProxy.setAutoCommit(ConnectionJavassistProxy.java) ~[cromwell.jar:0.19]; at slick.jdbc.JdbcBackend$BaseSession.startInTransaction(JdbcBackend",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/742:63,failure,failure,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/742,3,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,Post-Processing slows down to a crawl and fails with large globs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1248:22,down,down,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1248,1,['down'],['down']
Availability,Potential hotfix candidate but would be nice to know why these empty queues are appearing in the first place. An attempt to recover from (though probably not fix the underlying cause of) tokens going missing due to stack traces like:; ```; [cromwell-system-akka.actor.default-dispatcher-1158] ERROR akka.actor.OneForOneStrategy - dequeue on empty queue; java.util.NoSuchElementException: dequeue on empty queue; 	at scala.collection.immutable.Queue.dequeue(Queue.scala:155); 	at cromwell.engine.workflow.tokens.TokenQueue.recursingDequeue(TokenQueue.scala:63); 	at cromwell.engine.workflow.tokens.TokenQueue.dequeue(TokenQueue.scala:50); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1(RoundRobinQueueIterator.scala:46); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1$adapted(RoundRobinQueueIterator.scala:46); 	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:415); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1169); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159); 	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1058); 	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1052); 	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:144); 	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:132); 	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:104); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.findFirst(RoundRobinQueueIterator.scala:48); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:32); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:10); 	at scala.collection.Iterator$SliceIterator.next(Itera,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4909:124,recover,recover,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909,2,"['ERROR', 'recover']","['ERROR', 'recover']"
Availability,Power through non-findable terminals in error messages,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3048:40,error,error,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3048,1,['error'],['error']
Availability,Preempted machines with PAPI error code 10 not handled as preempted,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5136:29,error,error,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5136,1,['error'],['error']
Availability,Preemptible recovery from a checkpointing file [BW-460],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6137:12,recover,recovery,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6137,2,"['checkpoint', 'recover']","['checkpointing', 'recovery']"
Availability,Preemptible retry behavior for Error Code 13,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744:31,Error,Error,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744,1,['Error'],['Error']
Availability,Presumably to ensure we get the labels even when the workflow was invalid?. I think we can centaur that... I’m pretty sure we check metadata even on failure tests.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3228#issuecomment-362119021:149,failure,failure,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3228#issuecomment-362119021,1,['failure'],['failure']
Availability,Pretty much what it says. We know we want to add Error 13 to our JES retries. Do any other codes make the cut?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1909:49,Error,Error,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1909,1,['Error'],['Error']
Availability,"Pretty much yes, also like I said we use a publicly available jq image to parse the cwl.output.json on every task, which was meant to be a ""let's get CWL working on PAPIv2 "" but not a permanent solution",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3680#issuecomment-415014146:52,avail,available,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3680#issuecomment-415014146,1,['avail'],['available']
Availability,Prints out the gsutil failures when (de)localizing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4396:22,failure,failures,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4396,1,['failure'],['failures']
Availability,"Produces a 500 error like; ```; {; ""status"": ""error"",; ""message"": ""Statement cancelled due to timeout or client request""; }; ```; TODO: if the 55-second request timeout fires and kills the request, we should still make sure the 60-second query kill also gets logged somehow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5087:15,error,error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5087,2,['error'],['error']
Availability,Production is hitting an error handler [0] that attempts to stringify too large a value and causes the server to crash before it manages to emit the log. This is a common enough situation that we have a library function for it. [0] https://github.com/broadinstitute/cromwell/blob/aen_wx_892/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/stores/ValueStore.scala#L117,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6981:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6981,1,['error'],['error']
Availability,"Program_required_data/deFuse/defuse-data/gmap/est4/est4.ref153offsets64strm...done (331,380,368 bytes, 38.82 sec); Pre-loading ref positions, kmer 15, interval 3......done (625,955,632 bytes, 54.79 sec); Starting alignment; No paths found for 279; No paths found for 396; No paths found for 1308; No paths found for 1785; No paths found for 1880; No paths found for 1883; No paths found for 1947; No paths found for 1948; No paths found for 2184; No paths found for 2262; No paths found for 2403; No paths found for 2536; No paths found for 2577; No paths found for 2742; No paths found for 2756; No paths found for 2907; No paths found for 2919; No paths found for 2921; No paths found for 3058; No paths found for 3298; No paths found for 3837; No paths found for 3942; No paths found for 4020; No paths found for 4124; No paths found for 4274; Processed 405 queries in 703.84 seconds (0.58 queries/sec); Removed existing memory for shmid 262152; Removed existing memory for shmid 131076; ; real 14m1.724s; user 3m28.053s; sys 0m13.563s; ```. - The WDL I am using: [defuse.wdl.txt](https://github.com/broadinstitute/cromwell/files/2651338/defuse.wdl.txt). The commands I use when running this in the docker container interactively are the same. - I encounter the same error using Cromwell 32 and Cromwell 36. - A similar error was reported [here](https://bitbucket.org/dranew/defuse/issues/36/defuse-with-segmentation-error) for when deFuse was run on a server. I tried running the failing gmap command alone through Cromwell and it succeeded. - The breakpoints files that gmap uses as input for the failing steps differ (deFuse generates these files in earlier steps). - Many gmap steps are run in deFuse; some do succeed when run through Cromwell. - The full log for the Cromwell version: ; [defuse.log](https://github.com/broadinstitute/cromwell/files/2653318/defuse.log); - The full log for the docker version: ; [defuse.log](https://github.com/broadinstitute/cromwell/files/2653319/defuse.log)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:6450,error,error,6450,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,3,['error'],['error']
Availability,Programmer error and logging in PAPI request manager,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4671:11,error,error,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4671,1,['error'],['error']
Availability,Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:89); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:211); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at a,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:1721,robust,robustPoll,1721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,2,['robust'],['robustPoll']
Availability,"Proposed high-level flow:. Workflows + inputs + options are submitted. Any grossly malformed submission would quickly generate an error returned to the client immediately. Otherwise save everything to WORKFLOW_EXECUTION and WORKFLOW_EXECUTION_AUX at status Submitted, and return the generated UUID to the client. THAT’S IT. Do not validate workflows synchronously to submission, that’s expensive and difficult to throttle back. The returned UUID constitutes a pollable handle for the submission. Technically this UUID is now a submission ID rather than a workflow ID, though if the workflow is eventually validated that UUID could certainly be thought of as the workflow ID. When Cromwell is ready to validate a submission, the status of the WORKFLOW_EXECUTION can be moved to Validating and validation initiated. Assuming the validation succeeds the workflow would move to state Validated. Again when Cromwell is ready to run the workflow, it can be moved to state Running and the workflow would begin to run. Workflows that fail validation would get status Failed with text describing the nature of the failures in the FAILURE_MESSAGES table.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/564:130,error,error,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/564,2,"['error', 'failure']","['error', 'failures']"
Availability,"Provide a configurable knob which will limit the number of in flight calls per workflow. This includes scatters, e.g. if the limit is 5 and there's a 6-way scatter, at most 5 shards may be processed at once. This will likely be requested to be in place prior to or shortly after the GATK launch in early january. Edit: Some clarifications. As mentioned below this needs to be robust to the entirety of the root workflow, including all subworkflows. It also needs to be robust to restart, IOW on a restart of Cromwell if there were N jobs restarted the counter should start at MAX - N.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2846:376,robust,robust,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2846,2,['robust'],['robust']
Availability,"Put in testing for restarts. . A possible (and preferred by this author) solution would be to seed a database with a known state and make sure a launching cromwell picks things up properly. That sounds like it'd probably be in the Centaur-ish space, but perhaps not. Another possibility (and not preferred by this author) would be to have a system that submits stuff, shuts down that cromwell, launches another cromwell and goes to town.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1329:374,down,down,374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1329,1,['down'],['down']
Availability,Query endpoint `additionalQueryResultFields` parameter database error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3115:64,error,error,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3115,1,['error'],['error']
Availability,Query endpoint `includeSubworkflows` parameter database error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3873:56,error,error,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3873,1,['error'],['error']
Availability,"Question on using the default authentication for AWS. Basically I have a credentials file saved to *~/.aws/credentials* and it is looked up by cromwell for authentication to use the AWS batch service. For scalability I wanted to include a few different profiles to look up. *~/.aws/credentials*; ```; [service1]; aws_access_key_id = key1; aws_secret_access_key = pw1. [service2]; aws_access_key_id = key2; aws_secret_access_key = pw2; ```. However, I cannot seem to get cromwell config to set a profile name for `service2` as the expected authentication. The only profile it will look for is `default`. From my config it is set up as follows:. ```; aws {; application-name = ""CROMWELL-SERVER""; auths = [{; name = ""service2""; scheme = ""default""; }]. region = ""us-east-1""; }; ```. The error when I try to run a job is below which shows I am using a `profileName=default`. Looked through the code a bit but couldn't find where I can add that profilename to the config. If there is a way to change the profile name I would definitely use it, if not then I can set things up differently. Thanks. `cromwell_1 | Caused by: software.amazon.awssdk.core.exception.SdkClientException: Unable to load credentials from any of the providers in the chain AwsCredentialsProviderChain(credentialsProviders=[SystemPropertyCredentialsProvider(), EnvironmentVariableCredentialsProvider(), ProfileCredentialsProvider(profileName=default, profileFile=ProfileFile(profiles=[Profile(name=service1, properties=[aws_access_key_id, aws_secret_access_key]), Profile(name=service2, properties=[aws_access_key_id, aws_secret_access_key])])), ContainerCredentialsProvider(), InstanceProfileCredentialsProvider()]) : [SystemPropertyCredentialsProvider(): Unable to load credentials from system settings. Access key must be specified either via environment variable (AWS_ACCESS_KEY_ID) or system property (aws.accessKeyId)., EnvironmentVariableCredentialsProvider(): Unable to load credentials from system settings. Access key must be",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5452:783,error,error,783,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452,1,['error'],['error']
Availability,"Question; -----------------; Is there any plan on speeding up `java -jar $JAR run`? Pipeline developer would definitely benefits from a faster startup. Symptom; -----------------; ```bash; time java -jar cromwell-49.jar run sub-flow.wdl -i input.json; ```. ```; real	1m12.833s; user	1m12.148s; sys	0m7.644s; ```. ```; $ cat input.json ; {; ""hello_and_goodbye.hello_and_goodbye_input"":""test1""; }; ```. Detail; ----------; backend: local. File: `sub-flow.wdl`; ```wdl; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }; output {; String out = read_string(stdout()); }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5451:532,echo,echo,532,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5451,1,['echo'],['echo']
Availability,Quieten down that localization switchover,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1727:8,down,down,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1727,1,['down'],['down']
Availability,"Quite often I get ; ```; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Docker image quay.io/biocontainers/star:2.5.3a--0@sha256:352f627075e436016ea2c38733b5c0096bb841e2fadcbbd3d4ae8daf03ccdf1b has an invalid syntax.""; }; ],; ```; Here is example of the task that caused this error. What is interested is that cromwell even did not start this task, so I suspect there is something wrong with docker parsing (I have this issue for this container both in develop and latest release of cromwell). ```; task star {. Int numberOfThreads = 8; File file; File genomeDir. command {; STAR --runThreadN ${numberOfThreads} --genomeDir ${genomeDir} --readFilesCommand gunzip -c --readFilesIn ${file}; }. runtime {; docker: ""quay.io/biocontainers/star:2.5.3a--0@sha256:352f627075e436016ea2c38733b5c0096bb841e2fadcbbd3d4ae8daf03ccdf1b""; }. output {; String result = ""STAR WORKS!""; }. }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2254:26,failure,failures,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2254,2,"['error', 'failure']","['error', 'failures']"
Availability,Quotation marks when quoting source in error msg,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3725:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3725,1,['error'],['error']
Availability,"Quoth Dave: Green Team launched 50 workflows and could initially query the API despite some slowness. After they’ve been running for 45 mins or so, hitting the API is only intermittently successful:. ```; https://cromwell.gotc-staging.broadinstitute.org/api/workflows/v1/query; Ooops! The server was not able to produce a timely response to your request.; Please try again in a short while!; ```. ```; Unexpected error while awaiting Cromwell Workflow completion: Error hitting REST API: https://cromwell.gotc-staging.broadinstitute.org/api/workflows/v1/5296889b-8b88-41db-a5fa-d1071ac22a... => Unexpected response code: 502; ```. Just trying to get to the swagger page takes a couple of minutes to load or fails to load altogether. This is highly variable - about 2 hours in we still have 50 workflows running and API queries are coming back very quickly. In production using Cromwell 0.19, GotC routinely runs ~200-500 workflows simultaneously.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228405075:413,error,error,413,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228405075,2,"['Error', 'error']","['Error', 'error']"
Availability,"REST API correctly gives error on unknown status value:. ```; [conradL@qimr13054 cromwell-executions]$ curl localhost:8000/api/workflows/V1/query?status=blah; {; ""status"": ""fail"",; ""message"": ""Unrecognized status values: blah""; }; ```. and correctly gives results for status with exact case-sensitive spelling (""Failed""):. ```; [conradL@qimr13054 cromwell-executions]$ curl localhost:8000/api/workflows/V1/query?status=Failed; {; ""results"": [{; ""id"": ""8b8ce542-9e4a-4549-8fd3-c72ea325f392"",; ""status"": ""Failed"",; ""start"": ""2016-10-05T11:15:38.214+10:00"",; ""end"": ""2016-10-05T11:15:38.217+10:00""; }, {; ""id"": ""1784a44d-e623-42fd-bf7a-90b00d300017"",; ""status"": ""Failed"",; ""start"": ""2016-10-05T11:16:38.273+10:00"",; ""end"": ""2016-10-05T11:16:38.275+10:00""; }]; }; ```. But returns neither error nor results when given a status value that is only case-insensitively correct (""failed""):. ```; [conradL@qimr13054 cromwell-executions]$ curl localhost:8000/api/workflows/V1/query?status=failed; {; ""results"": []; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1514:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1514,2,['error'],['error']
Availability,REST Endpoint to shut down Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2595:22,down,down,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2595,1,['down'],['down']
Availability,"ROMIUMOS_SDK_GCS=https://storage.googleapis.com/chromiumos-sdk; + ROOT_OS_RELEASE=/root/etc/os-release; + KERNEL_SRC_DIR=/build/usr/src/linux; + NVIDIA_DRIVER_VERSION=418.40.04; + NVIDIA_DRIVER_MD5SUM=; + NVIDIA_INSTALL_DIR_HOST=/var/lib/nvidia; + NVIDIA_INSTALL_DIR_CONTAINER=/usr/local/nvidia; + ROOT_MOUNT_DIR=/root; + CACHE_FILE=/usr/local/nvidia/.cache; + LOCK_FILE=/root/tmp/cos_gpu_installer_lock; + LOCK_FILE_FD=20; + set +x; [INFO 2020-08-04 23:40:07 UTC] Checking if this is the only cos-gpu-installer that is running.; [INFO 2020-08-04 23:40:07 UTC] Running on COS build id 12871.1174.0; [INFO 2020-08-04 23:40:07 UTC] Checking if third party kernel modules can be installed; [INFO 2020-08-04 23:40:07 UTC] Checking cached version; [INFO 2020-08-04 23:40:07 UTC] Cache file /usr/local/nvidia/.cache not found.; [INFO 2020-08-04 23:40:07 UTC] Did not find cached version, building the drivers...; [INFO 2020-08-04 23:40:07 UTC] Downloading GPU installer ...; [INFO 2020-08-04 23:40:09 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/tesla/418.40.04/NVIDIA-Linux-x86_64-418.40.04.run; ls: cannot access '/build/usr/src/linux': No such file or directory; [INFO 2020-08-04 23:40:11 UTC] Kernel sources not found locally, downloading; [INFO 2020-08-04 23:40:11 UTC] Kernel source archive download URL: https://storage.googleapis.com/cos-tools/12871.1174.0/kernel-src.tar.gz. real	0m2.220s; user	0m0.183s; sys	0m0.338s; [INFO 2020-08-04 23:40:18 UTC] Setting up compilation environment; [INFO 2020-08-04 23:40:18 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain_env. real	0m0.126s; user	0m0.014s; sys	0m0.001s; [INFO 2020-08-04 23:40:18 UTC] Downloading toolchain from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain.tar.xz. real	0m11.907s; user	0m0.428s; sys	0m1.039s; [INFO 2020-08-04 23:41:17 UTC] Configuring environment variables for cross-compilation; [INFO 2020-08-04 23:41:17 UTC] Config",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:3740,Down,Downloading,3740,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,1,['Down'],['Downloading']
Availability,"Ran a workflow (see below) with nested scatters on Cromwell v38. The metadata for the calls seems a bit odd... It would be helpful to find out:; 1. Why the name of the call is `ScatterAt9_13`; 2. Why aren't the shards numbered as they are for a single scatter?. <img width=""570"" alt=""Screen Shot 2019-04-10 at 2 44 12 PM"" src=""https://user-images.githubusercontent.com/14941133/55905173-75746200-5b9f-11e9-8388-4ca4d4e887b5.png"">. ```; version 1.0. workflow nested_scatter {. Array[Int] indices = [1,2,3]; Int y = 55. scatter(a in indices) {; scatter(b in indices) {; Int x = a + b; call add { ; 	input: foo = x; }; }; }; }. task add {; 	; 	input { Int foo }; 	. 	command { echo foo }. 	runtime {; 		docker: ""ubuntu:latest"". 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4820:674,echo,echo,674,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4820,1,['echo'],['echo']
Availability,"Ran a workflow using V40 Cromwell on AWSBATCH that had as outputs (one outputfile.vcf for each of the shards in the workflow):. ```; output {; Array[File] outputs = task.outputvcf; }; ```. I used the following workflow options:; ```; {; ""workflow_failure_mode"": ""NoNewCalls"",; ""default_runtime_attributes"": {; ""maxRetries"": 1; },; ""final_workflow_outputs_dir"": ""s3://bucket/Cromwell/results"",; ""use_relative_output_paths"": ""false"",; ""final_workflow_log_dir"": ""s3://bucket/Cromwell/workflowLogs"",; ""final_call_logs_dir"": ""s3://bucket/Cromwell/workflowLogs""; }; ```. All calls of the workflow completed successfully but the workflow itself failed. . Error Message I got:; `""copying directories is not yet supported: s3://s3.amazonaws.com/bucket/Cromwell/results/workflowName/1ec38d0b-afc4-4cd5-90f1-f015395d6e36/call-task/shard-0/outputfile.vcf""`. Oddly enough, the correct prefixes for the output files were created in the correct S3 bucket, they just don't have an object there, and via the CLI they appear as directories. ??? . For the logs, a prefix was made that is empty, and the log file was written successfully to one level higher than the prefix it is supposed to be in. So instead of:; ```; s3://bucket/Cromwell/workflowLogs/workflowName/<workflowid>.log; ```; there is:; ```; s3://bucket/Cromwell/workflowLogs/workflowName/ (empty prefix); s3://bucket/Cromwell/workflowLogs/<workflowid>.log (successfully written file). ```; Thoughts?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982:648,Error,Error,648,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982,1,['Error'],['Error']
Availability,"Ran commit `34fcb6bf3b9e29557a7d9ac057e8ae07370180b9` on my laptop via `sbt clean && src/ci/bin/testCentaurBcs.sh` and tests passed. Dunno if there was a GitHub outage missing the event, but [the Travis CI test passed](https://travis-ci.com/broadinstitute/cromwell/builds/123676180). Merging.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5111#issuecomment-524391215:161,outage,outage,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5111#issuecomment-524391215,1,['outage'],['outage']
Availability,"Ran it on 25_hotfix, call logs available here:; https://console.cloud.google.com/storage/browser/cloud-cromwell-dev/cromwell_execution/ruchi/BrokenFilePath/16caeb92-3c39-496b-bf44-7cd1e5c33269/call-PadTargets/?project=broad-dsde-cromwell-dev. Failed to delocalize files:; ```; 2017/03/20 16:48:49 I: Running command: sudo gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/targets.padded.tsv gs://cloud-cromwell-dev/cromwell_execution/ruchi/BrokenFilePath/16caeb92-3c39-496b-bf44-7cd1e5c33269/call-PadTargets/targets.padded.tsv; 2017/03/20 16:48:51 E: command failed: CommandException: No URLs matched: /mnt/local-disk/targets.padded.tsv; CommandException: 1 file/object could not be transferred.; (exit status 1); ```. Stderr:; ```; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell_root/tmp.TdaNa3; Error: Could not find or load main class org.broadinstitute.hellbender.Main; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287825949:31,avail,available,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2078#issuecomment-287825949,2,"['Error', 'avail']","['Error', 'available']"
Availability,"Rather than putting up with this error because their config is out of date:; ```; [ERROR] [04/19/2017 18:14:32.636] [cromwell-system-akka.actor.default-dispatcher-4] [akka://cromwell-system/user/cromwell-service/ServiceRegistryActor/MetadataService] Error summarizing metadata; com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""SUMMARY_STATUS_ENTRY"" where (""SUMMARY_TABLE_NAME"" = 'WORKFLOW_METADATA_SUMMARY_' at line 1; ```. We should have a nice error message that tells people what to do about it",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2186:33,error,error,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2186,5,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"Re @aednichols :. >It seems like the root cause of the bug is an error by the [story](https://broadworkbench.atlassian.net/browse/BW-568) author 😄; >; > Behavior is undefined when groups are tied, whatever happens by default is fine. Are there any metrics we can add to look out in advance for disadvantaged users?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068003722:65,error,error,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6709#issuecomment-1068003722,1,['error'],['error']
Availability,"Re Kris's comment - sounds like a good idea. My only extra suggestion would be to make sure that the error reported to the user if the gcloud default auth isn't set up properly is as bulletproof as possible. Otherwise I can see some tedious ""why won't it work!"" head-scratching in the near future",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166588563:101,error,error,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166588563,1,['error'],['error']
Availability,"Re-enable and make the `""handle coercion failures gracefully""` test in `MaterializeWorkflowDescriptorActorSpec` work again",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1067:41,failure,failures,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1067,1,['failure'],['failures']
Availability,"Re: redundant test code, everything is separate as far as I can tell, including looking at the coverage counts in coveralls. The various tests I see are:. Existing in cromwell pre-PR:; - Test the cromwell swagger _JSON_ is valid according to the swagger specification; - Test that the cromwell swagger route serves up the json to the expected URL. Now as of this PR:. - Test that common code for serving up a swagger endpoint can serve; - json or yaml swagger; - [CORS](https://en.wikipedia.org/wiki/Cross-origin_resource_sharing) support; - Utility for redirecting browser requests to `/` to the correct swagger UI. Also included in this PR:; - Ensure that ""wrapping"" a route works, serving all wrapped routes under a new prefix like `/api/*`; - Ensure that common code for easier spray-can binding works",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065:4,redundant,redundant,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2093#issuecomment-289486065,1,['redundant'],['redundant']
Availability,"Re:. > The capoeira tests complete successfully but get unexpected cache hits. Caching is also tweaked in CI configs. For example:. https://github.com/broadinstitute/cromwell/blob/279909b1f35c8305dcfc23ac8534dcb00ce09771/src/ci/resources/local_provider_config.inc.conf#L6. Have you already tried the tests locally with the CI configs? For unicromtal, one can run the existing CI scripts with a bit of bootstrap:; - Setup vault; - Setup mysql locally (I'm using `brew install mysql`); - [Initialize a `travis` mysql user with granted permissions](https://dev.mysql.com/doc/refman/8.0/en/adding-users.html); - [Using the `travis` user create a `cromwell_test` schema](https://github.com/broadinstitute/cromwell/blob/279909b1f35c8305dcfc23ac8534dcb00ce09771/core/src/test/resources/application.conf#L24). From the cromwell source directory, with all of the above setup, one can try to run `src/ci/resources/testCentaurLocal.sh` and it will render the configs with vault and run the tests, including the restart tests that bring down/up cromwell. Also, if one just wants to ever use the CI configs with cromwell in IntelliJ, `sbt renderCiResources` will render configs into the folder `target/ci/resources`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472915580:1025,down,down,1025,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472915580,1,['down'],['down']
Availability,Readable error message for invalid indexing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3008:9,error,error,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3008,1,['error'],['error']
Availability,"Realized I had more logs that might be helpful from what we saw in one of the workflows that failed. This is a task that is pre-emptible; ```; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 WARN - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Job PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:1 failed with a retryable failure: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonRe",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:389,failure,failure,389,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490,2,"['error', 'failure']","['errors', 'failure']"
Availability,Rebased and a minor migration fix added. I also removed the non-default failure and error handling on the migration but did not add in a special error message.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-299560126:72,failure,failure,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2243#issuecomment-299560126,3,"['error', 'failure']","['error', 'failure']"
Availability,Recent change to JES travis test made it blind to failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1353:50,failure,failures,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1353,1,['failure'],['failures']
Availability,"Recently for an operation task that updating labels for ~2500 workflows, we have to write a loop to end ~2500 PATCH /label requests to the Cromwell, which took more than 3 hrs. (In Cromwell IAM, this is even worse since a single token will expire in 60mins, so you have to also deal with the token refreshment) . We tried to use multi-threading to speed it up but ended up getting transient 500 errors when using a thread pool with a size of >=4 threads. . So having this batch feature will make a lot of things much easier!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3755#issuecomment-451759631:395,error,errors,395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3755#issuecomment-451759631,1,['error'],['errors']
Availability,Recover from Docker image hash failures to fail the workflow.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/333:0,Recover,Recover,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/333,2,"['Recover', 'failure']","['Recover', 'failures']"
Availability,Recover from database failures when reading the workflow store. Develop edition,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2150:0,Recover,Recover,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2150,2,"['Recover', 'failure']","['Recover', 'failures']"
Availability,Recover from database failures when reading the workflow store. Hotfix edition,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2149:0,Recover,Recover,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2149,2,"['Recover', 'failure']","['Recover', 'failures']"
Availability,Recover support for Local PBE,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/666:0,Recover,Recover,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/666,1,['Recover'],['Recover']
Availability,Recovery functionality for HtCondor backend. Closes #1249.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1250:0,Recover,Recovery,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1250,1,['Recover'],['Recovery']
Availability,Recovery support for JES PBE,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/751:0,Recover,Recovery,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/751,1,['Recover'],['Recovery']
Availability,Recovery support for SGE Backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1162:0,Recover,Recovery,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1162,1,['Recover'],['Recovery']
Availability,Red thumb required for two minor WOM changes (two files changed at the bottom of the PR - one gets a scaladoc and the other gets an error message improvement),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3374#issuecomment-371528065:132,error,error,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3374#issuecomment-371528065,1,['error'],['error']
Availability,Redundant. we're already on 4.10.4,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5752#issuecomment-729792679:0,Redundant,Redundant,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5752#issuecomment-729792679,1,['Redundant'],['Redundant']
Availability,Refactored IO command creation to return errors instead of throw BT-8,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6019:41,error,errors,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6019,1,['error'],['errors']
Availability,"Refinement update:. We are going to solve the first two points by adding a sort of `input_errors` map with input names as keys and error(s) as values. The absence of `errors` and presence of `input_errors` indicates the DA case where WDL is good and inputs bad. ~The last issue will split off into soliciting community feedback from non-workbench users and writing a nicer wrapper endpoint that's more compatible with curl (it is Adam's opinion, potentailly poorly supported, that this is hard right now)~ see https://github.com/broadinstitute/cromwell/issues/4892",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685:131,error,error,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4824#issuecomment-486342685,2,['error'],"['error', 'errors']"
Availability,"Regarding Kris' comment, I'm game for removing features in general, including ""user"". Various optional tests will need some help though, as currently they use user credentials. They instead should switch to the service account. The application-default factory seemed to return a credential on our Travis, though I haven't yet investigated what that credential actually is. I'm also willing to see ""application-default"" be the GoogleCredentialFactory implicit default. Still, I understand if others vote for the config to be explicit, in which case I like the suggested name ""default"". I'll stand down on making changes for now while @mcovarr has the ticket. Will still look for docs on scopes though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166473338:596,down,down,596,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166473338,1,['down'],['down']
Availability,"Regarding aborts, @Horneth made significant changes to aborts in Cromwell 30 (coming soon!) that should make it easier and more reliable to abort jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2845#issuecomment-344392351:128,reliab,reliable,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2845#issuecomment-344392351,1,['reliab'],['reliable']
Availability,"Regardless of where the task is executing though, that `sync` is expensive, and the more cores and memory you have on execution nodes, and the less work the `command` is actually doing, the worse things become when lots of tasks get allocated to a given node.; Simple ""echo Hello, World"" tasks occupy one of our 56-core execution nodes for about 30 seconds when 56 of them are allocated to a node, nearly the whole time spent in sync. ; The answer could well be ""then stop doing that"", but I hope not!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284765336:269,echo,echo,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284765336,1,['echo'],['echo']
Availability,"Reinstates the ability to run at least the `simple_if` WDL from centaur:; ```wdl; task runMe {; command {; echo ""done""; }; output {; String s = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow simple_if {; if (true) {; call runMe as runMeTrue; }. if (false) {; call runMe as runMeFalse; }. output {; runMeTrue.s; runMeFalse.s; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2803:107,echo,echo,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2803,1,['echo'],['echo']
Availability,"Related maybe: https://github.com/broadinstitute/cromwell/issues/3905. When my job is killed by our cluster (SGE or Slurm) the rc file will not appear anymore. Cromwell then stay forever waiting on this file.; When manual writing this file (aka `echo ""2"" > rc`) cromwell does see this as a failed job. I have tested this also on a Local backend but here the problem does not exists. I think this is because cromwell stays connected to the process. I have tested cromwell 34 and the current develop branch (ce27a93). For testing I have used this workflow:; ```; version 1.0. workflow Test {; input {; }. call Echo as echo {; input:; }. output {; }; }. task Echo {; input {; }. command {; # killing bash process; kill -9 $$; echo test; }. output {; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4050:246,echo,echo,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4050,5,"['Echo', 'echo']","['Echo', 'echo']"
Availability,"Related to #1944 but with some options problems as well:. The following WDL prints nothing for `string_from_maybe_file` and a non-relativized path for `string_from_file` on JES, AWS, and Local (on Local the path is not relativized under the cromwell executions directory):. ```; task strings {; File file ; File? maybe_file. String string_from_file = file; String? string_from_maybe_file = maybe_file. command {; echo string_from_file: ${string_from_file} string_from_maybe_file: ${string_from_maybe_file}; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow w {; File file. call strings { input: file = file, maybe_file = file }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1945:413,echo,echo,413,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1945,1,['echo'],['echo']
Availability,"Relevant OpenWDL PR: ~~https://github.com/openwdl/wdl/pull/229~~ (rebased into) https://github.com/openwdl/wdl/pull/366. Adding a `sep` function to join arrays of string together. I've followed the general process from: https://github.com/broadinstitute/cromwell/pull/4409/files,. ---. ### Older discussion that has been resolved. Getting two errors, but not really sure why, but really having problems with the `sepFunctionEvaluator`, it's a two value function so I tried to use the `processTwoValidatedValues` from `wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators`, but I'm getting errors on the evaluateValue:. ```scala; val value1 = expressionValueEvaluator.evaluateValue(a.arg1, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); val value2 = expressionValueEvaluator.evaluateValue(a.arg2, inputs, ioFunctionSet, forCommandInstantiationOptions)(expressionValueEvaluator); processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; EvaluatedValue(WomString(arr1.value.mkString(sepvalue.value)), Seq.empty).validNel; }; ```. But I get the following error:. ```; [error] /Users/franklinmichael/source/cromwell/wdl/transforms/biscayne/src/main/scala/wdl/transforms/biscayne/linking/expression/values/BiscayneValueEvaluators.scala:164:64: type mismatch;; [error] found : common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomString]]; [error] required: common.validation.ErrorOr.ErrorOr[wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] (which expands to) cats.data.Validated[cats.data.NonEmptyList[String],wdl.model.draft3.graph.expression.EvaluatedValue[wom.values.WomArray]]; [error] processTwoValidatedValues[WomArray, WomString, WomString](value1, value2) { (arr1, sepvalue) =>; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494:343,error,errors,343,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494,2,['error'],['errors']
Availability,Remove MetadataPutAcks & Failures. Closes #1811,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1903:25,Failure,Failures,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1903,1,['Failure'],['Failures']
Availability,"Remove link to Building (which was in any case broken), and replace it earlier in the page with a link to the releases page. Most users will want to download the latest release of Cromwell rather than build it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6984:149,down,download,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6984,1,['down'],['download']
Availability,Remove logging that could error with multiple compute environments [BT-429],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6547:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6547,1,['error'],['error']
Availability,Remove redundant WaitingForQueueSpace execution status [BA-6487 prereq],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5590:7,redundant,redundant,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5590,1,['redundant'],['redundant']
Availability,Remove redundant WaitingForQueueSpace status [BW-387],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6034:7,redundant,redundant,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6034,1,['redundant'],['redundant']
Availability,"Remove redundant nested /project directory, ignore BSP [no JIRA]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5931:7,redundant,redundant,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5931,1,['redundant'],['redundant']
Availability,Remove the 'Some' from an error message,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1899:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1899,1,['error'],['error']
Availability,Remove the 'Some' from an error message. Closes #1893,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1898:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1898,1,['error'],['error']
Availability,Removing Await.results: 1 down 6 to go.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/233:26,down,down,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/233,1,['down'],['down']
Availability,"Removing a duplicate write of workflow state that seemed to be responsible for some Tyburn failures. The WorkflowActor already handles the persistence of its own state, WorkflowManagerActor doesn't need to be doing this as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/240:91,failure,failures,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/240,1,['failure'],['failures']
Availability,"Reopened PR because last build failed with strange error and after triggering re-build on travis everything was OK, but here status wasn't updated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-520349943:51,error,error,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-520349943,1,['error'],['error']
Availability,"Reopening this issue. . For the following WDL:; ```wdl; task echo {. command {; echo ""Hello World!""; }. runtime {; docker: ""ubuntu:latest""; disks: ""local-disk 100 HDD, /test1 10 SSD""; }; }. workflow wf_echo {; call echo; }; ```. I get an error that the specification expects the form of `disks: ""local-disk, /test1""`. If we are going to change the syntax of the value of `disks` runtime attribute, then we should change the label as well. `host_mount_point` or `docker_volumes` would be more appropriate for the way Batch works. . But in the interest of test, I change the WDL to match expectations:; ```wdl; task echo {. command {; echo ""Hello World!""; }. runtime {; docker: ""ubuntu:latest""; disks: ""local-disk, /test1""; }; }. workflow wf_echo {; call echo; }; ```. And the following `volumes` & `mountPoints` were created in the AWS Batch JobDefinition:. ```json; ""volumes"": [; {; ""host"": {; ""sourcePath"": ""/cromwell_root/wf_echo.echo-None-1""; },; ""name"": ""local-disk""; },; {; ""host"": {; ""sourcePath"": ""/test1/wf_echo.echo-None-1""; },; ""name"": ""d-c919a18cd1e1254f560bb64acc581574""; }; ],; ""mountPoints"": [; {; ""containerPath"": ""/cromwell_root"",; ""sourceVolume"": ""local-disk""; },; {; ""containerPath"": ""/test1"",; ""sourceVolume"": ""d-c919a18cd1e1254f560bb64acc581574""; }; ]; ```. The `volumes[].host.sourcePath` should instead be `/container_host_root/wf_echo.echo-None-1/cromwell_root` and `/container_host_root/wf_echo.echo-None-1/test1`. The defined `mountPoints` are correct.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-403851615:61,echo,echo,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3744#issuecomment-403851615,11,"['echo', 'error']","['echo', 'echo-None-', 'error']"
Availability,Replace no-longer-available tutum/curl for CI curl test [CROM-6757],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6360:18,avail,available,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6360,1,['avail'],['available']
Availability,Replaced use of Future.foreach with error handling,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2500:36,error,error,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2500,1,['error'],['error']
Availability,"Replacing `awaitCond` with `eventually` we should also get a better failure message than ""timeout expired""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1442:68,failure,failure,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1442,1,['failure'],['failure']
Availability,Report causal exceptions during Google auth failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2090:44,failure,failures,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2090,1,['failure'],['failures']
Availability,Report causal exceptions during Google auth failures. Closes #2090,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2091:44,failure,failures,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2091,1,['failure'],['failures']
Availability,Report invalid string along with error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4161:33,error,error,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4161,1,['error'],['error']
Availability,Report-and-Retry Travis centaur errors,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:32,error,errors,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['error'],['errors']
Availability,"Reported by @asmirnov239. Cromwell 30.2 doesn't seem to short-circuit for logical operators so that the following fails to evaluate when `optional_int` is undefined:. ```wdl ; Integer? optional_int; if (defined(optional_int) && select_first([optional_int]) == 2 ) {; #expresssions to do if optional_int is defined and equal to 2; }; ```. expected behavior: if `optional_int` is undefined, if statement evaluates to false. ; actual behavior: cromwell fails and stops with the error: . >Evaluating defined(gc_low_high_filter_params) && select_first([gc_low_high_filter_params]) == 2 failed: select_first failed. All provided values were empty. While it hasn't been tested, I presume that cromwell doesn't short-circut for the `||` operator...but it should!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3384:475,error,error,475,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3384,1,['error'],['error']
Availability,Reported by @fleharty - it appears that when call caching in symlink mode if the real file no long exists the caching will not fail. . Regardless of the mode we should be robust to cases where we're attempting to call cache but the underlying data is not there,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1671:171,robust,robust,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1671,1,['robust'],['robust']
Availability,"Reproduced the assembly failure in Travis on a different branch that was green last night. i.e., the problem isn't here, so I'm merging this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/582#issuecomment-202836710:24,failure,failure,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/582#issuecomment-202836710,1,['failure'],['failure']
Availability,Requesting re-review because this now includes a less-likely-to-be-made-redundant unit test,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5993#issuecomment-719024604:72,redundant,redundant,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5993#issuecomment-719024604,1,['redundant'],['redundant']
Availability,Resolve downstream Dependencies 0.22 closes #1690,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1697:8,down,downstream,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1697,1,['down'],['downstream']
Availability,Resolve downstream Dependencies develop,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1698:8,down,downstream,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1698,1,['down'],['downstream']
Availability,Resolve downstream dependencies 0.21,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1696:8,down,downstream,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1696,1,['down'],['downstream']
Availability,"Resolves (or at least provides resolution to) one class of ""my workflow never completes"" problems. We will *hopefully* only ever see this error during development, but it seems like a useful backstop condition to enforce in case it ever does happen to live workflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4486:138,error,error,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4486,1,['error'],['error']
Availability,Resolves one of the 2 major error modes I've been tracking,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/130:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/130,1,['error'],['error']
Availability,Restart / Recover should not kick off until services have initialized and Liquibase has run,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1196:10,Recover,Recover,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1196,1,['Recover'],['Recover']
Availability,Restart/recover migration. Closes #1119,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1340:8,recover,recover,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1340,1,['recover'],['recover']
Availability,Results from running this again with proper configuration: 75m down to 25. The entire run on the correct configuration appears to line up with the initial plateau from the `develop` results,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441:63,down,down,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5518#issuecomment-632253441,1,['down'],['down']
Availability,"Results:3:1]: JES Run ID is operations/EPfI6N2AKxi_iI64ku3M2xAgn5eRl70GKg9wcm9kdWN0aW9uUXVldWU; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.CNLoHAndSplitsCaller:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [info] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.PlotACNVResults:3:1]: JesAsyncBackendJobExecutionActor [a3dd8163:case_gatk_acnv_workflow.PlotACNVResults:3:1] Status change from - to Initializing; [2016-10-28 14:38:43,07] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:43,07] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:4:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:14154,error,errors,14154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['error'],['errors']
Availability,"Retry Call Caching, JES Error 404",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/517:24,Error,Error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/517,1,['Error'],['Error']
Availability,Retry HTTP 408 errors during DOS/DRS resolution BT-41,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6059:15,error,errors,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6059,2,['error'],['errors']
Availability,Retry PAPI backend creation failures [CROM-6791],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6487:28,failure,failures,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6487,1,['failure'],['failures']
Availability,Retry Travis downloads,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1791:13,down,downloads,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1791,1,['down'],['downloads']
Availability,Retry on PAPI v1 error code 10.14,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4363:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4363,1,['error'],['error']
Availability,Retry on error code 13 if preemptible,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/478:9,error,error,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/478,1,['error'],['error']
Availability,Retry on failures to get access token,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4272:9,failure,failures,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4272,1,['failure'],['failures']
Availability,Returns 404 errors on JES API calls. Likely due to new API?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/591:12,error,errors,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/591,1,['error'],['errors']
Availability,Revert changes introducing workflow pickup failures [BW-1128],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6697:43,failure,failures,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6697,1,['failure'],['failures']
Availability,Review note: ; ---. May be best to review the docs in their rendered form in case there are markdown errors that aren't obvious in the MD files:. * https://cromwell.readthedocs.io/en/cjl_execution_store_docs/developers/bitesize/workflowExecution/executionStore/; * https://cromwell.readthedocs.io/en/cjl_execution_store_docs/developers/bitesize/workflowExecution/valueStore/; * https://cromwell.readthedocs.io/en/cjl_execution_store_docs/developers/bitesize/workflowExecution/executionAndValueStoreExamples/,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6054:101,error,errors,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6054,1,['error'],['errors']
Availability,Reword error message when failure is due to an invalid return code from the script,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2740:7,error,error,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2740,2,"['error', 'failure']","['error', 'failure']"
Availability,Rewrite error message for type coersion,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1998:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1998,1,['error'],['error']
Availability,"Right now we are trying to display these failures to users in firecloud because they are currently suppressed and you can only see them if you inspect the json. I'd prefer our UI to not show raw JSON, so we want to format the responses. For now, we could format the known cases and show raw JSON otherwise and hope there aren't other raw JSON formats other than this. This is definitely not a showstopper for us, but not just a nice to have, I'd say it's a P2 on a scale from P1 to P3 with P0 for showstoppers. . We could also flatten ourselves, but there's the other case where it says ""timestamp"" and the singular ""failure"", so it's not always consistent.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282842176:41,failure,failures,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2039#issuecomment-282842176,2,['failure'],"['failure', 'failures']"
Availability,"Right now, only images for the `amd64` architecture are available on DockerHub. Given that Docker is well-supported on ASi/ARM chips, it would be helpful to have a native `arm64` image automatically built, rather than needing to run a non-native image unless I build it myself.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7107:56,avail,available,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7107,1,['avail'],['available']
Availability,"Right, I should have mentioned that my WDL pipeline failed. I received errors such as the following:; ```; ...; {; ""causedBy"": [; {; ""causedBy"": [; {; ""message"": ""504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media"",; ""causedBy"": []; }; ],; ""message"": ""Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""[Attempted 1 time(s)] - IOException: Could not read from gs://mccarroll-mocha/cromwell/cromwell-executions/mocha/86d47e9a-5745-4ec0-b4eb-0164f073e5f4/call-idat2gtc/shard-73/rc: 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media""; }; ],; ""message"": ""Workflow failed""; ```; And the pipeline did not proceed (even if all tasks run until that point seemed to be reported as completed successfully).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719:71,error,errors,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5344#issuecomment-760312719,4,"['down', 'error']","['download', 'errors']"
Availability,"Right, but I want to know what happened...not that a tool fails silently... On Tue, Jan 31, 2017 at 11:46 AM, Linlin Yan <notifications@github.com>; wrote:. > I see. My usual ""workaround"" for such fail (but continue) is like this:; >; > task foo {; > 	command {; > 		(echo foo; false) || (echo 1>&2 MSG; true); > 	}; > }; >; > workflow test {; > 	call foo; > }; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276419301>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0izLbBIY8rqgekOd7mNujSZ-DIRiks5rX2VXgaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276424241:268,echo,echo,268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276424241,2,['echo'],['echo']
Availability,"Right. When these issues come up they're part of a tension between people using docker and people using HPC on shared filesystems (who almost always are **not** using docker). . What we've done in the past has been to detect if the context is docker - if so, stick with something more permissive and if not lock it down. YMMV and all of that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394394286:315,down,down,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3721#issuecomment-394394286,1,['down'],['down']
Availability,Robust METADATA_VALUE embiggening. Closes #1607.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1609:0,Robust,Robust,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1609,1,['Robust'],['Robust']
Availability,Robustify Centaur tests to a restarting Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2352:0,Robust,Robustify,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2352,1,['Robust'],['Robustify']
Availability,Robustify aborts to PRECONDITION_FAILED [BT-450] [CROM-6829],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6568:0,Robust,Robustify,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6568,1,['Robust'],['Robustify']
Availability,Robustity of workflowTiming diagrams improved,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/380:0,Robust,Robustity,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/380,1,['Robust'],['Robustity']
Availability,Rollback underscores [24_hotfix],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1902:0,Rollback,Rollback,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1902,1,['Rollback'],['Rollback']
Availability,Rollback workbench-util to 0.6-65bba14,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6481:0,Rollback,Rollback,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6481,1,['Rollback'],['Rollback']
Availability,Root cause is absolutely our old friend `CromwellTestKitSpec`. Jenkins job # 1778; Jenkins job # 1838. Jenkins job # 2607 w/ the following .... ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 210 times over 3.3457535841 minutes. Last failure message: Submitted did not equal Succeeded.; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdlAndAssertOutputs(CromwellTestKitSpec.scala:344); at cromwell.WorkflowOutputsSpec.$anonfun$new$4(WorkflowOutputsSpec.scala:38); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.fo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4457:314,failure,failure,314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4457,1,['failure'],['failure']
Availability,"Run @ruchim 's Travis test in a different mode, or branch, that shuts down and brings back up. Does a count right after to check that the right number of jobs are recovered (no duplicates). TO DO:; - [ ] Make sure you have as many JES jobs as you think you have; - [ ] If not, fix it!; - [ ] If so, yay!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2111:70,down,down,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2111,2,"['down', 'recover']","['down', 'recovered']"
Availability,Run in local with docker - error waiting for container read: operation timed out,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370:27,error,error,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370,1,['error'],['error']
Availability,Run mode: executing WDL from directory relative to current fails with import resolver error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4052:86,error,error,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4052,1,['error'],['error']
Availability,"Runnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; :hmmm:. chrisl [4:50 PM]; that sort of looks like the kind of error message I put in when I’m 99% sure a situation is impossible…. mcovarr [4:50 PM]; workflows that are picked up but have old heartbeats will look eligible for pickup, but they won't actually get run and you'll see that message. chrisl [4:50 PM]; oh, or that :slightly_smiling_face:. kshakir [4:51 PM]; I see 13 of these:; ```; 024bf23f-b7a3-4ede-bddf-938321ac570f; 26707981-d32b-4814-ba1f-4e5f27f739dc; 52fe6d61-2ba8-4c79-8a50-e365b355e36b; 5cbeca9f-c686-45a9-ab57-167379029964; 627a48a3-1584-42de-9b57-ee7a859b08d1; 6ed1070c-e478-47f2-8ea9-7ccc656bbba9; 70786146-ac4e-4d26-9906-ba211fde03f9; 8de76a93-6b66-4c29-a2fe-31e6cd1f969e; 8f07ade2-0a6d-40df-b886-cf99e3a1ed13; 9bda3e3d-1e17-4406-87e3-9ec7f71f4822; cb4b3331-193a-4c22-a95d-40f1ac9b53d6; dc9ded6f-463f-4cb1-a71a-0503c53f702a; f6644044-f4af-412a-a978-12ff080af3e1; ```; (edited). mcovarr [4:51 PM]; yeah that's from the 2/3 of horizontal Cromwell we implemented. mcovarr [4:52 PM]; but not sure why this is happening in this specific case. kshaki",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3673:2041,error,error,2041,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3673,1,['error'],['error']
Availability,"Running 3step workflow on JES with a workflow options file that looks like this. ``` json; {; ""monitoring_script"": ""gs://sfrazer-dev/monitor.sh""; }; ```. the `monitor.sh` script contains:. ``` bash; echo this; echo is; echo a; echo test; ```. It seems that for my three invocations of this workflow, they all had the correct data in `monitoring.log` for the `ps` sub-command, but not the other two commands.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1158:199,echo,echo,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158,4,['echo'],['echo']
Availability,"Running Centaur has several ""noop"" tasks. When passed to AWS Batch, we see failing tests and errors from the AWS SDK that the Command field cannot have empty strings. Assuming Cromwell expects no output/return code 0, this can be special-cased in AWSBatchJob.scala.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3737:93,error,errors,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3737,1,['error'],['errors']
Availability,"Running a CWL workflow with local relative imports seems to fail. This is possibly related to #4308 but unsure. . Running with cromwell 36. . Process looks like the following; ```; $ git clone https://github.com/dockstore-testing/dockstore-workflow-md5sum-unified.git; $ cd dockstore-workflow-md5sum-unified; $ cwltool checker_workflow_wrapping_workflow.cwl md5sum.json; /usr/local/bin/cwltool 1.0.20180403145700; <snip>; Final process status is success; $ wget https://github.com/broadinstitute/cromwell/releases/download/36/cromwell-36.jar; $ java -jar cromwell-36.jar run https://raw.githubusercontent.com/dockstore-testing/dockstore-workflow-md5sum-unified/develop/checker_workflow_wrapping_workflow.cwl --inputs md5sum.json; <snip>; [2018-11-07 14:34:25,13] [info] Pre-Processing /tmp/cwl_temp_dir_7264114231127246601/cwl_temp_file_a3cb6a14-3672-4132-8a24-2e0a4e66ff96.cwl; [2018-11-07 14:34:34,94] [error] WorkflowManagerActor Workflow a3cb6a14-3672-4132-8a24-2e0a4e66ff96 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; running cwltool on file /tmp/cwl_temp_dir_7264114231127246601/cwl_temp_file_a3cb6a14-3672-4132-8a24-2e0a4e66ff96.cwl failed with Traceback (most recent call last):; File ""/home/dyuen/dockstore_tools/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 12, in cwltool_salad; File ""/home/dyuen/dockstore_tools/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/cwltool/load_tool.py"", line 279, in validate_document; File ""/home/dyuen/dockstore_tools/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/schema_salad/ref_resolver.py"", line 915, in resolve_all; File ""/home/dyuen/dockstore_tools/dockstore-workflow-md5sum-unified/cromwell-36.jar/Lib/schema_salad/ref_resolver.py"", line 1087, in validate_links; schema_salad.validate.V",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4366:514,down,download,514,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4366,2,"['down', 'error']","['download', 'error']"
Availability,"Running a slightly cleaned up version of your example:. ```; task my_task {; String a; String b = a + ""/"" + ""annotation.fa"". command {; echo ${b}; }; }. workflow my_workflow {; call my_task { input: a = ""my_path"" }; }; ```. I see this command go by in the logs:. ```; ... INFO - BackgroundConfigAsyncJobExecutionActor [...]: `echo my_path/annotation.fa`. ```. So this appears to be working as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2505#issuecomment-326050555:136,echo,echo,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2505#issuecomment-326050555,2,['echo'],['echo']
Availability,"Running in server mode, jobs that have localization error become immortal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/922:52,error,error,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/922,1,['error'],['error']
Availability,"Running locally a scatter job with 2102 jobs causes an out-of-memory error after finishing job 1427 (reproduced twice) . The command that I run was using the wrapper script from **brew**:. ```; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. Where the local configuration looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false; workflow-options.workflow-log-dir: ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/workflow-logs"". # Allows re-use of existing results for jobs you've already run; call-caching.enabled: true. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; # set the root directory to the run; root = ""/Volumes/Temp/E43CEE02/data/freqs/haf/base-all/w100000_2.0x/execution""; filesystems.local {; ## do not allow copy (too huge files); ## prefer hard-links, to don't remove data and kept analysis intact; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:69,error,error,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['error'],['error']
Availability,"Running the AWS backend with the latest develop Cromwell gives errors like this:. java.lang.Exception: Job 2de677d8-0842-4e17-ab26-288ffc3d8aaa failed for reason: unknown error: . {JobName: cromwell-job,JobId: 2de677d8-0842-4e17-ab26-288ffc3d8aaa,JobQueue: arn:aws:batch:us-east-1:369228243869:job-queue/mcovarr-queue-nouveau,Status: FAILED,StatusReason: **Container.image contains invalid characters.**,CreatedAt: 1488362254138,DependsOn: [],JobDefinition: arn:aws:batch:us-east-1:369228243869:job-definition/cromwell-job-definition:125,Parameters: {},Container: {**Image: library/python@sha256:d23845e4757f13266b42877c25b845e455127b85ec12e5d551bec5d8162e7cd4**,Vcpus: 1,Memory: 1907,Command: [/bin/sh, -c, /bin/bash /usr/share/iodir/91a31bc2-ad38-4853-8bd6-fa456c66021b/cromwell-executions/PairedEndSingleSampleWorkflow/91a31bc2-ad38-4853-8bd6-fa456c66021b/PairedEndSingleSampleWorkflow-CheckFinalVcfExtension-NA-1/script > /usr/share/iodir/91a31bc2-ad38-4853-8bd6-fa456c66021b/cromwell-executions/PairedEndSingleSampleWorkflow/91a31bc2-ad38-4853-8bd6-fa456c66021b/PairedEndSingleSampleWorkflow-CheckFinalVcfExtension-NA-1/stdout 2> /usr/share/iodir/91a31bc2-ad38-4853-8bd6-fa456c66021b/cromwell-executions/PairedEndSingleSampleWorkflow/91a31bc2-ad38-4853-8bd6-fa456c66021b/PairedEndSingleSampleWorkflow-CheckFinalVcfExtension-NA-1/stderr < /dev/null || echo -1 > /usr/share/iodir/91a31bc2-ad38-4853-8bd6-fa456c66021b/cromwell-executions/PairedEndSingleSampleWorkflow/91a31bc2-ad38-4853-8bd6-fa456c66021b/PairedEndSingleSampleWorkflow-CheckFinalVcfExtension-NA-1/rc],Volumes: [{Host: {SourcePath: /usr/share/iodir},Name: cromwell-volume}],Environment: [],MountPoints: [{ContainerPath: /usr/share/iodir,ReadOnly: false,SourceVolume: cromwell-volume}],Ulimits: [],}}. For this reason found in the ECS javadocs:. ```; Amazon ECS task definitions currently only support tags as image identifiers within a specified repository; (and not <code>sha256</code> digests); ```. Of course it would be preferable",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2044:63,error,errors,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2044,2,['error'],"['error', 'errors']"
Availability,"Running the `cd` command anywhere in the command block causes output files to not be found and for the workflow to throw an error:. [2018-01-08 15:05:21,44] [error] WorkflowManagerActor Workflow 6575132f-3c01-498f-ac7a-2a418568f002 failed (during ExecutingWorkflowState): Could not process output, file not found: /Users/jonn/Development/cromwell/cromwell-executions/Funcotator/6575132f-3c01-498f-ac7a-2a418568f002/call-MakeItFunky/execution/out.vcf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3116:124,error,error,124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3116,2,['error'],['error']
Availability,"Running the workflow; ```; version 1.0. workflow test {; input {; Array[File]? y = [""some/file/path.txt""]; }. output {; Array[File] x = select_first([y, []]); }; }; ```; on latest `develop` seems to work fine, producing outputs; ```; {; ""test.x"": [""some/file/path.txt""]; }; ```. Is this an accurate simplification of your problem case?. There is a good chance this bug is fixed in 37 onward as a result of https://github.com/broadinstitute/cromwell/pull/4324; >Fixed a regression in Cromwell 36 that could cause operations on empty arrays to fail with a spurious type error. I suspect your workflow got stuck after failing because the `WomArray` code [throws an exception](https://github.com/broadinstitute/cromwell/blob/develop/wom/src/main/scala/wom/values/WomArray.scala#L37) that screws up control flow. I believe this is a ""this should never happen"" case so we did not bother upgrading it to our fancier error handling that encodes failures in the type system to achieve predictable behavior.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247:568,error,error,568,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4755#issuecomment-474440247,3,"['error', 'failure']","['error', 'failures']"
Availability,"Running this WDL (with a wider scatter count... start with 250) fails with a database persistence error like:. > The size of BLOB/TEXT data inserted in one transaction is greater than 10% of redo log size. Increase the redo log size using innodb_log_file_size. On Google, the redo size is fixed at 536,870,912 bytes, which means the maximum BLOB/TEXT size is ~53mb. ```; task FileSpam {; String sample_name = ""DeliciousFileSpam""; Int index. command <<<; mkdir ${sample_name}_${index}; for i in `seq 0 900`; do; echo $i > ${sample_name}_${index}_$i.txt; echo $i > ${sample_name}_${index}/$i.txt; done; >>>; runtime {; docker: ""ubuntu:latest""; memory: ""3 GB""; cpu: ""1""; disks: ""local-disk 5 HDD""; preemptible: 3; }; output {; Array[File] outer = glob(""${sample_name}_${index}_*""); Array[File] inner = glob(""${sample_name}_${index}/*""); }; }. task MatrixRotation {; Array[Array[String]] input_matrix. command <<<; python <<CODE; import csv; import sys; with open('${write_tsv(input_matrix)}') as tsv_in:; input_matrix = [line.strip().split('\t') for line in tsv_in]; final_matrix = [["""" for x in range(len(input_matrix))] for y in range(len(input_matrix[0]))]; for x in range(len(input_matrix)):; for y in range(len(input_matrix[0])):; final_matrix[y][x] = input_matrix[x][y]; with open(""output.tsv"", ""w"") as tsv_out:; # lineterminator is a workaround for a cromwell bug that doesnt allow for '\r\n' line endings which this outputs by default; writer = csv.writer(tsv_out, delimiter='\t', lineterminator='\n'); writer.writerows(final_matrix); CODE; >>>. runtime {; docker: ""python:2.7""; memory: ""1 GB""; preemptible: 3; }. output {; File out = ""output.tsv""; Array[Array[String]] output_matrix = read_tsv(out); }; }. workflow DeliciousFileSpam {; Array[Int] indexing_list = [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,; 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,; 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,; 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,; 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,; 51, 52, 53, 54, 55, 56, 57, 58, 59",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/910:98,error,error,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/910,3,"['echo', 'error']","['echo', 'error']"
Availability,"Running this WDL (with a wider scatter count... start with 250) fails with a database persistence error like:. > The size of BLOB/TEXT data inserted in one transaction is greater than 10% of redo log size. Increase the redo log size using innodb_log_file_size. Which we can't do with CloudSQL on Google. task FileSpam {; String sample_name = ""DeliciousFileSpam""; Int index. command <<<; mkdir ${sample_name}_${index}; for i in `seq 0 900`; do; echo $i > ${sample_name}_${index}_$i.txt; echo $i > ${sample_name}_${index}/$i.txt; done. > > > runtime {; > > > docker: ""ubuntu:latest""; > > > memory: ""3 GB""; > > > cpu: ""1""; > > > disks: ""local-disk 5 HDD""; > > > preemptible: 3; > > > }; > > > output {; > > > Array[File] outer = glob(""${sample_name}_${index}__""); > > > Array[File] inner = glob(""${sample_name}_${index}/_""); > > > }; > > > }. task MatrixRotation {; Array[Array[String]] input_matrix. ```; command <<<; python <<CODE; import csv; import sys; with open('${write_tsv(input_matrix)}') as tsv_in:; input_matrix = [line.strip().split('\t') for line in tsv_in]; final_matrix = [["""" for x in range(len(input_matrix))] for y in range(len(input_matrix[0]))]; for x in range(len(input_matrix)):; for y in range(len(input_matrix[0])):; final_matrix[y][x] = input_matrix[x][y]; with open(""output.tsv"", ""w"") as tsv_out:; # lineterminator is a workaround for a cromwell bug that doesnt allow for '\r\n' line endings which this outputs by default; writer = csv.writer(tsv_out, delimiter='\t', lineterminator='\n'); writer.writerows(final_matrix); CODE; >>>. runtime {; docker: ""python:2.7""; memory: ""1 GB""; preemptible: 3; }. output {; File out = ""output.tsv""; Array[Array[String]] output_matrix = read_tsv(out); }; ```. }. workflow DeliciousFileSpam {; Array[Int] indexing_list = [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,; 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,; 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,; 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,; 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,; 51, 52, 53, 54, 55, 56, ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/911:98,error,error,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/911,3,"['echo', 'error']","['echo', 'error']"
Availability,"Running this on Cromwell 35 on PAPI v2 returns this error:; ```Task w.t:NA:1 failed. The job was stopped before the command finished. PAPI error code 5. Execution failed: selecting zone: no regions/zones match request```. AC: For both the PAPI v1/v2 backends, add more context to this error. Something along the lines of...; ```Unable to start job because the zones defined in the runtime parameter zones: ""$zones"" doesn't match zones/regions supported by GCE. Please resubmit the job with a list of supported zones/regions by consulting a list of options here: https://cloud.google.com/compute/docs/regions-zones/```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-424956804:52,error,error,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-424956804,3,['error'],['error']
Availability,"Running with release 24. As a minimal example, the following WDL:; ```; import ""foo.wdl"" as missing. workflow testMe {; 	call doNothing; }; task doNothing {; 	command {}; }; ```. and a zip file containing `baz.wdl` but not `foo.wdl`:; ```; [conradL@qimr13054 ~]$ unzip -l bar.zip ; Archive: bar.zip; Length Date Time Name; --------- ---------- ----- ----; 0 02-07-2017 13:47 bar/; 0 02-07-2017 13:47 bar/baz.wdl; --------- -------; 0 2 files; ```. submit to server:; ```; [conradL@qimr13054 ~]$ curl http://localhost:8000/api/workflows/V1 -FwdlSource=@badImport.wdl -FwdlDependencies=@bar.zip; {; ""id"": ""b701aafd-445c-4f49-8ba6-452d56e69fd3"",; ""status"": ""Submitted""; }; ```. causes the server process to die with this in the logs:; ```; 2017-02-07 13:52:41,842 cromwell-system-akka.actor.default-dispatcher-33 ERROR - guardian failed, shutting down system; wdl4s.exception.ValidationException: Failed to import workflow foo.wdl.:; File not found /tmp/640585481854205084.zip4511378926145376874/bar/foo.wdl; 	at wdl4s.WdlNamespace$.wdl4s$WdlNamespace$$tryResolve$1(WdlNamespace.scala:198); 	at wdl4s.WdlNamespace$$anonfun$17.apply(WdlNamespace.scala:208); 	at wdl4s.WdlNamespace$$anonfun$17.apply(WdlNamespace.scala:207); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.immutable.List.foreach(List.scala:381); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.immutable.List.map(List.scala:285); 	at wdl4s.WdlNamespace$.apply(WdlNamespace.scala:207); 	at wdl4s.WdlNamespace$.wdl4s$WdlNamespace$$load(WdlNamespace.scala:177); 	at wdl4s.WdlNamespace$.loadUsingSource(WdlNamespace.scala:173); 	at wdl4s.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:542); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$validateNamespaceWithImports$1.apply(MaterializeWorkflowDescriptorActor.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1958:810,ERROR,ERROR,810,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1958,2,"['ERROR', 'down']","['ERROR', 'down']"
Availability,Running workflows should heartbeat to workflow store,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3341:25,heartbeat,heartbeat,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3341,1,['heartbeat'],['heartbeat']
Availability,S3 File object as task output error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6716:30,error,error,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6716,1,['error'],['error']
Availability,S3 filesystem retry download,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946:20,down,download,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946,1,['down'],['download']
Availability,S3 permissions errors using Cromwell 37+,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4740:15,error,errors,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740,1,['error'],['errors']
Availability,S3 permissions errors using Cromwell 37+ BA-4740 (#4740),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5088:15,error,errors,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5088,1,['error'],['errors']
Availability,"SBT fails with an error that looks very related; ```; should evaluate a sep expression containing a sub-call to prefix correctly *** FAILED *** (50 milliseconds). [info] EvaluatedValue(WomString(-i a -i b -i c),List()) was not equal to EvaluatedValue(WomString(a b c),List()) (ErrorOrAssertions.scala:11). [info] org.scalatest.exceptions.TestFailedException:; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656432981:18,error,error,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656432981,2,"['Error', 'error']","['ErrorOrAssertions', 'error']"
Availability,"SBT failure is another `No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.` for those keeping track, the `SingleWorkflowRunnerActor` also failed previous to that",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-526379078:4,failure,failure,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150#issuecomment-526379078,1,['failure'],['failure']
Availability,SFS job recovery. Closes #1162 Closes #666,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1319:8,recover,recovery,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1319,1,['recover'],['recovery']
Availability,"SGE defaults to csh, and at least in my environment, even with #!/bin/bash bash was not launched in the job. This could well just be a quirk of my environment, and I am happy if you decide to not take this change. However it seems like a more generally robust default for different environments.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3489:253,robust,robust,253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3489,1,['robust'],['robust']
Availability,SGE resource 'cpu' binding error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3805:27,error,error,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805,1,['error'],['error']
Availability,"STAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 2019-01-31 20:30:56,617 INFO - changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne: Successfully released change log lock; 2019-01-31 20:30:56,631 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_I",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:1671,ERROR,ERROR,1671,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['ERROR'],['ERROR']
Availability,"SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. [Configuration file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/aws.conf). Running this workflow on AWS Batch (with cromwell-36.jar) consistently fails at the same point each time. . It gets through most (looks like all but one iteration) of the scatter loop that calls the `BaseRecalibrator` task. Then cromwell just sits for a long time (~1hr) with no Batch jobs running (or runnable or starting). Then cromwell calls the `RegisterJobDefinition` API of AWS Batch, and it always fails with the following error message:. ```; 2018-12-15 23:39:03,360 cromwell-system-akka.dispatchers.backend-dispatcher-258 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; software.amazon.awssdk.services.batch.model.ClientException: arn:aws:batch:us-west-2:064561331775:job-definition/PreProcessingForVariantDiscovery_GATK4-BaseRecalibrator not found or versions do not match (Service: null; Status Code: 404; Request ID: 9914238b-00c2-11e9-a13d-cdc28a8016c8); ```. Looking at cloudtrail, here is the event associated with that request ID:. [Event](https://gist.github.com/dtenenba/909f16e720a01b00a736cf6e60f7083a). If I pull out just the contents of the `requestParameters` section and call RegisterJobDefinition using the AWS CLI as follows, it works fine. ```; aws batch register-job-definition --cli-input-json file://event_history.json; {; ""jobDefinitionArn"": ""arn:aws:batch:us-west-2:064561331775:job-definition/PreProcessingForVariantDiscovery_GATK4-BaseRecalibrator:207"",; ""jobDefinitionName"": ""PreProcessingForVariantDiscovery_GATK4-BaseRecalibrator"",; ""revision"": 207; }; ```. Each run of this workflow creates many new revisions of this jo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496:2028,Error,Error,2028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496,1,['Error'],['Error']
Availability,"Sam is now up and accessible. Complete the wiring process for CromIAM to be able to talk to a live Sam. @cjllanwarne Can you fill in the blanks on what this entails?. (EDIT below by cjllanwarne):. - Have a look at the `handleRequest` method in `SamActor`. It's called by the actor's `receive`.; - Right now, the placeholders complete `Promises` after a set time. You'll want to message SAM, wait for a response, and then complete them more appropriately.; - NB in case you're wondering, the `Promise/Future` stuff is because the CromIamApiService is not an actor and so cannot receive responses in the usual actor-y way. Feel free to make it better, I ended up doing this because the actor-y way was adding in enormous/unnecessary overheads in terms of boilerplate... YMMV.; - FWIW even if `CromIamApiService` isn't an actor, I still suspect `SamActor` will end up spinning up individual worker actors. It's just the interface back to the service which isn't messages.; - The SamActor is parameterized with the `userIdHeader`. SamActorRequests will both have a `userIdtoken` included (oops, `RegisterWorkflow` calls it `user` but it's the same thing). When you make your request to same, you'll need to add the appropriate header (called whatever the actor was parameterized with) with the user token's value.; - Success or Failure completions, everything else *should* be wired up correctly already. You might want to double check that (especially for SAM rejections). If the intrepid person picking up this ticket is not me (@cjllanwarne), I'm happy to make this make more sense in person, if required.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2469:1324,Failure,Failure,1324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2469,1,['Failure'],['Failure']
Availability,"Same error, maybe you will see something in this full output from start of server:. ```; [info] SHA-1: dceb5f4f7fdf8646b3aa9ed1257ceb7f1b35f3c7; [info] Packaging /Users/tdyar/workspace/cromwell/server/target/scala-2.12/cromwell-33-7a41a75-SNAP.jar ...; [info] Done packaging.; [info] Done packaging.; [info] Done packaging.; [success] Total time: 189 s, completed Jun 7, 2018 12:13:55 PM; US-M094110:cromwell tdyar$ nano ~/workspace/cromwell-31/my-cromwell.conf; US-M094110:cromwell tdyar$ java -Dconfig.file=/Users/tdyar/workspace/cromwell-31/my-cromwell_test_develop.conf -jar /Users/tdyar/workspace/cromwell/server/target/scala-2.12/cromwell-33-7a41a75-SNAP.jar server; Picked up _JAVA_OPTIONS: -Dswing.systemlaf=com.sun.javax.swing.plaf.metal.CrossPlatformLookAndFeel; 2018-06-07 12:16:03,575 INFO - Running with database db.url = jdbc:hsqldb:mem:3922af96-263f-4846-9018-fb0a4968d4ab;shutdown=false;hsqldb.tx=mvcc; 2018-06-07 12:16:09,027 INFO - Successfully acquired change log lock; 2018-06-07 12:16:10,243 INFO - Creating database history table with name: PUBLIC.DATABASECHANGELOG; 2018-06-07 12:16:10,246 INFO - Reading from PUBLIC.DATABASECHANGELOG; 2018-06-07 12:16:10,417 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; 2018-06-07 12:16:10,419 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table EXECUTION created; 2018-06-07 12:16:10,420 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table JES_JOB created; 2018-06-07 12:16:10,421 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table LOCAL_JOB created; 2018-06-07 12:16:10,422 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: ChangeSet changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer ran successfully in 6ms; 2018-06-07 12:16:10,431 INFO - changelog.xml: changesets/db_sch",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:5,error,error,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['error'],['error']
Availability,"Same symptom as https://github.com/broadinstitute/cromwell/pull/7385, different cause. This is @sjfleming's report in the ticket. After:; ```; INFO - WorkflowManagerActor: Workflow b23e299b-5fbd-4b12-8389-9aa73321fba1 failed (during ExecutingWorkflowState): cromwell.engine.workflow.lifecycle.execution.WdlRuntimeException: Failed to evaluate 'break_with_stderr.load_data_csv' (reason 1 of 2): Evaluating select_first([stdout(), stderr()]) failed: stdout is not implemented at the workflow level, Failed to evaluate 'break_with_stderr.load_data_csv' (reason 2 of 2): Evaluating select_first([stdout(), stderr()]) failed: stderr is not implemented at the workflow level; INFO - WorkflowManagerActor: Workflow actor for b23e299b-5fbd-4b12-8389-9aa73321fba1 completed with status 'Failed'. The workflow will be removed from the workflow store.; ```; ```; {; 	""status"": ""Failed"",; 	""id"": ""b23e299b-5fbd-4b12-8389-9aa73321fba1""; }; ```. Before:; ```; ERROR - stdout is not implemented at the workflow level; java.lang.UnsupportedOperationException: stdout is not implemented at the workflow level; 	at cromwell.core.io.WorkflowCorePathFunctionSet.fail(CorePathFunctionSet.scala:12); 	at cromwell.core.io.WorkflowCorePathFunctionSet.stdout(CorePathFunctionSet.scala:20); 	at wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators$$anon$1.evaluateValue(EngineFunctionEvaluators.scala:54); 	at wdl.transforms.base.linking.expression.values.EngineFunctionEvaluators$$anon$1.evaluateValue(EngineFunctionEvaluators.scala:48); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateValue$(ValueEvaluator.scala:10); 	at wdl.model.draft3.graph.expression.ValueEvaluator$ops$$anon$1.evaluateValue(ValueEvaluator.scala:10); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.evaluateValue(package.scala:75); 	at wdl.draft3.transforms.linking.expression.values.package$$anon$1.e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7386:946,ERROR,ERROR,946,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7386,1,['ERROR'],['ERROR']
Availability,"Same thing with this error: ; ```; could not download return code file, retrying; com.google.cloud.storage.StorageException: 500 Internal Server Error; Backend Error; 	at com.google.cloud.storage.spi.DefaultStorageRpc.translate(DefaultStorageRpc.java:190); 	at com.google.cloud.storage.spi.DefaultStorageRpc.read(DefaultStorageRpc.java:482); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); 	at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); 	at com.google.cloud.RetryHelper.doRetry(RetryHelper.java:179); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:244); 	at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); 	at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:85); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); 	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); 	at java.nio.file.Files.read(Files.java:3105); 	at java.nio.file.Files.readAllBytes(Files.java:3158); 	at better.files.File.loadBytes(File.scala:163); 	at better.files.File.byteArray(File.scala:166); 	at better.files.File.contentAsString(File.scala:206); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor$$anonfun$cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$1.apply(JesAsyncBackendJobExecutionActor.scala:110); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$backend$impl$jes$JesAsyncBackendJobExecutionActor$$returnCodeContents$lzycompute(JesAsyncBackendJobExecutionActor.scala:110); 	at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.cromwell$back",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762:21,error,error,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1782#issuecomment-267025762,4,"['Error', 'down', 'error']","['Error', 'download', 'error']"
Availability,"Saw DSDEEPB-1549 was already filed. Minor time scaling issue for jenkins, then :+1: as far as I'm concerned. Btw, anyone have any idea what's up with travis-to-coveralls integration's SSL errors? We don't have coverage results anymore? :crying_cat_face:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/224#issuecomment-146276086:188,error,errors,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/224#issuecomment-146276086,1,['error'],['errors']
Availability,"Saw it again on C26 snowflake:. ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Read timed out""; }],; ""message"": ""Google credentials are invalid: Read timed out""; }]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886#issuecomment-300513496:38,failure,failures,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886#issuecomment-300513496,1,['failure'],['failures']
Availability,"Scattered workflow finished without error, but the output file is empty in the bucket",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006:36,error,error,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006,1,['error'],['error']
Availability,"Scott's on vaca and hasn't chimed in on the ""I'm available"" thread - you might want to recalibrate your wheel :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/341#issuecomment-166621309:49,avail,available,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/341#issuecomment-166621309,1,['avail'],['available']
Availability,Scratch that. Fresh git clone and build cleared the error for the non-AWS config. I must have screwed up the switch to develop from aws_backend branch. So my error **does** seem to be related to AWS code somehow! Sorry for the whipsaw...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395489025:52,error,error,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395489025,2,['error'],['error']
Availability,See #2970 for the current error case which we now think should be treated as preemptible.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2978:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2978,1,['error'],['error']
Availability,"See [this forum post](http://gatkforums.broadinstitute.org/firecloud/discussion/9851/cryptic-failure-messages). >message: Task slick.basic.BasicBackend$DatabaseDef$$anon$2@1fc7758d rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@39871354[Running, pool size = 200, active threads = 200, queued tasks = 1000, completed tasks = 1543924]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2381:93,failure,failure-messages,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2381,1,['failure'],['failure-messages']
Availability,"See [this travis build](https://travis-ci.org/broadinstitute/cromwell/builds/388562204) for an example where Papi V1 was retrying preemption and Papi V2 was failing. ```; 9605 2018-06-06 11:02:59,838 cromwell-system-akka.dispatchers.engine-dispatcher-32 ERROR - WorkflowManagerActor Workflow c9dfd3ed-8be8-413f-af46-4692142b3248 failed (during ExecutingWorkflowState): Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9606 Execution failed: action 14: unexpected exit status 1 was not ignored; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3732:254,ERROR,ERROR,254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"See below for motivating use case WDL (now a test). The throwaways need OGINs to reference the `i`, but we weren't recording the OGINs that they were making. The upshot was that multiple usages of the same variable in a nested scope was producing multiple OGINs for the same value, and that was causing the ""duplicate FQN"" errors to trigger. ```; workflow nested_lookups {; Int i = 27; if(true) {; Int? throwaway = i # Make sure this 'i' OGIN doesn't duplicate the nested m1's 'i' OGIN; Int? throwaway2 = i # Make sure this 'i' OGIN doesn't duplicate the nested m1's 'i' OGIN; if(true) {; if(true) {; call mirror as m1 { input: i = i}; }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2855:323,error,errors,323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2855,1,['error'],['errors']
Availability,"See https://github.com/broadinstitute/cromwell/issues/1736. It looks like there's been a regression in the versioning. The last jar was named ""cromwell-24.jar"" (https://github.com/broadinstitute/cromwell/releases/download/24/cromwell-24.jar) and according to #1736 that was intentional. The new jar is named ""cromwell-0.25.jar"" not ""cromwell-25.jar"" (https://github.com/broadinstitute/cromwell/releases/download/25/cromwell-0.25.jar). Is this a mistake? Shouldn't the file be named cromwell-25.jar?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2052:213,down,download,213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2052,2,['down'],['download']
Availability,"See https://github.com/broadinstitute/cromwell/pull/4022 - originally closed because I wasn't sure it was the right thing, and I didn't see a way to test it. I'm now more certain it's worth trying. Even if this doesn't actually fix the deadlock and we need to use one of the other solutions, I think this may still be worth having because it will save the RDBMS a lot of work coordinating transactions. [`autocommit` documentation](https://dev.mysql.com/doc/refman/8.0/en/innodb-autocommit-commit-rollback.html); [Slick transactions and pinned sessions](http://slick.lightbend.com/doc/3.2.0/dbio.html#transactions-and-pinned-sessions). Acceptance criteria:; - [ ] Not a performance regression - heartbeats are still efficiently written to the DB in batches; - [ ] Prevents the deadlock",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4249:497,rollback,rollback,497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4249,2,"['heartbeat', 'rollback']","['heartbeats', 'rollback']"
Availability,"See linked epic for related postmortem. * happens If # of VMs > maximum number of IPs available; * throttling of API calls to GCE prevents destruction of finished VMs (DB: t sure ; * Finished VMs are holding IP addresses, preventing new calls from obtaining them. # IP Exhaustion. * New networks are in /20 CIDR block, allowing 2^12 = 4096 IP addresses; * PAPI v1 is limited to default network; * PAPI v2 can specify network per project (TODO: confirm per project? per call?); * PAPI v2 non-default networks can use /16 and thus 65K IP addresses. # Context. * Can only occur when quota increase is requested to put max # cpus > available IPs. # Mechanics. * Pass in network name as workflow option from rawls -> cromwell. # Questions. * What caused the API throttling in the first place? We allocated too many VM's and just generally sent too much traffic to GCE?; * Does PAPI v2 address the IP exhaustion situation? Otherwise we have to manage the resources for it.; * Migration of old projects to use PAPI v2 ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3665:86,avail,available,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3665,2,['avail'],['available']
Availability,"Seeing issues around OWL parsing like #4210, may need to retry more. . ```text; Error loading schemas: Problem parsing file:./cwl/src/test/resources/cwl/ontology/EDAM.owl Could not parse ontology. Either a suitable parser could not be found, or parsing failed. See parser logs below for explanation. The following parsers were tried: 1) org.semanticweb.owlapi.manchestersyntax.parser.ManchesterOWLSyntaxOntologyParser@35c1a1aa 2) org.semanticweb.owlapi.rdf.turtle.parser.TurtleOntologyParser@4e879e3e 3) org.semanticweb.owlapi.oboformat.OBOFormatOWLAPIParser@6bd28ad6 4) org.semanticweb.owlapi.krss2.parser.KRSS2OWLParser@5326aaf7 Detailed logs: -------------------------------------------------------------------------------- Parser: org.semanticweb.owlapi.manchestersyntax.parser.ManchesterOWLSyntaxOntologyParser@35c1a1aa Stack trace: Encountered '<?xml version=""1.0""?>' at line 1 column 1. Expected either 'Ontology:' or 'Prefix:' (Line 1) org.semanticweb.owlapi.manchestersyntax.parser.ManchesterOWLSyntaxOntologyParser.parse(ManchesterOWLSyntaxOntologyParser.java:81) uk.ac.manchester.cs.owl.owlapi.OWLOntologyFactoryImpl.loadOWLOntology(OWLOntologyFactoryImpl.java:193) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.load(OWLOntologyManagerImpl.java:1071) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntology(OWLOntologyManagerImpl.java:1033) uk.ac.manchester.cs.owl.owlapi.OWLOntologyManagerImpl.loadOntologyFromOntologyDocument(OWLOntologyManagerImpl.java:974) cwl.ontology.Schema$.$anonfun$loadOntologyFromIri$5(Schema.scala:155) cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:85) cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:336) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:357) cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:303) -------------------------------------------------------------------------------- Parser: org.semanticweb.owlapi.rdf.turtle.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4372:80,Error,Error,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4372,1,['Error'],['Error']
Availability,"Seeing the same thing on grid engine with cromwell-37. grid engine job dispatches and completes just fine but cromwell throws the following:. ```; [2019-02-13 22:18:19,77] [info] DispatchedConfigAsyncJobExecutionActor [bc35173dmyWorkflow.myTask:NA:1]: job id: 8550357; [2019-02-13 22:18:19,78] [info] DispatchedConfigAsyncJobExecutionActor [bc35173dmyWorkflow.myTask:NA:1]: Status change from - to Running; [2019-02-13 22:18:20,81] [warn] DispatchedConfigAsyncJobExecutionActor [bc35173dmyWorkflow.myTask:NA:1]: Fatal exception polling for status. Job will fail.; java.util.concurrent.ExecutionException: Boxed Error; 	at scala.concurrent.impl.Promise$.resolver(Promise.scala:83); 	at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); 	at scala.concurrent.impl.Promise$KeptPromise$.apply(Promise.scala:402); 	at scala.concurrent.Promise$.fromTry(Promise.scala:138); 	at scala.concurrent.Future$.fromTry(Future.scala:635); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.standard.StandardAsyncExecutionActor.pollStatusAsync$(StandardAsyncExecutionActor.scala:697); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatusAsync(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll(StandardAsyncExecutionActor.scala:989); 	at cromwell.backend.standard.StandardAsyncExecutionActor.poll$(StandardAsyncExecutionActor.scala:983); 	at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:211); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustPoll$1(AsyncBackendJobExecutionActor.scala:76); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cro",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710:611,Error,Error,611,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560#issuecomment-463475710,1,['Error'],['Error']
Availability,Seems like I am having a related issue: https://github.com/openwdl/wdl/issues/339. When using `write_json` with an `Array[Object]` it results in this error: . ```; Failed to process expression 'write_json(list)' (reason 1 of 1): ; Invalid parameter 'IdentifierLookup(list)'. Expected 'Object' but got 'Array[Object]'; ```. The spec says this would be allowed [here](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#arrayobject-serialization-using-write_json). Although from @aednichols comment seems like this is not currently working?. I was using WDL v1.0 and Cromwell v47.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4219#issuecomment-545564714:150,error,error,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4219#issuecomment-545564714,1,['error'],['error']
Availability,"Seems to be a transient (GCS/JES) issue as it was reproducible multiple times the day the error was first seen. Since then it has not been reproducible. Closing issue, will reopen if it pops up again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/932#issuecomment-223980068:90,error,error,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/932#issuecomment-223980068,1,['error'],['error']
Availability,"Seems to generally improve centaur robustness, although it still drops the ball sometimes and the test script just exits for no apparent reason.; Fixes the `cwl_cache_between_workflows` and `cwl_cache_within_workflows` tests; Unit tests transient failures remain",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3827:35,robust,robustness,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3827,2,"['failure', 'robust']","['failures', 'robustness']"
Availability,"Seen in [Jenkins build 577](https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/577/) but also in a couple of my branches, where I seem to be able to reliably trigger it w/ some seemingly unrelated changes. . Sometimes they manifest as timeouts, in other cases the [wrong data is coming back](https://fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/577/testReport/junit/cromwell.webservice/MetadataBuilderActorSpec/MetadataParser_should_support_nested_lists/). In my branches I'm reliably able to get `should build workflow scope tree from metadata events` fail by simply changing the package of `CromwellApiServiceSpec` (see branch `jg_hmm`). That error is not in this jenkins run, but I've seen those other failures in some of my other experiments (see branch `jg_refactor_reality` although that's just a series of me making strange edits to see what happens)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4288:173,reliab,reliably,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4288,4,"['error', 'failure', 'reliab']","['error', 'failures', 'reliably']"
Availability,"Seen on Cromwell 26-c14e64d. Reported to us as GAWB-1867. Here's a WDL:. ```; task HelloTask; 	{; 	File InFile; 	command; 		{; 		echo ""A file is here !"" ;; 		echo ""Here is the ls"" ;; 		ls -alh * ; 		ls -alht ${InFile} ; 		head ${InFile}; 		}; 	output; 		{; 		}; 	meta {author : ""Eddie Salinas""}; 	runtime { docker: ""eddiebroad/public_test_dsdeepb_2332"" }; 	}; 	; workflow HelloWorkflow; 	{; 	File InFile; 	call HelloTask; 		 {; 		input:InFile=InFile; 		}; 	}; ```. If I accidentally pass an empty string to the input, the job stays running indefinitely:. ```; {; ""workflowName"": ""HelloWorkflow"",; ""submittedFiles"": {; ""inputs"": ""{\""HelloWorkflow.InFile\"":\""\""}"",; ""workflow"": ""task HelloTask\n\t{\n\tFile InFile\n\tcommand\n\t\t{\n\t\techo \""A file is here !\"" ;\n\t\techo \""Here is the ls\"" ;\n\t\tls -alh * \n\t\tls -alht ${InFile} \n\t\thead ${InFile}\n\t\t}\n\toutput\n\t\t{\n\t\t}\n \tmeta {author : \""Eddie Salinas\""}\n\truntime { docker: \""eddiebroad/public_test_dsdeepb_2332\"" }\n\t}\n\t\nworkflow HelloWorkflow\n\t{\n\tFile InFile\n\tcall HelloTask\n\t\t {\n\t\tinput:InFile=InFile\n\t\t}\n\t}"",; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""zones\"": \""us-central1-b us-central1-c us-central1-f\""\n },\n \""google_project\"": \""broad-dsde-dev\"",\n \""auth_bucket\"": \""gs://cromwell-auth-broad-dsde-dev\"",\n \""refresh_token\"": \""cleared\"",\n \""final_workflow_log_dir\"": \""gs://fc-463a83e5-a9b2-49bc-b9d2-0024e9b1afe6/bb61413a-3e29-44d1-beaa-5bbdd8cc711c/workflow.logs\"",\n \""account_name\"": \""obamacloud@gmail.com\"",\n \""jes_gcs_root\"": \""gs://fc-463a83e5-a9b2-49bc-b9d2-0024e9b1afe6/bb61413a-3e29-44d1-beaa-5bbdd8cc711c\"",\n \""read_from_cache\"": true\n}""; },; ""calls"": {; ""HelloWorkflow.HelloTask"": [{; ""preemptible"": false,; ""executionStatus"": ""Running"",; ""stdout"": ""gs://fc-463a83e5-a9b2-49bc-b9d2-0024e9b1afe6/bb61413a-3e29-44d1-beaa-5bbdd8cc711c/HelloWorkflow/f5c59dc3-fb98-42d9-9dbf-e9305d8e8793/call-HelloTask/HelloTask-stdout.log"",; ""shardIndex"": -1,; ""jes"": {; ""executionBucket""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2278:129,echo,echo,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2278,2,['echo'],['echo']
Availability,Send next workflow heartbeat after it's been processed,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3275:19,heartbeat,heartbeat,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3275,1,['heartbeat'],['heartbeat']
Availability,"Sending JMUI style call failures to Sentry.; Wired in the ability for Centaur integration tests to get data directly from the Cromwell database.; Added a `queryJobKeyValueEntries` to return all job key/values for a workflow.; Removed deprecation exception for old database config syntax.; Flatten metadata only during comparison, passing the original internally.; Removed secure env variables that were always true in Sentry.; Refactored centaur secure config rendering.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4055:24,failure,failures,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4055,1,['failure'],['failures']
Availability,"Sending error info to sentry during centaur testing before retrying.; Encrypting sensitive variables using a random key during centaur tests, jic they are sent to sentry.; Rendering secure resources during _all_ tests.; When secure variables cannot be rendered, only fail when secure variables are required, otherwise producing only info/warning messages.; Disabled caches during tests that read `backendStatus` call metadata.; Allow `test_cromwell.sh` to use a centaur config file.; Enable GcsPathBuilderFactory to retry more than zero times.; Lazy load centaur `*.inputs` & `*.options` so that they aren't required to load `*.test` files.; Relatedly, so that one doesn't (try to) accidentally commit the changes, `git rm` the options file that was being rendered.; Moved logback.xml out of transitive core library and into executables, next to application.conf files.; Pin `cwltool` version.; Use a workaround to pass `--timeout` through `run_test.sh` to `cwltest`.; Using `better.files` instead of `java.nio.Path`, and passing `IO` monads further up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4000:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4000,1,['error'],['error']
Availability,"Server mode is the intended, first-class usage scenario for Cromwell. It's really [no harder to use](https://cromwell.readthedocs.io/en/stable/tutorials/ServerMode/) than run mode and enables way more features. Run mode is in maintenance and we do not anticipate making enhancements.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-785281065:226,mainten,maintenance,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-785281065,1,['mainten'],['maintenance']
Availability,Shard's are also in the path. Also the zero `0` should be included in the attempt and shard regexes. In case the number of attempts is higher than 9.; I found this error while running the dev version of cromwell.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5456:164,error,error,164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5456,1,['error'],['error']
Availability,Should I keep the error message to `JES` or should it say `Pipelies API`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2251#issuecomment-299884751:18,error,error,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2251#issuecomment-299884751,1,['error'],['error']
Availability,Should address the cron failures by setting the maximum freeze scan interval to be less than the Carbonite-flavored Centaur's limit of patience.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5272:24,failure,failures,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5272,1,['failure'],['failures']
Availability,"Should help with debugging occasional failures like this in travis by replacing ""not found"" with ""this looks similar"":. ```; StatsDInstrumentationServiceActorSpec:; StatsDInstrumentationServiceActor; - should increment counters (1 second, 193 milliseconds); - should add count (1 second, 9 milliseconds); - should set gauges (1 second, 10 milliseconds); info- should set timings *** FAILED *** (3 seconds, 701 milliseconds); info Missing packet: prefix_value.cromwell.test_prefix.test.metric.bucket.timing.stddev:0.00|g (StatsDInstrumentationServiceActorSpec.scala:83); info org.scalatest.exceptions.TestFailedException:; info ...[0m[0m; info at cromwell.services.instrumentation.impl.statsd.StatsDInstrumentationServiceActorSpec.$anonfun$new$4(StatsDInstrumentationServiceActorSpec.scala:83); info at cromwell.services.instrumentation.impl.statsd.StatsDInstrumentationServiceActorSpec.$anonfun$new$4$adapted(StatsDInstrumentationServiceActorSpec.scala:83); info at scala.collection.immutable.Set$Set4.foreach(Set.scala:206); info at cromwell.services.instrumentation.impl.statsd.StatsDInstrumentationServiceActorSpec.$anonfun$new$2(StatsDInstrumentationServiceActorSpec.scala:83); info at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); info at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); info at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); info ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4387:38,failure,failures,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4387,1,['failure'],['failures']
Availability,Shut down if any write to the Database fails,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4761:5,down,down,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4761,1,['down'],['down']
Availability,Shut down on heartbeat staleness [BT-589],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6696:5,down,down,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6696,2,"['down', 'heartbeat']","['down', 'heartbeat']"
Availability,"Side note for those **only** looking to put time limits on commands, and **not** looking for a workaround for cloud bugs-- here's a more-portable time limit option: `timeout`. `timeout` differs slightly in each distro. Example docs:; - https://busybox.net/downloads/BusyBox.html#timeout; - https://www.gnu.org/software/coreutils/manual/html_node/timeout-invocation.html; - https://manpages.debian.org/stretch/coreutils/timeout.1.en.html. The command `timeout` will most likely be unable to help for ""stuck"" cloud jobs. For example, if the command `echo hello world` actually exits but for some unknown reason the cloud VM sticks around forever, then `timeout 5s echo hello world` will likely have the same issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4946#issuecomment-490059717:256,down,downloads,256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4946#issuecomment-490059717,3,"['down', 'echo']","['downloads', 'echo']"
Availability,Side note to this ticket: There's also the issue on how to handle catastrophic DB issues - e.g. DB goes down for a period of time. I've taken that up myself as a lower priority path which shall eventually turn into an issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2016#issuecomment-281697619:104,down,down,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2016#issuecomment-281697619,1,['down'],['down']
Availability,"Side note:. The reason you seem to be getting a DNS error `Name or service not known` is that when we take an import and try it against all of the available resolvers, we're using a function called `firstSuccess` that does exactly what you'd expect. If all the resolvers fail, the error you get is from the last resolver that we tried, not the one it ""should have used"" (since we have no idea which one that would be ahead of time).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-457245751:52,error,error,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-457245751,3,"['avail', 'error']","['available', 'error']"
Availability,Silence error messages on shutdown,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3620:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3620,1,['error'],['error']
Availability,"Similar to #1820 -- Talk with FireCloud Analysis PO ( @abaumann ) to specifically see if we can gather workflow failure cases for Chet's FireCloud 1000 sample run which had several non-user failures. Incorporate these findings into the same meeting as #1820 . With those errors, attempt to quantify how often this happens in FireCloud by mining the FireCloud Cromwell DBs (e.g. how often does a job fail with JES error code X). Use good judgement to strike a balance of effort vs value here.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1821:112,failure,failure,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1821,4,"['error', 'failure']","['error', 'errors', 'failure', 'failures']"
Availability,"Similar to #3998 (`backendStatus`), but for the metadata key `dockerImageUsed`. This call metadata key is written during job success by the engine. This key may be missing due to restarts of cromwell during centaur tests. Automated restarts of the centaur test end up call caching, where this key isn't written. As a call cache hit technically doesn't _have_ a dockerImage, it should be decided like in #3998 if the key `dockerImageUsed` should be written for cache hits. https://github.com/broadinstitute/cromwell/blob/9bee537c5f6a9ff4e8597f75b6844c0eaee721cc/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/EngineJobExecutionActor.scala#L279-L281. Example log of a failure during WIP of #3658 ; [dockerImageUsed_missing.txt](https://github.com/broadinstitute/cromwell/files/2284646/dockerImageUsed_missing.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4001:690,failure,failure,690,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4001,1,['failure'],['failure']
Availability,"Similar to other parser-related errors reported by Andrea in WB, [via google](https://www.google.com/search?q=owlapi+thread+safety) I'm not convinced the OWL API is thread safe. Some info/debug logging might expose if multiple threads are trying to access the OWL API, and synchronization might fix it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4372#issuecomment-437411171:32,error,errors,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4372#issuecomment-437411171,1,['error'],['errors']
Availability,Simplification of #5415 removing the `attempt-1` redirection. ~Starting out with Don't Look At Me because I anticipate there might be some test fixup before it's ready for prime time.~ Now available for looking at.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5429:189,avail,available,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5429,1,['avail'],['available']
Availability,"Since I always build `womtool` myself instead of downloading, I did notice this problem but assumed there was some magic that made the version set correctly for release builds.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4745#issuecomment-472882044:49,down,downloading,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4745#issuecomment-472882044,1,['down'],['downloading']
Availability,"Since it's been a month I thought I'd post an update. Main items:; - In general I'm having to make a lot more changes to the Scala code than I expected due to queries being written in a way that Postgres doesn't like. (This isn't a criticism, more of a heads-up.) Nothing functional, just refactoring.; - The way `Blob` is handled in Slick+Postgres turns out to be a massive pain. I'm not sure if Slick is lazy-loading these fields or I just don't understand how it works under the hood, but the workaround is that the blobs need to be accessed as part of a transaction, which involved some refactoring of downstream processing.; - Semi-related question: is there a reason why the entire contents of the `importsZip` need to be stored in the database? This quickly leads to an enormous METADATA_ENTRY table - possibly because I have call caching turned on, I haven't checked whether this is the cause yet.; - The auto-incremented fields that are `Option[Long]` in the data model can't be handled the same way in Postgres; I haven't decided whether this is simply different database behavior or a bug somewhere. Anyway I found a workaround for that too.; - I may have messed up and branched from `master` in my fork by mistake, and in any case I'm definitely out of sync with your `develop`. Do you have a preferred workflow to bring my branch up to date, i.e. to minimize the mess in the Git history? (Despite using Git daily I'm still not totally sure what ""best practice"" is.). At this point I can at least run a workflow using Postgres, minus call caching. I'm going to be focusing on completing and testing this in the next couple of weeks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402:606,down,downstream,606,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-486370402,1,['down'],['downstream']
Availability,"Since it's still crickets chirping upstream, and quay.io was down again yesterday, I decided to work a bit on this permanent cache thing. Nobody likes bash scripts. But are python scripts okay? I was wondering what you think about this python script @TMiguelT, @illusional, @vsoch ? The PR where it is created is [here](https://github.com/biowdl/singularity-permanent-cache/pull/1). It is open for feedback. The python script:; - Pulls images to a single location based on environment or command line flag.; - Uses `singularity pull` as the backend.; - Returns the location of the image to stdout.; - Checks if an image is present in the cache. If so, it returns it and does not use `singularity pull` in that case. It does not require any internet connection in that case.; - Utilizes a filelock to prevent cache corruption. It uses flock to do this.; - Has no dependencies. Only a modern version of python is needed (3.5 or higher).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054:61,down,down,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515#issuecomment-635927054,1,['down'],['down']
Availability,"Since read_json and write_json are not implemented yet, I'm documenting for my team which types we can serialize and de-serialize using the available library functions. I wrote a pair of WDLs, one to write data to files and another to read data from files:. [test_read_data.wdl.txt](https://github.com/broadinstitute/cromwell/files/901057/test_read_data.wdl.txt); [test_write_data.wdl.txt](https://github.com/broadinstitute/cromwell/files/901056/test_write_data.wdl.txt). When I tested these I noticed that I can read and write a single Boolean value, and I can write complex types with Booleans, but reading Array[Boolean] didn't work and neither did reading any Map[Boolean,?] or Map[?,Boolean]. . **What is the best way to read in complex types that involve Boolean?**. The error I got when trying to read Array[Boolean] looks like this:. > Error: No coercion defined from 'WdlString(true)' of type 'class wdl4s.values.WdlString' to WdlBooleanType$. The errors I would get for the complex types involving Boolean look like this:. > Error: Failed to coerce one or more keys or values for creating a Map[String, Boolean]: java.lang.IllegalArgumentException: No coercion defined from 'WdlString(true)' of type 'class wdl4s.values.WdlString' to WdlBooleanType$. > Error: Failed to coerce one or more keys or values for creating a Map[Int, Boolean]: java.lang.IllegalArgumentException: No coercion defined from 'WdlString(true)' of type 'class wdl4s.values.WdlString' to WdlBooleanType$. Here are the files generated by test_write_data.wdl that I can read back in with test_read_data.wdl:. [a_false.txt](https://github.com/broadinstitute/cromwell/files/901058/a_false.txt); [a_float.txt](https://github.com/broadinstitute/cromwell/files/901061/a_float.txt); [a_string.txt](https://github.com/broadinstitute/cromwell/files/901063/a_string.txt); [a_true.txt](https://github.com/broadinstitute/cromwell/files/901062/a_true.txt); [a_zero.txt](https://github.com/broadinstitute/cromwell/files/901059/a_zero.t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2152:140,avail,available,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2152,4,"['Error', 'avail', 'error']","['Error', 'available', 'error', 'errors']"
Availability,"Since the latter error message specifies that the input 'a' isn't declared in 'baz', I think it would be good to have the error messages paired, like so:. ```; Unable to load namespace from workflow: ERROR: Call supplies an input 'a' that isn't declared in the 'baz' task (line 6, col 38); ```. and . ```; Unable to load namespace from workflow: ERROR: Call supplies an output 'b' that isn't declared in the 'bar' task (line 4, col 47); ```. And while we are here, I'm curious--why do we display a `^` if it doesn't point to the proper column within the line?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112:17,error,error,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2211#issuecomment-298006112,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Since this is fairly backend-specific I've implemented as a `cromwell.backend.google.pipelines.common.api.RunStatus` instead of a `cromwell.core.ExecutionStatus`. It also reduces the scope of changes. Feedback welcome. Looks like this in metadata requests:; ```; ""calls"": {; ""sleepy_sleep.sleep"": [; {; ""preemptible"": false,; ""executionStatus"": ""Running"",; ""stdout"": ""gs://cloud-cromwell-dev-self-cleaning/cromwell_execution/ci/sleepy_sleep/058bff35-4a55-4c0f-9113-0885f4119cd9/call-sleep/stdout"",; ""backendStatus"": ""AwaitingCloudQuota"",; ""compressedDockerSize"": 28566425,; ""commandLine"": ""sleep 180;\nls -la"",; ""shardIndex"": -1,; ""jes"": {; ""executionBucket"": ""gs://cloud-cromwell-dev-self-cleaning/cromwell_execution/ci"",; ""endpointUrl"": ""https://lifesciences.googleapis.com/"",; ""googleProject"": ""broad-dsde-cromwell-dev""; },; ```; The Terra UI workflow dashboard currently makes calls that look like; ```; https://rawls.dsde-dev.broadinstitute.org/api/workspaces/general-dev-billing-account/anichols-post-ppw/submissions/a7cfb487-9c30-4fd7-b10f-19d6f3c1d192/workflows/f52e4e4b-0299-44b7-a16f-904d2ee3f1e9; ?includeKey=end; &includeKey=executionStatus; &includeKey=failures; &includeKey=start; &includeKey=status; &includeKey=submittedFiles:workflow; &includeKey=workflowLog; &includeKey=workflowName; &includeKey=callCaching:result; &includeKey=callCaching:effectiveCallCachingMode; ```; so it would be easy to add a `&includeKey=backendStatus` and logic to evaluate something like; ```; if (executionStatus == ""Running"" && backendStatus == ""AwaitingCloudQuota"") {; msg = ""Trying to run but needs quota""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6655:1166,failure,failures,1166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6655,1,['failure'],['failures']
Availability,"Since this post was about the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) being quite broken, here a few points:. 1) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; #Create a new service account called ""MyServiceAccount"", and from the output of the command, take the email address that was generated; EMAIL=$(gcloud beta iam service-accounts create MyServiceAccount --description ""to run cromwell"" --display-name ""cromwell service account"" --format json | jq '.email' | sed -e 's/\""//g'); ```; does not work. It errors out with:; ```; ERROR: (gcloud.beta.iam.service-accounts.create) argument NAME: Bad value [MyServiceAccount]: Service account name must be between 6 and 30 characters (inclusive), must begin with a lowercase letter, and consist of lowercase alphanumeric characters that can be separated by hyphens.; ```; I believe `MyServiceAccount` needs to change to `my-service-account` (similarly to how it is used [here](https://cromwell.readthedocs.io/en/stable/backends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:638,error,errors,638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,2,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,Since version 32 Womtool validate give this error on wdl file without a workflow: ; `Namespace does not have a local workflow to run`. Is this required by spec? Would be better to still accept this for reusable tasks like this: https://github.com/biowdl/tasks,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3762:44,error,error,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3762,1,['error'],['error']
Availability,"Since you specifically asked for a ""words"" review, I personally found the CHANGELOG comment came across as a bit defensive (and IMO slightly overplays the certainty of one new heuristic). My suggestion:. > Added a new heuristic for detecting preemption jobs in PAPIv2 based on the error message ""The assigned worker has failed to complete the operation"". Jobs with this message will from now on be treated as preempted rather than failed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511349623:281,error,error,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5072#issuecomment-511349623,1,['error'],['error']
Availability,"Single version across all artifacts in the repo/sbt aggregate project, published together and referring to each other.; Publishing releases and snapshots to Broad artifactory using lenthall's build bash ported to sbt, appropriate for publishing our multi-module, multi-build-type setup.; Stop wasting time assembling fat jars for artifacts _except_ the root.; Renamed Travis' build types from centaur/test to centaur/sbt.; Split up and thinned compile and test dependency statements, artifacts sometimes depend only on another artifacts tests, but not the same artifact's main code.; Just as many (most?) artifacts depend on other artifacts, rearranged dependencies in a similar way. Test dependencies are now embedded in the artifacts that use them. So jars like spray-testkit are no longer added to the classpath of the gcs filesystem, for example.; Centaur build in Travis now prints a heartbeat '…' every 60 seconds, instead of using the log destroying `travis_wait`.; No longer echoing line for line most bash scripts via `set -x`.; Fixed scaladoc in `SlickDatabase`.; Reverted `sbt-git` back to `0.7.1` due to issues with detached git submodules.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1141:889,heartbeat,heartbeat,889,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1141,2,"['echo', 'heartbeat']","['echoing', 'heartbeat']"
Availability,Slurm squeue check-alive method is not robust to failures of communication with slurm controller,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5400:19,alive,alive,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5400,3,"['alive', 'failure', 'robust']","['alive', 'failures', 'robust']"
Availability,"Small but annoying bug in WDL 1.0 support identified on the openWDL slack channel. The evaluation logic was all present and correct, only the type evaluators were not set up to determine the types of the expressions correctly, and returning errors inappropriately.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5598:241,error,errors,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5598,1,['error'],['errors']
Availability,"So I did some digging. The bad news: a `docker_pull` command will not work. It cannot be implemented at workflow initialization because at that point in time runtime attributes are not known. These are evaluated when the task is executed. This is due to inputs in WDL being dependent on the outputs of other tasks, which is what makes WDL great, so this cannot (easily) be fixed. So a docker_pull command would have to be executed at task execution time. But then it is redundant. This command can be part of the submit script. . Thanks @TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:470,redundant,redundant,470,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430,1,['redundant'],['redundant']
Availability,"So I moved the http{} stanza to the correct place in the cromwell_cori.conf file (it was there but in the wrong place). And I changed Local to Mylocal in lines 22 & 26 in the config file. I used this command ; ```; export _JAVA_OPTIONS=""--add-opens=java.base/sun.security.util=ALL-UNNAMED"". java -Dconfig.file=cromwell_docker.conf; -Dbackend.providers.MyLocal.config.dockerRoot=$(pwd)/cromwell-executions; -Dbackend.providers.MyLocal.config.root=$(pwd)/cromwell-executions; -jar ~/cromwell/cromwell-84.jar run fq_count.wdl -i fq_count.json; ```; (Note the change from Local to Mylocal in the command). This command should fail with the ""Could not build the path"" error. Would you please try again with this new cromwell_docker.conf file and new command?. [test-files.zip](https://github.com/broadinstitute/cromwell/files/10397528/test-files.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692:663,error,error,663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6977#issuecomment-1379722692,1,['error'],['error']
Availability,"So I tested that this will now correctly fail a workflow, and indeed:; ```; ERROR - WorkflowManagerActor Workflow f587f57a-2897-4450-a4e1-410442f2460c failed (during ExecutingWorkflowState): Variable 'xs' not found; wdl4s.exception.VariableNotFoundException$$anon$1: Variable 'xs' not found; ```. However as noted, this is a Cromwell runtime catch. It'd be much nicer as a WDL4S validation.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885:76,ERROR,ERROR,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1774#issuecomment-298086885,1,['ERROR'],['ERROR']
Availability,So I'm getting much different behavior than the ticket describes... basically this WDL file is hanging indefinitely. The first task finishes on both JES and local backends. The second behaves in very strange ways. It loops infinitely on JES and has a localization error on local. I suspect it might be because of the problem with the WDL file itself (see previous comment),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/891#issuecomment-224936166:264,error,error,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/891#issuecomment-224936166,1,['error'],['error']
Availability,"So I've got a workflow with tasks A, B, C, D. The outputs of the workflow are the outputs of the tasks. I ran the workflow on a bunch of samples, and because of an error in one of my tools, the task D failed one some (but not all) samples: 6/8 workflows succeeded, 2/8 failed on task D. FireCloud writes workflow outputs back to its data model when the workflow completes. This means that for 2/8 of my samples, _none_ of the intermediates were written back to the data model. It would be nice if FireCloud could say ""this workflow failed, but this subset of workflow outputs were successfully computed, so I'll write those back for you."" However, FireCloud can't do this, because calling Cromwell's `/outputs` endpoint only provides the list of workflow outputs if the workflow succeeds. If the workflow fails or is still running, it returns `""outputs"" : {}`. Could Cromwell perhaps determine ""this workflow output has been computed to its final state, and thus I can tell you about it even though the workflow hasn't completed [successfully]""?. Note that the Swagger documentation for the `/outputs` endpoint says:. > Retrieve the outputs for the specified workflow. Cromwell will return any outputs which currently exist even if a workflow has not successfully completed. This does not seem to be the case.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4139:164,error,error,164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4139,1,['error'],['error']
Availability,"So I've run across into an even more explicit scenario where File? probably should be accepted (or coerced). This doesn't work:. `String? tsv_arg = if defined(tsv_file_input) then basename(tsv_file_input) else """"`. basename() is only going to be run if File? tsv_file_input is defined. Wouldn't it make sense for that if-block to effectively coerce the File? into a File in the context of the then-block? Generally speaking when I write a program that says ""if variable is defined, do something with variable"" to just work, rather than throwing an error when the variable isn't defined, much less throwing an error when the variable might not be defined (which is what appears to be happening here).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1239868895:548,error,error,548,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1239868895,2,['error'],['error']
Availability,"So apparently with 200 we should be returning (at least) an Allow header with a list of available HTTP verbs per endpoint. Looking around the internet (google, reddit, github, bbc), a 405 Method Not Allowed seems to be the standard ""I can't be bothered to implement content for this HTTP verb that's never used"" response. But, on the other hand, if this is just there to Make Swagger Work and it needs to be 200, then :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/191#issuecomment-142074975:88,avail,available,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/191#issuecomment-142074975,1,['avail'],['available']
Availability,"So far cromwell is working great on our cluster. Thanks a lot for this splendid effort. However there is one issue we run into that other people might run into as well. Our cluster uses NFS as its filesystem backend. This means that if one node completes a job, it might take a while before the files that were created. In other workflows we can set an I/O timeout option: if the file did not appear within 3 minutes, the job failed. How do we do this in cromwell. All the settings I have available is `number-of-requests`, `per` and `number-of-attempts`. Currently we have; ```HOCON; io {; number-of-requests = 10; per = 10 seconds; number-of-attempts = 180; }; ```; This should make sure that failing files are attempted for 180 seconds, but this is not very elegant. There is a `timeout` option in I/O. But this is the timeout for the I/O operation to respond. If the file is not ""visible"" then the I/O operation will respond immediately, and the job will have failed. Is there a fix to this option in the current cromwell configuration?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3648:489,avail,available,489,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3648,1,['avail'],['available']
Availability,"So given the following directory structure : ; ```; base; ├── imported_workflow; │   ├── wdl_tasks; │   │   ├── taska.wdl; │   │   └── taskb.wdl; │   └── workflow_b.wdl; ├── task_wdls; │   ├── task1.wdl; │   └── task2.wdl; └── workflow.wdl; ```. In `workflow.wdl` I have; ```WDL; import ""task_wdls/task1.wdl"" as task1; import ""task_wdls/task2.wdl"" as task2; import ""imported_workflow/workflow_b.wdl"" as workflow_b; ```. In `imported_workflow/worflow_b.wdl` I have; ```WDL; import ""wdl_tasks/taska.wdl"" as taska; import ""wdl_tasks/taskb.wdl"" as taskb; ```. If I run cromwell from the directory `base` and run `workflow.wdl`. I will stumble upon an error. It will try to import `wdl_tasks/taska.wdl` from the base directory instead of from the `imported_workflow` directory where `workflow_b.wdl` is located. ```; Failed to import workflow wdl_tasks/taska.wdl.:; File not found /home/ruben/test/base/wdl_tasks/taska.wdl; ```. This has also been discussed at: https://gatkforums.broadinstitute.org/wdl/discussion/11330/how-to-import-workflows-that-also-have-imports-themselves. Evaluating import statements for the directory where cromwell is executed is ""bad"" because; * Where you run cromwell matters. Even if using the same workflows. In terms of reproducibility this is not very nice.; * You cannot see from the WDL file what the base importing directory was meant to be. This follows from where cromwell is run, and this is not specified in the file itself.; * Because of this context dependence, sub workflows are not modular building blocks which can be moved around freely between workflows. This severely handicaps the usefulness of WDL. Import statements should be evaluated from the files which they are in because:; * It makes sub-workflows modular building blocks that can be moved around; * It makes it easier to review and understand the import statements.; * It follows the principle of least surprise.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3241:647,error,error,647,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3241,1,['error'],['error']
Availability,"So here is a final update. I have tried running Cromwell with the following roles:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (lifesciences.workflowsRunner); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (iam.serviceAccountUser); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Creator (roles/storage.objectCreator); 4. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Viewer (roles/storage.objectViewer). And I have got the following error from Cromwell:; ```; java.lang.Exception: Task xxx.xxxNA:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Please check the log file for more details: xxx; ```; And the log just contains this cryptic message:; ```; yyyy/mm/dd hh:mm:ss Starting container setup.; ```; I have then tried to run Cromwell with the following roles:; 1. [Cloud Life Sciences](https://cloud.google.com/life-sciences/docs/concepts/access-control#roles) Workflows Runner (lifesciences.workflowsRunner); 2. [Service Account User](https://cloud.google.com/iam/docs/service-accounts#user-role) (iam.serviceAccountUser); 3. [Storage Object](https://cloud.google.com/storage/docs/access-control/iam-roles) Admin (storage.objectAdmin). And the workflow succeeded. To give a full explanation of the set of roles and permissions needed, I wrote a little python script `roles.py` that collects this information from Google:; ```; #!/bin/python3; import subprocess; import requests; import pandas as pd; import sys. token = subprocess.check_output([""gcloud"",""auth"",""print-access-token""]).decode(""utf8"").strip(); response = requests.get(""https://iam.googleapis.com/v1/roles"", headers={""accept"": ""application/json"", ""Authorization"": ""Bearer ""+token}, params={""pageSize"": 1000, ""view"": ""FULL""}); roles_json = response.json()['roles']; roles = [role['name'] for role in roles_json if 'includedP",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955:620,error,error,620,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955,3,['error'],['error']
Availability,So successes are not wrapped - only failures are (which the ticket was about). With the exception of the validate endpoint which has a special response format which would have been weird not updating.; It's easy enough to wrap the success too - It just feels risky too me to update the entire API responses format a week before firecloud goes live but if that's fine I can wrap the successes too and let them now. We should tell them anyway that the error format will change.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171069379:36,failure,failures,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171069379,2,"['error', 'failure']","['error', 'failures']"
Availability,"So the real reason of the failure I'm afraid is the JES one, which I don't think we can do much for except tell them about it ?; The Cromwell NPE is still a bug but I believe is an artifact of the job failing combined with the fact that the DB is swamped.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298670203:26,failure,failure,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228#issuecomment-298670203,1,['failure'],['failure']
Availability,"So this is not actually a flaky test, but an indication that our code is also failing about half the time in production because the error message is not what we expected?. If that is true, I think the release notes need to be filled out in the ticket so that this makes it into the Terra release notes as a fixed bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6556#issuecomment-960861067:132,error,error,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6556#issuecomment-960861067,1,['error'],['error']
Availability,"So, in my testing, this appears to only happen in the scatter is a download from s3 job. Is it possible that heavy network congestion could create this error? The error itself doesn't seem to be associated with or come from the download, but then again I'm not sure what it means.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5421#issuecomment-589380724:67,down,download,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421#issuecomment-589380724,4,"['down', 'error']","['download', 'error']"
Availability,"So, setting `concurrent-job-limit` to 8 did resolve my issue (and I hit another, unrelated issue instead). However, I don't believe this is the ideal way to resolve this. I (and I assume most other users) want maximum concurrency with my jobs, we just want to avoid this error. If we caught this `Too Many Requests` error, and just waited for a few seconds before retrying these requests, it would surely resolve this issue in a cleaner way.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-435558323:271,error,error,271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-435558323,2,['error'],['error']
Availability,Solution for 503 errors in cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2495:17,error,errors,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2495,1,['error'],['errors']
Availability,"Some SFS backends can kill jobs outside of Cromwell, leaving us waiting forever for an rc file that will never be created. . Idea: occasionally run the `check-alive` command to verify that long-running jobs are indeed still alive outside of restarting Cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2315:159,alive,alive,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2315,2,['alive'],['alive']
Availability,"Some genuine test failures and a lot of ToLs. Once you sort that all out, 👍 from me. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1836/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1836#issuecomment-273212258:18,failure,failures,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1836#issuecomment-273212258,1,['failure'],['failures']
Availability,"Some light reading for @Horneth and @kshakir. This is largely Frankensteining of ""Olde Style"" code. Known missing or broken, I need to confirm that appropriate tickets exist for the restoration of the following:; - [x] #753 Abort; - [x] #751 Recover; - [x] #749 Preemptibility; - [x] #785 Persistence of any data (note this would not be a ticket to create a KV / metadata service, but to integrate this backend with such a service); - [x] #809 Hashing (prereq for #750); - [x] #750 Caching; - [x] #806 implement Firecloud style auth upload / deletion (the code is not present here); - [x] #808 Retries were removed from command script upload and JES run creation. Lessons learned:; - [x] #811 #812 Actor Factories should be responsible for sanity-checking configs .; - [x] #813 Initialization actors should have the ability to create workflow-level resources that can be shared by the other actors collaborating in the workflow execution. e.g. GCS Filesystems need only be created once per workflow, not for every call.; - [x] It's not clear how workflow logging (or logging in general) should work.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/797:242,Recover,Recover,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/797,1,['Recover'],['Recover']
Availability,Some of the test failures seem very genuine... 😨,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268798342:17,failure,failures,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1801#issuecomment-268798342,1,['failure'],['failures']
Availability,Some refactoring: . * Separated the concepts of resolving and downloading.; * `DrsLocalizerMain#resolve` now chooses the `Downloader` implementation based on the content of the Martha response.; * The two downloader implementations support access and GCS URLs respectively.; * Tests for the downloader implementations have been separated from tests for the resolver.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6312:62,down,downloading,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6312,4,"['Down', 'down']","['Downloader', 'downloader', 'downloading']"
Availability,"Some sloppy experimental procedure is to blame for incorrect conclusions in the previous (deleted) comment. It's enough for the input and task name to be the same:; ```; workflow x {; call cram; call y { input:; cram = cram.scram; }; }. task cram {; command {; echo "".""; }; output {; String scram = "".""; }; }. task y {; String cram; command {; echo "".""; }; }; ```; ```; ERROR: Bad target for member access 'cram.scram': 'cram' was a String (line 4, col 21):. cram = cram.scram; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3811#issuecomment-418892826:261,echo,echo,261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3811#issuecomment-418892826,3,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability,"Something appears to be wrong in the credential building for PAPI v2 private Docker as turned up in @marctalbott's testing. It's not clear why this is not replicated by the `docker_hash_dockerhub_private_wf_options` Centaur test but from reading the code it does appear to be a real issue. ```; 2019-01-14 20:20:22,530 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; common.exception.AggregatedException: :; null; 	Unrecognized token 'user_service_account_json': was expecting ('true', 'false' or 'null'); at [Source: (ByteArrayInputStream); line: 1, column: 51]; 	at common.util.TryUtil$.sequenceIterable(TryUtil.scala:29); 	at common.util.TryUtil$.sequenceMap(TryUtil.scala:47); 	at cromwell.engine.backend.CromwellBackends.<init>(CromwellBackends.scala:14); 	at cromwell.engine.backend.CromwellBackends$.initBackends(CromwellBackends.scala:42); 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:62); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:96); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.runServer(CromwellEntryPoint.scala:50); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:15); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4553:319,ERROR,ERROR,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553,2,"['ERROR', 'down']","['ERROR', 'down']"
Availability,"Something seems to have changed in the import resolution logic between Cromwells 45.1 and 46, such that the latter doesn't resolve imports relative to the importing file (when neither are in the current working directory). ```. cat << 'EOF' > echo_task.wdl; version 1.0; task echo {; input {; String s; }. command {; echo ""~{s}""; }. output {; String t = read_string(stdout()); }; }; EOF; cat << 'EOF' > echo.wdl; version 1.0; import ""echo_task.wdl"" as lib; workflow echo {; input {; Array[String] ss; }; scatter (s in ss) {; call lib.echo { input:; s = s; }; }; }; EOF; mkdir -p subdir; cd subdir; # Succeeds:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-45.1.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); # Fails:; java -DLOG_LEVEL=info -DLOG_MODE=pretty -jar ../cromwell-46.jar run ../echo.wdl -i <(echo '{""echo.ss"": [""Alice"", ""Bob""]}'); ```. Cromwell 46 failure looks like:. ```; [2019-09-18 20:42:55,88] [info] WorkflowManagerActor Workflow 9c7ab81b-43b4-40ca-ab8a-99fe58326127 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to import 'echo_task.wdl' (reason 1 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'relative to directory [...]/subdir (escaping allowed)' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 2 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'entire local filesystem (relative to '/')' (reason 1 of 1): File not found: echo_task.wdl; Failed to import 'echo_task.wdl' (reason 3 of 3): Failed to resolve 'echo_task.wdl' using resolver: 'http importer (no 'relative-to' origin)' (reason 1 of 1): Relative path; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescripto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5182:276,echo,echo,276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5182,12,"['echo', 'failure']","['echo', 'failure']"
Availability,"Sometimes the timing diagram splits the results onto multiple lines, which then hides the lower part of the diagram because it's twice as long as expected. You end up with a mostly empty page with a small box that has as scroll wheel at the top of it. <img width=""1431"" alt=""screen shot 2018-04-05 at 4 06 59 pm"" src=""https://user-images.githubusercontent.com/4700332/38389168-956414ac-38eb-11e8-8459-dff52946091c.png"">. I noticed that the console for this diagram has the following errors:; ```; timing:20 Unable to add 'MarkDuplicates.SortSam's entry: 'RunningJob' because start-time 'Tue Apr 03 2018 23:54:20 GMT-0400 (EDT)'' is greater than end-time 'Tue Apr 03 2018 23:54:20 GMT-0400 (EDT)'; addDataTableRow @ timing:20; timing:20 Unable to add 'MarkDuplicates.PreSort's entry: 'RunningJob' because start-time 'Tue Apr 03 2018 17:35:05 GMT-0400 (EDT)'' is greater than end-time 'Tue Apr 03 2018 17:35:05 GMT-0400 (EDT)'; ```. I'm not sure if it's related to the display issue or not, but I don't see it on other workflows. ; See diagram here. https://cromwell-v30.dsde-methods.broadinstitute.org/api/workflows/v1/01c7d76f-5b2b-48cd-be08-ce75b923666e/timing. It's the same job from issue #3483 so there might just be something off about the whole job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3484:483,error,errors,483,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3484,1,['error'],['errors']
Availability,Sometimes users are interested in a particular workflow and would like to know when changes to some terminal state: failure or success. Allowing multiple users to register as interested in a workflow and emailing them when the workflow is reaches one of these states (maybe also registering the set of desired states to be notified about) would be useful.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1678:116,failure,failure,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678,1,['failure'],['failure']
Availability,"Sorry @aofarrel we need to look into this further, but on a brief first glance I'd bet this is an oversight in the miniwdl type checker, elaborated in the [linked issue](https://github.com/chanzuckerberg/miniwdl/issues/596). thanks @pshapiro4broad for the slack ping",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233430919:262,ping,ping,262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233430919,1,['ping'],['ping']
Availability,"Sorry all. I was WAY off base about that ""scalability"" thing. It turns out, if one shutdowns one's database pool, the database doesn't allow you to open any new connections. :blush:. Perhaps someday, someone will run `ab` against cromwell and see where it really does fall over, but today wasn't that day. Based on the exceptions I saw, I mistakenly thought it was an internal pool being starved, but when I actually attached a debugger, found out it was because the pool wasn't really _there_ anymore. So I closed the #199 with more extensive refactoring. Currently, even with simpler refactoring to remove calling `DataAccess.instance.shutdown()`, there seems to be something else weird in `data_access_singleton` that I need to figure out. I'm getting repeatable test failures on `WorkflowManagerActorSpec`'s ""should Try to restart workflows when there are workflows in restartable states"". Still debugging…",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495:771,failure,failures,771,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143084495,1,['failure'],['failures']
Availability,"Sorry for the noise. I've succeeded in creating an AMI where the scratch partition is called /cromwell_root. I have submitted a job which failed with the error under discussion before, and I'm waiting to see what happens....",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468834266:154,error,error,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468834266,1,['error'],['error']
Availability,"Sorry if this is a dumb question, but on a `SharedFilesystemBackend` that's using GCS inputs for a call, is it possible that `out` could be correctly interpreted as a ""GCS relative path""? i.e. wouldn't that have to be a local file? Per the docs and my understanding of this code, this only localizes down GCS inputs and doesn't delocalize outputs, and it didn't seem there's the concept of a GCS scratch bucket that could serve as the basis of a relative path. +1 to removing the possibility that JesBackend workflows could get their hands on a shared file system IO interface. :smile:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-162688568:300,down,down,300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305#issuecomment-162688568,1,['down'],['down']
Availability,"Sorry, I put the `select_first` in the wrong place. This works:; ```; String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input])) else """"; ```. My `test.wdl`:; ```; version 1.0. workflow W {; input { File? tsv_file_input }. String tsv_arg = if defined(tsv_file_input) then basename(select_first([tsv_file_input])) else """". call T { input: s = tsv_arg }; output { String out = T.out }; }. task T {; input { String s }; command { echo ~{s} }; output { String out = read_string(stdout()) }; }; ```; I checked and this works with both `miniwdl run` and `java -jar cromwell.jar run`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245918821:458,echo,echo,458,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1245918821,1,['echo'],['echo']
Availability,"Sorry, I use to search in previous issues but I didn't this time - my fault. Issue #3161 is the one that describes what I would like to have in cromwell. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3417#issuecomment-373403073:70,fault,fault,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3417#issuecomment-373403073,1,['fault'],['fault']
Availability,"Sorry, but this seems very related and I can't see why this is the problem:. ```; task trim_PE {; input {; File f1; File f2; Int? cpu=1; # Int? disk_space_gb=ceil(2*size(f1, ""GB"")+2*size(f2, ""GB"")) + 20; Int? machine_mem_gb; Int? preemptible_attempts; String? adap; String b1 = basename(basename(basename(f1, "".fastq""), "".fq""), "".gz""); String b2 = basename(basename(basename(f2, "".fastq""), "".fq""), "".gz""); String p1 = ""W7_"" + b1 + "".fq.gz""; String p2 = ""W7_"" + b2 + "".fq.gz""; String u1 = ""W7_"" + b1 + ""_unpaired.fq.gz""; String u2 = ""W7_"" + b2 + ""_unpaired.fq.gz""; }; ```. This yields an error:; ![image](https://user-images.githubusercontent.com/16489606/92958456-931bba00-f46a-11ea-97cb-ccc709929ad8.png). Is this a similar problem?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5694#issuecomment-691239382:587,error,error,587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5694#issuecomment-691239382,1,['error'],['error']
Availability,"Sorry. I didn't realize the configuration has changed between 0.19/0.19_hotfix and 0.20. What happened is that the config flag was ignored (though it would perhaps be preferable to throw an error or at least a warning.) and the cromwell tried each localization strategy until one succeeded, as described in the documentation. In my run, the files that were hard-linked live on the same storage device.; The files that were soft-linked live on another storage device, presumably because hard-linking failed. Using the new flag `-Dbackend.providers.Local.config.filesystems.local.localization.0=soft-link` ensures that all files are soft-linked as expected.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230920938:190,error,error,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1072#issuecomment-230920938,1,['error'],['error']
Availability,Space in checkpointFile name will make it fail,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7441:9,checkpoint,checkpointFile,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7441,1,['checkpoint'],['checkpointFile']
Availability,Spark backend script has syntax error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4611:32,error,error,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4611,1,['error'],['error']
Availability,"Specifically looking at the following: `gvcf_joint`, `prealign`, `rnaseq`, `somatic`, `svcall` from https://github.com/bcbio/test_bcbio_cwl. Note that there's a version of `somatic` with GS inputs available in the `gcp` subdir which might make testing smoother for that one. I've seen `prealign` work ok on PAPI2 but haven't had luck on anything else.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-425209964:197,avail,available,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-425209964,1,['avail'],['available']
Availability,Spike in backpressure and 403 copy failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4229:35,failure,failures,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4229,1,['failure'],['failures']
Availability,Spike: Investigate NPE causing workflow failure,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4772:40,failure,failure,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772,1,['failure'],['failure']
Availability,"Spoiler alert: The DB *does* have `WaitingForQueueSpace` statuses. Notes for reviewers:. * The metadata builder was throwing errors on old workflows which had previously generated `WaitingForQueueSpace` execution statuses (ie almost all of them).; * This PR fixes that by re-specifying that value as a valid status, even though nothing in the code _generates_ this status any more.; * The only ""live code"" changes are undoing the status deletion from https://github.com/broadinstitute/cromwell/pull/6047; * The test case is deliberately over-comprehensive (it makes sure the metadata builder can handle *ALL* current statuses). I'm trying to prevent future situations where statuses are removed from the list without realizing that that might break metadata builder logic.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6131:125,error,errors,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6131,1,['error'],['errors']
Availability,Spoke to @mcovarr in-person. My take on the goal of this PR is to catch database errors related to aborts and either report them back to the sender or log them. :+1: . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2154/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-293336655:81,error,errors,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2154#issuecomment-293336655,1,['error'],['errors']
Availability,Sporadic JES failure: error code 10 / message 15,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2233:13,failure,failure,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233,2,"['error', 'failure']","['error', 'failure']"
Availability,Staging error for listing attribute in Directory input type,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4670:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4670,1,['error'],['error']
Availability,"Staging input/output files from/to S3 is not yet implemented. If the command from the task is an echo that redirects STDOUT to a file (e.g. `echo 'hello world' > output`) this will fail when Cromwell tries to retrieve the file from S3. . If the command is a simple echo (e.g. `echo ""hello world"") the STDOUT is retrieved from CloudWatch Logs and the process should succeed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760#issuecomment-398846677:97,echo,echo,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760#issuecomment-398846677,4,['echo'],['echo']
Availability,StandardAsyncExecutionActor$$anonfun$executeAsync$1.apply(StandardAsyncExecutionActor.scala:242); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$executeAsync$1.apply(StandardAsyncExecutionActor.scala:242); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeAsync(StandardAsyncExecutionActor.scala:242); 	at cromwell.backend.impl.aws.AwsAsyncJobExecutionActor.executeAsync(AwsAsyncJobExecutionActor.scala:23); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.executeOrRecover(StandardAsyncExecutionActor.scala:502); 	at cromwell.backend.impl.aws.AwsAsyncJobExecutionActor.executeOrRecover(AwsAsyncJobExecutionActor.scala:23); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:52); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:80); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.backend.impl.aws.AwsAsyncJobExecutionActor.aroundReceive(AwsAsyncJobExecutionActor.scala:23); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:25,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1966:3224,robust,robustExecuteOrRecover,3224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1966,1,['robust'],['robustExecuteOrRecover']
Availability,StandardAsyncExecutionActor$class.handlePollSuccess(StandardAsyncExecutionActor.scala:356); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.handlePollSuccess(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$1.apply(StandardAsyncExecutionActor.scala:320); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$poll$1.apply(StandardAsyncExecutionActor.scala:314); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.standard.StandardAsyncExecutionActor$class.poll(StandardAsyncExecutionActor.scala:313); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.poll(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll$1.apply(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:37); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.withRetry(AsyncBackendJobExecutionActor.scala:41); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$class.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustPoll(AsyncBackendJobExecutionActor.scala:56); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:70); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:484); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:113); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.sc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1817:1974,robust,robustPoll,1974,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1817,1,['robust'],['robustPoll']
Availability,StandardAsyncExecutionActor.execute(StandardAsyncExecutionActor.scala:628); 	at cromwell.backend.standard.StandardAsyncExecutionActor.execute$(StandardAsyncExecutionActor.scala:627); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.execute(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recover$(StandardAsyncExecutionActor.scala:645); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$recoverAsync$1(StandardAsyncExecutionActor.scala:653); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.standard.StandardAsyncExecutionActor.recoverAsync$(StandardAsyncExecutionActor.scala:653); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.recoverAsync(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:949); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65);,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:1608,recover,recoverAsync,1608,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,2,['recover'],['recoverAsync']
Availability,Standardize PAPI detritus naming to match SFS for the benefit of the existing detritus filtering code. This also fixes the conflation of standard output and error from the user command with that of the whole exec script on PAPI that we've had since the beginning of time.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3695:157,error,error,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3695,1,['error'],['error']
Availability,"Starting Cromwell on Windows and running any CWL workflow causes the server to crash and exit:; ```; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-50]: ; java.lang.UnsatisfiedLinkError: could not locate stub library in jar file. Tried [jni/x86_64-Windows/jffi-1.2.dll, /jni/x86_64-Windows/jffi-1.2.dll]; ```; What's worse, if one starts Cromwell back up it picks up that same workflow from the workflow store and immediately crashes. Notably, one can't even run WDLs at this point because the server does not stay up. The only remedy is to edit rows in the DB. I realize running on Windows is a self-inflicted wound in my case, hence ""low priority"". It does seem likely that not every researcher who wants to use Cromwell works somewhere that can afford to issue Macs. [jython_trace.txt](https://github.com/broadinstitute/cromwell/files/3048143/jython_trace.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4802:110,error,error,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4802,1,['error'],['error']
Availability,"Starting to spider out the proofing of concept for a google-y metadata system into something which is actually storing events as well as providing a not horribly inefficient read access for the current set of metadata-y endpoints. A high level description: Stream metadata events out of Cromwell via Google PubSub, and store them in two locations. The first is a permanent event store which will be storing these events in an immutable fashion, which will allow us to be flexible with downstream presentation w/o information loss. The second will be a set of SQL tables which have been designed to provide efficient results for all of the standard Cromwell metadata endpoints such as metadata, status, outputs, etc. For Broad folks, more information is available [here](https://docs.google.com/document/d/1F5WsEAKvYx6njdF-yJZ4LHvT39KcErCdBpCOySq-NoQ/edit)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3243:485,down,downstream,485,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3243,2,"['avail', 'down']","['available', 'downstream']"
Availability,"Starting up a new cromwell-44 server on CentOS7 with MySQL v5.7 (mysql Ver 14.14 Distrib 5.7.27, for Linux (x86_64) using EditLine wrapper). migrations complete without error:; ```; 2019-07-21 23:34:35,925 INFO - DROP INDEX METADATA_JOB_AND_KEY_IDX ON cromwell.METADATA_ENTRY; 2019-07-21 23:34:35,935 INFO - Index METADATA_JOB_AND_KEY_IDX dropped from table METADATA_ENTRY; 2019-07-21 23:34:35,936 INFO - DROP INDEX METADATA_JOB_IDX ON cromwell.METADATA_ENTRY; 2019-07-21 23:34:35,947 INFO - Index METADATA_JOB_IDX dropped from table METADATA_ENTRY; 2019-07-21 23:34:35,948 INFO - ChangeSet metadata_changesets/metadata_index_removals.xml::metadata_index_removals::mcovarr ran successfully in 24ms; 2019-07-21 23:34:35,949 INFO - INSERT INTO cromwell.SQLMETADATADATABASECHANGELOG (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, `DESCRIPTION`, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('metadata_index_removals', 'mcovarr', 'metadata_changesets/metadata_index_removals.xml', NOW(), 8, '8:b32b63103dfbe3664806be3eccf78b09', 'dropIndex indexName=METADATA_JOB_AND_KEY_IDX, tableName=METADATA_ENTRY; dropIndex indexName=METADATA_JOB_IDX, tableName=METADATA_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3752074629'); 2019-07-21 23:34:35,956 INFO - Successfully released change log lock; 2019-07-21 23:34:36,224 WARN - Unrecognized configuration key(s) for Jes: filesystems.gcs.project, name-for-call-caching-purposes, slow-job-warning-time; 2019-07-21 23:34:36,976 INFO - Slf4jLogger started; 2019-07-21 23:34:37,408 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-673c553"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2019-07-21 23:34:37,771 cromwell-system-akka.actor.default-dispatcher-3 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:169,error,error,169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,1,['error'],['error']
Availability,"Statements=true""; user = ""fake""; password = ""fake""; driver = ""com.mysql.jdbc.Driver""; connectionTimeout = 5000; }; }. system {; # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; input-read-limits {; lines = 128000000; string = 128000000; json = 128000000; tsv = 128000000; map = 128000000; object = 128000000; }; }. ```. <!-- Paste/Attach your workflow if possible: -->; a modified version of : https://github.com/gatk-workflows/gatk4-data-processing. [workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log](https://github.com/broadinstitute/cromwell/files/2143529/workflow.0263ce1e-e1da-44c4-a49f-56fea7a6e1ea.log). A workflow is failing. It looks like cromwell attempts to localise some folder, ; ```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/0263ce1e-e1da-44c4-a49f-56fea7a6e1ea/call-SamToFastqAndBwaMem/inputs/-21323395/cromwell -> /share/ScratchGeneral/evaben/cromwell```; Is it attempting to localize my entire cromwell directory? IE the CWD of the cromwell server process?. This fails because a previously found workflow input (```/share/ScratchGeneral/evaben/cromwell/cromwell-executions/HaplotypeCallerGvcf_GATK4/f18cded7-24ae-470d-b58d-d87ce97f21cb/call-HaplotypeCaller/shard-6/inputs/share/ClusterShare/biodata/contrib/evaben/genome_one_na12878/reheader/NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam.bai```) is a symlink to a file that no long exists. (NA12878_V2.5_Robot_1.dedup.realigned.recalibrated.bam.bai`). In addition that file is totally unrelated to the workflow I am trying to run ( it is from a different workflow, and a different call, and the inputs.json and wdl are totally different). I am confused about what is happening, none of the actions the log is mentioning as errors seem to be things cromwell should be attempting. The cache does not become invalidated (as it seems to in other cases where the cached file has disappeared).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3825:2898,error,errors,2898,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3825,1,['error'],['errors']
Availability,StatsDProxy was deleted in these changes. StatsDProxy was an old UDP packet tee for the Cromwell perf environment we no longer use. It was written against version 1 of [fs2](https://fs2.io/#/) which unfortunately introduced all kinds of horrible conflicts at assembly time with the updated http4s fs2 version 2 dependencies (see CROM-6858 for sample assembly errors).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1010387159:359,error,errors,359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6646#issuecomment-1010387159,1,['error'],['errors']
Availability,"Still driving on the Wash U workflow, this is a pregull checkpoint with some ~hacks~ work I did for blockers:. We round trip inputs YAML through Circe which uses its own ""Big"" number types that stringify with scientific notation. Our existing WomInteger / WomLong code didn't deal with that so the changes here add some flexibility. CWL explicitly allows filled optional to non-optional assignments with warnings, with an error at runtime if it turns out the optional wasn't actually filled. I expect some discussion on this 🙂",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3430:56,checkpoint,checkpoint,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3430,2,"['checkpoint', 'error']","['checkpoint', 'error']"
Availability,"Still experiencing this problem. It seems we cannot use `Array[File]` inside `struct`s for now. . ```Test.wdl; version development; ​; workflow Test {; input {; String file_name = ""file.txt""; String file_contents = ""teste""; }; ​; call WriteFile {; input:; file_name=file_name,; file_contents=file_contents; }; ​; Array[File] array_file = [WriteFile.output_file, WriteFile.output_file]; ​; MultiTypeStruct test_struct = {; ""file_name"" : file_name,; ""file"" : WriteFile.output_file,; ""array_file"" : array_file; }; ​; output {; MultiTypeStruct multi_type_struct_test = test_struct; }; }; ​; struct MultiTypeStruct {; String file_name; File file; Array[File] array_file; }; ​; task WriteFile {; input {; String file_name; String file_contents; }; ​; command <<<; echo -e """"""~{file_contents}"""""" > ~{file_name}; >>>; ​; runtime {; docker: ""gcr.io/google.com/cloudsdktool/cloud-sdk:330.0.0-alpine""; preemptible: 3; }; ​; output {; File output_file = ""~{file_name}""; }; }. ```. You can easily see an error happening when running a simple workflow like this. As long as you have an `Array[File]` inside a `struct`, it will keep on failing. In my case, I'm using `version development`, and the last task on the workflow simply gets stuck with status `Running` while the workflow itself moves to status `Aborting` and stays stuck permanently in `Aborting` (never actually moving its status to `Aborted`). Experienced this issue with Cromwell versions 63 and 74, while using GCP lifescience v2 backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-1017704304:758,echo,echo,758,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4663#issuecomment-1017704304,4,"['echo', 'error']","['echo', 'error']"
Availability,Still getting this error today.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-856677347:19,error,error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-856677347,1,['error'],['error']
Availability,"Still looking at a Centaur test for this but I thought I'd let the seafowl have at it. Metadata looks like:. ```; {; ""workflowName"": ""three_step"",; ""submittedFiles"": {; ""workflow"": ""import \""3step-tasks.wdl\"" as tasks\nimport \""https://raw.githubusercontent.com/broadinstitute/centaur/591beaf8422af7c3faf51e437a91d94d13b76eba/src/main/resources/standardTestCases/aliased_subworkflows/subworkflow.wdl\"" as subworkflow\n\n\nworkflow three_step {\n call tasks.ps as ps\n call tasks.cgrep as cgrep {\n input: in_file = ps.procs\n }\n call tasks.wc as wc {\n input: in_file = ps.procs\n }\n output {\n cgrep.*\n wc.*\n }\n}\n"",; ""workflowType"": ""WDL"",; ""options"": ""{\n\n}"",; ""inputs"": ""{\""three_step.cgrep.pattern\"":\""mcovarr\""}"",; ""labels"": ""{}"",; ""imports"": {; ""https://raw.githubusercontent.com/broadinstitute/centaur/591beaf8422af7c3faf51e437a91d94d13b76eba/src/main/resources/standardTestCases/aliased_subworkflows/subworkflow.wdl"": ""task increment {\n Int i\n command {\n echo $(( ${i} + 1 ))\n }\n output {\n Int j = read_int(stdout())\n }\n runtime {\n docker: \""ubuntu:latest\""\n }\n}\n\nworkflow subwf {\n Array[Int] is\n scatter (i in is) {\n call increment { input: i = i }\n }\n output {\n Array[Int] js = increment.j\n }\n}\n"",; ""3step-tasks.wdl"": ""task ps {\n command {\n ps\n }\n output {\n File procs = stdout()\n }\n runtime {\n docker: \""ubuntu:latest\""\n }\n}\n\ntask cgrep {\n String pattern\n File in_file\n command {\n grep '${pattern}' ${in_file} | wc -l\n }\n output {\n Int count = read_int(stdout())\n }\n runtime {\n docker: \""ubuntu:latest\""\n }\n}\n\ntask wc {\n File in_file\n command {\n cat ${in_file} | wc -l\n }\n output {\n Int count = read_int(stdout())\n }\n runtime {\n docker: \""ubuntu:latest\""\n }\n}\n\n""; }; },; .; .; .; ```. So the http imports come in under ""submittedFiles"" which is maybe a little weird. But other options would have http imports either not being next to the file imports and/or having to change the metadata schema.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2802:973,echo,echo,973,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2802,1,['echo'],['echo']
Availability,"Still need to performance-test on alpha. Problematic pairs:; * fetch <-> heartbeat (already coordinated); * fetch <-> abort (newly coordinated); * fetch <-> delete (newly coordinated). Example queries from MySQL deadlock printout in prod:. **Abort**; ```; update ; `WORKFLOW_STORE_ENTRY` ; set ; `WORKFLOW_STATE` = 'Aborting' ; where ; `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '109a9d01-10b6-425d-8381-12a9d3a2c134'. ```; **Delete from workflow store**; ```; delete `WORKFLOW_STORE_ENTRY` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c4961523-321b-4172-abe8-e1e4eba94f43'. ```; **Fetch startable workflows**; ```; select ; `WORKFLOW_EXECUTION_UUID`, ; `WORKFLOW_DEFINITION`, ; `WORKFLOW_URL`, ; `WORKFLOW_ROOT`, ; `WORKFLOW_TYPE`, ; `WORKFLOW_TYPE_VERSION`, ; `WORKFLOW_INPUTS`, ; `WORKFLOW_OPTIONS`, ; `WORKFLOW_STATE`, ; `SUBMISSION_TIME`, ; `IMPORTS_ZIP`, ; `CUSTOM_LABELS`, ; `CROMWELL_ID`, ; `HEARTBEAT_TIMESTAMP`, ; `HOG_GROUP`, ; `WORKFLOW_STORE_ENTRY_ID` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; (`HEARTBEAT_TIMESTAMP` is null) ; or (; `HEARTBEAT_TIMESTAMP` < '2020-09-18 05:08:18.823'; ); ) ; and (; not (`WORKFLOW_STATE` = 'On Hold'); ) ; order by ; `SUBMISSION_TIME` ; limit ; 30 for ; update; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906:73,heartbeat,heartbeat,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906,1,['heartbeat'],['heartbeat']
Availability,"Stop downloading, compiling, and uploading dead jes code [BT-84]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6311:5,down,downloading,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6311,1,['down'],['downloading']
Availability,"Stopped closing a scala `Future` over akka's `context.become`.; Flipped the default for `requestsAbortAndDiesImmediately` from `false` to `true`.; When killing a Standard backend job with rADI false, both the rc and the standard error are required.; Writing the stderr on abort for the SFS backend.; When rADI is true, sending a backend status of `""Aborted""`.; In the engine, set and store the `ExecutionStatus.Aborted`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2032:229,error,error,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2032,1,['error'],['error']
Availability,"Straight from the log: ""Something has gone horribly wrong!"". Jenkins build 2475 (2 errors); Jenkins build 2470; Jenkins build 2426",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4520:83,error,errors,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4520,1,['error'],['errors']
Availability,"Strange ""Boxed Error"", probably authorization / config",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:15,Error,Error,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['Error'],['Error']
Availability,"Strangely, this non-input workflow and non-AWS configuration, lead to the same error, so seems to be something about my build, not likely the AWS Batch specific part:. [my-cromwell.conf.txt](https://github.com/broadinstitute/cromwell/files/2081156/my-cromwell.conf.txt); [myWorkflow.wdl.txt](https://github.com/broadinstitute/cromwell/files/2081161/myWorkflow.wdl.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395472255:79,error,error,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395472255,1,['error'],['error']
Availability,"StreamingInvokerAction.scala:20); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:944); 	at scala.collection.Iterator.foreach$(Iterator.scala:944); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.dbio.SynchronousDatabaseAction$$anon$6.run(DBIOAction.scala:469); 	at slick.dbio.SynchronousDatabaseAction$$anon$10.run(DBIOAction.scala:561); 	at slick.dbio.SynchronousDatabaseAction$$anon$7.run(DBIOAction.scala:486); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); 2019-07-21 23:34:40,417 cromwell-system-akka.dispatchers.service-dispatcher-14 ERROR - Failed to summarize metadata; java.sql.SQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""SUMMARY_STATUS_ENTRY"" where ""SUMMARY_NAME"" = 'WORKFLOW_METADATA_SUMMARY_ENTRY_I' at line 1; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:6748,ERROR,ERROR,6748,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,Submit docker error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862:14,error,error,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862,1,['error'],['error']
Availability,Submit large N (perhaps 10K) workflows as fast as we can; ## Measure; * Observe any errors from submit endpoint; * How well is the summarizer keeping up with Metadata; * All workflows complete in < X Seconds; * CPU/memory usage of the Cromwell server ; * CPU/memory usage of the Database (directly from stackdriver) (TODO: Not sure we care?); ## Possible solutions; ### Mock-ify the Google backend; ## Goal; Submit workflows is successful and all return in < X seconds,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4793:84,error,errors,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4793,1,['error'],['errors']
Availability,"Submitting a workflow with wrong credentials (eg invalid refresh token) triggers retries which are bound to fail and are significantly delaying the response from JesBackend, which in turn seems to lead to 502 proxy errors.; This might not be the only cause of this error but when submitting a few workflow with wrong token there was a flood of retries in cromwell logs which probably slows everything down.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/313:215,error,errors,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/313,3,"['down', 'error']","['down', 'error', 'errors']"
Availability,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:62,error,errors,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134,4,"['error', 'failure']","['error', 'errors', 'failure']"
Availability,"Suggested by @vdauwera regarding read_tsv() import functionality:. When you have a TSV with a header, it would be great if you could set a parameter to `header=true`, in order to have WDL ignore the first line. . Even better, if the `read_tsv` function could use the header fields to read the content into a map instead of an array. So you could say `sample[inputBam]` instead of having to take `sample[1]`, which is more susceptible to errors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1962:437,error,errors,437,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1962,1,['error'],['errors']
Availability,"Support for namespaced inputs was dropped from 48 as mentioned in BA-6149. . The code was reworked in #5214 which disallowed namespaced inputs. There was one line of code that actively prohibited namespaced inputs. Removing this line allows namespaced inputs again. The namespaces are a bit different from the ones in 47, so I documented it in the changelog. Also I wrote some documentation on the inputs as I couldn't find any on the [latest development documentation](https://cromwell.readthedocs.io/develop/). . I like the newer namespaces much better than the old ones. A great job! This PR makes sure the fruit of this effort can be plucked by the pipeline developers. If womtool inputs needs to give a cleaner output, then maybe we can change a few other things. Changing it in the inputs parsing is not desirable IMO because that also affects cromwell. In an ideal world all the namespaced inputs are available for advanced pipeline users in cromwell, while not cluttering the womtool inputs output (unless a flag is set).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5317:908,avail,available,908,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5317,1,['avail'],['available']
Availability,"Sure !. This is a minimal workflow that runs a task with a dynamic number of GPUs; ```version 1.0. workflow gpu_example {. call maybe_gpu {; input:; gpu_count = 0; }. }. task maybe_gpu {. input {; Int gpu_count; }. command {; echo 1; }. runtime {; docker: ""ubuntu:16.04""; gpuCount: gpu_count; gpuType: ""nvidia-tesla-t4""; }; }; ```. When ran with `gpu_count = 0`, the cromwell runtime validation fails because it is expecting a non-null integer.; ```; 2022-02-14 16:48:34,798 cromwell-system-akka.dispatchers.engine-dispatcher-7 INFO - WorkflowExecutionActor-45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 [UUID(45f6febb)]: Starting gpu_example.maybe_gpu; 2022-02-14 16:48:39,643 cromwell-system-akka.dispatchers.engine-dispatcher-25 INFO - Assigned new job execution tokens to the following groups: 45f6febb: 1; 2022-02-14 16:48:41,244 cromwell-system-akka.dispatchers.backend-dispatcher-31 ERROR - Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0; 2022-02-14 16:48:42,011 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowManagerActor: Workflow 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1$$anon$1: PipelinesApiAsyncBackendJobExecutionActor failed and didn't catch its exception. This condition has been handled and the job will be marked as failed.; Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting gpuCount runtime attribute value greater than 0. 2022-02-14 16:48:44,341 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - WorkflowManagerActor: Workflow actor for 45f6febb-8625-43ce-8bd5-fe0ab71d3fe7 completed with status 'Failed'. The workflow will be removed from the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757:226,echo,echo,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679#issuecomment-1039388757,2,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability,"Sure thing. Just ping me when you are back. Have a fun break. On Wed, Dec 21, 2016 at 3:28 PM, Jeff Gentry <notifications@github.com>; wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> Let's touch base after the; > break (since I'm already on mine). I have a feeling this is one of those; > things where everyone present will say ""I agree that the current solution; > is right but I disagree with everyone else's vision"" :) So it'd probably; > take a while before getting it right.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268630429>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk73KvoC7sKtWEqWT2w88t7Qh8h5Eks5rKYvRgaJpZM4LTPm1>; > .; >. -- ; Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268638416:17,ping,ping,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1804#issuecomment-268638416,1,['ping'],['ping']
Availability,Surface TES System Logs to Cromwell when TES backend returns task error status,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6972:66,error,error,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6972,1,['error'],['error']
Availability,"Suspicion confirmed! There is a collector key for each scatter that is a part of the execution store in Cromwell--and it's being abandoned in the ""NotStarted"" state when there is a failure in the Scatter in ContinueWhilePossible mode. Because there is a ""NotStarted"" key in the store--the workflow remains in the ""Running"" state with a job that's not really an executable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2297#issuecomment-310204092:181,failure,failure,181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2297#issuecomment-310204092,1,['failure'],['failure']
Availability,Sync centaur timeouts with heartbeats in test.inc.sh,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3874:27,heartbeat,heartbeats,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3874,1,['heartbeat'],['heartbeats']
Availability,Synch this error message with SFS for failing centaur test,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1331:11,error,error,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1331,1,['error'],['error']
Availability,"Syntax that was accepted in draft-2:. ```; workflow w {. call empty {}. }. task empty {; command {}; output {}; }; ```. But when the exact workflow is run as version 1.0, it fails:. ```; [2018-12-18 11:01:42,27] [info] MaterializeWorkflowDescriptorActor [7ded9503]: Parsing workflow as WDL 1.0; [2018-12-18 11:01:42,46] [error] WorkflowManagerActor Workflow 7ded9503-e73c-4cca-ab37-5066c64780ef failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; ERROR: Unexpected symbol (line 4, col 15) when parsing 'call_body'. Expected input, got ""}"". call empty {}; ^. $call_body = :lbrace :input :colon $_gen18 :rbrace -> CallBody( inputs=$3 ); ; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:213). ```. and works again if `call empty {}` is changed to `call empty`. This shouldn't be a required change to update to v1.0, and would benefit from a fix!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4501:321,error,error,321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4501,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"System: mac; Cromwell: 36; Mode: Server; Backend: Local; Mysql: 5.5 or 5.7. ---. ### error info; ```; 2018-11-12 21:58:31,646 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; 2018-11-12 21:58:31,747 cromwell-system-akka.dispatchers.engine-dispatcher-27 ERROR - Error trying to fetch new workflows; com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '""WORKFLOW_STORE_ENTRY"" where ((""HEARTBEAT_TIMESTAMP"" is null) or (""HEARTBEAT_TIM' at line 1; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 	at java.lang.reflect.Constructor.newInstance(Constructor.java:422); 	at com.mysql.jdbc.Util.handleNewInstance(Util.java:425); 	at com.mysql.jdbc.Util.getInstance(Util.java:408); 	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:944); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3978); 	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3914); 	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2530); 	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2683); 	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2495); 	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1903); 	at com.mysql.jdbc.PreparedStatement.execute(PreparedStatement.java:1242); 	at com.zaxxer.hikari.pool.ProxyPreparedStatement.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java); 	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:38); 	at slick.jdbc.StatementInvoker.iteratorTo(StatementInvoker.scala:21); 	at sl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4382:85,error,error,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4382,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"T INTO public.databasechangelog (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, DESCRIPTION, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('add_hog_group_in_workflow_store', 'cjllanwarne', 'changesets/add_hog_group_in_workflow_store.xml', NOW(), 32, '8:618f223b37b310ec4ba7a1a89eb37e09', 'addColumn tableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,335 INFO - alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:34937,down,down,34937,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,1,['down'],['down']
Availability,TEST Integration errors: Chaos Cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2113:17,error,errors,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2113,1,['error'],['errors']
Availability,TEST Recovering Jobs: Turn Cromwell off/on,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2111:5,Recover,Recovering,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2111,1,['Recover'],['Recovering']
Availability,"TL;DR: Boot disk size is increased automatically in Papi v2 if the (approximated) docker image size is > than what is requested (or the default one). Only works for Dockerhub and GCR at this moment because quay doesn't support yet manifest v2. - Refactor of the DockerHashActor: got rid of akka stream, replaced with fs2 streams and cats.effect.IO. Easier to read/write, modify and maintain; - Instead of just getting the digest from the manifest header response, parse the content and get the size of the layer. The sum of the sizes == compressed size of the image; - The size of the layers is only available in the version 2 of the manifest schema https://docs.docker.com/registry/spec/manifest-v2-2/; GCR and Dockerhub support it, quay.io not yet (soon according to their support); - Approximate the uncompressed size using a configurable compression factor. If the approximation is higher than the requested boot disk size, use that instead (only for Papi v2); - The refactoring allows for: no need for explicitly differentiation gcr zones, which means `gcr.io`, `us.gcr.io`, `eu.gcr.io` and `asia.gcr.io` all work; - The refactoring also isolate each registry from each other with their own thread pool. Which means if say dockerhub is having very long response time all of sudden, gcr and quay aren't impacted",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4472:600,avail,available,600,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4472,1,['avail'],['available']
Availability,"TOL and assuming the investigation requested above confirms it would be helpful:. The current blacklisting implementation asks ""did a read from this source bucket result in a 403?"" If the answer is no, Cromwell tries to copy from that source bucket at full blast. However if Cromwell's first attempts to read from that source bucket do in fact result in 403s, there can be a large number of failed copies before blacklisting kicks in. One possibility would be modifying the question to ""what is the status of this source bucket with respect to 403s?"" with valid responses of ""known good"", ""known bad"" and ""I don't know"". . For ""I don't know"" Cromwell could be more cautious in its handling of that source bucket and launch a single ""canary"" copy attempt. Based on the result of that canary attempt Cromwell could choose ""known good"" or ""known bad"" and blacklisting could proceed the same as it does today with fewer copy failures.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4768#issuecomment-475667497:921,failure,failures,921,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4768#issuecomment-475667497,1,['failure'],['failures']
Availability,"TOL: Is this intended to be a user-available script or just for jenkins to run? If the latter, is the cromwell repo the best place for it? And should we have ""don't use this yourself from the command line!!"" warnings on the script?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5455#issuecomment-602869790:35,avail,available,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5455#issuecomment-602869790,1,['avail'],['available']
Availability,"Tagged all database specific tests as `IntegrationTest`.; As mysql now has 5s to timeout, increased the testing timeout used to detect if mysql is available.; Removed unused code and optimized imports.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/398:147,avail,available,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/398,1,['avail'],['available']
Availability,"Take the following simply WDL:; ```; version 1.0. workflow main {; call main {; input:; }; }. task main {; input {; String str; }. command <<<; >>>; }; ```. It will validate with Womtool:; ```; $ java -jar womtool-85.jar validate main.wdl ; Success!; ```. However, if you try to run it:; ```; $ java -jar cromwell-85.jar run main.wdl ; ...; Required workflow input 'main.main.str' not specified; ...; ```; It will immediately fail without being executed. Couldn't the same code in Cromwell that spots these errors be included in Womtool?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7139:507,error,errors,507,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7139,1,['error'],['errors']
Availability,"Task.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```. I tried reading the code but I don't know scala so did not get very far:; https://github.com/broadinstitute/cromwell/blob/f1cce2cd2723b849c4d8f285510f30913ec188a0/filesystems/sra/src/main/scala/cromwell/filesystems/sra/SraPathBuilder.scala. The input I feed in the following:; ```; ""Mutect2.tumor_reads"": ""sra://SRR9247315/NWD751606.b38.irc.v1.cram"",; ```. The whole json input looks like this:; ```; {; ""Mutect2.gatk_docker"": ""broadinstitute/gatk:4.1.4.1"",. ""Mutect2.intervals"": ""gs://gatk-best-practices/somatic-b37/whole_exome_agilent_1.1_refseq_plus_3_boosters.Homo_sapiens_assembly19.baits.interval_list"",; ""Mutect2.scatter_count"": 50,; ""Mutect2.m2_extra_args"": ""--downsampling-stride 20 --max-reads-per-alignment-start 6 --max-suspicious-reads-per-alignment-start 6"",; ""Mutect2.filter_funcotations"": ""True"",; ""Mutect2.funco_reference_version"": ""hg19"",; ""Mutect2.funco_data_sources_tar_gz"": ""gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.6.20190124s.tar.gz"",; ""Mutect2.funco_transcript_selection_list"": ""gs://broad-public-datasets/funcotator/transcriptList.exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt"",. ""Mutect2.ref_fasta"": ""gs://gatk-best-practices/somatic-b37/Homo_sapiens_assembly19.fasta"",; ""Mutect2.ref_dict"": ""gs://gatk-best-practices/somatic-b37/Homo_sapiens_assembly19.dict"",; ""Mutect2.ref_fai"": ""gs://gatk-best-practices/somatic-b37/Homo_sapiens_assembly19.fasta.fai"",; ""Mutect2.tumor_reads"": ""sra://SRR9247315/NWD751606.b38.irc.v1.cram"",; ""Mutect2.tumor_reads_index"": ""sra://SRR9247315/NWD751606NWD751606.b38.irc.v1.cram.crai"",. ""Mutect2.pon"": ""gs://gatk-best-practices/somatic-b37/Mutect2-exome-panel.vcf"",; ""Mutect2.pon_idx"": ""gs://gatk-best-practices/somatic-b37/Mutect2-exome-panel",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680:3418,down,downsampling-stride,3418,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-679399680,1,['down'],['downsampling-stride']
Availability,Task.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). ```. The job finally ends with errors:; ```; [error] WorkflowManagerActor Workflow 6bd79e09-cb56-480f-be46-0b2419591b3f failed (during ExecutingWorkflowState): java.lang.RuntimeException: AwsBatchAsyncBackendJobExecutionActor failed and didn't catch its exception.; 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:183); 	at cromwell.backend.standard.StandardSyncExecutionActor$$anonfun$jobFailingDecider$1.applyOrElse(StandardSyncExecutionActor.scala:180); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:298); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:431); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:521); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; Expecting cpu runtime attribute value greater than 0; Expecting cpuMin runtime attribute value greater than 0; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(Valida,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4591:5054,Fault,FaultHandling,5054,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4591,1,['Fault'],['FaultHandling']
Availability,Tasks cannot call cache if their names are different (unless the tasks have no inputs!). The following tasks cannot call cache:. ```; task foo {; Int i; command { echo ${i} }; }; ```. ```; task bar {; Int i; command { echo ${i} }; }; ```. The call caching simpletons shouldn't include the call name in the input hash keys:. ```; +----------------------------+-----------------------------------------+----------------------------------+-----------------------+; | CALL_CACHING_HASH_ENTRY_ID | HASH_KEY | HASH_VALUE | CALL_CACHING_ENTRY_ID |; +----------------------------+-----------------------------------------+----------------------------------+-----------------------+; ....; | 137 | input: File foo.i | 778138a97b315ec95f374425255f8b9e | 12 |; | 138 | input: File bar.i | 778138a97b315ec95f374425255f8b9e | 13 |; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1989:163,echo,echo,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1989,2,['echo'],['echo']
Availability,Tasks with large files (>100GB) in input take a lot of time to start even if nothing (or almost nothing) happens in the command. I believe something should be optimized there because having a cache does not make sense when computing inputs hashes (or whatever slows thing down) is comparable or even slower to taking results out of cache,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6213:272,down,down,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6213,1,['down'],['down']
Availability,"Terra/Cromwell workflows using data that has been exported from the UChicago Gen3/Windmill system or the HCA Data Browser with DRS URI data references frequently (always?) fail in the Ammonite script that performs the DRS resolution/localization. Failed workflows using DRS URI data references most often have error messages and logs as shown below. These examples are from the Terra workspace `firecloud-cgl/20190701 Test` in which a small number of files were exported from Windmill to Terra, and an md5sum workflow was exported from Dockstore. These same error messages and log entries have been seen in many other similar workspaces over the last couple/few months (no data before that). @abaumann has been recently and actively involved in the investigation of this problem, and has access to this workspace. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/01 22:54:02 Starting container setup.; 2019/07/01 22:54:11 Done container setup.; 2019/07/01 22:54:17 Starting localization.; 2019/07/01 22:54:24 Localizing input dos://dg.4503/1406db81-91d7-4e57-ada3-40487199ed06 -> /cromwell_root/topmed-irc-share/genomes/NWD522711.b38.irc.v1.cram; Compiling (synthetic)/ammonite/predef/interpBridge.sc; ```. or. ```; Task ga4ghMd5.md5:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. The assigned worker has failed to complete the operation	. 2019/07/10 19:25:06 Starting container setup.; 2019/07/10 19:25:14 Done container setup.; 2019/07/10 19:25:20 Starting localization.; 2019/07/10 19:25:26 Localizing input dos://dg.4503/1cba8116-a3d1-41e6-aab3-428e4f42e916 -> /cromwell_root/topmed-irc-share/genomes/NWD735861.b38.irc.v1.cram.crai; Compiling (synthetic)/ammonite/predef/interpBridge.sc; Compiling (synthetic)/ammonite/predef/DefaultPredef.sc; ```. In some cases, additional information is logged, as in the following example where Ammonit",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5069:310,error,error,310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5069,3,['error'],['error']
Availability,Test failure is unrelated to this PR. Will investigate that separately.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/215#issuecomment-145959320:5,failure,failure,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/215#issuecomment-145959320,1,['failure'],['failure']
Availability,"Test failures unrelated to this PR should be fixed, as of the merge I did just now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7434#issuecomment-2168684880:5,failure,failures,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7434#issuecomment-2168684880,1,['failure'],['failures']
Availability,Test reliability: filesystem startup timeouts and docker health check errors [BA-6164],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5341:5,reliab,reliability,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5341,2,"['error', 'reliab']","['errors', 'reliability']"
Availability,"Tested by localizing a Blob file from Terra Prod, on a patched Cromwell in Terra Dev. This PR with foreign Blob URL:. ```; 2023-12-20 18:12:17.197 Tes.Runner.Transfer.BlobOperationPipeline[0] Completed download. Total bytes: 7,811,366,912 Filename: /mnt/batch/tasks/workitems/TES-ybjxkg-D5_v2-4yab26tn3af2kf6dfa755sbg5oeqevqw-6cylhedz/job-1/a7123170_f41bbba17a6f4409940127a60234695d-1/wd/wd/cromwell-executions/localizer_workflow/a7123170-1652-45b8-a8ba-c7bef84acac4/call-localizer_task/inputs/lz304a1e79fd7359e5327eda.blob.core.windows.net/sc-705b830a-d699-478e-9da6-49661b326e77/inputs/Rocky-9.2-aarch64-dvd.iso; 2023-12-20 18:12:17.200 Tes.Runner.Transfer.ProcessedPartsProcessor[0] All parts were successfully processed.; 2023-12-20 18:12:17.200 Tes.Runner.Transfer.PartsReader[0] All part read operations completed successfully.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.PartsWriter[0] All part write operations completed successfully.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Pipeline processing completed.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Waiting for processed part processor to complete.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Processed parts completed.; 2023-12-20 18:12:17.204 Tes.Runner.Executor[0] Executed Download. Time elapsed: 00:00:13.0435715 Bandwidth: 571.12 MiB/s; 2023-12-20 18:12:17.208 Tes.RunnerCLI.Commands.CommandHandlers[0] Total bytes transferred: 7,811,369,114; /cromwell-executions/localizer_workflow/a7123170-1652-45b8-a8ba-c7bef84acac4/call-localizer_task/execution; ```. This PR with a regular HTTPS URL from the 'net:; ```; 2023-12-20 18:42:08.430 Tes.Runner.Transfer.BlobOperationPipeline[0] Completed download. Total bytes: 1,553,924,096 Filename: /mnt/batch/tasks/workitems/TES-ybjxkg-D5_v2-4yab26tn3af2kf6dfa755sbg5oeqevqw-6cylhedz/job-1/f9b357bc_8d135cf26c4345599dbd046d5892d274-1/wd/wd/cromwell-executions/localizer_workflow/f9b357bc-4a13-4923-9b90-0f707ae9f435",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7347:202,down,download,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7347,1,['down'],['download']
Availability,"Tested locally with a long pause inserted into the metadata write actor. Shutting down and restarting after the subworkflow ""finished"" but before the final state metadata was written did not cause inconsistency in statuses written to the DB.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6433:82,down,down,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6433,1,['down'],['down']
Availability,"Tested the WDL below with 831512a and got the error below:. ```wdl; version 1.0. workflow wdl_v1_tests {; scatter (x in [0]) {; scatter (y in [0]) {; call input_default_not_used; }; }; }. task input_default_not_used {; input { String greeting = ""hello"" }; command { echo ~{greeting} }; runtime { docker: ""bash"" }; }; ```. ```; [2018-06-08 01:30:26,49] [error] WorkflowManagerActor Workflow 58ccc276-40f7-447c-bbff-87a47aa7163e failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; key not found: wdl_v1_tests.input_default_not_used.greeting; scala.collection.immutable.Map$Map1.apply(Map.scala:111); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$8(ScatterElementToGraphNode.scala:103); scala.collection.immutable.List.map(List.scala:283); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.$anonfun$convertInnerScatter$7(ScatterElementToGraphNode.scala:102); cats.data.Validated.map(Validated.scala:194); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convertInnerScatter(ScatterElementToGraphNode.scala:99); wdl.draft3.transforms.wdlom2wom.graph.ScatterElementToGraphNode$.convert(ScatterElementToGraphNode.scala:31); wdl.draft3.transforms.wdlom2wom.graph.WorkflowGraphElementToGraphNode$.convert(WorkflowGraphElementToGraphNode.scala:49); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:82); common.validation.ErrorOr$ShortCircuitingFlatMap$.flatMap$extension(ErrorOr.scala:27); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.graphNodeCreationFold$1(WorkflowDefinitionElementToWomWorkflowDefinition.scala:77); wdl.draft3.transforms.wdlom2wom.WorkflowDefinitionElementToWomWorkflowDefinition$.$anonfun$makeWomGraph$6(WorkflowDefinitionElement",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3751:46,error,error,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3751,3,"['echo', 'error']","['echo', 'error']"
Availability,"Tested with Cromwell 52, 53. `memory-retry` does not work as expected. First of all it's activated only when `continueOnReturnCode` is set as `true` or list of some return codes.; I think this is intended as described in the documentation but why?. This is very weird. In most cases, return code of OOM is just 137.; Why don't we have something like `memoryRetryReturnCode`. I think it's too dangerous too set `continueOnReturnCode` as `true`.; Cromwell will pass any failure in all tasks.; So I set it as `[0, 137]` to catch `SIGKILL` due to OOM.; I also tried with `true` though. Here is my simple OOM tester WDL. I tested it with PAPIv2 beta based on Life Sciences API. ```wdl; version 1.0. workflow mem_retry {; call fail_oom; }. task fail_oom {; command {; set -e; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero # <====== This WDL works fine without this line; }; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; continueOnReturnCode: [0, 137]; }; }; ```. Google backend (PAPI2 beta) in `backend.conf`, ; ```; config {; memory-retry {; error-keys = [""OutOfMemoryError"", ""Killed""]; multiplier = 1.5; }; }; ```. STDERR of task:; ```; $ gsutil cat gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/stderr; /cromwell_root/script: line 28: 17 Killed tail /dev/zero; ```. RC of task. It's weird that this is not caught in `metadata.json`.; ```; $ gsutil cat gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/rc; 137; ```. `memory_retry_rc`: So Cromwell found that it's failed due to OOM.; ```; $ gsutil cat gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/memory_retry_rc; 0; ```. `metadata.json`; ```; {; ""workflowName"": ""mem_retry"",; ""workflowProcessingEvents"": [; {; ""timestamp"": ""2020-08-29T00:00:38.724Z"",; ""cromwellVersion"": ""53"",; ""cromwellId"": ""cromid-0a29b92""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815:468,failure,failure,468,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815,1,['failure'],['failure']
Availability,"Tested with the app, confirmed that public files are able to be read and that we hit the expected TES error when trying to read other workspace blobs that are not private",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7140#issuecomment-1589517248:102,error,error,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7140#issuecomment-1589517248,1,['error'],['error']
Availability,Testing out disabling CWL in FC so I set the CWL language factory to `enabled=false`. The error message was very wrong:. ```; cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; WDL draft 3 is not enabled; ```. Due to this [wonky bit](https://github.com/broadinstitute/cromwell/blob/develop/languageFactories/language-factory-core/src/main/scala/cromwell/languages/StandardLanguageFactoryConfig.scala#L10) of code.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3921:90,error,error,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3921,1,['error'],['error']
Availability,"Thank you for that information, its helpful to gage the impact of this bug. I do think the error rate here is higher than we'd like, and so its worth addressing sooner.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4148#issuecomment-424960611:91,error,error,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4148#issuecomment-424960611,1,['error'],['error']
Availability,"Thank you for the quick reply, Adam!. I wish the error message was a little more helpful, but this was definitely the issue and I have it working now!! :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134986904:49,error,error,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134986904,1,['error'],['error']
Availability,Thank you for the thumbs! A friendly reminder that the related https://github.com/broadinstitute/firecloud-develop/pull/3194 needs similar thumbing to actually make this reference image available in Terra.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1387372574:186,avail,available,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6975#issuecomment-1387372574,1,['avail'],['available']
Availability,"Thanks - btw, looking at the code, it doesn't seem like our _specific_ error message - `IOException: Could not read from gs://<...>: 503 Service Unavailable Backend Error` - is not covered by the tests. I see a similar test for 500, should I add one for 503 as well?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557696421:71,error,error,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557696421,2,"['Error', 'error']","['Error', 'error']"
Availability,"Thanks @aednichols, looks like I didn't scroll down enough - I think @cjllanwarne added this test at the very start so wasn't on my horizon to check. I've pushed a fix, and will confirm when it's fixed. Edit: Initial logs look like it's passing. Edit 2: Looks like `womtool/src/test/resources/validate/biscayne/valid/*` must have a workflow to validate - fixed and running tests again. ```; [info] - should be able to output a graph for biscayne workflow: 'sep_function' *** FAILED *** (59 milliseconds); [info] In WDL not in Graph: Set(); In Graph not in WDL: Set(SepTestInInterpolatorBlock) Set() was not equal to Set(""SepTestInInterpolatorBlock"") (WomtoolValidateSpec.scala:84); [info] org.scalatest.exceptions.TestFailedException:; [info] ...; [info] at womtool.WomtoolValidateSpec.$anonfun$new$14(WomtoolValidateSpec.scala:84); [info] at org.scalatest.Assertions.withClue(Assertions.scala:1221); [info] at org.scalatest.Assertions.withClue$(Assertions.scala:1208); [info] at org.scalatest.FlatSpec.withClue(FlatSpec.scala:1685); [info] at womtool.WomtoolValidateSpec.$anonfun$new$12(WomtoolValidateSpec.scala:84); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] ...; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553:47,down,down,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656436553,1,['down'],['down']
Availability,Thanks @carbocation - based on what you're saying it sounds like those run creation requests are in fact succeeding but just taking longer than Cromwell's request timeout to respond. For whoever picks up this ticket: I believe that wiring through an option to increase [timeouts on the requests to Google](https://developers.google.com/api-client-library/java/google-api-java-client/errors) (and make the value configurable) is hopefully sufficient for fixing this error.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914#issuecomment-488437019:383,error,errors,383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914#issuecomment-488437019,2,['error'],"['error', 'errors']"
Availability,"Thanks @cjllanwarne , . > 2019-04-24 10:49:25,556 INFO - DispatchedConfigAsyncJobExecutionActor [UUID(917dfbca)JointGenotyping.ImportGenotypeGVCFs:7640:1]: Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts). So I guess I have not managed to enable polling after all! Edit: I found that I mispelt the configuration line, I wonder if there was a message telling me about misspelling in the logs. I had not heard of exit-poll-timeout, I assume you mean exit-code-timeout-seconds, or is this the `unrelated to this timeout`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-486025965:248,alive,alive,248,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-486025965,1,['alive'],['alive']
Availability,"Thanks @geoffjentry !. Yeah, I'd realised that I needed to pass an absolute path into the workflow I'm trying to run, which works for running on our local SLURM cluster, but definitely wouldn't be portable or the ""right"" way to create the workflow. It sounds like the Directory type would be the right way to do this in future, and as you say, passing a Directory down to a task to pick the needed files out. . In the meanwhile, do you know if Cromwell's CWL implementation supports the CWL Directory type? Switching to CWL might also make sense for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526#issuecomment-452452934:364,down,down,364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526#issuecomment-452452934,1,['down'],['down']
Availability,"Thanks @illusional !. Here is the complete error and I can confirm there is no valid value for docker in the run time section. It also completely hangs when I run this and I have to kill the process. The Local provider is placed right after the Slurm provider in the provider block. ```; [2020-09-17 21:41:42,92] [info] MaterializeWorkflowDescriptorActor [866769d0]: Call-to-Backend assignments: hostremoval_subworkflow.interleave_task -> Local, geneprediction_subworkflow.prodigal_task -> Local, qc_subworkflow.flash_task -> Local, assembly_subworkflow.blast_task -> Local, metaGenPipe.merge_task -> Local, geneprediction_subworkflow.diamond_task -> Local, assembly_subworkflow.metaspades_task -> Local, assembly_subworkflow.megahit_task -> Local, hostremoval_subworkflow.hostremoval_task -> Local, geneprediction_subworkflow.collation_task -> Local, assembly_subworkflow.idba_task -> Local, qc_subworkflow.trimmomatic_task -> Local, metaGenPipe.taxonclass_task -> Local, qc_subworkflow.fastqc_task -> Local, metaGenPipe.multiqc_task -> Local; [2020-09-17 21:41:42,97] [error] Error parsing generated wdl:; task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --en",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938:43,error,error,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862#issuecomment-694515938,1,['error'],['error']
Availability,"Thanks @kshakir! . Mint team just noticed a similar issue for a few of our workflows, where the workflow status in Cromwell is ""running"" but the VM instance is no longer running. These specific workflows were ""stuck"" on the first task and did not start any subworkflows. When trying to abort the workflow, I get the following error: . ```; {; ""status"": ""error"",; ""message"": ""Couldn't abort 467b64cc-9aa4-4eaf-85ef-16ed4d540d4c because no workflow with that ID is in progress""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3654#issuecomment-402854046:326,error,error,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3654#issuecomment-402854046,2,['error'],['error']
Availability,"Thanks @mcovarr for your response. I realized that after my initial post and created a labels.json with the following contents:. ```; {; ""google_labels"": {; ""my-private-network"": ""xxx"",; ""my-private-subnetwork"": ""yyy""; }; }; ```. where xxx and yyy are my actual vpc network and subnet names in GCP. Then I added the ""-l labels.json"" option to the cromwell run command but that still gives me the same error. Am I missing something here? Apologies but this is what I am understanding from the posts/docs that needs to happen but won't work when I try it. Am I supposed to create some label in the actual GCP account as well?. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905064870:401,error,error,401,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477#issuecomment-905064870,1,['error'],['error']
Availability,"Thanks @mcovarr, that version of the WDL does indeed work now when run locally with docker (it didn't in 24, which is why I retried using task declarations on @LeeTL1220's suggestion - doing so caused the Firecloud failure reported in [GAWB-1704](https://broadinstitute.atlassian.net/browse/GAWB-1704) to match the local docker error I reported here). Unfortunately, it still fails in Firecloud with the error I initially reported in the [Firecloud forum](http://gatkforums.broadinstitute.org/gatk/discussion/8864/how-can-a-method-configuration-locate-a-file-generated-by-wdl-method-write-lines-array-file) and what @jmthibault79 is seeing:; > write_lines creates two temp files, (a) with unlocalized file paths as described above, and (b) with correctly localized file paths; > file (a) gets localized; > file (b) does not get localized; > the script correctly looks for file (b) but can't find it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-294191000:215,failure,failure,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906#issuecomment-294191000,3,"['error', 'failure']","['error', 'failure']"
Availability,"Thanks @mr-c, I modified the example a bit to be compatible with the classes present in our JVM and I do now see a difference between `Constructor` and `SafeConstructor` that suggests we could have been exposed before the change. (Deliberately ommited `autoCommit` because it seems to be unsupported in our JVM and causes a much less interesting error.); ```; cwlVersion: v1.0; class: Workflow; inputs: []; steps: []; outputs: []. hints:; - class: foo; bar: !!com.sun.rowset.JdbcRowSetImpl; dataSourceName: ldap://attacker/obj; ```. Old Cromwell, workflow succeeds with just some extra log messages:. ```; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.yaml.snakeyaml.constructor.BaseConstructor (file:/Users/anichols/Downloads/cromwell-69.jar) to constructor com.sun.rowset.JdbcRowSetImpl(); WARNING: Please consider reporting this to the maintainers of org.yaml.snakeyaml.constructor.BaseConstructor; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; ```; ```; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: checking item; ../../../var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/cwl_temp_file_ef439db0-21c5-4035-87b2-0613819fc113.cwl:8:3: Field `class` contains undefined reference to `file:///var/folders/xj/rglhyd6s2lbbrz8r53vn_rww0000gp/T/cwl_temp_dir_8673604963791319219/foo`; ```. New Cromwell, workflow is rejected:. ```; Workflow input processing failed:; could not determine a constructor for the tag tag:yaml.org,2002:com.sun.rowset.JdbcRowSetImpl; in 'reader', line 9, column 8:; bar: !!com.sun.rowset.JdbcRowSetImpl; ^; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194:346,error,error,346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6510#issuecomment-932368194,2,"['Down', 'error']","['Downloads', 'error']"
Availability,"Thanks @pshapiro4broad . Yes, I agree that this should be fixed. That error is being caught by our ""oops we have no idea what happened"" when clearly there's some repeatable code path causing an error here where we *should* know what's going on. It'd be good to fix this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002:70,error,error,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-318426002,2,['error'],['error']
Availability,"Thanks @pshapiro4broad, sorry for detached example filename. I think I see more now what's happening now, but I'm less convinced it's the right behaviour. I believe the outputs section is using whatever is provided in the input, where I would expect it to be using the localised path. You're correct, if you use the full path in the inputs json (eg: `/path/to/inputs.json`), you get that full path in the outname. I'll illustrate it a little bit differently, note that I've annotated a container in the basenametest task of the following workflow:. **outputsnametest.wdl**; ```wdl ; version development . workflow basenameconnectiontest {; call createFile {}; call basenametest {; input:; inp=createFile.out; }. output {; String outname = basenametest.outname; String outbasename = basenametest.outbasename; String out = basenametest.out; }; }. task createFile {; input {}. command <<<; echo ""Hello, Broad!""; >>>. output {; File out = stdout(); }; }. task basenametest {; input {; File inp; }. command <<<; echo '~{inp}'; echo '~{basename(inp)}'; >>>; ; output {; String outname = inp; String outbasename = basename(inp); String out = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }; ```. This time running with no inputs:. ```bash; java -jar cromwell-52 run outputsnametest.wdl; ```. ```json; {; ""outputs"": {; ""basenameconnectiontest.outbasename"": ""stdout"",; ""basenameconnectiontest.out"": ""/cromwell-executions/basenameconnectiontest/51e0cb4d-f706-4540-9303-ed9c70a8764e/call-basenametest/inputs/-43085659/stdout\nstdout"",; ""basenameconnectiontest.outname"": ""/Users/franklinmichael/Desktop/tmp/wdltests/cromwell-executions/basenameconnectiontest/51e0cb4d-f706-4540-9303-ed9c70a8764e/call-createFile/execution/stdout""; },; ""id"": ""51e0cb4d-f706-4540-9303-ed9c70a8764e""; }; ```. I would expect that the outname be the localised pathname (in the container), in this case that would be `/cromwell-executions/..../`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-677957329:887,echo,echo,887,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5785#issuecomment-677957329,3,['echo'],['echo']
Availability,"Thanks @wleepang for the response. I was using the custom AMI as specified in the link. But I also had a LaunchTemplate that will mount EFS to batch computes. The problem was my userData in the LT had this line ""yum update"" that updated everything including the ECS-agent to the latest version and then it failed with the specified error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4604#issuecomment-464252953:332,error,error,332,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4604#issuecomment-464252953,1,['error'],['error']
Availability,Thanks Denis - this was my fault :confounded:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/414#issuecomment-178252223:27,fault,fault,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/414#issuecomment-178252223,1,['fault'],['fault']
Availability,Thanks again for looking into the null hash issue. I unfortunately don't have a small reproducible case but have a larger case that seems to always hit this problem if the traceback isn't enough information to debug. It's the `somatic-giab-mix` CWL validation set from here:. https://github.com/bcbio/bcbio_validation_workflows#somatic-genome-in-a-bottle-mixture. and the first errors start to occur ~2hr into the run. Sorry this isn't a minimal case but hope it's useful when you have an opportunity to look into it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-387799021:378,error,errors,378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-387799021,1,['error'],['errors']
Availability,"Thanks for getting back to me, Ruchi.; Truth be told, I only started seriously using Cromwell since 10 days ago, hence I’ve no idea about the rate before that. Sorry. . The error rate is rather low (whether acceptable, I’m not sure), considering I’m seeing this many failures out of ~ 4000 VM instances.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4148#issuecomment-424946693:173,error,error,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4148#issuecomment-424946693,2,"['error', 'failure']","['error', 'failures']"
Availability,Thanks for paying down the doc debt and creating the runtime attributes section! I would only request a couple of minor adjustments to the example I wrote then :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/192#issuecomment-144363200:18,down,down,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/192#issuecomment-144363200,1,['down'],['down']
Availability,"Thanks for reporting, @asmoe4. A few questions:; - Which Cromwell version are you using?; - Have you ever run this workflow successfully on a previous version of Cromwell?; - Does the workflow run on a backend other than AWS? You could try e.g. the [local backend](https://cromwell.readthedocs.io/en/stable/backends/Local/). ---. Developer notes:; - I've never seen the error message `error in opening zip file` before, a quick Google suggests it's coming straight out of `java.util.zip`; - The only recent zip change I can think of is one that Shouldn't Break Anything, https://github.com/broadinstitute/cromwell/pull/4399. If this is a regression (based on reported Cromwell version), that's where I'd start looking.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4509#issuecomment-451199608:370,error,error,370,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4509#issuecomment-451199608,2,['error'],['error']
Availability,"Thanks for reporting, @vruano - I've modified the title to reflect your discovery that this was caused by an empty scatter and that better logs from Cromwell would be nice when debugging. I think the yellow triangle in the UI would be a FireCloud change - perhaps you'd want to ask them to make a change too to make this even more visible? (the Cromwell team can only change what logs we produce and the error messages on failure, not how the UI interprets what we give them!)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4136#issuecomment-424011899:404,error,error,404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136#issuecomment-424011899,2,"['error', 'failure']","['error', 'failure']"
Availability,"Thanks for the clarification. I get same errors with `version development`.; ; > Since you specified version 1.0 it is not weird that it crashes on stuff that is not valid WDL 1.0. I find this a little surprising. Maybe it is a documentation issue. The 1.0 spec says object literal and then shows an example using map literal? . > Struct Assignment from Object Literal; > Structs can be assigned using an object literal. When Writing the object, all entries must conform or be coercible into the underlying type they are being assigned to; > ; > Person a = {""name"": ""John"",""age"": 30}. In any case I think I was confusing Object type with object literal. I took following to mean object notation was also being removed:. > Be careful when using Object. They are superceded by 'struct' in WDL 1.0 and are being removed outright in WDL 2.0. Can probably resolve this if this is intended usage.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5414#issuecomment-599692822:41,error,errors,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5414#issuecomment-599692822,1,['error'],['errors']
Availability,Thanks for the comments. I pushed the README changes and Funnel is now being downloaded from the broad's fork. . It looks like both the Local and Tes Centaur build are failing now though?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280774491:77,down,downloaded,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280774491,1,['down'],['downloaded']
Availability,Thanks for the heads up. We haven't gotten around to move our cluster to 51 yet. Pinging @DavyCats so he is also aware.; We will investigate the issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-638049145:81,Ping,Pinging,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-638049145,1,['Ping'],['Pinging']
Availability,Thanks for the link to the centaur docs! I'll work on adding a testcase for this. Looks like that CI failure is a timeout error? Not sure what to make of that... but I'll see what happens after adding the new testcase.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525773176:101,failure,failure,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5143#issuecomment-525773176,2,"['error', 'failure']","['error', 'failure']"
Availability,"Thanks for the ping, it had slipped my mind.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6959#issuecomment-1343079134:15,ping,ping,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6959#issuecomment-1343079134,1,['ping'],['ping']
Availability,Thanks for the quick replies. You're saying this happens for only a few shards in the same scatter ? If that's the case it would suggest this is some sort of transient failure of gsutil to authenticate properly but I'm not sure why that would result in this error message,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435949112:168,failure,failure,168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-435949112,2,"['error', 'failure']","['error', 'failure']"
Availability,"Thanks for the report. The ""error"" is reproducible and should be cleaned up, along with the docs. Your original issue would be better discussed over in the [WDL forums](https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team). If the hint below doesn't help, please make a forum post and drop a link here so that others may follow it back. I can post screen shots in the forum plus other users can follow what's going on. 1. To find the button you're probably looking for click on the first ""POST"" and then ""Try it out"". Again happy to post screenshots over in the forum.; 2. The ""error"" seen is a swagger-online error, not a swagger-ui error.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3869#issuecomment-403295623:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3869#issuecomment-403295623,4,['error'],['error']
Availability,Thanks for the response @danbills - this is on a compute node which only has access to the outside network through an http(s)/ftp proxy so it can't do name resolution for outside addresses. Why does cromwell try to resolve that particular address? We don't have docker available and I tried with a config file that only defines a local and slurm backend and still get the same exceptions.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462393738:269,avail,available,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462393738,1,['avail'],['available']
Availability,"Thanks for the update! We will assign reviewers to give proper feedback, as soon as someone is available.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488296313:95,avail,available,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488296313,1,['avail'],['available']
Availability,"Thanks for this Ales,. I wonder if it's a slurm thing?; https://slurm.schedmd.com/resource_limits.html. Some key things to look at from here:; https://slurm.schedmd.com/slurm.conf.html. MaxJobCount - Number of jobs in the active database - this includes recently finished jobs, new jobs won't go into pending.; MaxSubmitJobs - Can be set for a per user too. Could Cromwell be continuously trying to submit jobs and waiting until a new position is available?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-652768986:447,avail,available,447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-652768986,1,['avail'],['available']
Availability,"Thanks for your reply. I set the sql_mode ```SET GLOBAL sql_mode = 'ANSI_QUOTES';``` , than a new error occur. But when I change ``` driver = ""slick.driver.MySQLDriver$"" ``` to ```profile = ""slick.jdbc.MySQLProfile$""``` in cromwell.conf, all going well. ```; database {; # driver = ""slick.driver.MySQLDriver$""; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true&useSSL=false""; user = ""user""; password = ""123456""; connectionTimeout = 5000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4382#issuecomment-438664522:98,error,error,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4382#issuecomment-438664522,1,['error'],['error']
Availability,"Thanks much for testing this out. I'm happy to help with whatever I can for supporting this. I haven't seen this previously and am kind of surprised that it hits memory issues. This is a tiny test dataset so I'm not sure why it hits a 4Gb limit. It shouldn't use much memory at all.The error comes from within pyflow, which is an internal workflow system manta uses for running:. https://github.com/Illumina/pyflow/blob/aac143d6b95ddfdc1dad7b2a7226b03a41379b58/pyflow/src/pyflow.py#L3660. I wish it told us the memory it thought the system had and what it wants so we'd have more idea of what is happening. I don't think Cromwell is doing anything wrong here and asking for more memory would be the first thing I'd try as well. Let me know if this doesn't fix and we can try to explore more. Thanks again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435950771:286,error,error,286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435950771,1,['error'],['error']
Availability,"Thanks much for the helping with debugging on this. . Beyond the hash failure from Cromwell the other errors I get are all from the workflow itself due to not preserving the original file names. The numerical hashes for files get passed directly into the downstream tools, stripping off any extensions or other identifying information. This results in tool confusion, like tabix can't tell a file wasn't already gzipped:; ```; ValueError: Unexpected tabix input: /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/cromwell_work/cromwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-prep_samples/shard-0/execution/bedprep/cleaned-8539016497173364825.gz; ```; or bwa can't find all the other associated indices:; ```; bwa mem /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/cromwell_work/cromwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-alignment/shard-1/wf-alignment.cwl/96d7b606-e0fe-4305-a586-e0fc4acf76f8/call-process_alignment/shard-0/inputs/1628767813 [...]. [E::bwa_idx_load_from_disk] fail to locate the index files; ```; Is it expected to lose the original input file names when passing through the pipeline. A lot of tools are sensitive to these and this might be the underlying issue. Regarding the configuration, without `http {}` in under `engine -> filesystems` I get a complaint about it not being supported, even with `http {}` under `backend -> providers -> Local -> config -> filesystems`:; ```; java.lang.IllegalArgumentException: Either https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa exists on a filesystem not supported by this instance of Cromwell, or a failure occurred while building an actionable path from it. Supported filesystems are: LinuxFileSystem. Failures: LinuxFileSystem: Cannot build a local path from https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa (RuntimeException) Please refer to the documentation for more information on h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320:70,failure,failure,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320,3,"['down', 'error', 'failure']","['downstream', 'errors', 'failure']"
Availability,"Thanks so much for the pointer to the hashing and details about adjusting that. You're exactly right that was what was happening. I saw the hash error and saw no new jobs launched, thought it had failed, so must have stopped the tasks before they finished the md5 summing and continued. I ran our analysis both with md5 checksumming and path based hashing, and it does save some time. The md5 based one took 6:43 and path based was 5:55. Doing based on paths works great for us so I'll swap to that as the default in bcbio pipelines. For database storage, are their known deficiencies with file based HSQL over MySQL? Do I have ways to mitigate those through different settings? We're planning to use MySQL in some circumstances but for providing something generally for users the file-based database will be the default and I'd like to make that as good and error free as possible. Is leaving this open for the `null` hash issue worthwhile? I also got some other hash errors on a different run that didn't affect completion but might be influencing re-use and storage that I could raise an issue on, but might be due to file based storage. If there are general things I can tweak and test I can do that first prior to opening additional issues. Thank you again for the help.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386666743:145,error,error,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386666743,3,['error'],"['error', 'errors']"
Availability,"Thanks so much, unfortunately there aren't any other errors. The runner kicks off 3 more jobs (for 3 variant callers on run on this failed cache input). Those all complete and then the main process just stops. There are no more submissions to the cluster or anything beyond the main job waiting. Here is the remainder of the log if it's helpful:; ```; [2018-05-02 15:22:58,85] [info] 9fa3ab92-97fd-4bed-a636-6eaf38941141-SubWorkflowActor-SubWorkflow-variantcall:1:1 [[38;5;2m9fa3ab92[0m]: Starting get_parallel_regions; [2018-05-02 15:22:58,85] [info] b0777d55-4f75-47aa-9655-3119936b10a5-SubWorkflowActor-SubWorkflow-variantcall:2:1 [[38;5;2mb0777d55[0m]: Starting get_parallel_regions; [2018-05-02 15:22:58,85] [info] b4328660-38fb-4bd7-8220-cd2f47bb26b2-SubWorkflowActor-SubWorkflow-variantcall:0:1 [[38;5;2mb4328660[0m]: Starting get_parallel_regions; [2018-05-02 15:22:59,95] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2mb4328660[0mget_parallel_regions:NA:1]: Unrecognized runtime attribute keys: memoryMax, tmpDirMin, cpuMax, cpuMin, tmpDirMax, outDirMin, memoryMin, outDirMax; [2018-05-02 15:22:59,95] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2m9fa3ab92[0mget_parallel_regions:NA:1]: Unrecognized runtime attribute keys: memoryMax, tmpDirMin, cpuMax, cpuMin, tmpDirMax, outDirMin, memoryMin, outDirMax; [2018-05-02 15:22:59,98] [[38;5;220mwarn[0m] DispatchedConfigAsyncJobExecutionActor [[38;5;2mb0777d55[0mget_parallel_regions:NA:1]: Unrecognized runtime attribute keys: memoryMax, tmpDirMin, cpuMax, cpuMin, tmpDirMax, outDirMin, memoryMin, outDirMax; [2018-05-02 15:23:00,78] [info] DispatchedConfigAsyncJobExecutionActor [[38;5;2m9fa3ab92[0mget_parallel_regions:NA:1]: [38;5;5m'bcbio_nextgen.py' 'runfn' 'get_parallel_regions' 'cwl' 'sentinel_runtime=cores,1,ram,3839.9999999999995' 'sentinel_parallel=batch-split' 'sentinel_outputs=region_block' 'sentinel_inputs=batch_rec:record'[0m; [2018-05-02 15:23:00,79] [info] Di",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386387039:53,error,errors,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584#issuecomment-386387039,1,['error'],['errors']
Availability,"Thanks to @mcovarr we *think* we figured out the cause of the memory leak.; The `WorkflowExecutionActor` was sending ""snapshots"" of its current internal data (including execution store and output store) to the `EJEA`and `SWEA` so they could pass it on to the JobPreparationActor (resp. `SubWorkflowPreparationActor`) so they could evaluate inputs using the OutputStore.; This PR rewires things so that the WEA data does not escape the WEA. Instead actors needing the OutputStore for input evaluation request it at the right time and it gets sent across but never stored in the downstream actors, therefore not holding references and allowing for proper GC.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2542:577,down,downstream,577,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2542,1,['down'],['downstream']
Availability,Thanks very much for the error report and fix! 😄 :+1: . [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2591/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2591#issuecomment-326653496:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2591#issuecomment-326653496,1,['error'],['error']
Availability,"Thanks! I did see the trailing `/` is added by Cromwell: https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/VpcAndSubnetworkProjectLabelValues.scala#L15. I tried to set by literals as the following:. ```; virtual-private-cloud {; network-name = ""$NETWORK-NAME""; subnetwork-name = ""$SUBNETWORK-NAME""; auth = ""application-default""; }; ```; where `$NETWORK-NAME` and `$SUBNETWORK-NAME` are replaced by the values of `my-private-network` and `my-private-subnetwork` labels, and hidden here. but my server failed immediately when starting:. ```; 2024-08-20 21:43:02 main WARN - Failed to build GcpBatchConfigurationAttributes on attempt 1 of 3, retrying.; cromwell.backend.google.batch.models.GcpBatchConfigurationAttributes$$anon$1: Google Cloud Batch configuration is not valid: Errors:; Virtual Private Cloud configuration is invalid. Missing keys: `network-label-key`.; ```. It looks like the GCP Batch config requires `network-label-key`, which is not optional...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641:864,Error,Errors,864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7500#issuecomment-2299820641,1,['Error'],['Errors']
Availability,"Thanks, @geoffjentry. The quota failures were on the JES side, I think, and were reported back to Cromwell; I'm not sure how you would be able to detect a failure of one type versus another. Probably good to follow the conversation [here](https://groups.google.com/forum/#!topic/google-genomics-discuss/OL18jPoPvPE), as it sounds like Google is still sorting out a few things.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645132:32,failure,failures,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260645132,2,['failure'],"['failure', 'failures']"
Availability,"Thanks, @wleepang. I had specified the correct `user_data` but had neglected to switch from the default `/scratch`. As of today, this worked for me after downloading the python AMI creation script you noted. ```; python create-genomics-ami.py \; --user-data cromwell-genomics-ami.cloud-init.yaml \; --key-pair-name KEYNAME \; --scratch-mount-point /cromwell_root \; --profile default \; --ami-description ""AMI for use with Cromwell""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4322#issuecomment-434894610:154,down,downloading,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4322#issuecomment-434894610,1,['down'],['downloading']
Availability,"Thanks. I think I have a clue, your lowest level exception says; ```; 504 Gateway Timeout\nGET https://storage.googleapis.com/download/storage/v1/b/mccarroll-mocha/o/cromwell%2Fcromwell-executions%2Fmocha%2F86d47e9a-5745-4ec0-b4eb-0164f073e5f4%2Fcall-idat2gtc%2Fshard-73%2Frc?alt=media; ```; which does not match the pattern; ```; "".*Could not read from gs.+504 Gateway Timeout.*""; ```; introduced in https://github.com/broadinstitute/cromwell/pull/5344",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760319699:126,down,download,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760319699,1,['down'],['download']
Availability,"Thanks. That seems kind of googly, not sure if it maps to AWS concepts. Would I put a region in there (like us-west-2) or an availability zone, such as us-west-2a? I thought that AZ's were governed by the VPC used by the compute environment and thus could not be influenced by anything in aws.conf, a WDL, or workflow options....",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493266406:125,avail,availability,125,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4974#issuecomment-493266406,1,['avail'],['availability']
Availability,"Thanks. What's strange is that many other analyses _were_ being run successfully at the time. If the PAPI retry mechanism is somewhat deficient, maybe it makes sense for Cromwell to mask that by resubmitting the analysis? E.g. there could be a config setting for the number of times to retry analyses that fail for transient-looking reasons.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001#issuecomment-495744779:182,mask,mask,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001#issuecomment-495744779,1,['mask'],['mask']
Availability,That certainly looks reasonable despite all the Travis redness. I restarted all the failures and assuming everything clears up :+1:. [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2951/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2951#issuecomment-348069333:84,failure,failures,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2951#issuecomment-348069333,1,['failure'],['failures']
Availability,That error is a failure of jes to initialize a vm for you in the first place. There are no logs to write beyond what's in there. I just took a quick glance but it looks like you're specifying a file which doesn't exist,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243440920:5,error,error,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243440920,2,"['error', 'failure']","['error', 'failure']"
Availability,That is correct - this already uses the metadata endpoint to get its data. The fancification can indeed happen anywhere - you can download the HTML and put a metadata file next to it and it works just the same,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/266#issuecomment-153127594:130,down,download,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/266#issuecomment-153127594,1,['down'],['download']
Availability,"That is surprising, the order should be preserved. Is the order you're seeing consistently the same or does it randomly change if you run it several times ?; If you have a small set of inputs that reproduces the error that would be helpful too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368074313:212,error,error,212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3314#issuecomment-368074313,1,['error'],['error']
Availability,"That makes sense, and I understand the concerns around call caching discussed in the linked issue. If this ENV injection will never be supported is there another recommended method for a workflow to pass information about itself outside cromwell as this seems to be something many people have requested (dating back at least 6 years based on that issue). Right now, as far as I'm aware, the only option is to poll the REST API which is suboptimal if you're running many workflows at once, and also means that the external service must be authed to either Terra or wherever your standalone cromwell server lives. It would be very useful for those of us that already have systems for tracking metadata, sample information, etc if cromwell had the ability to notify those systems when results were available somehow. Either through a step in the workflow itself as requested above, or perhaps via webhooks or similar. If not the injection solution above, is anything like that on the roadmap, or is this just not something the team is planning on addressing? Everyone has limited resources and I get that certain things just aren't a priority.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591332855:795,avail,available,795,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1591332855,1,['avail'],['available']
Availability,"That was actually my first thought. But with this:; ```; version 1.0. workflow main {; call main {; input:; x = 1; }; }. task main {; input {; Int x; }. command <<<; echo ~{if (x == 1) then 1 else 0}; >>>; }; ```; when I parse it:; ```; $ java -jar womtool-52.jar validate main.wdl ; ERROR: Unexpected symbol (line 16, col 16) when parsing '_gen23'. Expected rparen, got """". echo ~{if (x == 1) then 1 else 0}; ^. $e = :lparen $_gen23 :rparen -> TupleLiteral( values=$1 ); ```; I was under the impression that Cromwell automatically adds parentheses but I am not really sure how it actually works.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667242825:166,echo,echo,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5602#issuecomment-667242825,3,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability,"That's a good point, plugging an offset into [Khalid's playground](https://scastie.scala-lang.org/79qIiIkIRReJLtbyvWQ2cg) makes it explode; ```; val timeStr2 = ""2020-01-15T17:46:25.694148-02:00""; Try(LocalDateTime.parse(timeStr2)); ```; ```; Failure(java.time.format.DateTimeParseException:; Text '2020-01-15T17:46:25.694148-02:00' could not be parsed, unparsed text found at index 26):; scala.util.Try[java.time.LocalDateTime]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5821#issuecomment-685044992:242,Failure,Failure,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5821#issuecomment-685044992,1,['Failure'],['Failure']
Availability,That's probably going to bork a lot more stuff than just our tests. I can see why that change was put in but I don't think we can just blindly accept that behavior w/o running it past @kcibul - it's likely to cause a lot of commotion downstream.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-253886201:234,down,downstream,234,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1581#issuecomment-253886201,1,['down'],['downstream']
Availability,"That's true, from reading. >outside the command block, it has no affect. it does seem that it's an inadvertent no-op rather than an error. Good take!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6789#issuecomment-1177975945:132,error,error,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6789#issuecomment-1177975945,1,['error'],['error']
Availability,"The 'should' string is:; ```; should abort a workflow mid run and restart immediately abort.restart_abort_tes *** FAILED ***; ```. The error message is:; ```; Metadata mismatch for calls.scheduled_abort.aborted.executionStatus - expected: ""Failed"" but got: ""Aborted""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3392:135,error,error,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3392,1,['error'],['error']
Availability,The (edit by Chris: ~~Workflow Actor~~ WorkflowExecutionActor) should take action based on the kind of failure returned by the BE and retry if so indicated,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/655:103,failure,failure,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/655,1,['failure'],['failure']
Availability,"The AWS cromwell-aio template for Cromwell 43 (latest) returned the following error after a workflow is submitted. fyi. This template works fine with Cromwell 42. 2019-07-02 19:16:36,824 cromwell-system-akka.dispatchers.api-dispatcher-73 INFO - Unspecified type (Unspecified version) workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 submitted; 2019-07-02 19:16:37,222 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - 1 new workflows fetched by cromid-271b774: 10f172e8-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,239 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor Starting workflow UUID(10f172e8-b7ba-416f-964e-22ab8c7b38e3); 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor Successfully started WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3; 2019-07-02 19:16:37,248 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2019-07-02 19:16:37,271 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2019-07-02 19:16:37,932 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(AwsAuthMode.scala:116); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credential$lzycompute(AwsAuthMode.scala:127); 	at cromwell.cloudsupport.aws.auth.DefaultMode._credent",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:78,error,error,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['error'],['error']
Availability,The AWS dependencies needs to be pruned. The entire known universe of AWS libraries [were pulled in](https://github.com/broadinstitute/cromwell/blob/9bee537c5f6a9ff4e8597f75b6844c0eaee721cc/project/Dependencies.scala#L230) during the work-in-progress towards a new backend for cromwell. The image below was produced using [GrandPerspective](http://grandperspectiv.sourceforge.net/) on the expanded cromwell 33 jar. It shows the amount of AWS libraries that have been assembled transitively by that one `aws-sdk-java` [blanket](https://mvnrepository.com/artifact/software.amazon.awssdk/aws-sdk-java/2.0.0-preview-9) dependency. ![cromwell_expanded](https://user-images.githubusercontent.com/791985/43986825-3ccde314-9ce5-11e8-8260-c9bbb66d3623.png). The dependencies should be slimmed down to only the required libs.; We don't need to have AWS [Route53](https://aws.amazon.com/route53/) etc. zipped in the jar to run workflows on the upcoming backend.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3909#issuecomment-412242086:784,down,down,784,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3909#issuecomment-412242086,1,['down'],['down']
Availability,The CI unit tests are failing with:; > /home/travis/build/broadinstitute/cromwell/engine/src/test/scala/cromwell/webservice/SwaggerServiceSpec.scala:103:36: constructor Constructor in class Constructor is deprecated. I get the same build error locally. The deprecated class comes from SnakeYAML. Looking into it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6927#issuecomment-1270500310:238,error,error,238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6927#issuecomment-1270500310,1,['error'],['error']
Availability,"The CWL [File](http://www.commonwl.org/v1.0/CommandLineTool.html#File) type requires a number of additional attributes to pass conformance tests. [Previously](https://travis-ci.org/broadinstitute/cromwell/jobs/298402505) Cromwell was returning:. ```json; {; ""ac159f70-2fcf-469f-a23f-e53b894533b1908791632707518014.output_file"": ""/home/travis/build/broadinstitute/cromwell/cromwell-executions/ac159f70-2fcf-469f-a23f-e53b894533b1908791632707518014/ac159f70-2fcf-469f-a23f-e53b894533b1/call-ac159f70-2fcf-469f-a23f-e53b894533b1908791632707518014/execution/error.txt""; }; ```. When the tests are expecting output like:. ```json; {; ""output_file"": {; ""checksum"": ""sha1$f1d2d2f924e986ac86fdf7b36c94bcdf32beec15"",; ""class"": ""File"",; ""location"": ""error.txt"",; ""size"": 4; }; }; ```. [Currently](https://travis-ci.org/broadinstitute/cromwell/jobs/302086244) there does not seem to be any outputs generated. After discussions, the `WomSingleFile` will be updated to support the new File attributes. TBD: CWL conformance tests must pass, but metadata responses must be backwards compatible for WDL run in Firecloud. Acceptance criteria; - [ ] CWL conformance test # 10 passes; - [ ] Metadata responses for Firecloud are still backwards compatible",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2899:554,error,error,554,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2899,2,['error'],['error']
Availability,"The Centaur failures here are real, ~reverting to Draft~ closing until I sort this out.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6437#issuecomment-879017997:12,failure,failures,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6437#issuecomment-879017997,1,['failure'],['failures']
Availability,"The CopyWorkflowActor regularly gets timeouts when trying to copy the gigabytes of data that are typically associated with production workflows. Also this duplicates the amount of disk space used for a workflow. . This remedies that problem by hardlinking the files. It is much much faster, and the cromwell-executions folder can be safely removed afterward. This is very beneficial for people who run cromwell on a cluster backend in `run` mode. Ping @illusional . I have included a test case.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6672:447,Ping,Ping,447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6672,1,['Ping'],['Ping']
Availability,"The Cromwell 59 JAR download from GitHub is broken. Also, how does one access the JIRA board? I created an account and tried to access it, but got a permission denied message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6500:20,down,download,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6500,1,['down'],['download']
Availability,"The DNS name `batch.default.amazonaws.com` does not resolve - perhaps you need to change a value of `default` in the config to something else. For example, `batch.us-east-1.amazonaws.com` resolves fine (though predictably doesn't respond to ping, load a web page, etc.).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4334#issuecomment-434316568:241,ping,ping,241,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4334#issuecomment-434316568,1,['ping'],['ping']
Availability,"The DOS schema changed to a pattern not queryable by JSON Paths-- specifically returning a nested array-of-objects where the array is unordered but only a certain object-value with a prefix should be returned... Found that someone had made the more flexible JQ filter syntax available, so switched to that library. I've still seen no evidence that the DOS schema won't change again, but hopefully the JQ filters will allow flexibly plucking out the right field as the schema hardens. The other alternative was submitting another PR if/when the schema changes again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4127#issuecomment-423331432:275,avail,available,275,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4127#issuecomment-423331432,1,['avail'],['available']
Availability,The EJEA is sending RecoverJobCommand or ExecuteJobCommand depending on if the workflow is restarting or not. So I think this is already done.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/664#issuecomment-233432623:20,Recover,RecoverJobCommand,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/664#issuecomment-233432623,1,['Recover'],['RecoverJobCommand']
Availability,"The GCP Batch PR has seen a number of seemingly unrelated failures building the DRS localizer [(Github actions link)](https://github.com/broadinstitute/cromwell/actions/runs/5580938822/jobs/10198518044?pr=7177) and I saw them locally too, both on `develop` and the Batch branch [(Slack link)](https://broadinstitute.slack.com/archives/G01D73CM63S/p1689702982488299?thread_ts=1689702892.491819&cid=G01D73CM63S). ```; W: GPG error: http://security.ubuntu.com/ubuntu jammy-security InRelease: At least one invalid signature was encountered.; E: The repository 'http://security.ubuntu.com/ubuntu jammy-security InRelease' is not signed. process ""/bin/sh -c apt-get -y update"" did not complete successfully; ```. It seems that specifying the OS explicitly instead of implicitly helps work around the problem. I confirmed that yesterday's build of the localizer uses the same base so this is not a radical change. [Nightly:](https://hub.docker.com/layers/broadinstitute/cromwell-drs-localizer/86-af9660e/images/sha256-ee39681ef7287e904fdde01874b5dfa80b045aed28b9cc4b017554bdb40015f1?context=repo); ```; docker inspect broadinstitute/cromwell-drs-localizer:86-af9660e; ""Labels"": {; ""org.opencontainers.image.ref.name"": ""ubuntu"",; ""org.opencontainers.image.version"": ""22.04""; }; ```; Local:; ```; docker inspect broadinstitute/cromwell-drs-localizer:86-813fc98-SNAP; ""Labels"": {; ""org.opencontainers.image.ref.name"": ""ubuntu"",; ""org.opencontainers.image.version"": ""22.04""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7179:58,failure,failures,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7179,2,"['error', 'failure']","['error', 'failures']"
Availability,The HPC tutorial config causes the server to error.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3488:45,error,error,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3488,1,['error'],['error']
Availability,"The HTTP library we use [0] does not support proxies [1], therefore it is not possible for Cromwell to support them either without a whole-library replacement. The certificate error is normal and a red herring, it occurs because certs apply to domain names and not IP addresses. I can reproduce it locally with no proxy. [0] https://github.com/broadinstitute/cromwell/blob/17efd599d541a096dc5704991daeaefdd794fefd/project/Dependencies.scala#L166; [1] https://github.com/http4s/blaze/issues/656",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7136#issuecomment-1544540814:176,error,error,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7136#issuecomment-1544540814,1,['error'],['error']
Availability,"The Horicromtal Deadlock test started failing the morning of Monday, 2/26 with Docker pull failures:; ```; Head ""https://registry-1.docker.io/v2/dockercloud/haproxy/manifests/latest"":; error parsing HTTP 429 response body:; invalid character 'S' looking for beginning of value: ; ""Server capacity exceeded.\n""; ```; I was able to pull the [`dockercloud/haproxy` image](https://hub.docker.com/r/dockercloud/haproxy) locally but found that it was last updated 6 years ago. My suspicion is that ancient images are stored in a much less hot level of cache in the bowels of Docker Hub and may be more susceptible to capacity issues and timeouts. In order to adopt a current, official HAProxy image, I had to make a very basic config and we were off to the races. This is because the `dockercloud` image was a bit customized with special sauce to automatically configure itself by detecting running Docker containers. As a bonus, the new Alpine-based image is actually smaller than the ancient one, albeit only 25 MB vs. 43 MB.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7376:91,failure,failures,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7376,2,"['error', 'failure']","['error', 'failures']"
Availability,The Java SDK for AWS produces multiple JARs with some path conflicts. For example:; ```; [error] deduplicate: different file contents found in the following:; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/software/amazon/awssdk/ses/2.0.0-preview-9/ses-2.0.0-preview-9.jar:codegen-resources/service-2.json; [error] /Users/angel/Library/Caches/Coursier/v1/https/repo1.maven.org/maven2/software/amazon/awssdk/snowball/2.0.0-preview-9/snowball-2.0.0-preview-9.jar:codegen-resources/service-2.json; # ...; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3515:90,error,error,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3515,3,['error'],['error']
Availability,"The Local build did fail, due to an unrelated hiccup. A restart of the build cleared the error. Meanwhile, **J**ES Centaur failed due to #1717, but **T**ES passed just fine. Everything looking great. Thanks again for all of your contributions!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280815262:89,error,error,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1979#issuecomment-280815262,1,['error'],['error']
Availability,"The Methods Cromwell recently experienced an outage nearly identical to the Terra one, where requests to GCS and the database timed out. Like on Terra, a server restart fixed it. Their server is running version `54-97597a4` that [definitely has](https://github.com/broadinstitute/cromwell/commits/54_hotfix) the [PR](https://github.com/broadinstitute/cromwell/pull/5994) we did to handle null messages. In this PR I fixed another possible source of NPEs in `cromwell.engine.io.gcs.GcsBatchFlow#recoverCommand`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6118:45,outage,outage,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6118,2,"['outage', 'recover']","['outage', 'recoverCommand']"
Availability,"The OGE back end shell script that is submitted to the task scheduler on SGE/OGE back ends is potentially missing a sync statement. . `; (; cd /SingleSampleWF/ffc0dbea-a292-4e24-833a-d2010d373600/call-DeliverReports/execution; sync; ). mv /SingleSampleWF/ffc0dbea-a292-4e24-833a-d2010d373600/call-DeliverReports/execution/rc.tmp /SingleSampleWF/ffc0dbea-a292-4e24-833a-d2010d373600/call-DeliverReports/execution/rc; `. We've had cases where job completed successfully, OGE reports the job as exiting normally, but the rc file is written with a '79' (undocumented behavior until we located it in the source code). And yet on examination of the logs, there were no errors, and the rc file was moved from rc.tmp (which is no longer there). Our assumption is that the mv command is not getting flushed to our storage system, and if cromwell polls for zombie tasks before that happens we will see it as a job failure. . See https://github.com/broadinstitute/cromwell/blob/68949364c7ffab8450116baa8e2a4e276bb16d70/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L465. which is not followed by SCRIPT_EPILOGUE that would add the required sync command after the mv.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6988:663,error,errors,663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6988,2,"['error', 'failure']","['errors', 'failure']"
Availability,The PAPIv2 version of `interpretOperationStatus` calls `ErrorReporter#toUnsuccessfulRunStatus` instead of `RunStatus.UnsuccessfulRunStatus#apply`.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5129:56,Error,ErrorReporter,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5129,1,['Error'],['ErrorReporter']
Availability,"The Rawls behavior is not a simple ""retry on any error"". Here's what I'm seeing:; * User aborts a submission in FireCloud.; * Rawls marks submission as `Aborting`; * In a background actor, Rawls finds all workflows inside `Aborting` submissions and sends an abort request to Cromwell.; * In a background actor, Rawls periodically queries Cromwell for the status of each active workflow, then updates its db based on Cromwell's response; * if all workflows in a submission are complete (failed, succeeded, aborted), the submission is marked as complete. In the aberrant workflow cases I checked, Cromwell is returning a status of `Running` or `Submitted` from its workflow-status endpoint, but returns a 404 from its abort endpoint. I suspect that 1) the abort request will never succeed; 2) the workflow status endpoint's response will never change, and therefore 3) Rawls will never update its db and will be stuck trying to abort the workflow forever.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480:49,error,error,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4497#issuecomment-448063480,1,['error'],['error']
Availability,"The ServiceRegistry requires all services sitting behind it to at least be aware of the graceful shutdown infrastructure, e.g. handling a `ShutdownCommand` even if the service doesn't need to be graceful about its shutting downing. It'd be nicer if this could be made to not be the case",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2575:223,down,downing,223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2575,1,['down'],['downing']
Availability,"The SharedFileSystem (SFS) backend that is the basis for the local and SGE backends currently has a race condition in the script:. ``` bash; #!/bin/sh; cd $cwd; $instantiatedCommand; echo $$? > rc; ```; 1. The `echo` needs to write to a `rc.tmp` and then use the atomic `mv rc.tmp rc`. Otherwise cromwell will (very very rarely?) pickup the existence of the file before bash has written, flushed, and closed the rc contents. This is not an issue on JES because the API is waiting for the script to exit, not the appearance of the `rc` file.; 2. The `rc` path should be absolute, whether inside or outside of docker. `echo $$? > $cwd/rc` may be enough? Not an issue on JES as it only writes to the dockerized rc path.; 3. The SFS (and probably JES for consistency, and backends in general?) should run the WDL command in a subshell, either with a (yet-another) call to bash, or using [bashisms](https://github.com/koalaman/shellcheck/wiki/SC2103#correct-code). This would better protect that the WDL from killing the current shell without writing an `rc`. A proper centaur test would contain _diabolical_ WDL such as:. ```; command {; exit; }; ```. ```; command {; mkdir newdir; cd newdir; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1383:183,echo,echo,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1383,3,['echo'],['echo']
Availability,"The SingleWorkflow mode, via `java -jar cromwell.jar run …`, does not always wait for metadata to be completely processed. Thus when the actor exits, it may write out incomplete metadata json. This mainly seen during intermittent test failures that are looking for expected metadata from the output file:. ```scala; [info] SingleWorkflowRunnerActorWithMetadataSpec:; [info] A SingleWorkflowRunnerActor should ; [info] - successfully run a workflow outputting metadata *** FAILED *** (6 seconds, 143 milliseconds); [info] java.util.NoSuchElementException: None.get; [info] at scala.None$.get(Option.scala:347); [info] at scala.None$.get(Option.scala:345); [info] at cromwell.engine.workflow.SingleWorkflowRunnerActorSpec$OptionJsValueEnhancer$.toStringValue$extension(SingleWorkflowRunnerActorSpec.scala:44); [info] at cromwell.engine.workflow.SingleWorkflowRunnerActorSpec$OptionJsValueEnhancer$.toOffsetDateTime$extension(SingleWorkflowRunnerActorSpec.scala:43); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1932:235,failure,failures,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1932,1,['failure'],['failures']
Availability,"The Travis complaint appears to be a genuine failure, investigating",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-703704308:45,failure,failure,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5906#issuecomment-703704308,1,['failure'],['failure']
Availability,"The WDL is part of an entire pipeline, that I can't post here. But I can share this:; [WDLTesting.zip](https://github.com/broadinstitute/cromwell/files/6827255/WDLTesting.zip). ```; $ $CROMWELL_HOME/womtool validate WDLTesting/src/wdl/Workflow.wdl ; Failed to import 'WDLTesting/src/wdl/WriteTask.wdl' (reason 1 of 1): ERROR: Unexpected symbol (line 11, col 2) when parsing 'setter'. Expected equal, got ""String"". 	String	input2 = ""Default""; ^. $setter = :equal $e -> $1; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881118722:319,ERROR,ERROR,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438#issuecomment-881118722,1,['ERROR'],['ERROR']
Availability,"The `79` code is explained [here](https://github.com/broadinstitute/cromwell/blame/84/supportedBackends/sfs/src/main/scala/cromwell/backend/impl/sfs/config/ConfigAsyncJobExecutionActor.scala) and this might relate to an error in the command defined in the `check-alive` variable that is used to check whether a task is still alive. What could have happened is that the task completed correctly but when Cromwell went to check for the task being still running it received a temporary error which it interpreted as the task had failed, while the task might have completed successfully. A workaround is to substitute the `check-alive` function with something more complicated. On SLURM I have replaced:; ```; check-alive = ""squeue -j ${job_id}""; ```; with:; ```; check-alive = ""squeue -j ${job_id} || if [[ $? -eq 5004 ]]; then true; else exit $?; fi""; ```; The `5004` value corresponds to the SLURM error code SLURM_PROTOCOL_SOCKET_IMPL_TIMEOUT as implemented in [slurm_errno.h](https://github.com/SchedMD/slurm/blob/master/slurm/slurm_errno.h) and [slurm_errno.c](https://github.com/SchedMD/slurm/blob/master/src/common/slurm_errno.c). But I am not sure whether this workaround is sufficient",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093:220,error,error,220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6988#issuecomment-1622129093,8,"['alive', 'error']","['alive', 'error']"
Availability,"The `DrsCloudNioFileSystemProvider` was wrapping the retries of the `DrsPathResolver` with another set of `CloudNioRetry` retries. The product of these two retries at the previous configuration values would wait around 35 minutes (~:20 + 10 x ~3:30) to fail for each doomed attempt. That combined with a fairly wide scatter and a typo'd DRS path for `file` in code like . ```; task size {; input {; File file; }. Int file_size = ceil(size(file)); ...; }; ```; would completely block all 10 of the `IoActor`s [NIO threads](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/server/CromwellRootActor.scala#L108). These changes remove the nested retries in the engine and dial back the patience for retries. If we want the retries to be more patient we'll probably have to make other code that is competing for `IoActor` threads more patient as well. Utility files for reproducing this error can be cherry picked from commit `ff7bddc8830802f7a606177d0eaf19c8f47ca865`. I don't know how to programatically link Google accounts to NIH accounts in Bond to be able to include this Centaur test in CI, though maybe we don't need to be linked to make sure this negative case errors within a reasonable timeout?. Workflow`9635fbf0-00b1-4635-b482-5a782cda5cd5` induced this problem in production, its metadata shows multiple `HaplotypeCaller` shards erroring out with ; ```; Failed to evaluate input 'disk_size' (reason 1 of 1): [Attempted 1 time(s)] - RuntimeException: Unexpected response during DRS resolution: RuntimeException: Could not access object 'drs://dg4.DFC/...'. Status: 500, reason: 'Internal Server Error', Martha location: 'https://.../martha_v3', message: 'Received error while resolving DRS URL. getaddrinfo ENOTFOUND dg4.dfc'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6439:919,error,error,919,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6439,5,"['Error', 'error']","['Error', 'error', 'erroring', 'errors']"
Availability,"The `cwl_dynamic_initial_workdir` fails with below stack trace:; ```; 2019-03-13 12:29:04,388 cromwell-system-akka.dispatchers.backend-dispatcher-88 ERROR - BackgroundConfigAsyncJobExecutionActor [UUID(c9194073)main:NA:1]: Error attempting to Execute; java.lang.Exception: Failed command instantiation; 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:581); 	at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand$(StandardAsyncExecutionActor.scala:515); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand$lzycompute(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.instantiatedCommand(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents(StandardAsyncExecutionActor.scala:317); 	at cromwell.backend.standard.StandardAsyncExecutionActor.commandScriptContents$(StandardAsyncExecutionActor.scala:316); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.commandScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents(SharedFileSystemAsyncJobExecutionActor.scala:175); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.writeScriptContents$(SharedFileSystemAsyncJobExecutionActor.scala:174); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.cromwell$backend$sfs$BackgroundAsyncJobExecutionActor$$super$writeScriptContents(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents(BackgroundAsyncJobExecutionActor.scala:12); 	at cromwell.backend.sfs.BackgroundAsyncJobExecutionActor.writeScriptContents$(BackgroundAsyncJobExecutionActor.scala:11); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.writeS",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:149,ERROR,ERROR,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,2,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"The `evaluateFiles` method is used to figure out what files are going to be produced or are referenced by an expression, before the task has been run.; In this context it doesn't make sense to do the work of evaluating `glob` which will fail since the task hasn't run.; Instead, fail immediately. When that happens, we fallback to a second algorithm to evaluate the files ([draft-2](https://github.com/broadinstitute/cromwell/blob/541636734705b7d93321a31ac5817f96b275eb0f/wdl/model/draft2/src/main/scala/wdl/draft2/model/expression/FileEvaluator.scala#L52), [draft-3](https://github.com/broadinstitute/cromwell/blob/541636734705b7d93321a31ac5817f96b275eb0f/wdl/model/draft3/src/main/scala/wdl/model/draft3/graph/expression/FileEvaluator.scala#L25)). This should fix the `dontglobinputs` transient centaur failures. With the caveat that the real underlying issue is this: https://github.com/broadinstitute/cromwell/issues/4209",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4208:805,failure,failures,805,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4208,1,['failure'],['failures']
Availability,"The `graph` action in womtool works fine, but when adding the `--all` argument, it gives the error:; ```; java -jar womtool-44.jar --all graph host_workflow.wdl ; Error: Unknown option --all; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5126:93,error,error,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5126,2,"['Error', 'error']","['Error', 'error']"
Availability,The `sbt` tests are reliably failing; any idea why?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3968#issuecomment-410352775:20,reliab,reliably,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3968#issuecomment-410352775,1,['reliab'],['reliably']
Availability,"The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543922163:110,error,errors,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-543922163,2,['error'],['errors']
Availability,"The `thread.sleep` command would need to be added to whichever actor(s) is actually submitting messages to the API. This doesn't strike me as too onerous for the developers, but you're right, it's definitely part of the scala and not the config files. Some minimal exception catching is also called for. Rather than throttling concurrent _jobs_ it probably makes more sense to limit the number and frequency of concurrent _workflow submissions_:; ```# Cromwell ""system"" settings; system {; ; # Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000 # No practical limit on the number of total workflows. # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 4 # Too conservative?. # Number of seconds between workflow launches; new-workflow-poll-rate = 5 # Too conservative?; }; ```; This should stagger submissions without limiting the total amount of work being done. The number of threads available to the backend-dispatcher also appears to settable. You could create an artificial bottleneck there to protect AWS's API.`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436674279:1011,avail,available,1011,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-436674279,1,['avail'],['available']
Availability,"The `validateWomNamespace` method was using `NoIoFunctionSet` instead of the available ioFunctions, causing the workflow in the centaur test to fail",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3715:77,avail,available,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3715,1,['avail'],['available']
Availability,The batched heartbeat writer and workflow picker upper both try to lock multiple rows in the workflow store table inside a transaction and were often observed to deadlock. These two workflow store accesses are now routed through an actor that effectively serializes access to the workflow store table (other accesses are not affected). If this manages to run the gauntlet of gulls [batch abort](https://github.com/broadinstitute/cromwell/issues/3753) would likely need to be added to this system. Known shortcomings:; - ~~Should probably give more thought as to the thread on which the blocking happens.~~ now on the IO dispatcher; - ~~Should consider actor supervision because if this one actor ever dies that will be bad times.~~ default Akka supervision is reasonable here; - ~~May keep one writer Cromwell from tripping over itself but wouldn't keep multiple writer Cromwells from tripping over each other~~ ticketed in #3795,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3761:12,heartbeat,heartbeat,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3761,1,['heartbeat'],['heartbeat']
Availability,"The build failures are in unrelated Swagger tests, rebasing on develop should get this green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-341493941:10,failure,failures,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-341493941,1,['failure'],['failures']
Availability,The case I inspected has error in only one of the 17 shards (of the same scatter). For example:. ```bash; $ for i in $(ls shard-*/*.log); do echo $i; grep Requester $i; done; BaseRecalibrator-0.log; BaseRecalibrator-10.log; BaseRecalibrator-11.log; BaseRecalibrator-12.log; BaseRecalibrator-13.log; BaseRecalibrator-14.log; BaseRecalibrator-15.log; BaseRecalibrator-16.log; BaseRecalibrator-1.log; BaseRecalibrator-2.log; BaseRecalibrator-3.log; BaseRecalibrator-4.log; BaseRecalibrator-5.log; BaseRecalibrator-6.log; BaseRecalibrator-7.log; BaseRecalibrator-8.log; BaseRecalibrator-9.log; ServiceException: 401 Requester pays bucket access requires authentication.; ServiceException: 401 Requester pays bucket access requires authentication.; ServiceException: 401 Requester pays bucket access requires authentication.; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-436261622:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336#issuecomment-436261622,2,"['echo', 'error']","['echo', 'error']"
Availability,"The cause and effect:. - An assumption in the token dispenser actor (that none of the internal queues were ever empty) turned out to not always be true; - As a result, not infrequently an attempted `dequeue` would cause the `JobExecutionTokenDispenserActor` to crash and be restarted - zeroing out all known token dispensations *and* all known actors waiting for tokens.; - The overall consequence of this was that jobs would submit their request for exeution tokens and be added to a queue. But that queue was lost when the `JobExecutionTokenDispenserActor` was restarted and therefore the jobs would sit forever waiting for a token which was never sent to them. The fix:; - First, add a sanity check before calling `dequeue`. If the queue is empty, don't do it. But hopefully will now be a redundant check thanks to:; - Second, one situation was identified which would lead to this state when token-requesting-actors were aborting before a token was dispensed. If that left the token queue for that hog group empty then the queue was not being correctly removed from the `JobExecutionTokenDispenserActor` - thus leaving an empty queue behind and triggering the ""dequeue on an empty queue"" bug.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4908#issuecomment-488345810:792,redundant,redundant,792,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4908#issuecomment-488345810,1,['redundant'],['redundant']
Availability,"The centaur test `custom_mount_point` is failing on JES. That's very sad. Make it work again! You can run it by checking out centaur and running:. ```; sbt ""test-only * -- -n custom_mount_point""; ```. The error:. ```; - should successfully run custom_mount_point *** FAILED *** (2 minutes, 1 second); java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/949:205,error,error,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/949,1,['error'],['error']
Availability,"The centaur test `jesexercises` is failing on JES. That's very sad. Make it work again! You can run it by checking out centaur and running:. ```; sbt ""test-only * -- -n jesexercises""; ```. The error:. ```; - should successfully run jesexercises *** FAILED *** (4 minutes, 40 seconds); java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/951:193,error,error,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/951,1,['error'],['error']
Availability,"The centaur test `lots_of_inputs` is failing on JES. Sad! Make it great again! You can run it by checking out centaur and running:. ```; sbt ""test-only * -- -n lots_of_inputs""; ```. The error:. ```; - should successfully run lots_of_inputs *** FAILED *** (53 minutes, 12 seconds); java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; ```. WARNING! It's also taking almost an hour to run...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/956:186,error,error,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/956,1,['error'],['error']
Availability,"The centaur test `passingfiles` is failing on JES. That's very sad. Make it work again! You can run it by checking out centaur and running:. ```; sbt ""test-only * -- -n passingfiles""; ```. The error:. ```; - should successfully run passingfiles *** FAILED *** (3 minutes, 30 seconds); java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/950:193,error,error,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/950,1,['error'],['error']
Availability,"The centaur test `sizeenginefunction` is failing on JES. That's very sad. Make it work again! You can run it by checking out centaur and running:. ```; sbt ""test-only * -- -n sizeenginefunction""; ```. The error:. ```; - should successfully run sizeenginefunction *** FAILED *** (1 minute, 30 seconds); java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/952:205,error,error,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/952,1,['error'],['error']
Availability,"The centaur test `workspaceenginefunctions` is failing on JES. That's very sad. Make it work again! You can run it by checking out centaur and running:. ```; sbt ""test-only * -- -n workspaceenginefunctions""; ```. The error:. ```; - should successfully run workspaceenginefunctions *** FAILED *** (10 seconds, 11 milliseconds); java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/953:217,error,error,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/953,1,['error'],['error']
Availability,"The centaur test `write_lines` is failing on JES. That's very sad. Make it work again! You can run it by checking out centaur and running:. ```; sbt ""test-only * -- -n write_lines""; ```. The error:. ```; - should fail during execution write_lines *** FAILED *** (6 minutes, 0 seconds); java.lang.Exception: Unexpected terminal status Succeeded but was waiting for Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/955:191,error,error,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/955,1,['error'],['error']
Availability,"The centaur test `write_tsv` is failing on JES. That's very sad. Make it work again! You can run it by checking out centaur and running:. ```; sbt ""test-only * -- -n write_tsv""; ```. The error:. ```; - should successfully run write_tsv *** FAILED *** (2 minutes); java.lang.Exception: Unexpected terminal status Failed but was waiting for Succeeded; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/954:187,error,error,187,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/954,1,['error'],['error']
Availability,"The changes for this fix are just a subset of the ones in open request #6058, hence this request is redundant now,",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-759823836:100,redundant,redundant,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5807#issuecomment-759823836,1,['redundant'],['redundant']
Availability,"The clean up instructions to delete the files should go in here; https://github.com/broadinstitute/cromwell/blob/bda6d9043de866b1542a58555dcb9c6070a7c7b5/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L164. After the delocalization happens. On Wed, Oct 28, 2020 at 9:42 PM Luyu <notifications@github.com> wrote:. > However, AWS Batch backend ignores script-epilogue as unrecognized. Do you; > have any suggestions?; >; > Yes, the script epilogue is exactly where the change should be. The script; > is generated by AwsBatchJob.scala; > … <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isn’t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark …; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:976,down,down,976,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965,1,['down'],['down']
Availability,"The code looks nice, but I'm not sure what to make of that build failure. Can Travis possibly be too slow to get a connection from an in-memory database in less than a second, or is something else happening here?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/231#issuecomment-146670780:65,failure,failure,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/231#issuecomment-146670780,1,['failure'],['failure']
Availability,"The combination of a separate metadata database and -DLOG_LEVEL=DEBUG is causing an out of memory error when the log output exceeds the max size of a byte array. We are executing Cromwell locally in run mode with a long running workflow (about 24 hrs) that includes several scatter-gather blocks. We switched to a separate metadata database to address the Java heap problem that occurs with the in-memory database. We enabled debug logging to try and troubleshoot an unrelated problem. Most of the log output at the increased level is appears to be from HSQL. After running for about 8 hrs, the following error appears in the output and Cromwell hangs:; ```; Exception in thread ""Exec Stream Pumper"" java.lang.OutOfMemoryError: Required array length 2147483639 + 39 is too large; 	at java.base/jdk.internal.util.ArraysSupport.hugeLength(ArraysSupport.java:649); 	at java.base/jdk.internal.util.ArraysSupport.newLength(ArraysSupport.java:642); 	at java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:100); 	at java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:132); 	at org.apache.commons.io.output.ProxyOutputStream.write(ProxyOutputStream.java:92); 	at org.apache.commons.io.output.TeeOutputStream.write(TeeOutputStream.java:68); 	at org.apache.commons.exec.StreamPumper.run(StreamPumper.java:108); 	at java.base/java.lang.Thread.run(Thread.java:1623); ```. We are running Cromwell using Dockstore as a wrapper using the following command:; ```; dockstore workflow launch --local-entry BiobankScrubWorkflow.wdl --json inputs.json > dockstore.log 2>&1 &; ```. At the time the OOME occurs, the size of the dockstore.log file is approx 2147485425 bytes. Based on the ""Saving copy of Cromwell stdout to..."" messages at the end of a successful Cromwell run, it would appear that Cromwell is internally buffering the stdout and stderr streams to save at the end of the run. So when the size of the stdout or stderr exceeds the Java buffer max size, the ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7217:98,error,error,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7217,2,['error'],['error']
Availability,"The comment in the keel PR asserts that the hashes returned in `Docker-Content-Digest` and within the body are the same, but in my limited testing on Docker Hub, Quay and GCR that did not seem to be the case. If the body value actually represents something other than the image digest it may cause downstream issues for Cromwell to treat as such. . Basically this issue needs more investigation. Since ECR support is apparently not as a high a priority as originally thought this issue has been deprioritized for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-487639234:298,down,downstream,298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-487639234,1,['down'],['downstream']
Availability,The concurrent-job-limit now available to all backends.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2547:29,avail,available,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2547,1,['avail'],['available']
Availability,"The constructor signature needs to be exactly that since it's built reflectively, and like Miguel said it's better if it shuts itself down with the ShutdownCommand. Plus it's literally 5 lines so.. 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3455#issuecomment-376551184:134,down,down,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3455#issuecomment-376551184,1,['down'],['down']
Availability,The coursier plugin is a drop-in replacement for resolution and download of ivy artifacts. Resolution and downloading are done in parallel. https://github.com/coursier/coursier,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3015:64,down,download,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3015,2,['down'],"['download', 'downloading']"
Availability,The current call caching fails to correctly ignore whitespace. The following command blocks will not call cache together:. ```; command {; echo hello world; }; ```. ```; command { echo hello world }; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1988:139,echo,echo,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1988,2,['echo'],['echo']
Availability,"The current heartbeat-scanning code is incorrect and will attempt to relaunch workflows an hour after they start. Fortunately there is an error check that prevents workflows from actually being restarted but this situation is still less than ideal since these workflows cycle back to the top of the runnable queue an hour later (running workflows will have submission times earlier than submitted-but-never-started workflows and the sort is by submission time). Also some semi-alarming warning messages are generated for these failed starts. These changes should fix this problem by actually writing workflow heartbeats for running workflows. The current vitality logic is simply ""is this workflow actor not dead"".",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3493:12,heartbeat,heartbeat-scanning,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3493,3,"['error', 'heartbeat']","['error', 'heartbeat-scanning', 'heartbeats']"
Availability,"The description of the problem can be found here; https://broadworkbench.atlassian.net/browse/BA-5881; Unfortunately, there is no direct way to reproduce `500 Internal Server Error Backend Error`. Therefore, it is unclear how to show that this PR solves the problem.; However, since this error causes IOException, we think that reproducing some other IOException with this file should reproduce the problem close enough. If you are interested, we have an [experimental branch](https://github.com/EpamLifeSciencesTeam/cromwell/pull/9) for reproducing IOException. In that branch, Cromwell stops during execution, giving us time to delete the `rc` file if we want to. It allows us to see that this fix actually works and Cromwell indeed tries to read the file multiple times.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113:175,Error,Error,175,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113,3,"['Error', 'error']","['Error', 'error']"
Availability,"The docker attribute specified in the ""default_runtime_attributes"" in cromwell.conf does not get picked up. My workflow fails with error: . ```; 2019-12-19 03:04:56,497 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - WorkflowManagerActor Workflow 759c0000-c343-4466-a26b-aa627785; 89b0 failed (during InitializingWorkflowState): Task gcpp has an invalid runtime attribute docker = !! NOT FOUND !!; ```; The relevant portion of cromwell.conf; ```; computecfg_00 {; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; config {; numSubmitAttempts = 10; numCreateDefinitionAttempts = 10; root = ""xxxxxx""; auth = ""default""; default-runtime-attributes {; queueArn = ""xxxxxx""; docker = ""ubuntu:latest""; }; ...; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5329:131,error,error,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5329,1,['error'],['error']
Availability,The dockerScripts build seems to fail due to some connection error. Hopefully it will succeed after a restart. ; I am happy that upgrading the Betterfiles dependency to a new major version release did not cause any issues in the rest of cromwell.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5312#issuecomment-575089532:61,error,error,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5312#issuecomment-575089532,1,['error'],['error']
Availability,"The docs are correct, the local docker backend does not recognize CPU and memory attributes, because it's impossible to implement with the Docker Desktop API. And even if it was, it would probably not ship because the local backend is intended as a down-featured sandbox environment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1031557550:249,down,down-featured,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4413#issuecomment-1031557550,1,['down'],['down-featured']
Availability,"The documentation for the new HPC backend 'check-alive' feature is a bit scattered an incomplete. https://cromwell.readthedocs.io/en/stable/backends/HPC/; > This option will implicitly enable polling with the check-alive option. https://github.com/broadinstitute/cromwell/releases:; > When the value exit-code-timeout-seconds is set, check-alive command is now only called once every timeout interval instead of each poll. https://github.com/broadinstitute/cromwell/blob/73ad264b4c7919d0bbd344fecbc903f819f5e16c/cromwell.example.backends/SGE.conf#L27; > # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). I had a look at the code and PRs implementing the feature, but am not confident I can even see how the feature is implemented, so cannot write the docs as a PR myself!. I had assumed, when you specify `exit-code-timeout-seconds`, cromwell would poll `check-alive` at that interval, and after 2 failed `check-alive`s with no rc file, would mark the job as failed. Now I think there is some other polling mechanism, perhaps to cap polling load per backend instead of scaling it with number of jobs. . Is it possible to revisit that documentation and actually explain what is happening? Alternatively just a link here to the `unrelated to this timeout` documentation?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877:49,alive,alive,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877,6,['alive'],['alive']
Availability,"The documentation has a nice section on [how to use Service Accounts with Cromwell](https://cromwell.readthedocs.io/en/develop/backends/Google/), with the Google Cloud backend. However, what it doesn't do is explain the roles/permissions that such an account needs. It would be appreciated if we had a list of permissions we could apply to our Service Accounts to know that we had the absolute minimum required for Cromwell to control jobs (probably separate lists for filesystem access and job management). Currently, the roles I've applied to my Service Account are:. * Compute Instance Admin (v1); * Genomics Pipelines Runner; * Service Account User; * Storage Object Admin. This works, but I know that these roles are quite permissive. Ideally I'd be able to lock it down to permissions that stop it from deleting buckets etc.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304:771,down,down,771,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304,1,['down'],['down']
Availability,"The downloadable jar on Github was intentionally kept the same, and that's the URL used in the Homebrew formula. This whole incident has illustrated the need for better release & version management going forward but that doesn't exist right now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316066639:4,down,downloadable,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463#issuecomment-316066639,1,['down'],['downloadable']
Availability,"The error is different nowadays and apparently downstream of the fail. The underlying issue appears to be with not hydrating a `WomMaybePopulatedFile` output from a dependent job on restart: . ```; java.lang.UnsupportedOperationException: value is not available: WomMaybePopulatedFile(None,None,None,None,None,List()); 	at wom.values.WomMaybePopulatedFile.$anonfun$value$2(WomFile.scala:244); 	at scala.Option.getOrElse(Option.scala:121); 	at wom.values.WomMaybePopulatedFile.value(WomFile.scala:244); 	at cwl.internal.EcmaScriptEncoder.encodeFile(EcmaScriptEncoder.scala:102); 	at cwl.internal.EcmaScriptEncoder.encodeFileOrDirectory(EcmaScriptEncoder.scala:90); 	at cwl.internal.EcmaScriptEncoder.encode(EcmaScriptEncoder.scala:39); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$3(EcmaScriptUtil.scala:105); 	at scala.collection.MapLike$MappedValues.$anonfun$foreach$3(MapLike.scala:253); 	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:789); 	at scala.collection.immutable.Map$Map2.foreach(Map.scala:146); 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 	at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:253); 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59); 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50); 	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:186); 	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:44); 	at scala.collection.TraversableLike.to(TraversableLike.scala:590); 	at scala.collection.TraversableLike.to$(TraversableLike.scala:587); 	at scala.collection.AbstractTraversable.to(Traversable.scala:104); 	at scala.collection.TraversableOnce.toList(TraversableOnce.scala:294); 	at scala.collection.TraversableOnce.toList$(TraversableOnce.scala:294); 	at scala.collection.AbstractTraversable.toList(Traversable.scala:104); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4(EcmaScrip",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787:4,error,error,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787,3,"['avail', 'down', 'error']","['available', 'downstream', 'error']"
Availability,"The error messages from Cromwell when it can't find input files are opaque:. Response body:; {; ""status"": ""error"",; ""message"": ""404 Not Found\n{\n \""code\"" : 404,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Not Found\"",\n \""reason\"" : \""notFound\""\n } ],\n \""message\"" : \""Not Found\""\n}""; } . Response header:; {; ""Date"": ""Fri, 18 Nov 2016 21:15:39 GMT"",; ""Server"": ""spray-can/1.3.2"",; ""X-Frame-Options"": ""SAMEORIGIN"",; ""Access-Control-Max-Age"": ""1728000"",; ""Access-Control-Allow-Methods"": ""GET,POST,PUT,PATCH,DELETE,OPTIONS,HEAD"",; ""Content-Type"": ""application/json; charset=UTF-8"",; ""Access-Control-Allow-Origin"": ""*"",; ""Access-Control-Expose-Headers"": ""Link"",; ""Connection"": ""close"",; ""Access-Control-Allow-Headers"": ""authorization,content-type,accept,origin"",; ""Content-Length"": ""232""; } ```. Response Code: 500. It would be very helpful if it could specify what files are missing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1691:4,error,error,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1691,3,['error'],"['error', 'errors']"
Availability,"The error occurs again, I read this [thread](https://gatkforums.broadinstitute.org/wdl/discussion/9436/error-running-cromwell-27-snap-build-from-master), and configure the local mysql database rather in-memory database.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-440913258:4,error,error,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4403#issuecomment-440913258,2,['error'],"['error', 'error-running-cromwell-']"
Availability,"The error you see in centaur looks like. > Metadata mismatch for failures.0.message - expected: ""Task invalid_runtime_attributes has an invalid runtime attribute continueOnReturnCode = \""oops\"""" but got: ""None.get""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1272#issuecomment-238694173:4,error,error,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1272#issuecomment-238694173,2,"['error', 'failure']","['error', 'failures']"
Availability,The failure looks like a random TravisCI glitch.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5168#issuecomment-529671972:4,failure,failure,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5168#issuecomment-529671972,1,['failure'],['failure']
Availability,"The failure rate was getting unreasonable so I figured someone should do something. It might take a bit longer, but at least it'll more likely return green. ---. Once build activity died down after everyone went home, I was able to isolate the imapct of conformance PapiV2 by running it on its own: virtually none (my run started at 7:10). <img width=""898"" alt=""screen shot 2018-10-12 at 7 39 48 pm"" src=""https://user-images.githubusercontent.com/1087943/46898326-a37a2300-ce56-11e8-9c0a-ff5f33ade931.png"">. It's still possible that other jobs suck up quota which in turn affects conformance PapiV2 even if it uses very little itself. I asked for more quota again, because my first request didn't give us enough breathing room (screenshot _after_ first increase):; <img width=""976"" alt=""screen shot 2018-10-12 at 6 24 21 pm"" src=""https://user-images.githubusercontent.com/1087943/46898377-15526c80-ce57-11e8-8af6-f7844e84b843.png"">",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4248:4,failure,failure,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4248,2,"['down', 'failure']","['down', 'failure']"
Availability,"The file it was trying to read is gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/9a7a405c-6b14-48cc-87e8-84ee4e7ef074/call-PreBqsrCheckContamination/PreBqsrCheckContamination-stdout.log. The task uses `read_float (stdout)` in its output block. . ```; 2016-06-01 09:47:17,888 cromwell-system-akka.actor.default-dispatcher-16 ERROR - CallActor [UUID(9a7a405c):PreBqsrCheckContamination]: Failing call: Error reading gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/9a7a405c-6b14-48cc-87e... at position 0; cromwell.util.AggregatedException: Error reading gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/9a7a405c-6b14-48cc-87e... at position 0;   at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:118) ~[cromwell.jar:0.19];   at cromwell.util.TryUtil$.sequenceMap(TryUtil.scala:130) ~[cromwell.jar:0.19];   at cromwell.engine.backend.jes.JesBackend.postProcess(JesBackend.scala:625) ~[cromwell.jar:0.19];   at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:664) ~[cromwell.jar:0.19];   at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:659) ~[cromwell.jar:0.19];   at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19];   at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19];   at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19];   at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19];   at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19];   at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19];   at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/928:355,ERROR,ERROR,355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/928,3,"['ERROR', 'Error']","['ERROR', 'Error']"
Availability,"The first class of failure is similar to the second, we write outputs and execution events before attempting to write status. The code is written to `setOutputs` and `setExecutionEvents` and does not deal well with preexisting data. Similar fixes as per the second failure class should apply here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215134213:19,failure,failure,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215134213,2,['failure'],['failure']
Availability,"The following WDL fails to parse because of the expressions in the array accesses. ```; workflow foo {; Array[String] inputs = [ ""A0"", ""A1"", ""B0"", ""B1"", ""C0"", ""C1"" ]. scatter(i in range(length(inputs) / 2)) {; String item0 = inputs[i * 2]; String item1 = inputs[i * 2 + 1]; call bar { input: item0 = item0, item1 = item1 }; }; }. task bar {; String item0; String item1; command { echo ""<< item0: ${item0}, item1: ${item1} >>"" }; output {; String combined = read_string(stdout()); }; }; ```. The error given is:; ```; cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unable to load namespace from workflow: ERROR: Unexpected symbol (line 5, col 29) when parsing 'e'. Expected rsquare, got *. String item0 = inputs[i * 2]; ^. $e = :identifier <=> :lparen $_gen18 :rparen -> FunctionCall( name=$0, params=$2 ). ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2599:380,echo,echo,380,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2599,3,"['ERROR', 'echo', 'error']","['ERROR', 'echo', 'error']"
Availability,"The following WDL has issues on JES, AWS, and Local backends. The expectation would be that all occurrences of `file` and `maybe_file` would interpolate as relativized file paths. ```; task files {; File file ; File? maybe_file. command {; echo file: ${file} maybe_file: ${maybe_file} ${""file with concatenation: "" + file} ${""maybe_file with concatenation: "" + maybe_file}; }; runtime {; docker: ""ubuntu:latest""; }; }. workflow w {; File file. call files { input: file = file, maybe_file = file }; }; ```. On JES with Cromwell 7c52320b3844fb83959a784a16c613c62b8bec1c, the ""maybe_file with concatenation"" leaves a residual `gs://` path; all other interpolations are correctly relativized. On AWS with Cromwell 9341a4dac6145233f2a33b092a8fc443c18744ea, both concatenations leave residual `s3://` paths. On Local with Cromwell 7c52320b3844fb83959a784a16c613c62b8bec1c and an input file under my home directory, this throws an exception with the following trace:. ```; 2017-02-02 11:55:36,701 cromwell-system-akka.dispatchers.backend-dispatcher-44 ERROR - BackgroundConfigAsyncJobExecutionActor [UUID(5fdb357a)w.files:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	at sun.nio.fs.UnixPath.subpath(UnixPath.java:346); 	at sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	at cromwell.backend.io.JobPathsWithDocker.toDockerPath(JobPathsWithDocker.scala:35); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$class.toUnixPath(SharedFileSystemAsyncJobExecutionActor.scala:107); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.toUnixPath(ConfigAsyncJobExecutionActor.scala:113); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLineValueMapper$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor$$anonfun$commandLineValueMapper$1.apply(SharedFileSystemAsyncJobExecutionActor.scala:127); 	at wdl4s.command.ParameterCommandPart.instantiat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1944:240,echo,echo,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1944,1,['echo'],['echo']
Availability,"The following WDL reproduces the problem with the JES backend, though the magic number is consistently 1899 files. Local backend does not have this problem. . ```; task make_files {; command <<<; mkdir files; for i in {1..5000}; do echo ""foo"" > files/$i.txt; done; >>>; output {; Array[File] files = glob(""files/*.txt""); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow piles_of_files {; call make_files; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/699#issuecomment-210234833:232,echo,echo,232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/699#issuecomment-210234833,1,['echo'],['echo']
Availability,The following WDL runs to a certain point and then fails (when running the STAR aligner) without writing errors to stdout/stderr (which it does when running out of resources when run locally). Example input JSON also attached. [samtofastq_star_rsem.txt](https://github.com/broadinstitute/cromwell/files/215739/samtofastq_star_rsem.txt); [inputs.txt](https://github.com/broadinstitute/cromwell/files/215783/inputs.txt),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/698#issuecomment-209056982:105,error,errors,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/698#issuecomment-209056982,1,['error'],['errors']
Availability,"The following config used to work (0.22):; ```; PBS {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int memory_mb = 1000; String pbs_cpu = ""1""; String? pbs_email; String? pbs_queue; String pbs_walltime = ""1:00:00""; """"""; }; }; ```. Note the specification of `pbs_email` and `pbs_queue` as Optional String type. On upgrading to 23 and starting the server, the process hangs and this appears in the logs:; ```; [ERROR] [12/05/2016 13:28:57.994] [cromwell-system-akka.dispatchers.engine-dispatcher-30] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/WorkflowInitializationActor-8def86a5-13b1-4dc4-9cdf-d0ae2eedd7c9/PBS] Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; java.lang.RuntimeException: Unsupported config runtime attribute WdlOptionalType(WdlStringType) pbs_email; 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclaration(DeclarationValidation.scala:41); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$$anonfun$fromDeclarations$1.apply(DeclarationValidation.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.backend.impl.sfs.config.DeclarationValidation$.fromDeclarations(DeclarationValida",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1737:487,ERROR,ERROR,487,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1737,1,['ERROR'],['ERROR']
Availability,"The following minimal example fails on version 30-4fa75da:; ```; workflow Test {; String? testString. if(true) {; if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; {; ""Test.testString"": ""test""; }; ```; ---; The failures look like this:; ```; ""failures"": [{; ""causedBy"": [{; ""causedBy"": [],; ""message"": ""Unable to build WOM node for If '$if_0': Two or more nodes have the same FullyQualifiedName: ^.testString""; }]; }]; ```; ---; I think it's got something to do with evaluating `defined(testString)` within the inner conditional, because the following two wdl examples, with the same inputs as before, do not fail:; ```; workflow Test {; String? testString. if(true) {; Boolean testStringDefined = defined(testString); if (testStringDefined) {; call testTask {; input:; testString = testString; }; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ```; workflow Test {; String? testString. if (defined(testString)) {; call testTask {; input:; testString = testString; }; }; }. task testTask {; String? testString. command {; echo ""Hello world""; }. runtime {; docker: ""ubuntu""; }; }; ```; ---; This was discovered during a run of [Arrays.wdl](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/ArraysWf.wdl) with [SimpleInput.json](https://github.com/broadinstitute/dsde-pipelines/blob/develop/genomes_in_the_cloud/arrays/tests/testing_input_files/SimpleInput.json). Let me know if there's any more information you need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3190:252,echo,echo,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3190,5,"['echo', 'failure']","['echo', 'failures']"
Availability,"The following stack trace appears:. ```; 2017-01-27 11:43:10,369 cromwell-system-akka.dispatchers.engine-dispatcher-19 ERROR - WorkflowManagerActor Workflow 268771d3-0303-45b1-ba0f-9c93c27b6784 failed (during ExecutingWorkflowState): JobStore write failure: Invalid position in SerialClob object set; java.lang.Exception: JobStore write failure: Invalid position in SerialClob object set; 	at cromwell.engine.workflow.lifecycle.execution.EngineJobExecutionActor$$anonfun$11.applyOrElse(EngineJobExecutionActor.scala:239); 	at cromwell.engine.workflow.lifecycle.execution.EngineJobExecutionActor$$anonfun$11.applyOrElse(EngineJobExecutionActor.scala:235); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.execution.EngineJobExecutionActor.akka$actor$LoggingFSM$$super$processEvent(EngineJobExecutionActor.scala:29); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.execution.EngineJobExecutionActor.processEvent(EngineJobExecutionActor.scala:29); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$class.aroundReceive(Actor.scala:496); 	at cromwell.engine.workflow.lifecycle.execution.EngineJobExecutionActor.aroundReceive(EngineJobExecutionActor.scala:29); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); 	at akka.actor.ActorCell.invoke(ActorCell.scala:495); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerTh",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1919:119,ERROR,ERROR,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1919,3,"['ERROR', 'failure']","['ERROR', 'failure']"
Availability,"The following task delocalizes the output file: . ```; task IndexVCF {; File combined_gvcf; Int disk_size. command {; /usr/gitc/tabix ${combined_gvcf}; }; runtime {; docker: ""broadinstitute/genomes-in-the-cloud@sha256:d7aa37fc8351074a2d6fb949932d3283cdcefdc8e53729dcf7202bee16ab660a""; memory: ""13 GB""; cpu: ""1""; disks: ""local-disk "" + disk_size + "" HDD""; }; output {; File gvcf_index = ""${combined_gvcf).tbi""; }; }; ```. In V25 the output file delocalizes correctly, but if you try to use this output file as input to a future task, it doesn't get the full google bucket path, it just has the string interpolation of the path that was on the VM in the IndexVCF task. . Instead the file shouldn't delocalize and a helpful error message should be provided that indicates that a File input shouldn't be interpreted as a String in the output section because it's using that local VM's relative path at that point (rather than the full google bucket path).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2236:721,error,error,721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2236,1,['error'],['error']
Availability,"The following tests failed in the `centaurPAPIv2` run in our CI (this doesn't run for external contributors for credentials reasons). This build does have issues with transient failures at times, but these failures all appear to be label-related:. * jes_labels; * google_labels_bad; * google_labels_sub; * google_labels_good",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5000#issuecomment-497256233:177,failure,failures,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5000#issuecomment-497256233,2,['failure'],['failures']
Availability,"The following wdl validates and starts ""running"" even though nothing can happen:. ```; workflow CromwellSimpleCircularGraphBug {; 	; 		call Echo as A {input: string = item1}; 		String item2=A.out; ; 		call Echo as B{input: string = item2}; 		String item1=B.out; }. task Echo {; 	String string; 	command {; 		echo ${string}; 	}; 	runtime {; docker: ""ubuntu:latest""; 	}; 	output {; 		String out=read_string(stdout()); 	}; }; ```. This version is a bit trickier, perhaps. It also is ""running"":; ```; workflow CromwellCircularGraphWithScatterBug {; 	; scatter(item1 in list1) {; 		call Echo as A {input: string = item1}; 		String list2=A.out; }. scatter(item2 in list2) {; 		call Echo as B{input: string = item2}; 		String list1=B.out; } ; }. task Echo {; 	String string; 	command {; 		echo ${string}; 	}; 	runtime {; docker: ""ubuntu:latest""; 	}; 	output {; 		String out=read_string(stdout()); 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2279:140,Echo,Echo,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2279,8,"['Echo', 'echo']","['Echo', 'echo']"
Availability,"The following workflow always fails on JES if call-caching is enabled. File paths are getting horribly mangled and the getHash fails. ```; task mirror {; File x; command {; echo noop; }; output {; File y = x; }; }. workflow call_caching {; File x_in; Array[Int] blahs = [1,2,3,4,5,6,7,8,9,10]. call mirror { input: x = x_in }. scatter ( blah in blahs ) {; call mirror as cached_mirror1 { input: x = mirror.y }; call mirror as cached_mirror2 { input: x = cached_mirror1.y }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/531:173,echo,echo,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/531,1,['echo'],['echo']
Availability,"The following workflow and subworkflow wdls do not work in cromwell v29 hot fix (3be5b8c0537ef051dece9fbad63a154b72ad510d): . ```; import ""subworkflow.wdl"" as sub. workflow wf {; 	Array[Float] arr = [1.0,2.0,3.0]. 	call sub.subwf {; 		input:; 			input_array = arr; 	}; }; ```. ```; workflow subwf {; 	Array[Float] input_array. 	call SumFloats {; 		input:; 			sizes = input_array,; 			preemptible_tries = 0; 	}. 	String size = if SumFloats.total_size < 2.0 then ""Small"" else ""Big"". 	call print {; 		input: ; 			s = size; 	}; }. task SumFloats {; 	Array[Float] sizes; 	Int preemptible_tries. 	command <<<; 	python -c ""print ${sep=""+"" sizes}""; 	>>>; 	output {; 		Float total_size = read_float(stdout()); 	}; 	runtime {; 		docker: ""python:2.7""; 		preemptible: preemptible_tries; 	}; }. task print {; 	String s. 	command {; 		echo ${s}; 	}. 	runtime {; 		docker: ""python:2.7""; 	}; }; ```. I get errors such as the following:. ```; message: ""Could not find size in input section of call wf.subwf""; message: ""Could not find SumFloats in input section of call wf.subwf""; message: ""No declaration named SumFloats for call wf.subwf""; message: ""Input evaluation for Call wf.subwf failed.""; message: ""Couldn't resolve all inputs for wf.subwf at index None.""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2753:821,echo,echo,821,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2753,2,"['echo', 'error']","['echo', 'errors']"
Availability,"The following workflow failed in cromwell (3da6e34e-a2a1-49fb-8a84-4d62aada9b3d) on production. . ```; {; ""preemptible"": true,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/3da6e34e-a2a1-49fb-8a84-4d62aada9b3d/call-ApplyBQSR/shard-15/ApplyBQSR-15-stdout.log"",; ""backendStatus"": ""Success"",; ""shardIndex"": 15,; ""outputs"": {. },; ""runtimeAttributes"": {; ""preemptible"": ""3"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 200 HDD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""broadinstitute/genomes-in-the-cloud:2.0.0"",; ""cpu"": ""1"",; ""zones"": ""us-central1-c"",; ""memory"": ""3.5 GB""; },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""input_bam"": ""SortAndFixSampleBam.output_bam"",; ""ref_fasta"": ""ref_fasta"",; ""ref_dict"": ""ref_dict"",; ""recalibration_report"": ""GatherBqsrReports.output_bqsr_report"",; ""disk_size"": ""agg_medium_disk"",; ""output_bam_basename"": ""recalibrated_bam_basename"",; ""input_bam_index"": ""SortAndFixSampleBam.output_bam_index"",; ""ref_fasta_index"": ""ref_fasta_index"",; ""sequence_group_interval"": ""subgroup""; },; ""returnCode"": 0,; ""failures"": [{; ""failure"": ""Read timed out"",; ""timestamp"": ""2016-04-23T18:29:53.764Z""; }],; ""jobId"": ""operations/EK2_jqLEKhiCmbzn296gnv4BIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl"",; ""backend"": ""JES"",; ""end"": ""2016-04-23T18:29:54.000Z"",; ""stderr"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/3da6e34e-a2a1-49fb-8a84-4d62aada9b3d/call-ApplyBQSR/shard-15/ApplyBQSR-15-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://broad-gotc-prod-storage/cromwell_execution/PairedEndSingleSampleWorkflow/3da6e34e-a2a1-49fb-8a84-4d62aada9b3d/call-ApplyBQSR/shard-15/ApplyBQSR-15.log""; },; ""start"": ""2016-04-23T17:56:02.000Z""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738:1146,failure,failures,1146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738,2,['failure'],"['failure', 'failures']"
Availability,"The following workflow failed in cromwell (4ef40f07-ff52-426b-9610-3c9dc66ec67e) on production. Looking metadata we have no logs for the step that failed. ```; {; ""executionStatus"": ""Failed"",; ""shardIndex"": 5,; ""outputs"": {. },; ""runtimeAttributes"": {. },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""input_bam"": ""SortAndFixSampleBam.output_bam"",; ""ref_fasta"": ""ref_fasta"",; ""ref_dict"": ""ref_dict"",; ""disk_size"": ""agg_medium_disk"",; ""dbSNP_vcf"": ""dbSNP_vcf"",; ""known_snps_sites_vcf"": ""known_snps_sites_vcf"",; ""dbSNP_vcf_index"": ""dbSNP_vcf_index"",; ""known_indels_sites_vcf_index"": ""known_indels_sites_vcf_index"",; ""input_bam_index"": ""SortAndFixSampleBam.output_bam_index"",; ""recalibration_report_filename"": ""sample_name + \"".recal_data.csv\"""",; ""known_snps_sites_vcf_index"": ""known_snps_sites_vcf_index"",; ""ref_fasta_index"": ""ref_fasta_index"",; ""sequence_group_interval"": ""subgroup"",; ""known_indels_sites_vcf"": ""known_indels_sites_vcf""; },; ""failures"": [{; ""failure"": ""Call failed to initialize: Could not persist runtime attributes: Timeout after 5059ms of waiting for a connection."",; ""timestamp"": ""2016-04-23T09:14:54.651Z""; }],; ""backend"": ""JES"",; ""end"": ""2016-04-23T09:14:56.000Z"",; ""attempt"": 1,; ""executionEvents"": [],; ""start"": ""2016-04-23T09:14:45.000Z""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737:953,failure,failures,953,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737,2,['failure'],"['failure', 'failures']"
Availability,"The following workflow fails:. This workflow fails:. ```; task MakeMeAFile {; command <<<; echo FILECONTENT > output.txt; >>>. runtime {; docker: ""ubuntu:latest""; memory: ""1 GB""; preemptible: 3; }. output {; String out = read_string(""output.txt""); }; }. task ReadMeAFile {; File infile. command <<<; cat ${infile}; >>>. runtime {; docker: ""ubuntu:latest""; memory: ""1 GB""; preemptible: 3; }. output {; File out = read_string(stdout()); }; }. workflow FileMakeAndRead {; call MakeMeAFile; call ReadMeAFile { input: infile = MakeMeAFile.out }; }; ```. Because:. ```; ""failures"": [; ""Could not find suitable filesystem to parse output.txt""; ]; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/891:91,echo,echo,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/891,2,"['echo', 'failure']","['echo', 'failures']"
Availability,"The following workflow should produce an output array containing a single string. Instead it produces an empty array. This problem occurs when using the JES backend but not the local backend. ```; task x {; command {; echo hello > hello; }; output {; Array[String] a = glob(""hello""); }; runtime {; docker: ""ubuntu:latest""; }; }; workflow w {; call x; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/828:218,echo,echo,218,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/828,1,['echo'],['echo']
Availability,"The functionality provided in this PR would be helpful to one of our users and I would love to see it merged, but this PR has languished for over 2 years. Looking it over, I have 3 questions for @illusional which may effect getting this merged:. 1. Would it make sense to change the proposed option from skipping the lookup entirely, to allowing the lookup to happen, but ignore the failure if we have a hash?; 2. Would having tests for this change make it more palatable to the maintainers?; 3. Maybe redo the PR against the current state of the repo so that there are not 2 years worth of conflicts to resolve before a merge?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435350438:383,failure,failure,383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6140#issuecomment-1435350438,1,['failure'],['failure']
Availability,"The goal here is to gather reliability metrics on Cromwell. The theme here is basic.. the data is the most important artifact. Create something (e.g. a bash or python script) that ; ; - launches N (e.g. 50) workflows; - polls for workflow completion; - upon workflow completion, log workflow id + status (success/failure) and launch a new workflow. If a workflow fails, perhaps also grab the metadata for the workflow and write it out (although we could also query Cromwell for this later). This should run until we kill it. . Then start it up and let it run. Each day investigate workflow failures and generate a findings document which will feed into reliability ticket creation",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1822:27,reliab,reliability,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1822,4,"['failure', 'reliab']","['failure', 'failures', 'reliability']"
Availability,The hash failures are expected with http inputs and should not be the cause of your workflow failure. Also we don't currently support `http` in engine filesystems. Do you see any other error messages than might provide some insight into what's happening?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425981166:9,failure,failures,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425981166,3,"['error', 'failure']","['error', 'failure', 'failures']"
Availability,"The idea being, sending an abort message does not get an `AbortSuccess` or `AbortFailed` response directly. Instead, the actor being aborted will cause its operation to fail and the parent should expect an ""Aborted"" response instead of a Success or Failure (although a Success or Failure may still be returned anyway). In this way, it's acceptable for a backend implementor to simply do nothing when `abort` is received. The parent will just continue waiting for a result of some sort.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/759:249,Failure,Failure,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/759,2,['Failure'],['Failure']
Availability,"The labels PATCH endpoint will always return a 500 status code, no matter what exactly the error is. For example, using fake id “abc” returns status code 500, and using an unrecognized workflow id “774eeeac-aaaa-bbbb-8bf7-061b87ad19da” will also return 500. This may not be what we expected. . The only way to get a 400 code is to make a request with invalid json input, such as {“”:”test_label”}. But we couldn't get a 404 code as we expected, could someone get this fixed?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2961:91,error,error,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2961,1,['error'],['error']
Availability,"The limit of 300 is unsufficient for most tools, which log some startup messages before starting the actual work. This exceeds 300 characters easily. It results in not being able to see the actual error that crashed the job. This is quite annoying. Especially when the job fails on a CI job and you can't look into the logs on the remote CI server. 3000 is just another arbitrary limit, but hopefully better. EDIT: Example: https://github.com/biowdl/germline-DNA/pull/78/checks?check_run_id=1549055042",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6132:197,error,error,197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6132,1,['error'],['error']
Availability,"The main reason was that I don't have a native environment with python3 + the latest openssl that was usable with the github auth requirements. I have python3 + openssl that I've built, but need xmlsec which segfaulted when I used the centos6 version with the hand-built python environment. I started down the path of building xmlsec which didn't immediately work properly so wanted to stop spiraling down the rathole of building chained dependencies.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685:301,down,down,301,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537249685,2,['down'],['down']
Availability,"The mint team ran a group of ~200 10x workflows where 18 failed due to PAPI error code 10.14. Many of the tasks were stuck waiting for quota. For example, the following non-preemptible task ran for 3h 51m before failing with this error:. `CellRanger.cellranger_count:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 14: VM ggp-16203705785274897556 stopped unexpectedly.` . It would be useful if the `max_retries` runtime parameter included retrying tasks that fail with the above error, so that the workflow does not need to be manually re-run after failing. **Configuration info**:; Cromwell version: Cromwell-as-a-Service (caas-prod) version `35-5f86a05-SNAP`; Backend: PAPI v1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4363:76,error,error,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4363,4,['error'],['error']
Availability,"The most frequent use case I can imagine would be to iterate over all items in an array inside a `for` loop of some kind. However, I was under the impression that `for` loops weren't being implemented in WDL?. I agree with @vdauwera's suggestion on naming, provided that finding the aggregate file size of an array of files is something we want to do. If not, then extra naming distinctions would be confusing. (I can see a user trying to use `size()` rather than `array_size()`, particularly if they have previously written in a language that uses `size()` to check the length of an array. Personally, I often write things as I think they might be, and if there isn't an error thrown (i.e. `function size() does not exist`) and the behavior is not as I expect (returning a file size rather than an array length), I will spend a while trying to troubleshoot the error somewhere else, as I assume `size()` worked the way I wanted it to.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270197593:672,error,error,672,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1604#issuecomment-270197593,2,['error'],['error']
Availability,The most recent PR build succeeded but GitHub apparently did not take note of that during its earlier downtime.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6714#issuecomment-1071072611:102,downtime,downtime,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6714#issuecomment-1071072611,1,['downtime'],['downtime']
Availability,The motivating cases for this are the metadata-fetching performance improvements under development. We would not only like to fetch metadata but time how long it takes to retrieve. However the perf VM is currently shut down after the workflow completes and its metadata is extracted so this is unpossible without at least hacking the perf script a little.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4251:219,down,down,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4251,1,['down'],['down']
Availability,The new error message for this is `Task wf_hello.hello:NA:1 failed: error code 5. Message: Some(no zones available)`; Is it acceptable @katevoss ?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289600666:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1915#issuecomment-289600666,3,"['avail', 'error']","['available', 'error']"
Availability,"The next error is:; ```; 2019-01-31 19:38:58,499 INFO - changelog.xml: changesets/replace_empty_custom_labels.xml::custom_labels_not_null::rmunshi: ChangeSet changesets/replace_empty_custom_labels.xml::custom_labels_not_null::rmunshi ran successfully in 661ms; 2019-01-31 19:38:58,563 ERROR - changelog.xml: changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Change Set changesets/failure_metadata.xml::failure_to_message::cjllanwarne failed. Error: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 2019-01-31 19:38:58,618 INFO - changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Successfully released change log lock; 2019-01-31 19:38:58,637 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$Enhanc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:9,error,error,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,10,"['ERROR', 'Error', 'down', 'error', 'failure']","['ERROR', 'Error', 'down', 'error', 'failure', 'failures']"
Availability,The nucleus team is available again ...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/910#issuecomment-224591221:20,avail,available,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/910#issuecomment-224591221,1,['avail'],['available']
Availability,The official word from Google is that the OS installed on Pipelines API v1 workers has aged enough that driver support is no longer available. The best I can do on behalf of Cromwell is recommend switching to v2; if you have any further questions feel free to reopen!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4935#issuecomment-489215959:132,avail,available,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4935#issuecomment-489215959,1,['avail'],['available']
Availability,"The one Travis failure in ""push"" occurs in the ""fail slow"" test :frowning:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/593#issuecomment-199443309:15,failure,failure,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/593#issuecomment-199443309,1,['failure'],['failure']
Availability,"The only JES failure is `sizeenginefunction` which seems to be suffering from some file path chimerism. In my selfish desire to move forward with Cromwell unification, I'd be more than OK with disabling that temporarily and merging what's here. 😄",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2680#issuecomment-334931335:13,failure,failure,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2680#issuecomment-334931335,1,['failure'],['failure']
Availability,"The only theoretical downside is that it could take longer before we realize it's a cache miss and fall back to running the job. In practice though, this query which is suppose to make us ""fail faster"" is under performing so badly that it's effectively slowing us down. So it's a net positive in call caching time and resources spent. The alternative would be to replace it with something more performant, which we can always do later if the need arises.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4121#issuecomment-422900723:21,down,downside,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4121#issuecomment-422900723,2,['down'],"['down', 'downside']"
Availability,"The output was very verbose, so I didn't find the actual compilation error. I think I fixed it now. We'll see if the tests pass this time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493629183:69,error,error,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493629183,1,['error'],['error']
Availability,"The outputs of an optional scattered task that does **not** run should be undefined. Instead, Cromwell seems to think it is defined, and it has a length. This can cause all sorts of issues, such as breaking downstream tasks that are only supposed to run if the optional upstream task has run, and some very odd error messages. Simple example:; ```; # task_a and task_b are mutually exclusive scattered tasks; Array[File?] vcfs = select_first([task_a.vcf_out, task_b.vcf_out]); ```; Due to this bug, vcfs will yield an empty array if task_a did not run, even though task_b did run. This gets quite messy if you need to process the output of mutually exclusive tasks later. More involved example: ; ```; # variant_call_after_earlyQC_filtering is an optional task, so variant_call_after_earlyQC_filtering.errorcode is an optional type; if(defined(variant_call_after_earlyQC_filtering.errorcode)) {. # variant_call_after_earlyQC_filtering is a scattered task, so variant_call_after_earlyQC_filtering.errorcode is an array; # this length check should be redundant with the defined check earlier, but neither of them seem to work properly; if(length(variant_call_after_earlyQC_filtering.errorcode) > 0) {; 	; # get the first (0th) value and coerce it into type String; 	String coerced_vc_filtered_errorcode = select_first([variant_call_after_earlyQC_filtering.errorcode[0], ""FALLBACK""]); 	call echo as echo_a {input: integer=length(variant_call_after_earlyQC_filtering.errorcode), string=variant_call_after_earlyQC_filtering.errorcode[0]}; 	call echo as echo_b {input: string=coerced_vc_filtered_errorcode}; call echo_array as echo_c {input: strings=variant_call_after_earlyQC_filtering.errorcode}; }; }; ```. Output:; * echo_a will echo ""1"" for input _integer_ and an empty string for input _string_; * echo_b will echo ""FALLBACK"" for input _string_; * echo_c will cause an error ; * `""message"":""Cannot interpolate Array[String?] into a command string with attribute set [PlaceholderAttributeSet(None,None,",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7201:207,down,downstream,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7201,4,"['down', 'error']","['downstream', 'error', 'errorcode']"
Availability,"The parameters available to config scripts are mostly undocumented. A list can be reverse engineered using the cromwell.examples.conf file, and by forcing the parser to crash - which prints the generated submit wdl script to the log. I could then build this config:. ```; submit-docker = """"""; /usr/bin/env ${job_shell} ${script}; echo ${job_name}; echo ${cwd}; echo ${out}; echo ${err}; echo ${script}; echo ${job_shell}; echo ${docker_cid}; echo ${docker_cwd}; """"""; submit = """"""; /usr/bin/env ${job_shell} ${script}; echo ${job_name}; echo ${cwd}; echo ${out}; echo ${err}; echo ${script}; echo ${job_shell}; """"""; ```. (note while String job_id is available in the submit task, using it causes a crash-loop). . The values of cwd and docker_cwd make sense, cwd (in both scripts) refers to the 'shared file system' path to the cromwell created directory, and docker_cwd strips everything before 'cromwell-executions'. I do not see the necessity for docker_cwd, but fair enough. However 'out', 'err' and 'script' do not follow the pattern. In 'submit' they are sfs paths, but in submit-docker they are docker paths. This kind of inconsistency, combined with lack of documentation makes cromwell extremely frustrating to use.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4212:15,avail,available,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4212,16,"['avail', 'echo']","['available', 'echo']"
Availability,"The path is declared as a File. I even tried running the same thing by removing `?` to make non optional. But got the same error. The task definition is as follow:. ```; task FilterByOrientationBias {; File ref_fasta; File ref_index; File ref_dict; File ref_alt; File? intervals; File oncefiltered_mutect2_vcf; File oncefiltered_mutect2_vcf_index; File? pre_adapter_matrix. String? java_options; String? gatk_options; String? mutect2_extra_filtering_args. String out_base = basename(oncefiltered_mutect2_vcf ,"".mutect2.oncefiltered.vcf.gz""); String output_vcf = out_base + "".mutect2.twicefiltered.vcf.gz""; String output_vcf_index = out_base + "".mutect2.twicefiltered.vcf.gz.tbi"". Array[String] artifact_modes. command {; gatk FilterByOrientationBias \; ${""--java-options "" + java_options} \; -V ${oncefiltered_mutect2_vcf} \; -O ${output_vcf} \; ${""-P "" + pre_adapter_matrix} \; -R ${ref_fasta} \; ${""-L "" + intervals} \; -AM ${sep=' -AM ' artifact_modes} \; ${gatk_options}; }. runtime {; }. output {; File twice_filtered_vcf = ""${output_vcf}""; File twice_filtered_vcf_index = ""${output_vcf_index}""; File twice_filtered_pre_adapter_matrix = ""${pre_adapter_matrix}""; }; }. ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436279203:123,error,error,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436279203,1,['error'],['error']
Availability,The performance of method `processRunnableTaskCallInputExpression` first appeared on our radar in https://github.com/broadinstitute/cromwell/pull/5048. It came up again yesterday when we had a high CPU event on all three runners. I took a thread dump during the event and there were 44 threads busy with; ```; at wom.graph.CommandCallNode.toString(CallNode.scala:68); at java.lang.String.valueOf(String.java:2994); at java.lang.StringBuilder.append(StringBuilder.java:131); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processRunnableTaskCallInputExpression(WorkflowExecutionActor.scala:549); ```. I think we are making the mistake of calculating a potentially very large `toString` by using the default case class implementation. I do have the full stack dump available in case anyone is curious. ![Screen Shot 2019-09-11 at 5 00 45 PM](https://user-images.githubusercontent.com/1087943/64735012-b60d3a00-d4b5-11e9-8f60-e9a55fd20f07.png),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5172:787,avail,available,787,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5172,1,['avail'],['available']
Availability,"The problem is that when I started to use Cromwell I was under the impression that I would get shielded from having to learn all the complexities of a given backend and Cromwell would take care of it. I have zero familiarity with permissions in Google Cloud. The admin that set up my account also had no idea what `compute.zones.list` mean. I don't see this in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; //",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:620,ERROR,ERROR,620,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,2,['ERROR'],['ERROR']
Availability,"The problem turns out to be broader than globs as joint genotyping pointed out. The way we handle large arrays, particularly large file arrays chews up a massive amount of ram- in the case of a single large workflow it can be impossible to run. In a multi tenant case it can be death by a thousand cuts. . This will absolutely need to be improved before we get too far down the caas journey",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-328528593:369,down,down,369,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-328528593,1,['down'],['down']
Availability,"The pros / cons sort of mirror each other.; One pro of changing it is what @ffinfo suggested where you don't have to wait for the whole subworkflow to start downstream tasks.; However if your subworkflow is a coherent unit in the sense that it only really is successful if all of its calls complete successfully it might not be the desired behavior.; For example in the WDLs above, if task `Cat` is an expensive operation and the sleep task ends up failing, you could potentially have wasted time running `Cat` unnecessarily.; Of course this can be mitigated by having `Cat` depend on `Sleep`, but it's some sort of ""fake"" dependency. To be fair this behavior already exists with scatters so it might not be that much of a deal, but I remember it was brought up at the time. I think people could be surprised either way. Another not-quite-similar-but-related example is streaming of files from one task to the other, which [CWL lets you specify explicitly](https://www.commonwl.org/v1.0/CommandLineTool.html#CommandInputParameter) (see `streamable` field).; You could imagine a scenario like this (if WDL had a similar streamable concept):. ```wdl. task A {; command {; ./my_script_generating_data.sh > streamable_out; echo ""hello"" > out; }; output {; File streamable_out = ""streamable_out""; File out = ""out""; }; parameter_meta {; streamable_out: {; ""streamable"": true; }; }; }. task B {; File in; command {; cat in | my_script_reading_data.sh; }; }. workflow w {; call A; call B { input: in = A.streamable_out }; call B as B2 { input: in = A.out }; }; ```. Where A and B would actually run simultaneously but B2 would have to wait for A to complete.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3814#issuecomment-400468499:157,down,downstream,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3814#issuecomment-400468499,2,"['down', 'echo']","['downstream', 'echo']"
Availability,"The query endpoint responds with a 400 error when trying filter by ""On Hold"" status. curl -X GET ""https://[cromwell_url]/api/workflows/v1/query?status=On%20Hold"" -H ""accept: application/json"". 400; {; ""status"": ""fail"",; ""message"": ""Unrecognized status values: On Hold""; }. This makes it difficult to use On Hold for queuing because you can't find the On Hold workflows unless you query for everything and then filter the results by status on the client side, which is not very efficient.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3650:39,error,error,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3650,1,['error'],['error']
Availability,"The regular status polling is totally separate from this option. . Regular polling backs off, so doesn't have a single ""interval"" value to configure or report (ie it starts off polling with short intervals but slows down as the job runs for longer and longer). That's true for all jobs across all backends. The `is-alive` check is a fixed (configurable) duration, and it's the timeout between `is-alive` returning false, and the job being considered failed, that are the same. Does that make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-492253707:216,down,down,216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4877#issuecomment-492253707,3,"['alive', 'down']","['alive', 'down']"
Availability,"The removed `-Xms2g` was saying ""Never run sbt, ever, without less than 2g of memory"". Meanwhile, Intellij has its own ""[Maximum Heap Size](https://www.jetbrains.com/help/idea/sbt.html#82b10b37)"" configuration value for the amount of memory required to import an sbt project. The IDE doesn't use this for running tests, so it does not need to be as large as the sbt options one uses for `sbt test` from the command line. The net effect of having the Intellij maximum less than 2g and the sbt opts _minimum_ at 2g caused a cryptic error of: . ```; Error while importing sbt project:. Error occurred during initialization of VM; Initial heap size set to a larger value than the maximum heap size; ```. This PR still leaves .sbtopts maximum amount of memory for running `sbt test` at 4g. It just no longer states that the JVM should start at 2g of memory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4476#issuecomment-445947767:530,error,error,530,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4476#issuecomment-445947767,3,"['Error', 'error']","['Error', 'error']"
Availability,"The report in CROM-6808 includes two cases, these changes would only address the second. I'm not sure what would be causing the first case (an `Exception` with message text containing the text `Please try again.` that wasn't retried), so this PR may not be the final word on PAPI initialization failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6572:295,failure,failures,295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6572,1,['failure'],['failures']
Availability,"The resolution of #2035 exposed this issue. A centaur issue was mitigated by https://github.com/broadinstitute/centaur/pull/165. A proper fix in the [StandardBackend](https://github.com/broadinstitute/cromwell/blob/335e9838abce1405f0d26fe18c5dba2dfa7ad0f5/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L173) will enable a centaur test to run a task with command:. ```; command {; echo ""hello"" > tmp; }; ```. This regression centaur test should run on all backends.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2047:417,echo,echo,417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2047,1,['echo'],['echo']
Availability,"The results above were obtained with Cromwell 29. It seems that the issue still exists in Cromwell 35:; ### Length(); ```; [2018-10-08 13:39:46,36] [error] WorkflowManagerActor Workflow 88434c0b-2595-4aee-b044-932eb0ba59f4 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unable to build WOM node for Declaration 'num': Cannot build expression for 'Test_optional.num = length(strings1)': Unexpected arguments to function length. length takes a parameter of type Array but got: Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))); ```; ### Indexing; ```; [2018-10-08 13:40:22,66] [error] WorkflowManagerActor Workflow 506ae394-a8b2-428c-a80a-532e0a158438 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unable to build WOM node for Scatter '$scatter_0': Unable to build WOM node for WdlTaskCall 'testtask': Cannot build expression for 'Test_optional.testtask.str = strings1[idx]': Invalid indexing target. You cannot index a value of type 'Array[String]?'; ```; ### Zip(); ```; [2018-10-08 13:38:36,15] [error] WorkflowManagerActor Workflow 6d784fbe-2db0-4215-a03d-e2c40c95218a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unable to build WOM node for Declaration 'string_pair': Cannot build expression for 'Test_optional.string_pair = zip(strings1, strings2)': Unexpected zip parameters: Vector(Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType))), Success(WomOptionalType(WomMaybeEmptyArrayType(WomStringType)))); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-427939436:149,error,error,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218#issuecomment-427939436,3,['error'],['error']
Availability,"The singleton `JobExecutionTokenDispenserActor` seems to crash quite a bit with no cause currently known. The default Akka supervision strategy causes JETD to be restarted, but the reincarnated JETD knows nothing of its predecessors' token allocations. This will cause tokens to be dispensed as if none are outstanding and returned tokens to produce error messages like:. ```; 2019-04-26 19:40:20 [cromwell-system-akka.actor.default-dispatcher-1109] ERROR c.e.w.t.JobExecutionTokenDispenserActor - Job execution token returned from incorrect actor: 202ffd24-a696-4edc-b832-a75f1127fdb4-EngineJobExecutionActor-Gatk3BamToGVCF.PrintReads:1:1; ```. JETD restarts in production can be found by looking for `""log token queue events""` in Kibana where there is no corresponding Cromwell restart.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4908:350,error,error,350,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4908,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,The slick exceptions could technically cause this but no this is more generally due to the metadata building process being very expensive therefore causing all kinds of timeouts / errors.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-321682962:180,error,errors,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2519#issuecomment-321682962,1,['error'],['errors']
Availability,The source code used for localizing and executing the Cromwell exec script on BCS is embedded within tar.gz file in the cromwell code base. To make this code easier to update the code should be moved out of the tar and live as individual files within the source code. - https://github.com/broadinstitute/cromwell/blob/31/supportedBackends/bcs/src/main/scala/cromwell/backend/impl/bcs/worker.tar.gz. A/C:; - The source code for the BCS worker is available in the cromwell source tree ; - Running `sbt assembly` still creates a an embedded resource `worker.tar.gz` that the BCS backend can upload to OSS.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3521:445,avail,available,445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3521,1,['avail'],['available']
Availability,"The spec does say Array[?] is allowed so this should work. Tested on: d9b9262 and release 35. Test wdl:; ```wdl; version 1.0. workflow Test {; input {; }. # This fails currently; Array[SomeStruct] array = read_json(""test.json""). # This does work; SomeStructs array2 = read_json(""test2.json""). output {; }; }. struct SomeStruct {; String string; }. struct SomeStructs {; Array[SomeStruct] array; }; ```. test.json; ```json; [{""string"": ""bla""}]; ```. test2.json; ```json; {""array"": [{""string"": ""bla""}]}; ```. error:; ```; [2018-10-09 10:16:43,95] [error] WorkflowManagerActor Workflow 04cdb52a-6e98-4674-b313-f8f4d406ffb2 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to process workflow definition 'Test' (reason 1 of 1): Failed to process declaration 'Array[SomeStruct] array = read_json(""test.json"")' (reason 1 of 1): Cannot coerce expression of type 'Object' to 'Array[WomCompositeType {; string -> String ; }]'; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4219:507,error,error,507,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4219,2,['error'],['error']
Availability,"The specification according to the documentation is `${true=""--enabled"", false=""--disabled"" boolean_var}`, and the false tag is optional. However, I get this error when attempting this syntax:. ```. cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unable to load namespace from workflow: ERROR: Both 'true' and 'false' attributes must be specified if either is specified:. --tumorBam=${tumor_bam} --normalBam=${normal_bam} ${true='--targeted' targeted} \; ^. at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:186); at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:156); at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:151); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at akka.actor.FSM.processEvent(FSM.scala:663); at akka.actor.FSM.processEvent$(FSM.scala:660); at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:114); at akka.actor.LoggingFSM.processEvent(FSM.scala:799); at akka.actor.LoggingFSM.processEvent$(FSM.scala:781); at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:114); at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor.aroundReceive(Actor.scala:513); at akka.actor.Actor.aroundReceive$(Actor.scala:511); at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:114); at akka.actor.ActorCell.r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2594:158,error,error,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2594,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"The test failure looks like it might be real: `should parse all manner of well-formed auths *** FAILED *** (1 second, 53 milliseconds)`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1429#issuecomment-247718213:9,failure,failure,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1429#issuecomment-247718213,1,['failure'],['failure']
Availability,The test_read_data.wdl has lines commented out that resulted in errors I'm reporting here.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2152#issuecomment-292029467:64,error,errors,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2152#issuecomment-292029467,1,['error'],['errors']
Availability,The travis failure looks unrelated but sort of scary... perhaps one for the spreadsheet...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3389#issuecomment-371888809:11,failure,failure,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3389#issuecomment-371888809,1,['failure'],['failure']
Availability,"The urgency of this particular fix came as we starting adding more valid CWL to PAPI and the tests were creeping past 70 minutes. I'm open to parallelizing local too. The workflow is reusable and there's currently nothing technically stopping us from switching local to parallel also. Cromwell's ""randomization"" (aka internally using unordered sets/hashmaps) when launching scatter jobs makes debugging the entire suite of tests wicked painful. It's hard to tell when a failure occurs what test was running. While CWL is in a state of flux I kind of like slowly working my way through the serial logs of local-conformance when I break something. My 2¢/ToL",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3187#issuecomment-360253011:470,failure,failure,470,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3187#issuecomment-360253011,1,['failure'],['failure']
Availability,"The valid WDL file below fails to compile. ```wdl; workflow x {; call cram; call y { input:; cram = cram.cram; }; }. task cram {; command {; echo "".""; }; output {; String cram = "".""; }; }. task y {; String cram; command {; echo "".""; }; }; ```. Running it with Cromwell version 32 gives:. ```; Workflow input processing failed:; ERROR: Bad target for member access 'cram.cram': 'cram' was a String (line 4, col 33):. cram = cram.cram; ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3811:141,echo,echo,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3811,3,"['ERROR', 'echo']","['ERROR', 'echo']"
Availability,"The way this works for now is:; In JES configuration you need to specify an `authenticationMode` parameter which can take one of those 2 values: `service_account` or `refresh_token`; `service_account` works exactly the same way as it did before; `refresh_token` will require 2 fields to be passed as workflow options:; `account_name` and `refresh_token`. Depending on what is available, the JES backend will create, only if necessary and at workflow initialization time, a json file (gcloudauth.json) and upload it to the root directory of the workflow.; Currently this json file can contain 2 types of information:; - Docker credentials: These are optional and can be specified in the `jes` section in application.conf. They look like this:. ```; dockerAccount = ""my.docker@account.com""; dockerToken = ""mydockertoken""; ```. If they are specified a ""docker"" value will be added to gcloudauth.json containing those values.; This will allow using a private docker image that would not be accessible otherwise.; This functionality was mostly already there from Kristian PR, I just moved the credential location from gcs to conf.; - User credentials: These need to be added as workflow options : . ```; {; ""account_name"": ""myaccount@broadinstitute.org"",; ""refresh_token"": ""refresh_token""; }; ```. Again if in RefreshToken mode, they need to be there for every workflow call or an exception will be thrown at initialization time.; If they are there they will be added to the gcloudauth.json in the same way docker crednetials are, under a ""gcloud"" value. You should then get permission to localize input files that are only accessible to this user. So depending on what is provided (docker info, user info), you can get a gcloudauth.json with both credentials, only one, or no file at all. In any case the JES backend will try to delete this file when the workflow reaches a terminal state.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/190:376,avail,available,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/190,1,['avail'],['available']
Availability,"The wdl below doesn't pass input processing. It's a valid bash command. I suspect it has to do with the @ sign? . This wdl (also copied below) reproduces the issue:; /humgen/gsa-hpprojects/dev/tsato/m2-evaluation/wdl/test/hello.wdl; /humgen/gsa-hpprojects/dev/tsato/m2-evaluation/wdl/test/hello.json. #### wdl ####. task hello {; String addressee; ; command <<<; echo ""Hello ${addressee}!""; array=(one two three); for i in ${array[@]}; do; echo $i; done; >>>. runtime {; docker: ""ubuntu:latest""; }; output {; String salutation = read_string(stdout()); }; }. workflow hello_and_goodbye {; String hello_and_goodbye_input. call hello {input: addressee = hello_and_goodbye_input }. output {; String hello_output = hello.salutation; }; }. #### json ####; {; ""hello_and_goodbye.hello_and_goodbye_input"": ""Takuto""; }; #### error message ####; ""failures"": [{; ""message"": ""Workflow input processing failed:\nUnable to load namespace from workflow: Unrecognized token on line 7, column 22:\n\n for i in ${array[@]}; do\n ^""; }]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1819:363,echo,echo,363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1819,4,"['echo', 'error', 'failure']","['echo', 'error', 'failures']"
Availability,"The workflow itself seems to succeed, but Centaur thinks there is a failure due to #962.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224113013:68,failure,failure,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/955#issuecomment-224113013,1,['failure'],['failure']
Availability,The workflow; ```; version 1.0. workflow member_access {; Object myObj = object { an_int: 5 }; if (myObj.an_int == 10) {; Boolean asdf = true; }; }; ```; fails to validate with error; ```; Failed to create wom Graph (reason 1 of 1):; Failed to process workflow definition 'member_access' (reason 1 of 1):; Invalid type for condition variable: Any; ```; It appears that the type checker is not correctly evaluating the type of values reached via member access.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3790:177,error,error,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3790,1,['error'],['error']
Availability,"Then; ```; 2019-01-31 20:10:51,323 INFO - changelog.xml: changesets/failure_metadata.xml::remove_failure_timestamp::cjllanwarne: ChangeSet changesets/failure_metadata.xml::remove_failure_timestamp::cjllanwarne ran successfully in 5ms; 2019-01-31 20:10:51,428 ERROR - changelog.xml: changesets/failure_metadata.xml::causedByLists::cjllanwarne: Change Set changesets/failure_metadata.xml::causedByLists::cjllanwarne failed. Error: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 2019-01-31 20:10:51,492 INFO - changesets/failure_metadata.xml::causedByLists::cjllanwarne: Successfully released change log lock; 2019-01-31 20:10:51,531 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:259,ERROR,ERROR,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,6,"['ERROR', 'Error', 'down', 'failure']","['ERROR', 'Error', 'down', 'failures']"
Availability,"Theory 1:; I've also seen this issue. I've set our `concurrent-job-limit` parameter in the backend to 150 and yet the number of concurrent jobs (including those pending) seems to stay at around 60-65. . Could the other 90 be from tasks that still need to be 'ticked off' as complete?. My suspicion on this is that the `check-alive` parameter in the config is set to `squeue -j ${job_id}`.; On our cluster we have around 3 hours to continue running commands like `scontrol show job` to see the metadata on the job and find logs. This is useful but, `squeue -j ${job_id}` still returns true well and truly after the job has completed/failed. Could you try massively increasing the job limit (to say 10000) and see if that changes anything?. Theory 2:; Your configuration file could need a scale up - it may be that the number of system io requests require increasing:. ```; system {; io {; number-of-requests = 100000; per = 100 seconds; number-of-attempts = 50; }; ```. Will allow your job to make 1000 requests per second. For some of those batch calling jobs with many vcf inputs, it may be taking some time for the server to set up the task?. Theory 3:; Your duplication-strategy is causing lag.; Can you confirm that in providers.slurm.filesystems that you have hard-link or soft-link as the top priority for the localization and caching settings, over copying?. Alexis.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952:325,alive,alive,325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218#issuecomment-554183952,1,['alive'],['alive']
Availability,"Theory: something in the new SBT is meaning that some background thread/process is no longer shutting down the same way as before. This means that:; * (a) we get a bunch of resource leaks when `-Dsbt.classloader.close=false` is not set in the SBT options; * (b) when `-Dsbt.classloader.close=false` *is* set in sbt options, the thread remains running in the background preventing the tests from exiting on completion.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5211#issuecomment-541103425:102,down,down,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5211#issuecomment-541103425,1,['down'],['down']
Availability,"There are again some issues with Travis build (strange errors) , I'll reopen PR to trigger it again.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527854752:55,error,errors,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-527854752,1,['error'],['errors']
Availability,"There are at least four spots in `WorkflowExecutionActor` where `==` is invoked on `Scope`s. This is not so good as `==` in Scala does a deep equals, and the `Scope` implementations are case classes with compiler-generated `equals` and `hashCode` implementations. These implementations invoke `equals`/`hashCode` on their constructor fields, including `Task` which contains a lot of hairy stuff including ASTs. This is extra bad in `WorkflowExecutionActor` since these `==`s are invoked synchronously with message processing to update the execution store, and all this computation is effectively single-threaded within a workflow. In practice, for a 20K wide scatter this causes the `WorkflowExecutionActor` to spend upwards of 20 minutes from the time work finishes in (mock) JES to actually succeeding the workflow. I _think_ this might be fixed by chasing down all the spots where `==` is used on `Scope`s and replacing that with call FQN + index + attempt comparisons instead.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1457:859,down,down,859,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1457,1,['down'],['down']
Availability,"There are multiple related issues in this ticket. The common thread is: a WDL author expects to be able to write compound WDL statements in a task `output` section. However, there are certain WDL statements that fail parsing when strung together, but _will_ work if the statement is broken into multiple variables. The original issue identifies problems in `output` using `glob()` or `Map[,]`. In terms of a fix, I'm guessing for the right dev this is a medium<sup>1</sup> sized task, but would probably be lower on the list of TODOs as there exists a workaround. This ""workaround"" works, where all three `output` variables are relatively simple:; ```wdl; task x {; command {; echo 0 > intFile.txt; echo hello > outFile.txt; }; runtime { docker: ""ubuntu"" }; output {; Int intermediateInt = read_int(""intFile.txt""); Array[File] intermediateOuts = glob(""outFile.txt""); File out = intermediateOuts[intermediateInt]; }; }. workflow glob_indexing { call x }; ```. Starting to compress the output block into two statements, where the latter is a compound expression, this still parses and runs:; ```wdl; output {; Int intermediateInt = read_int(""intFile.txt""); File out = glob(""outFile.txt"")[intermediateInt]; }; ```. Regarding the problems with `Map[,]` this _does_ work:; ```wdl; output {; Map[String, File] intermediateMap = {""a"": ""outFile.txt""}; File out = intermediateMap[""a""]; }; ```. HOWEVER, this doesn't work, currently failing with the error `Workflow input processing failed: <string:8:20 lbrace ""ew==""> (of class wdl4s.parser.WdlParser$Terminal)`:. ```wdl; output {; File out = {""a"": ""outFile.txt""}[""a""]; }; ```. And going back to globbing, the error with globs is _slightly_ better. This doesn't work, either:; ```wdl; output {; File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; }; ```. And fails with the ""prettier"" message at the moment:. ```; ERROR: Unexpected symbol (line 8, col 48) when parsing 'e'. Expected rsquare, got (. File out = glob(""outFile.txt"")[read_int(""intFile.txt"")];",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829:677,echo,echo,677,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829,2,['echo'],['echo']
Availability,"There are some real-looking test failures that might be due to the removal of the execution status setting logic from the backend info update method. Probably the way to fix these is to change the test expectations, but some investigation seems warranted.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/215#issuecomment-145589209:33,failure,failures,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/215#issuecomment-145589209,1,['failure'],['failures']
Availability,"There are two failures in WorkflowManagerActorSpec, but I believe they are both due to the asynchronicity of setting status that should be fixed (inelegantly) by #224.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/228#issuecomment-146242763:14,failure,failures,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/228#issuecomment-146242763,1,['failure'],['failures']
Availability,"There are two general cases where the `WorkflowExecutionActor` switches the workflow to a failed state:; - [`handleRetryableFailure()`](https://github.com/broadinstitute/cromwell/blob/f64c16e62c84b66ddba14705bede5e6fde8376b0/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L292-L306); - [`handleExecutionFailure()`](https://github.com/broadinstitute/cromwell/blob/f64c16e62c84b66ddba14705bede5e6fde8376b0/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/WorkflowExecutionActor.scala#L240-L255). In _both_ cases, the method:; - is triggered by the failure of an individual job, but; - fails the entire workflow, and; - doesn't send signals to other jobs to stop. Thus other jobs stay running, while the workflow gets deregistered from the system. Because the workflow manager can no longer delegate aborting the running jobs, this issue may also be related to #1414 and #1504.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2029:613,failure,failure,613,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2029,1,['failure'],['failure']
Availability,"There are use cases where we'd want to be able to shut down a Cromwell instance without stopping running jobs. Notably, we want to redeploy without interrupting any long-running tasks (Cromwell can then pick back up where it left off when it restarts). I think we should make this optional.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173967513:55,down,down,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-173967513,1,['down'],['down']
Availability,"There have been many requests over time to provide data on CPU, memory, and disk usage. . In an ideal world this would be a rich time series of data, in a less ideal world this would be max with a couple of periodic data points, and in a much more practical world it'd just be max usage. The last one is the definition of done here but if one wants to get ambitious ....... The PAPI backend already allows for a custom script to be attached, we should be putting in something on our own for any backend which runs things via unix command lines (ie spark jobs and such likely don't make sense). At least one group is using the following in production, this is likely a good start if not completely AOK. ``````#!/bin/bash; echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(free -h | grep Mem | awk '{ print $2 }'); echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); echo ; echo --- Runtime Information ---. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(top -bn 2 -d 0.01 | grep '^%Cpu' | tail -n 1 | awk '{print $2}')%; echo \* Memory usage: $(free -m | grep Mem | awk '{ OFMT=""%.0f""; print ($3/$2)*100; }')%; echo \* Disk usage: $(df | grep cromwell_root | awk '{ print $5 }'); }. while true; do runtimeInfo; sleep 300; done```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2507:721,echo,echo,721,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507,13,['echo'],['echo']
Availability,"There is a Pull request in for AWS CLI call retry's which will mitigate; some of the problem. Currently full retries of tasks are not supported via; Cromwell Server coordinating with the AWS Batch backend. Having said that,; you could identify the AWS Batch Job Description and edit it to create a; new revision such that the revision uses the AWS Batch retry strategy. This; will mean that AWS Batch will retry any job that doesn't exit cleanly; (return code 0 or container host is terminated) up to a max number of; times. When that happens, the job ID remains the same so as far as Cromwell; knows it is the same job. I haven't had a chance to test this out myself; but it's on my to do list. Let me know if you try it. If it works the same; approach would allow for recovery in the case of Spot interruption. https://docs.aws.amazon.com/batch/latest/userguide/job_retries.html; https://docs.aws.amazon.com/batch/latest/userguide/job_definition_parameters.html#retryStrategy. On Wed, Oct 14, 2020 at 2:40 PM Richard Davison <notifications@github.com>; wrote:. > Originally posted this two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and th",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:770,recover,recovery,770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780,1,['recover'],['recovery']
Availability,"There is a bug in the owl library where ontology parsing can sometimes fail, causing tests to fail.; See https://github.com/protegeproject/webprotege/issues/298. For now retry parsing a few times. Should fix the `cwl_format***` transient centaur failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4210:246,failure,failures,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4210,1,['failure'],['failures']
Availability,"There is a claim of 5–10% faster compiler performance since 2.12.8, which seems worthwhile. Not super scientific, but a build test before and after (with warmed JVM) went down from about 106 to 97 seconds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5106:171,down,down,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5106,1,['down'],['down']
Availability,"There is a fairly large need to support custom container engines in Cromwell, for those HPC systems that cannot run docker. This is discussed in the PR here: #4635. . Currently the only hook we have for custom container engines is the `submit-docker` field, which is a script that runs when a task is run that specifies a docker image. This lets us download the image from docker, and convert it to a custom format (probably the Singularity SIF format) before submitting a job to the queue that runs the image. However, in the case of a scatter job, this means downloading the same Docker image N times, converting it to SIF format N times, and using up N times as much storage as we would like. This issue is discussed in my comment [here](https://github.com/broadinstitute/cromwell/pull/4635#issuecomment-463890367), and a number of preceding comments. If we had another hook for the `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`, called `pull-docker` that was run each time a Docker image needs to be pulled, then we could resolve this issue. The hook could run each time a new image is encountered in a given workflow, but only once each time, so that a scatter job would result in only one call to the hook. We would then have to find some way for the image built in the `pull-docker` hook to be communicated to the `submit-docker` hook. The default value of `pull-docker` would be some kind of no-op, because this is not needed using Docker itself. However, it would be invaluable for custom engines.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4673:349,down,download,349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4673,2,['down'],"['download', 'downloading']"
Availability,"There is a potential race condition (which seems to have occurred at least once in our integration test) where Cromwell shuts down after submitting a job to PAPI but before writing the operation ID to the DB.; In this case, upon restart, the job will be submitted a second time, using the same bucket, causing all sorts of issues as 2 jobs are now concurrently reading from and writing to the same files.; [Example bucket](https://console.cloud.google.com/storage/browser/cloud-cromwell-dev/cromwell_execution/travis/dontglobinputs/c10b823f-7f7c-4442-b0d1-a90f0f4e39a7/call-globtask/?project=broad-dsde-cromwell-dev&organizationId=548622027621)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4209:126,down,down,126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4209,1,['down'],['down']
Availability,"There is a requirement from CCC to be able to identify through logs why localization functionality fallback to an specific strategy. A simple solution would be to log failures in each strategy invoke but I'm open to any other alternative. OPEN: I made use of StrictLogging but if you think I should use a different implementation, just let me know and I will change it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1643:167,failure,failures,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1643,1,['failure'],['failures']
Availability,"There is a second issue that came up after Google fixed the first issue that is oddly ironic given your comment. If they hit API quotas, it appears that they were immediately failing the operations, leading to some odd error messages passed through to Cromwell (and then to me). In a sense, they were failing too quickly. Tough balancing act, it seems.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260528439:219,error,error,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260528439,1,['error'],['error']
Availability,"There is evidence in centaur that CWL fully qualified names may not be stored in the database in a way CWL workflows survive a restart. Currently this occurs because the suite-of-restart-tests are running sequentially, while at the same time the suite-of-_non_-restart tests are running. If a restart test accidentally restarts during `three_step_cwl`, one might see the stack trace below. . A/C: Explicit centaur test that restarts in the middle of a CWL workflow. Example stack trace:; ```java; 2017-12-06 04:38:35,249 cromwell-system-akka.dispatchers.engine-dispatcher-29 INFO - WorkflowExecutionActor-20f2c75f-5250-4525-8e30-2330f25dbbec [UUID(20f2c75f)]: Restarting calls: ps:NA:1; 2017-12-06 04:38:35,277 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - JobStoreReadFailure; java.util.NoSuchElementException: key not found: ps-stdOut; 	at scala.collection.immutable.Map$Map1.apply(Map.scala:108); 	at cromwell.core.simpleton.WomValueBuilder$.$anonfun$toWdlValues$5(WomValueBuilder.scala:147); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.Map$Map1.foreach(Map.scala:120); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at cromwell.core.simpleton.WomValueBuilder$.toWdlValues(WomValueBuilder.scala:147); 	at cromwell.core.simpleton.WomValueBuilder$.toJobOutputs(WomValueBuilder.scala:133); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$2(SqlJobStore.scala:74); 	at scala.Option.map(Option.scala:146); 	at cromwell.jobstore.SqlJobStore.$anonfun$readJobResult$1(SqlJobStore.scala:70); 	at scala.util.Success.$anonfun$map$1(Try.scala:251); 	at scala.util.Success.map(Try.scala:209); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:289); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); 	at scala.concurrent.impl.Promise.$a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012:765,ERROR,ERROR,765,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012,1,['ERROR'],['ERROR']
Availability,"There seem to be intermittent Centaur failures on `invalid_runtime_attributes`. I investigated this for a while and found the following:. `WorkflowActor` will always try to run finalization, even if initialization fails. And if initialization fails there will be no initialization data, which means a `.get` on the optional initialization data will throw. Unfortunately this is exactly what `toJes` in `JesBackendLifecycleActorFactory` is doing:. ``` scala; def toJes = genericInitializationData collectFirst { case d: JesBackendInitializationData => d } get; ```. My guess is that this doesn't fail 100% of the time because there's already failure metadata generated (intentionally), and there's a race condition as to which failure event shows up first. If the intentional failure is generated first the test would pass.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1272:38,failure,failures,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1272,4,['failure'],"['failure', 'failures']"
Availability,"There seems to be a failure mode where a workflow fails during centaur JES testing because the JES job can't find the auth JSON:. ```; java.lang.Exception: Task readFromCache.find:NA:1 failed. JES error code 3. Message: unable to load extra config: download from ""gs://cloud-cromwell-dev/cromwell_execution/travis/readFromCache/009bccc8-8e1b-49b3-bc7c-7d15c8255482/009bccc8-8e1b-49b3-bc7c-7d15c8255482_auth.json"" failed: exit status 1: Copying gs://cloud-cromwell-dev/cromwell_execution/travis/readFromCache/009bccc8-8e1b-49b3-bc7c-7d15c8255482/009bccc8-8e1b-49b3-bc7c-7d15c8255482_auth.json...; / [0 files][ 0.0 B/ 133.0 B] ; NotFoundException: 404 gs://cloud-cromwell-dev/cromwell_execution/travis/readFromCache/009bccc8-8e1b-49b3-bc7c-7d15c8255482/009bccc8-8e1b-49b3-bc7c-7d15c8255482_auth.json does not exist.; ```. This happened on the readFromCacheFalse JES centaur test. logs are at `gs://cloud-cromwell-dev/cromwell_execution/travis/centaur_workflow/61e52cbe-50f5-4e45-b8fb-a61c1f902426/call-centaur/cromwell_root/logs/`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2405:20,failure,failure,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2405,3,"['down', 'error', 'failure']","['download', 'error', 'failure']"
Availability,There was a [bug](https://github.com/broadinstitute/cromwell/issues/3102) in the centaurCwlConformancePAPI test script that would have caused a false failure for you. The fixes for this bug are on develop so when you rebase this should no longer be an issue.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3101#issuecomment-355339696:150,failure,failure,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3101#issuecomment-355339696,1,['failure'],['failure']
Availability,"There were a **lot** of general cleanup rabbit holes I wanted to go down but figured I'd stick w/ a safe harbor of ""do what the ticket asks"" and then follow up with a series of more targeted clean up PRs. If people would prefer (or start asking for changes along those lines) I'll pull this back and do this. Along similar lines there's a clear overlap erupting between wes2cromwell code and cromiam, however only in places where I could literally use cromiam code as-is did I do so (and even then I left it in cromiam instead of a shared package). While I was working on this it became clear that the ideal shared abstraction is still too early to tell so I'd rather see it play out a bit before going down that road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3901:68,down,down,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3901,2,['down'],['down']
Availability,"There's an Akka Streams AbruptTerminationException being thrown by some process being shutdown unexpectedly. It's not causing any actual issues for users, however it's presence in the log is distracting. We should make sure that whatever process is throwing this error should be shut down more gracefully. ```[2017-06-05 16:56:14,16] [error] Outgoing request stream error; akka.stream.AbruptTerminationException: Processor actor [Actor[akka://cromwell-system/user/StreamSupervisor-1/flow-6-0-unknown-operation#-1334744097]] terminated abruptly; [ERROR] [06/05/2017 16:56:14.156] [cromwell-system-akka.actor.default-dispatcher-37] [akka.actor.ActorSystemImpl(cromwell-system)] Outgoing request stream error (akka.stream.AbruptTerminationException)```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2340:263,error,error,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2340,6,"['ERROR', 'down', 'error']","['ERROR', 'down', 'error']"
Availability,"There’s a new response from JES that should be retryable: ; JES error code 2. Message: Instance failed to start due to preemption.” . THE CATCH: Henry chatted with Google, and it sounds like JES error code 2 isn’t *always* about preemption. (But in the past few weeks we’ve had a handful of cloud workflows failing each day from this response, always with the message about preemption). So to be safe, we want to make this one retryable based on the combination of the code + the message. . Example: ; UUID: 7da0394b-371f-4fdb-ae70-737833c4fbfa; OPERATION_ID: operations/EKmIx96ALBjh373VhLH0ui8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. Relevant workflow metadata: ; ""failures""; : [; {; ""causedBy"": [],; ""message"": ""Task PairedEndSingleSampleWorkflow.CollectUnsortedReadgroupBamQualityMetrics:2:1 failed. JES error code 2. Message: Instance failed to start due to preemption.""; }; ],",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2970:64,error,error,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2970,4,"['error', 'failure']","['error', 'failures']"
Availability,"These changes appear to break submissions in service account mode. Removing the `GenomicsScopes.all` from `GoogleScopes` fixes the problem but (presumably) will break `application-default`:. ```; 2015-12-21 14:05:11,203 cromwell-system-akka.actor.default-dispatcher-2 WARN - JesBackend [UUID(60f8d0d3)]: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""serviceAccount.scopes\"": element 1: invalid scope name: https://www.googleapis.com/auth/cloud-platform"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""Invalid value for field \""serviceAccount.scopes\"": element 1: invalid scope name: https://www.googleapis.com/auth/cloud-platform"",; ""status"" : ""INVALID_ARGUMENT""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""serviceAccount.scopes\"": element 1: invalid scope name: https://www.googleapis.com/auth/cloud-platform"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""Invalid value for field \""serviceAccount.scopes\"": element 1: invalid scope name: https://www.googleapis.com/auth/cloud-platform"",; ""status"" : ""INVALID_ARGUMENT""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[google-ht",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486:340,error,errors,340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486,2,['error'],['errors']
Availability,"These changes are side effects of my investigation into the `invalidate_bad_caches_jes_no_copy`; test on PAPIv1:. - Don't retry `invalidate_bad_caches_jes_no_copy` because it will never work the second time (even on v2); - Print out any failure metadata we get if we expected `Succeeded` to speed up the debug cycle; - Don't log the ""metadata mismatch"" as errors just because we're waiting for metadata consistency.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4951:237,failure,failure,237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4951,2,"['error', 'failure']","['errors', 'failure']"
Availability,"These changes get us incrementally closer to GHA migration by trimming down to what is absolutely necessary. I did the [87 release](https://github.com/broadinstitute/cromwell/releases/tag/87) with the exact version of the WDL in this PR, so I know it works. - Drop support for minor releases, unused under our modern release cadence; - Drop the Homebrew section of the WDL, they do it automatically for us and our instructions have said ""ignore this"" for several years now; - Remove references to `firecloud-develop` and Jenkins 🚀; - Document side effects for each task in release WDL",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7422:71,down,down,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7422,1,['down'],['down']
Availability,"These removed calls to `setAccessible` are NOT technically illegal yet. They do not modify [JDK classes](https://openjdk.java.net/jeps/396) and the third-party libraries that are modified at runtime are currently weakly encapsulated. Still, if these libraries suddenly switch to modules then [`setAccessible` will then be illegal](https://docs.oracle.com/javase/9/docs/api/java/lang/reflect/AccessibleObject.html#setAccessible-boolean-). This PR uses alternatives for some of the calls to `setAccesible` in this repo, either through basic refactoring or using new APIs that weren't available back when the original workaround was implemented.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6663#issuecomment-1024960996:582,avail,available,582,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6663#issuecomment-1024960996,1,['avail'],['available']
Availability,These workflows all failed with the stacktrace error in the comments section. These were run on gotc-prod. 836783df-339e-4631-a7cd-038072899edb - operations/EIfzx8m1Khjt87q2wPLY8CAgn6KQ6Z4NKg9wcm9kdWN0aW9uUXVldWU. 9925f072-debf-4e4c-98aa-01db0a34a62f - operations/EOqx-cy1KhioppLVjZKx1XQgn6KQ6Z4NKg9wcm9kdWN0aW9uUXVldWU. f7c465bb-e5e5-4cb8-b493-82f072a40040 - operations/EIL4ks21Khi1zfCMm4HHpO8BIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/600:47,error,error,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/600,1,['error'],['error']
Availability,They upped our QPS on our projects so we can't actually reproduce 429 errors in our projects anymore with any tests that we currently have(we were able to do it before with our 50 workflow test). You would have to write some wdl that scatters wide and send in a number of them at the same time to cromwell in some non-upped QPS project.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271635305:70,error,errors,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271635305,1,['error'],['errors']
Availability,"Things Changed; - added a cromwell-master, which refreshes status metadata. There can be only one.; - created a pool of cromwell_norefresh, which has status refresh turned off, these can scale; - found race condition when multiple cromwells try to create the liquibase lock table at once, configured to have master go first; - updated scripts/compose to handle the above two kinds of cromwell; - increased to 100 batches of 10 workflow each; - changed timeout script to show number of completed workflows and break when done; - delete database at start of run, so the above works; - ran heartbeats in auto-commit mode rather than in a single transaction; - dump out logs at end of run for debugging. Things I'd like to share; - Lock Ordering in SELECT...FOR UPDATE no es bueno, there are great feature in MYSQL v8 (SKIP LOCKED) but we can't use those yet; - how to configure mysql for query logging, and what it shows; - heartbeat batches were never a batched update, just a big transaction; - slick terminology can give give the wrong intuition; - impact of cleaning db before each run; - No deadlocks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4508:587,heartbeat,heartbeats,587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4508,2,['heartbeat'],"['heartbeat', 'heartbeats']"
Availability,"This PR addresses an issue found during testing of the creation of the custom AMI for the AWS backend. The stderr redirect to /dev/null was put in place to eliminate duplicate logging (proxy and task container). However, redirecting stderr to /dev/null also suppresses some docker error messages; most notably instances where the command executable does not exist in the image. It is not possible to segregate the docker error messages from stderr output from the task. This PR errs on the side of potential duplicate logging in order to catch docker-based errors.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4110:281,error,error,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4110,3,['error'],"['error', 'errors']"
Availability,"This PR addresses two bugs from WX-1260:; 1. An `echo` command that included asterisks wasn't wrapped in double quotes, causing an issue in some environments (the user's) but not others (the developer's). Added double quotes so this issue occurs in no environments. 2. The Script Preamble that exports the SAS token environment variable was running in a bash subshell, which means that the environment variable it populated wasn't available in the user's parent shell that is actually executing the task.; - To fix this, I added the option for script preambles to be executed in a bash subshell, or not. My thinking:; - It's generally good hygiene for scripts to run in their own subshell, and I didn't want to change anything about the GCP behavior, so I left that functionality as is.; - I didn't want to write a sas token to file in the subshell for the parent shell to read. Writing tokens to file seems not great for security.; - In order for the environment variable to be visible to the user command, I allowed the TES script to run in the parent shell. This bug revealed a gap in my testing: I had been confirming that my script could acquire the token successfully and correctly, but I hadn't actually tried to use that token inside the user command block. The concept of subshells eluded me at the time. After making this change, I've confirmed that the environment variable is indeed useable in the command block.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7326:49,echo,echo,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7326,2,"['avail', 'echo']","['available', 'echo']"
Availability,"This PR adds `WORKFLOW_NAME`, `TASK_INPUTS`, `TASK_DISKS`, and `MONITORING_CONFIG` environment variables for `MonitoringAction` in PAPIv2 backend. These variables are used to pass details about task inputs and disk mappings (both in JSON form), along with an image-specific config string (e.g. `project-id.dataset-id.table-id` for `quay.io/broadinstitute/cromwell-monitor-bigquery`), into the container specified through the existing `monitoring_image` option. It also adds `bigquery.insertdata` OAuth scope, to be used for streaming monitoring data into BigQuery (@adrazhi seems to approve scope extension).; ; This PR will enable us to:; - stream monitoring data at scale into BQ (much more so than was possible through Stackdriver),; - build detailed models for prediction of runtime resource utilization, using BQ or external tools (e.g. Looker); - easily detect runtime failure modes such as running OOM. (Please see https://github.com/broadinstitute/cromwell-task-monitor-bbq for more info on BQ use case). However, the proposed changes are not specific to BQ (apart from the scope), and could be used for other `monitoring_image` implementations in the future, thanks to the new `monitoring_config` option for PAPIv2 backend. **Please note**: this is an initial implementation that's **not yet ready for a merge**. For example, `TASK_INPUTS` are not serialized correctly yet. We intend to add more commits to implement it fully. However, we're soliciting early feedback and review. Interested parties: @kshakir, @benjamincarlin, @rexwangcc, @mohawkTrail, @ruchim, @abaumann",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028:875,failure,failure,875,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028,1,['failure'],['failure']
Availability,"This PR adds the ability to launch a workflow as part of the Jenkins job. To do this you need to:. - create a workflow script which can download the necessary workflow files (source, input, options,..) and make `curl POST` request to Cromwell; - put the script inside cromwell -> scripts -> perf -> vm_scripts -> workflow_scripts folder; - pass the name of this script through `WORKFLOW_SCRIPT` param while building the Jenkins job",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4090:136,down,download,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4090,1,['down'],['download']
Availability,This PR cherry picks the below features/functionalities from Cromwell 37 onto 36_hotfix:. - Statsd logging in CromIAM [link](https://github.com/broadinstitute/cromwell/pull/4293); - Call cache copy fail blacklisting [link](https://github.com/broadinstitute/cromwell/pull/4359); - Forbidden failures are recognized/treated as IO Failures [link](https://github.com/broadinstitute/cromwell/pull/4376); - Auto-sizing boot disk in PAPI v2 [link](https://github.com/broadinstitute/cromwell/pull/4472); - Allow for a 'name-for-call-caching-purposes' to override the backend name [link](https://github.com/broadinstitute/cromwell/pull/4490); - User-service-account auth for Pipelines API v2 [link](https://github.com/broadinstitute/cromwell/pull/4566); - Add the Token Queue info to Cromwell log [link](https://github.com/broadinstitute/cromwell/pull/4567); - Labels query performance update [link](https://github.com/broadinstitute/cromwell/pull/4610); - Workflow validation for labels PATCH goes to summary not metadata [link](https://github.com/broadinstitute/cromwell/pull/4617); - CI Updates [link](https://github.com/broadinstitute/cromwell/commit/1a739fc7aabb1c557d96b57944c50ddc2006236a),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4675:290,failure,failures,290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4675,2,"['Failure', 'failure']","['Failures', 'failures']"
Availability,"This PR documents how to use the singularity cache in a way that every image only gets pulled once. It also documents why the `--containall` flag should be used with singularity at all times. Pinging @illusional @TMiguelT and @vsoch as heavy singularity promotors. What do you think of this?. EDIT: I cancelled the tests, as this is documentation and should not influence the Cromwell code. EDIT2: Fixes #5063",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5515:192,Ping,Pinging,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5515,1,['Ping'],['Pinging']
Availability,"This PR fixes [this](https://github.com/broadinstitute/cromwell/issues/4086) bug. It now handles the cases when empty input file or invalid json (but valid yaml) inputs is passed to Cromwell, and returns better error messages for them. Closes #4086",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4375:211,error,error,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4375,1,['error'],['error']
Availability,"This PR includes the majority of the work required to implement the [Struct Literal Syntax of WDL 1.1 ](https://github.com/openwdl/wdl/blob/main/versions/1.1/SPEC.md#struct-literals). At a high level, we can now parse the new syntax, and turn that into a value that can be validated & used. This PR _does not_ include the strict type checking defined in the spec. Since that will require some small tweaks to the linking step (and for the sake of the reviewers), I figure it's better to do that in a subsequent PR. In practice, this means that errors aren't thrown in certain situations where you would expect them to be thrown. Examples:. ```; struct MyStructType {; Int myIntMember; String? myStringMember; }. // the new syntax; MyStructType a = MyStructType{myIntMember: 4, myStringMember: ""Hi!""}. // still works just like any old struct value; MyStructType b = a. // note that omission of optional members is allowed by the spec; MyStructType c = MyStructType{myIntMember: 3}. // Literals are values; Int someInt = (MyStructType{myIntMember: 5, myStringType: ""Bye!""}).myIntMember. // As of this PR, this will not throw an error (but it technically should); Boolean illegalBool = (MyStructType{myIntMember: 5, myStringType: ""Bye!"", iMadeUpThisBool: false}).iMadeUpThisBool; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7391:544,error,errors,544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7391,2,['error'],"['error', 'errors']"
Availability,"This PR is a continuation of PR https://github.com/broadinstitute/cromwell/pull/4938. The idea is to propagate line numbers to the WOM structures, so you could report an error to the user with correct source locations. . In dxWDL, I need this also to recover the original ordering of the source code. The WOM structure is a partially sorted graph. For example, in a program like: . ```wdl; workflow foo {; call A; call B; }; ```. you don't know what came first, `A` or `B`, because they are unordered.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493627722:170,error,error,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-493627722,2,"['error', 'recover']","['error', 'recover']"
Availability,"This PR is a lot less scary than it looks. Most of the changed lines spring from two changes (and the changes are actually make everything a lot simpler!):. - `TaskDefinition` is split into two: `CallableTaskDefinition` which can be called, and `ExecutableTaskDefinition` which can be executed.; - `Callable` now doesn't have a `graph: ErrorOr[Graph]`. Instead the new, more specific `ExecutableCallable` has a `graph: Graph` method.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2793:336,Error,ErrorOr,336,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2793,1,['Error'],['ErrorOr']
Availability,This PR replaces the Java APIs with `gsutil` commands to download a GCS file during DRS localization. Closes https://broadworkbench.atlassian.net/browse/WA-168,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5552:57,down,download,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5552,1,['down'],['download']
Availability,"This PR updates the fetchWorkflowsToStart method to first find a hog group with lowest actively running workflows and then fetch workflows to start from that hog group. . Note to reviewers: I eventually had to separate the big select query into 2 parts - first find the hog group with lowest actively running workflows and then fetch workflows for that hog group because the outer select query had `forUpdate` and inner select query with `groupBy` clause which is not allowed in Postgres DB and throws the below error.; ```; cromwell-system-akka.dispatchers.engine-dispatcher-27 ERROR - Error trying to fetch new workflows; org.postgresql.util.PSQLException: ERROR: FOR UPDATE is not allowed with GROUP BY clause; ```. SQL generated from Slick:; Query to find hog group with lowest count of actively running workflows; ```; select ; x2.x3 ; from ; (; select ; `HOG_GROUP` as x3, ; count(1) as x4 ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; not (`WORKFLOW_STATE` = 'On Hold'); ) ; and (; not (; `HOG_GROUP` in ('Zardoz'); ); ) ; group by ; `HOG_GROUP` ; order by ; count(1); ) x2, ; (; select ; `HOG_GROUP` as x5, ; count(1) as x6 ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; (; (`HEARTBEAT_TIMESTAMP` is null) ; or (; `HEARTBEAT_TIMESTAMP` < {ts '2022-02-16 10:26:31.39' }; ); ) ; and (; not (`WORKFLOW_STATE` = 'On Hold'); ); ) ; and (; not (; `HOG_GROUP` in ('Zardoz'); ); ) ; group by ; `HOG_GROUP` ; order by ; count(1); ) x7 ; where ; x2.x3 = x7.x5 ; order by ; (x2.x4 - x7.x6), ; x2.x3 ; limit ; 1; ```. Query to select startable workflows for above hog group; ```; select ; `WORKFLOW_EXECUTION_UUID`, ; `WORKFLOW_DEFINITION`, ; `WORKFLOW_URL`, ; `WORKFLOW_ROOT`, ; `WORKFLOW_TYPE`, ; `WORKFLOW_TYPE_VERSION`, ; `WORKFLOW_INPUTS`, ; `WORKFLOW_OPTIONS`, ; `WORKFLOW_STATE`, ; `SUBMISSION_TIME`, ; `IMPORTS_ZIP`, ; `CUSTOM_LABELS`, ; `CROMWELL_ID`, ; `HEARTBEAT_TIMESTAMP`, ; `HOG_GROUP`, ; `WORKFLOW_STORE_ENTRY_ID` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; (; (`HEARTBEAT_TIMESTAMP` is null)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6680:512,error,error,512,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6680,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,"This WDL validates and even executes ""successfully"" but it shouldn't:. ```; task hello {; command {; #nothing; }; output {; String out = ""out""; }; }. task bye {; String instring = ""bye""; command {; echo ${instring}; }; output {; String out = read_string(stdout()); }; }. workflow w {; call hello; call bye { input: instring = hello }; }; ```. `call bye { input: instring = hello }` is invalid",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2394:198,echo,echo,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2394,1,['echo'],['echo']
Availability,"This WDL:. ```; task nofiles {; command {; echo ignored; }. output {; Array[File] nofiles = glob(""*.no.such.files""); }. runtime {; docker: ""ubuntu:latest""; }; }. workflow w {; call nofiles. output {; Array[File] out = nofiles.nofiles; }; }; ```. Causes this error:. ```; 2017-01-23 15:14:06,797 cromwell-system-akka.dispatchers.backend-dispatcher-36 INFO - JesAsyncBackendJobExecutionActor [UUID(7e4afc46)w.nofiles:NA:1]: JesAsyncBackendJobExecutionActor [UUID(7e4afc46):w.nofiles:NA:1] Status change from Running to Failed; 2017-01-23 15:14:07,862 cromwell-system-akka.dispatchers.engine-dispatcher-8 ERROR - WorkflowManagerActor Workflow 7e4afc46-a1ff-424f-aecf-4ce742ed5fa8 failed (during ExecutingWorkflowState): It appears that some of the expected output files for task w.nofiles:NA:1 did not exist when the command exited.; A few things to try; 1) Check that the output section in your WDL is correct. Remember that all output files declared in a task must exist when the command exits.; 2) Check that the return code is available and is valid with respect to your command expected exit code; 3) Look into the stderr (gs://miguel-cromwell-dev/w/7e4afc46-a1ff-424f-aecf-4ce742ed5fa8/call-nofiles/nofiles-stderr.log) file for evidence that some of the output files the command is expected to create were not created.; Failed to delocalize files: failed to copy the following files: ""/mnt/local-disk/glob-ae4ebb9050f92748c9c41ab4aa60afc9/* -> gs://miguel-cromwell-dev/w/7e4afc46-a1ff-424f-aecf-4ce742ed5fa8/call-nofiles/glob-ae4ebb9050f92748c9c41ab4aa60afc9/ (cp failed: gsutil -q -m cp -L /var/log/google-genomics/out.log /mnt/local-disk/glob-ae4ebb9050f92748c9c41ab4aa60afc9/* gs://miguel-cromwell-dev/w/7e4afc46-a1ff-424f-aecf-4ce742ed5fa8/call-nofiles/glob-ae4ebb9050f92748c9c41ab4aa60afc9/, command failed: CommandException: No URLs matched: /mnt/local-disk/glob-ae4ebb9050f92748c9c41ab4aa60afc9/*\nCommandException: 1 file/object could not be transferred.\n); Check the content of stderr for",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1896:43,echo,echo,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1896,3,"['ERROR', 'echo', 'error']","['ERROR', 'echo', 'error']"
Availability,This [PR](https://github.com/broadinstitute/cromwell/pull/3813) added _some_ retries around (de)localization. Definitely might not be enough but it's worth looking if it helped with those failures.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742#issuecomment-401892591:188,failure,failures,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742#issuecomment-401892591,1,['failure'],['failures']
Availability,This affects to HtCondor backend. Given:; Following WDL task. ``` ; task Example {; command {; echo foo;; mkdir testFolder; }; } ; ```. When:; Runs workflow using HtCondor backend. Then:; It creates testFolder in docker host machine. Expected result:; Whole command executed within the container.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1719:95,echo,echo,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1719,1,['echo'],['echo']
Availability,"This allows this workflow to run:. ```; task t {; Array[String] s; command {echo ${sep=',' s}}; output {String o = read_string(stdout())}; }. workflow w {; Array[String] x = read_lines(""gs://sfrazer-dev/array.txt""); call t {input: s=x}; }. ```. So this is the minimum implementation I could come up with... looking for design feedback too, this could be simple or become full-blown pluggable filesystems",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/824:76,echo,echo,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/824,1,['echo'],['echo']
Availability,"This appears to be the way it already works. ``` scala; private def preempted(errorCode: Int, errorMessage: Option[String], jobDescriptor: BackendCallJobDescriptor, logger: WorkflowLogger): Boolean = {; def isPreemptionCode(code: Int) = code == 13 || code == 14. try {; errorCode == 10 && errorMessage.isDefined && isPreemptionCode(extractErrorCodeFromErrorMessage(errorMessage.get)) && jobDescriptor.preemptible; } catch {; case _: NumberFormatException | _: StringIndexOutOfBoundsException =>; logger.warn(s""Unable to parse JES error code from error message: ${errorMessage.get}, assuming this was not a preempted VM.""); false; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215214007:78,error,errorCode,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/744#issuecomment-215214007,8,['error'],"['error', 'errorCode', 'errorMessage']"
Availability,"This auth is used to create pipelines and manipulate auth JSONs.; auth = ""application-default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""us-west1"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default"". # Google project which will be billed for the requests; project = ""gred-cumulus-sb-01-991a49c4"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:3488,down,downloading,3488,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['down'],['downloading']
Availability,"This bug was caused by ""userland"" (WDL) code throwing an exception where we did not expect it, causing issues for the ""kernel"" (Cromwell). Now we encapsulate possible exceptions in an `ErrorOr` so they can be safely evaluated in the `case Left(f) =>`. As a stylistic point, I changed `WomMap()` to `WomMap.apply()` to make it more obvious that we're really doing a lot of custom stuff and it's not just a regular old constructor. As a bonus, the new code detects all previous stuck-aborting workflows and fails them. After:; ```; INFO - WorkflowManagerActor: Workflow 1432d67e-3e95-40c8-acbd-d42f75040f1b failed (during ExecutingWorkflowState): cromwell.engine.workflow.lifecycle.execution.WdlRuntimeException: Failed to evaluate 'example2' (reason 1 of 1): Evaluating { ""second"": test, ""lowerLayer"": example1 } failed: Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; INFO - WorkflowManagerActor: Workflow actor for 1432d67e-3e95-40c8-acbd-d42f75040f1b completed with status 'Failed'. The workflow will be removed from the workflow store.; ```; ```; {; 	""status"": ""Failed"",; 	""id"": ""1432d67e-3e95-40c8-acbd-d42f75040f1b""; }; ```. Before:; ```; ERROR - Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; java.lang.UnsupportedOperationException: Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types in map values: [WomString(Hello World), WomObject(Map(first -> WomString(Hello World), number -> WomInteger(2)),WomCompositeType(Map(first -> WomStringType, number -> WomIntegerType),Some(firstLayer)))]; 	at wom.values.WomMap.<init>(Wo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385:185,Error,ErrorOr,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385,1,['Error'],['ErrorOr']
Availability,This certainly hasn't improved test reliability and I have more pressing things to look at just now,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5161#issuecomment-529006581:36,reliab,reliability,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5161#issuecomment-529006581,1,['reliab'],['reliability']
Availability,"This change attempts to fix the deadlock between [starting workflows](https://github.com/broadinstitute/cromwell/blob/develop/database/sql/src/main/scala/cromwell/database/slick/WorkflowStoreSlickDatabase.scala#L65) and [writing heartbeats](https://github.com/broadinstitute/cromwell/blob/develop/database/sql/src/main/scala/cromwell/database/slick/WorkflowStoreSlickDatabase.scala#L75) by removing transaction semantics from the heartbeat write query. This way, the second query no longer locks multiple rows at once. I am using Slick's [`withPinnedSession`](http://slick.lightbend.com/doc/3.2.0/dbio.html#transactions-and-pinned-sessions) to preserve the efficiency gain of having all the queries in a single session. The MySQL query log shows that `transactionally` and `withPinnedSession` both cause queries to execute in a single session, as evidenced by the setting of session variable `autocommit`:. - `database.run(action.transactionally)`:; ```; Query SET autocommit=0; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c8482924-ef9e-4b3f-930c-ab5f023eeb78'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'e79a1ee7-dd21-4a55-b52d-03f50031b75e'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.19",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4022:229,heartbeat,heartbeats,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022,2,['heartbeat'],"['heartbeat', 'heartbeats']"
Availability,"This change enables blob filesystems to be opened that do not belong to the workspace, including public containers, and containers the requesting user has access to via WSM. This involves frequent 'refreshing' of the stored open filesystem in the underlying NIO implementation. To minimize the number of redundant requests to WSM, the SAS token for each open filesystem is stored and checked for expiration before attempting to reopen a previously open filesystem.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7140:304,redundant,redundant,304,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7140,1,['redundant'],['redundant']
Availability,"This change makes workflows fail slow instead of fail fast. I'm uncomfortable committing this as-is because I don't fully understand when startRunnableCalls is used - and is it possible to miss the ""are we done (including failures)"" check. This _does_ work when a single call fails as part of a longer workflow. No other cases have been tested. Can anyone give me a quick tour of the control flow in WorkflowActor to make sure I haven't missed anything?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/436:222,failure,failures,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/436,1,['failure'],['failures']
Availability,This changes makes these error messages visible in the separate workflow log file we make available to users.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7220:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7220,2,"['avail', 'error']","['available', 'error']"
Availability,"This compiler flags allows the compiler to take types of kind `F[_,_]` and infers `F[_]` by parameterizing the right most argument and fixing the other ones. This allows us to omit type args when traversing to `Either` and `ErrorOr`, because scala can now see them as shape `F[_]` instead of ""you gave me a type w/ 2 type args and I was expecting one"". **I did a find/replace on all our traverses, this may be controversial**, chime in if you disagree. [Longer explanation](https://gist.github.com/djspiewak/7a81a395c461fd3a09a6941d4cd040f2)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3580:224,Error,ErrorOr,224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3580,1,['Error'],['ErrorOr']
Availability,"This currently only throws an exception when it tries to call the task. Instead, it should not be able to generate a validated WOM graph in the first place:; ```wdl; workflow oops {; call oopsie; }. task oopsie {; String str; command { echo ${str} }; runtime { docker: docker_image }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3326:236,echo,echo,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3326,1,['echo'],['echo']
Availability,"This does appear to work for 3 step but I'd like to add metadata assertions to the existing Centaur test assuming the PR doesn't get gulled out of existence. TODO. - [x] Set parent workflow into subworkflows; - [x] Fix input path localization; - [x] Make ""please turn on Docker"" an explicit thing in `core`; - [x] Unbreak all the conformance tests that are broken on this branch; - [x] Centaur assertions that the expected Docker images are being used; - [ ] Turn on conformance tests in backends that require Docker; - [ ] Fix globs on JES; - [ ] Fix inputs on JES; - [ ] Return validation failures to the caller",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3040:591,failure,failures,591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3040,1,['failure'],['failures']
Availability,This does assert at least that the value is valid Json. We may consider going to a coproduct in the future but this will get us past parsing failure in the conformance tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2987:141,failure,failure,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2987,1,['failure'],['failure']
Availability,"This enables tests on a lot of Slick code that's not actually used yet in the New Worlde (nothing writes to the core engine tables since only Recover needs that and Recover hasn't been implemented). So these changes are valuable iff the New Worlde ultimately uses an engine Slick API that looks a lot like that in the Olde Worlde. My guess is that will end up being true, but at this point that's only a guess. Some things here will certainly be nixed (ExecutionEvents) or are likely to be heavily modified (anything caching-related).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/942:142,Recover,Recover,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/942,2,['Recover'],['Recover']
Availability,"This fails at runtime but we shouldn't even allow it to validate:. ```wdl; version 1.0; struct Foo {; Int foo_int; }. [...]. task bar {; input {; Foo f; }; command {; echo ~{f} # Bad interpolation, should be ~{f.foo_int}; }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3917:167,echo,echo,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3917,1,['echo'],['echo']
Availability,"This failure was spotted again, reopening",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4223#issuecomment-430263735:5,failure,failure,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4223#issuecomment-430263735,1,['failure'],['failure']
Availability,"This fixes the docker deadlock by doing 2 things:. - makes sure that all `HttpResponse`s are either consumed or discarded to avoid https://github.com/akka/akka/issues/19538; - removes the decoupling between the `tokenFlow` and the `manifestFlow`; This is believed to be the main cause of the problem. Decoupling the token request from the manifest request can create a situation where all the connections are being used for token requests, and no connection is available to make a manifest request which makes the stream freeze.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2287:461,avail,available,461,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2287,1,['avail'],['available']
Availability,This fixes the response error codes. It will now return `404 Not Found` for an unrecognized workflow ID and `400 Bad Request` for a malformed or invalid workflow ID.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3918:24,error,error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3918,1,['error'],['error']
Availability,"This happened after we turned off call caching to work around DSDEEPB-2938 for now and this error popped up in what seems to be post success processing of the task. Logs are in the comment. This was run on gotc-prod against JES-staging. ``` scala; 34108:2016-03-10 22:53:11,258 cromwell-system-akka.actor.default-dispatcher-8 INFO - JES Run [UUID(7721686a):StripBamExtension:8]: Status change from - to Initializing; 50265:2016-03-10 22:55:07,880 cromwell-system-akka.actor.default-dispatcher-27 INFO - JES Run [UUID(7721686a):StripBamExtension:8]: Status change from Initializing to Running; 64201:2016-03-10 22:57:00,293 cromwell-system-akka.actor.default-dispatcher-27 INFO - JES Run [UUID(7721686a):StripBamExtension:8]: Status change from Running to Success. 2016-03-10 22:57:05,450 cromwell-system-akka.actor.default-dispatcher-27 ERROR - CallActor [UUID(7721686a):StripBamExtension:8]: Failing call: 500 Internal Server Error; Internal Error; cromwell.util.AggregatedException: 500 Internal Server Error; Internal Error; at cromwell.util.TryUtil$.sequenceIterable(TryUtil.scala:112) ~[cromwell.jar:0.19]; at cromwell.util.TryUtil$.sequenceMap(TryUtil.scala:124) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend.postProcess(JesBackend.scala:623) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:662) ~[cromwell.jar:0.19]; at cromwell.engine.backend.jes.JesBackend$$anonfun$executionResult$1.apply(JesBackend.scala:657) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable.run_aroundBody0(Future.scala:24) ~[cromwell.jar:0.19]; at scala.concurrent.impl.Future$PromiseCompletingRunnable$AjcClosure1.run(Future.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureIn",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/577:92,error,error,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/577,4,"['ERROR', 'Error', 'error']","['ERROR', 'Error', 'error']"
Availability,This has the potential to reduce QPS errors enormously.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229210088:37,error,errors,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229210088,1,['error'],['errors']
Availability,"This implements the specification for unspecified task inputs https://github.com/openwdl/wdl/pull/359 . . It has been backported to wdl 1.0 because it enables useful functionality (I.e. allowing nested optional inputs to be defined if this is specified in the meta section of the workflow). . It may break some workflows that have their required inputs nested deeply inside subworkflows and tasks, but Broad's own WDL linter for IntelliJ has been warning against this behavior for a while now. . To me it always was a bit weird that Cromwell allowed `""my_workflow.my_subworkflow.my_required_input"": 5` in the input because it was required, but not `""my_workflow.my_subworkflow.my_optional_input"": 5` because it was optional. Now both are not allowed by default. And the optional nested input is only available after explicitly enabling it in the top-level workflow.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5523:800,avail,available,800,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5523,1,['avail'],['available']
Availability,This is a [Not Acceptable](http://stackoverflow.com/questions/14251851/what-is-406-not-acceptable-response-in-http) error. The content types that Green said it would accept in the response were not compatible with the type of response Cromwell produced. It may be that Cromwell experienced a real error (which was hopefully logged) and returned a text/plain error response which I don't think you'd accept.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216659028:116,error,error,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216659028,3,['error'],['error']
Availability,"This is a bug report that seems to be caused by the `None` type introduced in version 1.1 not being handled properly when assigned to fields in custom structs. Consider the following minimal example:; ```; version development. struct OptionalFiles {; 	File required; 	File? optional; }. workflow TestNone {; 	input {; 		Array[File] letters = [""a.txt"", ""b.txt"", ""c.txt""]; 	}. 	scatter (letter in letters) {; 		OptionalFiles opt = {""required"": letter, ""optional"": None}; 	}. 	output {; 		Array[OptionalFiles] out = opt; 	}; }; ```; This is run in a directory containing empty files `a.txt`, `b.txt`, and `c.txt`. Assigning `None` to the `optional` field in type `OptionalFiles` seems to cause the following unhandled stacktrace:. ```; [2023-11-07 14:51:14,22] [info] MaterializeWorkflowDescriptorActor [4e522458]: Call-to-Backend assignments:; [2023-11-07 14:51:17,38] [error] Cannot construct WomMapType(WomStringType,WomOptionalType(WomAnyType)) with mixed types in map values: [WomOptionalValue(WomSingleFileType,Some(WomSingleFile(c.txt))), WomOptionalValue(WomAnyType,None)]; java.lang.UnsupportedOperationException: Cannot construct WomMapType(WomStringType,WomOptionalType(WomAnyType)) with mixed types in map values: [WomOptionalValue(WomSingleFileType,Some(WomSingleFile(c.txt))), WomOptionalValue(WomAnyType,None)]; 	at wom.values.WomMap.<init>(WomMap.scala:65); 	at wom.values.WomMap$.apply(WomMap.scala:50); 	at wom.values.WomMap$.coerceMap(WomMap.scala:30); 	at wom.values.WomMap$.apply(WomMap.scala:46); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.$anonfun$evaluateValue$12(LiteralEvaluators.scala:89); 	at cats.data.Validated.map(Validated.scala:559); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.evaluateValue(LiteralEvaluators.scala:86); 	at wdl.transforms.base.linking.expression.values.LiteralEvaluators$$anon$5.evaluateValue(LiteralEvaluators.scala:74); 	at wdl.model.draft3.graph.expression.ValueEvaluator$Ops.evaluateV",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7249:868,error,error,868,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7249,1,['error'],['error']
Availability,"This is a general form of the workflow that I had this issue with:. ```; workflow wf {; Boolean condition; Array[String] files. scatter (row in files) {; if(condition) {; call A; }; if(!condition) {; call B; }; }; }; ```. The tasks themselves finish, however I get the following error:. `Could not construct array of type WdlMaybeEmptyArrayType(WdlOptionalType(WdlFileType)) with this value: List(WdlSingleFile(/dsde/working/asmirnov/evaluations/CovariateCorrection/scripts/test_wdl/cromwell-executions/cnv_coverage_workflow/c71f4899-6031-4a21-b9b4-1dafd659cd05/call-CalculateTargetCoverage/shard-0/execution/TCGA-02-2483-10A-01D-1494-08.coverage.tsv), WdlSingleFile(/dsde/working/asmirnov/evaluations/CovariateCorrection/scripts/test_wdl/cromwell-executions/cnv_coverage_workflow/c71f4899-6031-4a21-b9b4-1dafd659cd05/call-CalculateTargetCoverage/shard-1/execution/TCGA-02-2485-10A-01D-1494-08.coverage.tsv))`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1936:279,error,error,279,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1936,1,['error'],['error']
Availability,"This is a port of the original pull [request](https://github.com/broadinstitute/wdl4s/pull/256) to the Cromwell tree. I hope this was done correctly, while addressing the review comments. Thanks for pointing out how to use `Try` and `should equal` in the testing framework. There are two outstanding comments, which I am answering here, since I am assuming you want to dismantle the old wdl4s git repository. . 1) TypeEvaluator.scala; *Q: Isn't the caller perhaps trying to ascertain if the expression actually can be coerced to the declared type?* ; A: That could be added as an assert. . 2) WdlSubworkflowWomSpec.scala; *Q: The magic conversion of a single Thing to an Array[Thing] was a feature someone explicitly asked for way back when. I never liked this feature but I'm sure there's WDL out there that expects this to work.*; A: I can see why a user might want that. A side effect is that Array[T] can automatically be coerced to Array[Array[T]]. I ran into a case where Array[String] was coerced into Array[Array[File]]. For example, the workflow below executes under Cromwell v0.29, involving the coercion of `Array[Int]` to `Array[Array[Int?]]`.; ; ```; # Trying out file copy operations; task Num {; Array[Array[Int?]] numbers; command {; }; output {; Array[Array[Int?]] result = numbers; }; }. workflow w {; Array[Int] primes = [2, 3, 5, 7, 11]; call Num { input: numbers = primes }; output {; Num.result; }; }; ```. This raises two issues in my mind: ; 1) There are many ways to convert a single dimensional array into a two dimensional array. Which one does the WDL language specify? ; 2) A user can mistakenly pass an incorrectly typed argument to a task without getting a compiler error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2807:1697,error,error,1697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2807,1,['error'],['error']
Availability,"This is a proposal on how to solve the ""insufficient data written"" error that is due to the batch request doing PAPI job creation / polling being too large. The actual limit number is unknown yet (ticket is open with google).; Depending on their answer the solution might look different but this would be one way to fix it.; I tested it with @ruchim's WDL that reproduces this problem and didn't see it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2554:67,error,error,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2554,1,['error'],['error']
Availability,"This is a regression-- we should have a centaur test case that goes through this scenario, so this failure is surprising and worth investigating.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4772#issuecomment-476677441:99,failure,failure,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772#issuecomment-476677441,1,['failure'],['failure']
Availability,This is a small batch to fix #4857 by implementing recoverAsync in AwsBatchAsyncBackendJobExecutionActor. I have tested this in our environment and it appears to work.; Implementation is based on pattern in other AsyncBackendJobExecutionActor classes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216:51,recover,recoverAsync,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216,1,['recover'],['recoverAsync']
Availability,"This is against 29 but could also be an issue with the planned structure for 30. For comparison, a somewhat similar issue with subworkflow declaration evaluation [in 29](https://github.com/broadinstitute/wdl4s/pull/257) will inform the way subworkflow declaration evaluation should work in 30. An uninitialized optional declaration in a subworkflow kills the SubworkflowExecutionActor whether it is referenced or not. The workflow [here](https://github.com/broadinstitute/centaur/pull/242/files) is able to run against 29 hotfix with the patches in the [wdl4s](https://github.com/broadinstitute/wdl4s/pull/257) and associated cromwell PRs. But if the uninitialized optional declaration on [this line](https://github.com/broadinstitute/centaur/pull/242/files#diff-cc04c14d68a6a1a6d8d8366fc0c2f88cR48) is uncommented, the workflow fails. (Deliberately not quoting since lines don't wrap and anyway the thumbs downs are apropos). 2017-11-14 18:00:05,062 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2017-11-14 18:00:05,093 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - MaterializeWorkflowDescriptorActor [UUID(4b725606)]: Call-to-Backend assignments: decls.sub_decls.second_task -> Local, decls.sub_decls.first_task -> Local; 2017-11-14 18:00:06,129 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - WorkflowExecutionActor-4b725606-6d2a-4cf2-b23b-e5971f52b7dd [UUID(4b725606)]: Starting calls: SubWorkflow-sudecls:-1:1; 2017-11-14 18:00:06,130 cromwell-system-akka.actor.default-dispatcher-50 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreRegisterSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#-388497585] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-4b725606-6d2a-4cf2-b23b-e5971f52b7dd/WorkflowExecutionActor-4b725606-6d2a-4cf2-b23b-e5971f52b7dd/SubWorkflowExecutionActor-SubWorkflow-sudecls",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2902:907,down,downs,907,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2902,1,['down'],['downs']
Availability,"This is also something we would want for our cluster. Users are not allowed to run docker containers on our HPC (that would give them root access :scream:). So we use cromwell on the command line instead. We modified our cromwell configuration to run with singularity containers instead of docker containers. Setting up a MySQL server somewhere is not easy for the average user. Furthermore we used to set up a MySQL server for all users on the cluster, but that meant they had to share a database user and password (they were all using the same configuration). This caused a lot of issues. . Mostly cromwell is run project based. So the call-caching is only interesting for that particular project. Also using a file-based database will automatically ensure that only people with rights to the share the project is on will have access. This is also of importance in a shared cluster environment. At LUMC we currently implement this by using the HSQLDB in-memory database with a persistence file. This has some disadvantages:; 1. Cromwell needs more memory compared to using a MySQL server; 2. The HSQLDB persistence files are huge, badly compressed (when compression is used). 3 GB is normal. Tarring and zipping will get this down to under <50 mb... ; 3. It is slower than using a MySQL server. I think that SQLite will solve problem 1 and 2. (3 is inherent to using a file-based DB). . Having a database is better than to have nothing at all, which is why many users are pining for SQLite. When looking into it I found the option for the persistence file. Although SQLite will be much better, this does not require any extra effort from the Cromwell developers and can already help out a lot of users. I will document how we did this in the Cromwell documentation so everyone can use this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002:1228,down,down,1228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564437002,1,['down'],['down']
Availability,"This is an attempt to make the separation between (JES / Local / SGE) and File systems (GCS, SharedFilesystem) more obvious. It enables implementation of more Backend / File systems combinations.; For example this wdl runs, on a Local cromwell instance :; Wdl: . ```; task hello {; String addressee; command {; echo ""Hello ${addressee}!""; }; output {; String fileoutput = read_string(stdout()); String salutation = read_string(""gs://tjeandet/wdl_input_public.txt""); }; }. workflow hello {; File infile; String instring = read_string(infile); call hello {input: addressee = instring}; }; ```. Inputs:. ```; {; ""hello.infile"": ""gs://tjeandet/wdl_input_public.txt""; }; ```. It also works with user-defined authentication for GCS (refresh token).; More tests / cleaning is needed before it's reviewable but I wanted to put it out there before Thanksgiving.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/305:311,echo,echo,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/305,1,['echo'],['echo']
Availability,"This is an exceptionally annoying error, any more thoughts on how to potentially fix this? Should we go through the pipelines team?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-347860094:34,error,error,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-347860094,1,['error'],['error']
Availability,This is an issue with the error reporting. I have not checked (and I do not remember how to replicate).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1963#issuecomment-304968047:26,error,error,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1963#issuecomment-304968047,1,['error'],['error']
Availability,"This is being discussed with faces. First with Hussein and his PO to see; if this is a priority for FireCloud. ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org. On Tue, Oct 25, 2016 at 11:21 AM, Chris Llanwarne notifications@github.com; wrote:. > Pinging @kcibul https://github.com/kcibul for priority; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-256067052,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ABW4g1wBXEQL8kkTuRTeebKXBoJ8xfOpks5q3h56gaJpZM4KfN-O; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-256125203:336,Ping,Pinging,336,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1617#issuecomment-256125203,1,['Ping'],['Pinging']
Availability,"This is causing a problem for me trying to write a workflow. I want to use [string interpolation](https://github.com/openwdl/wdl/blob/main/versions/1.1/SPEC.md#expression-placeholders-and-string-interpolation), which is a WDL 1.1 feature. It is not specified as being available in WDL 1.0 outside of commands, but Cromwell happens to support it in all strings in 1.0 (and maybe also draft-2?). If I put a `version 1.1` statement in my workflow, Cromwell won't parse it, because it knows it doesn't support 1.1. But if I put a `version 1.0` statement, my workflow isn't actually compliant with the spec, because 1.0 doesn't say that this feature is available. So I have a workflow that both Cromwell and any 1.1-compliant runner can run, except I can't write a correct version statement for it, or Cromwell will reject it. One solution might be to tell Cromwell that it does support version 1.1, at least partially, instead of rejecting all 1.1 workflows.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1499403575:268,avail,available,268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1499403575,2,['avail'],['available']
Availability,This is currently plan A for addressing the 10 TB database limit. Plan B would definitely have non-trivial downtime.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4736#issuecomment-472141525:107,downtime,downtime,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4736#issuecomment-472141525,1,['downtime'],['downtime']
Availability,This is failing travis but my guess is that that's not my fault?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5224#issuecomment-542328679:58,fault,fault,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5224#issuecomment-542328679,1,['fault'],['fault']
Availability,"This is great, thanks @delocalizer !. The downside of you doing those followups is that now I don't have the open PR staring me in the face reminding me I have to do something ;) I'll try to put in some content re Firecloud in the near future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259814290:42,down,downside,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1653#issuecomment-259814290,1,['down'],['downside']
Availability,"This is great, thanks for making the documentation awesome!; Only thing I could improve is to explicitly specify which base the memory_*b conversion uses, (I think both are available? MiB and MB). And to specify the rounding behaviour of 'Int memory_gb', probably ceil.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4212#issuecomment-468100117:173,avail,available,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4212#issuecomment-468100117,1,['avail'],['available']
Availability,This is like #957 but for Failure Events. Failure Events are published as metadata but no longer rate their own separate table.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/964:26,Failure,Failure,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/964,2,['Failure'],['Failure']
Availability,"This is likely a bogus failure, I went ahead and restarted the job",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4697#issuecomment-470680968:23,failure,failure,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4697#issuecomment-470680968,1,['failure'],['failure']
Availability,"This is more of an epic and not a right now thing, but I wanted to capture the thought for future consideration. In the author's not-so-humble opinion we log way too much stuff, to the point that logs aren't particularly useful unless you know exactly what you're looking for. We also know that Cromwell's logs are blowing apart loggly and stuff like that. We also manage to log some stuff (e.g. stacktraces) multiple times for one incident. We also have many issues where logs would be super helpful yet all we see is something like ""an error occurred"" without any context. We should sit down as a group and with focus groups of downstream clients (e.g. firecloud, gotc) and go through what we're logging and look for ways to both massively debulk our logs as well as making sure that we're logging the most useful stuff.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/778:538,error,error,538,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/778,3,"['down', 'error']","['down', 'downstream', 'error']"
Availability,"This is not working `${true='--enable-foo', false='--disable-foo' Boolean yes_or_no}` despite the documentation stating it does. Whether the statement is in the workflow or task context doesn't matter. Number of commas in the statement didn't help. Running on cromwell-40.jar. workflow yes_or_no {; 	 Boolean true_or_false. 	 String var = ${true=""Foo"" false='Bar' Boolean true_or_false}; 	; 	 call example_demo {; 		 message = var; 	 }; }. task example_demo {; 	; 	 String message. 	 command {; 		 echo ${message}; 	 }; }. Input is:. {; 	 ""yes_or_no.true_or_false"": true; }. Error thrown:. [2019-05-10 19:22:29,21] [error] WorkflowManagerActor Workflow 7f2796cb-ef24-470c-8663-5d497056fd44 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Unrecognized token on line 4, column 15:. 	String var = ${true=""Foo"" false='Bar' Boolean true_or_false}; ^; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:185); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:180); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.processEvent(FSM.scala:684); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:136); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.proces",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4963:498,echo,echo,498,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4963,3,"['Error', 'echo', 'error']","['Error', 'echo', 'error']"
Availability,"This is old and can be closed. @Thib already explained how to do examine; log files. On Wed, Aug 31, 2016 at 2:20 PM, Thib notifications@github.com wrote:. > Failed to delocalize files Looks like JES couldn't delocalize a file that; > was expected as an output.; > The task probably failed to produce that output hence the failure.; > I thought the logs were copied regardless in that case but apparently not..; > ; > —; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243854114,; > or mute the thread; > https://github.com/notifications/unsubscribe-auth/ACDXk-NAFrpk76r_5joMUbWTtGqkJvXAks5qlcXqgaJpZM4JwiCG; > . ## . Lee Lichtenstein; Broad Institute; 75 Ames Street, Room 7003EB; Cambridge, MA 02142; 617 714 8632",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243855172:323,failure,failure,323,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1357#issuecomment-243855172,1,['failure'],['failure']
Availability,"This is on PAPI side and has already been reported and acknowledged by Aaron. ```; pulling image: docker pull: running [""docker"" ""pull"" ""ubuntu@sha256:de774a3145f7ca4f0bd144c7d4ffb2931e06634f11529653b23eba85aef8e378""]: exit status 1 (standard error: ""error pulling image configuration: received unexpected HTTP status: 502 Bad Gateway...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4258:243,error,error,243,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4258,2,['error'],['error']
Availability,"This is pretty cool. The only thing I'd throw in is that for the run() function this to me is like the one I pointed to @mcovarr the other day. I think it's fine as-is but I'd ask for another pass through to see if there are places where breaking it down a bit makes sense. If not, no biggie.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/265#issuecomment-153541769:250,down,down,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/265#issuecomment-153541769,1,['down'],['down']
Availability,"This is pretty old and may have already been fixed. On Mar 9, 2017 09:48, ""Ruchi"" <notifications@github.com> wrote:. > @LeeTL1220 <https://github.com/LeeTL1220> I'm unable to reproduce the; > error you saw. Have you seen the same issue with more recent versions of; > Cromwell? What version were you using when you saw the initial error?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285371298>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXk782vulMdWbl5LYOzgueOZhAz_3bks5rkBEtgaJpZM4JmxQ5>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285409887:192,error,error,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1302#issuecomment-285409887,2,['error'],['error']
Availability,"This is ready for review, remaining test failures are being handled in other PRs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7130#issuecomment-1540843190:41,failure,failures,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7130#issuecomment-1540843190,1,['failure'],['failures']
Availability,"This is related (at least) to https://github.com/broadinstitute/cromwell/issues/2446. Examine the workflow `wf` below.; ```wdl; workflow wf {; String? who. call hello { input:; who = who; }; }. task hello {; String? who = ""world"". command {; echo ""Hello, ${who}""; }; output {; String result = read_string(stdout()); }; }; ```. When running `wf` with Cromwell v32 the result is `hello.result = ""Hello, world""`. However, since we are calling the task with an empty value, it should override `who`, and the the result should be (I think) `Hello,""`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3819:242,echo,echo,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3819,1,['echo'],['echo']
Availability,"This is sent out by the `EmptySubWorkflowStoreActor` but but not waited for by the `SubWorkflowExecutionActor` (which indeed ignores it if it ever receives one). This is awkward because it means we sometimes get ""Message not delivered"" error logs. We should either remove this message entirely or, if it's used by tests, we should only send one out if the receiver is actually waiting for it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2185:236,error,error,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2185,1,['error'],['error']
Availability,"This is something that would be quite useful. There's a lot of simple things that are just easier (and cheaper!) to run or debug locally, but doing so right now is a bit of a mess. Several times I have had Cromwell tasks [get sigkilled on local runs](https://github.com/aofarrel/analysis_pipeline_WDL/issues/5), or even choke up Docker to the point of being unable to enter containers both within (containerized tasks get stuck on WaitingForReturn code but will never anything within the task, even a simple echo) and outside of Cromwell (docker run, etc) without a full restart of the Docker system. I am not certain if limiting resources would solve my specfic probems, but at the very least it would be closer to the cloud experience without the associated cost. It also could be used to test out rough approximations of memory and disk requirements for things would normally be used on the cloud, again saving money.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021:508,echo,echo,508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803035021,1,['echo'],['echo']
Availability,"This is the error that I'm getting:. ```; 2019/10/03 19:00:01 Running user action: docker run -v /mnt/local-disk:/cromwell_root --entrypoint= hisplan/samtools@sha256:12d34a022f43e87e6c51f4be29705ccb70d2562a2046f3746f04cca88674a2e9 /bin/bash /cromwell_root/script; [main] unrecognized command '/bin/bash'; ```. Even though the container does have `/bin/bash`, this error occurs if the container image is built with `ENTRYPOINT`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-538183152:12,error,error,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2461#issuecomment-538183152,2,['error'],['error']
Availability,This is what a (simulated) failure looks like for the two areas edited by this PR:; https://travis-ci.org/broadinstitute/cromwell/builds/206678315,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2045#issuecomment-283404662:27,failure,failure,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2045#issuecomment-283404662,1,['failure'],['failure']
Availability,"This issue was prompted by the following thread on the forum: https://gatkforums.broadinstitute.org/wdl/discussion/10347/localization-via-hard-link-has-failed. By default, cromwell does not specify a user when running docker, which leads to two issues:. Firstly, it will run each tasks as whatever user the docker image specifies, typically 'root' as this is the default. This means that depending on how the docker image was build, a completely random user is suddenly the owner of your output files. For example, if the user inside the docker container has UID 1042, this could map to a completely unrelated user on the host system, who is suddenly the owner of your output files. Related to this issue is the fact that all output files have to be world-readable by default, or else the cromwell process will not have read access to the files it has just created (which are now owned by UID 1042). This is not desireable when cromwell is run on a system with many users, some of which should not have read access to eachothers data (for example when working with patient data). As a solution, I propose to change the default docker invocation to run the analysis as $EUID by default. This works even when the EUID is not mapped to a valid user within the docker image, and ensures that the cromwell user is the owner of all the files generated by docker. From man bash: ""EUID Expands to the effective user ID of the current user, initialized at shell startup. This variable is readonly."" , so it should be available on every system. This solution makes cromwell act more secure by default, and will also solve the issue of copying over data files as discussed on the forum and in #2620",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2658:1508,avail,available,1508,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2658,1,['avail'],['available']
Availability,This looks like a useful fix. Is there a development build of the JAR file available anywhere?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4437#issuecomment-456294327:75,avail,available,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4437#issuecomment-456294327,1,['avail'],['available']
Availability,This looks like the preemption error codes are different in PAPI v2. I don't think we expected this,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3732#issuecomment-395066256:31,error,error,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732#issuecomment-395066256,1,['error'],['error']
Availability,"This may be a known issue, in which case feel free to link this to that. If not... I ran a workflow on the DSDE-Methods server, which is running on P.API, and it failed when a file didn't exist. I was running a CNV pipeline to create a Panel of Normals, and a few of the bams (and their bai's) in the list did not exist. From @LeeTL1220: Cromwell can't localize a file because the file does not exist. As a result, instead of saying it, it does not create workflow root directory, doesn't make any usable log files.; The error message is just the name of the file, rather than to say that this file is missing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2774:521,error,error,521,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2774,1,['error'],['error']
Availability,"This may be a possible dupe. Below is a stack trace of a `java.nio.Path` operation that threw an IOException the first time, but would have likely succeeded if tried again. Ideally the read/write operation, wdl task, whole workflow, etc. should be retried at some later asynchronous time. In the particular case below, the JES initialization actor was attempting to write the authentication file when it received a common 503 error:. ```; 2017-01-23 05:37:49,143 cromwell-system-akka.dispatchers.engine-dispatcher-45 ERROR - WorkflowManagerActor Workflow b08cdbf3-08ea-4bfa-a612-c4452f120c84 failed (during InitializingWorkflowState): Failed to upload authentication file; java.io.IOException: Failed to upload authentication file; 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1.apply(JesInitializationActor.scala:61); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile$1.apply(JesInitializationActor.scala:58); 	at scala.Option.foreach(Option.scala:257); 	at cromwell.backend.impl.jes.JesInitializationActor.cromwell$backend$impl$jes$JesInitializationActor$$writeAuthenticationFile(JesInitializationActor.scala:58); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.apply(JesInitializationActor.scala:52); 	at cromwell.backend.impl.jes.JesInitializationActor$$anonfun$beforeAll$1.apply(JesInitializationActor.scala:51); 	at scala.util.Try$.apply(Try.scala:192); 	at cromwell.backend.impl.jes.JesInitializationActor.beforeAll(JesInitializationActor.scala:51); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$initSequence$1$$anonfun$apply$1.apply(BackendWorkflowInitializationActor.scala:156); 	at cromwell.backend.BackendWorkflowInitializationActor$$anonfun$initSequence$1$$anonfun$apply$1.apply(BackendWorkflowInitializationActor.scala:155); 	at scala.concurrent.Future$$anonfun$flat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1890:426,error,error,426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1890,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"This might be conflating two issues, but in case it is related, another error we are consistently seeing that seems dependent on which docker container we use (which may be a red herring, but that's all I can narrow it down to), we'll run something and get this error: ; ```; ""callCaching"": {; ""hashFailures"": [; {; ""message"": ""[Attempted 1 time(s)] - NoSuchFileException: s3://s3.amazonaws.com/some-bucket/cromwell-tests/Kraken2_test_input.fastq.gz"",; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""s3://s3.amazonaws.com/some-bucket/cromwell-tests/Kraken2_test_input.fastq.gz""; }; ]; }; ],; ```. Meanwhile, the input location of the input file is this:; ```; ""inputs"": {; ""input_fastq"": {; ""format"": null,; ""location"": ""s3://some-bucket/cromwell-tests/Kraken2_test_input.fastq.gz"",; ""size"": null,; ""secondaryFiles"": [],; ""contents"": null,; ""checksum"": null,; ""class"": ""File""; },; ```; So it's being given a valid S3 URL but then when it's trying to get the hash, it's looking at an invalid S3 URL (the one with s3.amazonaws.com isn't valid, but wasn't supplied by us). Thoughts? Is this a separate issue?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457651066:72,error,error,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-457651066,3,"['down', 'error']","['down', 'error']"
Availability,"This might be related to #1782 somehow, I believe I've seen these in some of the errorstorms which wind up w/ the symptoms described in #1782",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-267834315:81,error,errorstorms,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1436#issuecomment-267834315,1,['error'],['errorstorms']
Availability,"This might be the only remaining type of transient failure which hasn't been patched, but it does pop up fairly frequently. It looks like the usual pattern of retries would work here. ```; cromwell.core.CromwellFatalException: java.util.NoSuchElementException; 	at cromwell.core.CromwellFatalException$.apply(core.scala:17); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$2.applyOrElse(Retry.scala:37); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); 	at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); 	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.util.NoSuchElementException; 	at java.util.ArrayList$Itr.next(ArrayList.java:854); 	at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:43); 	at scala.collection.IterableLike$class.head(IterableLike.scala:107); 	at s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1966:51,failure,failure,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1966,3,"['failure', 'recover']","['failure', 'recoverWith']"
Availability,This one is different from previous JES issues that were error 500. So this issue might be a dupe.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256639456:57,error,error,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256639456,1,['error'],['error']
Availability,"This one's a bit odd. Submitting workflows with imports to release 24 server intermittently gives errors in the logs. A simple workflow: ; `cat goodImport.wdl`:. ```; import ""bar.wdl"" as doIt. workflow testMe {; 	call doIt.doIt; }; ```; And a bunch of wdl tasks in a folder, only one of which is the actual dependency (`bar.wdl`); ```; conradL@qimr13054 ~]$ unzip -l foo.zip ; Archive: foo.zip; Length Date Time Name; --------- ---------- ----- ----; 0 02-07-2017 14:46 foo/; 99 02-07-2017 14:45 foo/bar7.wdl; 98 02-07-2017 14:00 foo/bar.wdl; 99 02-07-2017 14:46 foo/bar8.wdl; 99 02-07-2017 14:45 foo/bar2.wdl; 100 02-07-2017 14:45 foo/bar10.wdl; 99 02-07-2017 14:46 foo/bar9.wdl; 99 02-07-2017 14:45 foo/bar1.wdl; 99 02-07-2017 14:45 foo/bar3.wdl; 99 02-07-2017 14:45 foo/bar5.wdl; 99 02-07-2017 14:45 foo/bar4.wdl; 99 02-07-2017 14:45 foo/bar6.wdl; --------- -------; 1089 12 files; ```. The content of all the task dependencies is just a variation on:; ```; [conradL@qimr13054 ~]$ cat foo/bar.wdl ; task doIt {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; }; ```. Submit to the server:; ```; curl http://localhost:8000/api/workflows/V1 -FwdlSource=@goodImport.wdl -FwdlDependencies=@foo.zip; ```. Now tailing the server logs, the first time this is submitted, the workflow succeeds and the log shows nothing out of the ordinary. But ""sometimes"" (meaning, I can submit it 5 times and not see it, or twice and see it both times) I see this:; ```; 2017-02-07 15:01:10,781 cromwell-system-akka.dispatchers.service-dispatcher-30 ERROR - Sending Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-84a51727-cfda-41e7-a03c-9e3af35eb0dc/MaterializeWorkflowDescriptorActor#972983209] failure message MetadataPutFailed(PutMetadataAction(Stream(MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar6.wdl),Some(MetadataValue(task doIt6 {; 	command { echo ""Help, world!"" }; 	output { String mes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:98,error,errors,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['error'],['errors']
Availability,"This potential bug was encountered while writing a CWL task that required a pair of inputs. Namely, Cromwell is requiring a parameter that is both 1) not required per CWL spec and 2) not even used. Working and failing examples given below. . I am on a Linux Mint (19) machine, using `cromwell-36.jar` (downloaded October 29, 2018) with java10:; ```; $ java -version; openjdk version ""10.0.1"" 2018-04-17; OpenJDK Runtime Environment (build 10.0.1+10-Ubuntu-3ubuntu1); OpenJDK 64-Bit Server VM (build 10.0.1+10-Ubuntu-3ubuntu1, mixed mode); ```. For both the working and failing CWL files, I use the following input:; ```; {; ""paired_parameters"": {; ""itemA"": ""one"",; ""itemB"": ""two""; }; }; ```; Below is the **successful** CWL file. In particular, note the `inputs.type.name` which is set to a garbage value.; ```; {; ""cwlVersion"": ""v1.0"",; ""class"": ""CommandLineTool"",; ""inputs"": [; {; ""id"": ""paired_parameters"",; ""type"": {; ""type"": ""record"",; ""name"": ""SOME JUNK VALUE"",; ""fields"": [; {; ""name"": ""itemA"",; ""type"": ""string"",; ""inputBinding"": {; ""prefix"": ""-A="",; ""separate"": false; }; },; {; ""name"": ""itemB"",; ""type"": ""string"",; ""inputBinding"": {; ""prefix"": ""-B="",; ""separate"": false; }; }; ]; }; }; ],; ""outputs"": {; ""example_out"": {; ""type"": ""stdout""; }; },; ""stdout"": ""output.txt"",; ""baseCommand"": ""echo""; }; ```; This was run with: `java -jar cromwell-36.jar run works.json --inputs inputs.json`. There are two issues:; - clearly the `name` key is being ignored. Since it is not required (see next item), this is by itself quite minor.; - a `name` key is *not* required per the CWL spec (https://www.commonwl.org/v1.0/CommandLineTool.html#InputRecordSchema). As mentioned, ignoring the `name` parameter is probably acceptable, BUT if I remove that parameter, the execution fails. The failing example is the same, but with ` ""name"": ""SOME JUNK VALUE"",` removed:; ```; $ diff works.json fails.json ; 9d8; < ""name"": ""SOME JUNK VALUE"",; ```; The stack trace reports:; ```; [2018-10-30 21:46:32,22] [error]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4338:302,down,downloaded,302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4338,1,['down'],['downloaded']
Availability,"This removes several classes of confusing `ERROR` log messages that we're receiving under healthy conditions. For example, when calling `checkAccess` in the filesystem provider, which is intended to be used to detect whether a path exists, the library would log an error containing the path string. This resulted in several `ERROR` level log messages every time we created a directory structure.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6982:43,ERROR,ERROR,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6982,3,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"This requirement boils down to declaring multiple sources of an input and providing them to a `valueFrom` expression to determine the actual value of the input. We should pass another valueFrom-related test (\#63) when ExpressionTool support is merged, which is imminent.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3204:23,down,down,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3204,1,['down'],['down']
Availability,This resolves the below error (which comes up when you run CromIAM):. `error while starting up loggers akka.ConfigurationException: Logger specified in config can't be loaded [akka.event.slf4j.Slf4jLogger] due to [java.lang.ClassNotFoundException: akka.event.slf4j.Slf4jLogger]`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4254:24,error,error,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4254,2,['error'],['error']
Availability,"This second class of failure seems straightforward: there's code in `WorkflowActor` to `setRuntimeAttributes` that makes no allowance for the attributes already having been written, but this scenario can definitely occur. For a master-like branch, the most expedient solution is probably to turn these inserts into upserts. For develop it would be nice to write these attributes in an event-sourcish way that for writes doesn't clobber existing values, but which always takes the latest values for reads. Still investigating the first class of failure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215125595:21,failure,failure,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215125595,2,['failure'],['failure']
Availability,"This seems like a bug. In my inputs.json, I have an input file that is a URL. Cromwell (v84) fails to pull it when I'm not using ""Local"" as a backend. The only difference between a working case and failed case is the name of the backend in the cromwell config. If you just change the name from ""Local"" to ""MyLocal"", it will fail. For example,. this works; ```; default = ""Local""; providers; {; Local; ```. And this fails; ```; default = ""MyLocal""; providers; {; MyLocal; ```. Command to reproduce error. export _JAVA_OPTIONS=""--add-opens=java.base/sun.security.util=ALL-UNNAMED"". java -Dconfig.file=cromwell_docker.conf \; -Dbackend.providers.Local.config.dockerRoot=$(pwd)/cromwell-executions \; -Dbackend.providers.Local.config.root=$(pwd)/cromwell-executions \; -jar ~/cromwell/cromwell-84.jar run fq_count.wdl -i fq_count.json. Error:; java.lang.IllegalArgumentException: Could not build the path ""https://portal.nersc.gov/cfs/m342/jaws/test_data/sample.fastq"". It may refer to a filesystem not supported by this instance of Cromwell. Supported filesystems are: MacOSXFileSystem. Failures: . Required files; [test-files.zip](https://github.com/broadinstitute/cromwell/files/10387814/test-files.zip)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6977:497,error,error,497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6977,3,"['Error', 'Failure', 'error']","['Error', 'Failures', 'error']"
Availability,"This seems like a pretty recoverable error. It fails the zamboni workflow but not the cromwell workflow. So it can easily be overcome with a reconsider in zamboni. Ideally the Zamboni workflow would catch and be robust to these sorts of things, or at least log the response.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216873574:25,recover,recoverable,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216873574,3,"['error', 'recover', 'robust']","['error', 'recoverable', 'robust']"
Availability,This seems to be fixing the symptom rather than the cause?. Is it not the file evaluators themselves which should be evaluating things that look like `glob` into `WomGlobFile`s immediately rather than ever calling down into the `IoFunction`s?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4208#issuecomment-427133149:214,down,down,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4208#issuecomment-427133149,1,['down'],['down']
Availability,"This seems to be interestingly connected with the spec change https://github.com/openwdl/wdl/pull/315 (so cc @patmagee). As that PR is currently written, we would be fine to do the scheme like this, because the `memory` section says ""you can provide as much memory as you want, as long as it's over this amount"", but it feels like we're in danger of writing non-portable WDLs like this because the incentive is to write a small value first and rely on the doubling to catch you if necessary. FWIW I'd rather go down the route of:; - `memory` is treated as the ""guaranteed to work"" ceiling amount; - We could start by having a much lower `memory_to_try_first` attribute representing the first value to try; - If the task fails, we can then double it from the low baseline, until either the task succeeds or we reach the `memory` ceiling, and at that point we don't try any further. Does that make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499235634:511,down,down,511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499235634,1,['down'],['down']
Availability,This seems to have been broken by a rebase. Closed for repairs.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3609#issuecomment-388126038:55,repair,repairs,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3609#issuecomment-388126038,1,['repair'],['repairs']
Availability,"This seems too simple to be correct, would appreciate a quick looking-over. `1st-workflow.cwl` is an expected failure case.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4285:110,failure,failure,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4285,1,['failure'],['failure']
Availability,"This seems weird but Cromwell shouldn't crash. In Jeff's Wash U testing instance, `docker attach` to the container and then:. ```; java -jar /root/cromwell/target/scala-2.12/cromwell-*-SNAP.jar run \; /root/mgi/cancer-genomics-workflow/unaligned_bam_to_bqsr/downsample_workflow.cwl \; --inputs /root/mgi/inputs_downsample.yml -t CWL; ```; Pretty soon there's this exception:. ```; [2018-02-25 11:18:35,97] [error] WorkflowManagerActor Workflow d35a3d0d-f48d-429f-9120-31cf67bd3e55 failed (during ExecutingWorkflowState): actor name [d35a3d0d-f48d-429f-9120-31cf67bd3e55-EngineJobExecutionActor-downsample:0:1] is not unique!; akka.actor.InvalidActorNameException: actor name [d35a3d0d-f48d-429f-9120-31cf67bd3e55-EngineJobExecutionActor-downsample:0:1] is not unique!; 	at akka.actor.dungeon.ChildrenContainer$NormalChildrenContainer.reserve(ChildrenContainer.scala:129); 	at akka.actor.dungeon.Children.reserveChild(Children.scala:134); 	at akka.actor.dungeon.Children.reserveChild$(Children.scala:132); 	at akka.actor.ActorCell.reserveChild(ActorCell.scala:370); 	at akka.actor.dungeon.Children.makeChild(Children.scala:272); 	at akka.actor.dungeon.Children.actorOf(Children.scala:44); 	at akka.actor.dungeon.Children.actorOf$(Children.scala:43); 	at akka.actor.ActorCell.actorOf(ActorCell.scala:370); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.startEJEA(WorkflowExecutionActor.scala:546); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun; ```. There's some odd looking CWL here in `/root/mgi/cancer-genomics-workflow/unaligned_bam_to_bqsr/downsample_workflow.cwl` but conformance test 75 does something very similar and we're passing that so the ""one element array of arrays"" thing may be a red herring. ```; steps:; downsample:; scatter: [bam]; scatterMethod: dotproduct; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3318:407,error,error,407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3318,4,"['down', 'error']","['downsample', 'error']"
Availability,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3366:1629,robust,robustness,1629,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366,1,['robust'],['robustness']
Availability,This should also fix transient failures of the `abort a workflow mid run and restart immediately abort.restart_abort_tes` centaur test,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4197:31,failure,failures,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4197,1,['failure'],['failures']
Availability,"This should be a dynamic assert, based on the results of the task.; validation can take a while, so I would like the output to be taken and; given to the next task in the workflow while the validation is happening. I; would also like to control the response of the run (fail and stop, fail but; continue, warn and continue, do not validate) if there's an assertion; failure. On Tue, Jan 31, 2017 at 12:57 AM, Linlin Yan <notifications@github.com>; wrote:. > Sounds like some syntactic sugars are expected to simplify the fail method; > declaration.; >; > In addition, will such 'assert' be dynamic (in run-time) or static (in; > parse-time, before running any task)?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276281986>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0is0FEJp23Rx_5cXqrSPWKK8d2h_ks5rXs1ggaJpZM4JJrWM>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089:366,failure,failure,366,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1146#issuecomment-276362089,1,['failure'],['failure']
Availability,This should help with error messages when Cromwell shuts down,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3620:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3620,2,"['down', 'error']","['down', 'error']"
Availability,"This started more as a POC that I had in mind but it ended up being a lot less refactoring than I anticipated so I'm making a PR for it.; Following the way we can plug services and languages this allows to plug in filesystems. All you need is a `PathBuilderFactory`.; How to make a `PathBuilderFactory` could still be made simpler but that's a separate issue.; This has the advantage that a filesystem can automatically be added to Cromwell engine or standard backend without code changes.; Available filesystems are defined in the config with their corresponding class, and the engine and backends can pick which ones they want to enable.; It removes some dependency of `engine` over the individual filesystem sub projects but it's not all the way there yet.; Thinking about PAPI2 this possibly opens the door to automatically support new filesystems for (de)localization as well if we were to go down that road.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3496:491,Avail,Available,491,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3496,2,"['Avail', 'down']","['Available', 'down']"
Availability,This strawman was brave... but it wasn't good enough. Since this no longer represents our current thinking I'm going to take this strawman down.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-237313379:139,down,down,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-237313379,1,['down'],['down']
Availability,"This sucked up a few hours of time with the debugger, we need a better error message/handling. If you misconfigure Cromwell (in my case using a service account) whereby the auth specified in JES.filesystems.gcs.auth is unable to write to the bucket specified in JES.config.root. Specifically, I found that the uploadCommandScript was dying silently in the JABJEA and my workflow just stopped running",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1669:71,error,error,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1669,1,['error'],['error']
Availability,"This task failed after a restart of Cromwell with the following stack trace. We have seen similar errors before - #588. This was only after one restart though. ```; 2016-05-24 16:59:43,266 cromwell-system-akka.actor.default-dispatcher-6 INFO - JesBackend [UUID(fd62961b):ApplyBQSR:11]: Starting call with pre-emptible VM. 2016-05-24 16:59:43,431 cromwell-system-akka.actor.default-dispatcher-17 INFO - WorkflowActor [UUID(fd62961b)]: persisting status of ApplyBQSR:11 to Running. 2016-05-24 16:59:43,526 cromwell-system-akka.actor.default-dispatcher-6 INFO - JES Run [UUID(fd62961b):ApplyBQSR:11]: Status change from - to Success. 2016-05-24 16:59:44,173 cromwell-system-akka.actor.default-dispatcher-13 ERROR - WorkflowActor [UUID(fd62961b)]: Completion work failed for call ApplyBQSR:11.; com.mysql.jdbc.exceptions.jdbc4.MySQLIntegrityConstraintViolationException: Duplicate entry '741-PairedEndSingleSampleWorkflow.ApplyBQSR-recalibrated_bam-11-' for key 'UK_SYM_WORKFLOW_EXECUTION_ID_SCOPE_NAME_ITERATION_IO'; at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[na:1.8.0_72]; at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[na:1.8.0_72]; at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[na:1.8.0_72]; at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[na:1.8.0_72]; at com.mysql.jdbc.Util.handleNewInstance(Util.java:400) ~[cromwell.jar:0.19]; at com.mysql.jdbc.Util.getInstance(Util.java:383) ~[cromwell.jar:0.19]; at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:973) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3847) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3783) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2447) ~[cromwell.jar:0.19]; at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2594) ~[cromwell.jar:0.19]; at com.mysql.jdbc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/880:98,error,errors,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/880,2,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,"This task is covered by the DSDE-Docs Epic ""[Update Cromwell Documentation](https://github.com/broadinstitute/dsde-docs/issues/1514)"", which includes moving most of the content to the WDL website and [slimming down the README](https://github.com/broadinstitute/dsde-docs/issues/1515). Closing this issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1343#issuecomment-268012330:210,down,down,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1343#issuecomment-268012330,1,['down'],['down']
Availability,"This test appears to not deal with eventual consistency correctly and sporadically fails for what looks like bogus reasons:. ```; [info] WorkflowExecutionActorSpec:; [info] WorkflowExecutionActor; [info] - should retry a job 2 times and succeed in the third attempt *** FAILED ***; [info] cromwell.core.package$CromwellFatalException: org.scalatest.exceptions.TestFailedException: ""Running"" was not equal to ""Preempted""; [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:44); [info] at cromwell.core.retry.Retry$$anonfun$withRetry$3.applyOrElse(Retry.scala:43); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:344); [info] at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:343); [info] at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); [info] at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); [info] at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); [info] ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1311:636,recover,recoverWith,636,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1311,2,['recover'],['recoverWith']
Availability,"This test was introduced in 2019 to verify an improved error message when localization fails ([issue](https://github.com/broadinstitute/cromwell/issues/4603), [PR](https://github.com/broadinstitute/cromwell/pull/4718)). It works by localizing an exactly 1 GB file onto a 1 GB disk, that presumably has other stuff on it, and verifying that it fails in the expected way. Unfortunately, the out-of-disk condition seems to trigger behavior in PAPI that causes the task to hang for half an hour!. Since PAPI is on its way out and this error message still seems to be checked in [a different test case](https://github.com/broadinstitute/cromwell/pull/4718/files#diff-a348345a036a680f8e29070f0972f61b09f3f640f26b188504d69d5a7f71b554), I am recommending we just delete the test instead of spending any more time on this. ```; > gcloud beta lifesciences operations describe projects/1005074806481/locations/us-central1/operations/8650136336352694244 --format=json; {; ""done"": true,; ""error"": {; ""code"": 9,; ""message"": ""Execution failed: generic::failed_precondition: while running \""-c /bin/bash /cromwell_root/gcs_localization.sh\"": unexpected exit status 1 was not ignored""; },; ""metadata"": {; ""@type"": ""type.googleapis.com/google.cloud.lifesciences.v2beta.Metadata"",; ""createTime"": ""2023-12-04T20:36:45.056562Z"",; ""endTime"": ""2023-12-04T21:10:43.697318162Z"" # <- WTF!!; }; [...]; ```. ```; Long duration; Warning: arning] Using a password on the command line interface can be insecure.; +--------------------------------------+-----------------+----------------------------+----------------------------+; | name | RUNTIME_MINUTES | start | end |; +--------------------------------------+-----------------+----------------------------+----------------------------+; | localize_file_larger_than_disk_space | 35 | 2023-12-05 01:01:27.836000 | 2023-12-05 01:37:10.789000 |; | lots_of_inputs | 32 | 2023-12-05 01:02:03.292000 | 2023-12-05 01:34:26.490000 |; | draft3_call_cache_capoeira | 27 | 2023-12-05 01:03:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7330:55,error,error,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7330,3,['error'],['error']
Availability,"This ticket is a reminder to discuss what @cjllanwarne was saying Friday afternoon. I don't believe this is a release blocker, but noting it for future consideration:. The interplay between restart and caching is possibly not ideal. When restarting with cache read turned on, Cromwell will begin potentially expensive hash calculations on jobs that may have previously been running. However, Cromwell currently does this calculation before checking if jobs were actually running and recoverable, which if they were would make hash calculation unnecessary. . On the other hand, perhaps determining whether a job is running in the backend is more expensive than calculating hashes. Shrug. . Anyway, the current scheme is likely more accidental than intentional and would benefit from some discussion.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1441:483,recover,recoverable,483,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1441,1,['recover'],['recoverable']
Availability,"This tight-looping of the `could not download return code file, retrying:` is typical of a problem that's been fixed in C24. ; If you see the 'never finishing' problem again in C24+, it's probably due to a different cause so please repost with new logs and metadata files (sorry!). Closing this ticket.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1928#issuecomment-276105148:37,down,download,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1928#issuecomment-276105148,1,['down'],['download']
Availability,"This updates the 2 caching tests that we have been seeing cache hit failure with so that they do not attempt to read from the cache on their first run. For commentary, see https://broadworkbench.atlassian.net/browse/CROM-6807?focusedCommentId=52973. Proof that the different options are being respected:. 2021-11-04 18:54:05,200 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - BT-322 3a536714:cacheBetweenWF.getAverage:-1:1 is eligible for call caching with **read = false** and write = true. 2021-11-04 18:54:55,200 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - BT-322 ce702d83:cacheBetweenWF.getAverage:-1:1 is eligible for call caching with **read = true** and write = true",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6557:68,failure,failure,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557,1,['failure'],['failure']
Availability,This was also reported [http://gatkforums.broadinstitute.org/wdl/discussion/9576/is-this-error-caused-by-a-job-submission-failure#latest](here) by @MatthewMah,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2189#issuecomment-302100731:89,error,error-caused-by-a-job-submission-failure,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2189#issuecomment-302100731,1,['error'],['error-caused-by-a-job-submission-failure']
Availability,"This was deliberately ""not allowed"", to try to force people to pass through the inputs and outputs to their workflows (ie so that the interface to a workflow was stable even if extra tasks were added or removed from its internal workings). In other words to encourage:. ```wdl; version 1.0. workflow Test2 {; input {; String? passthrough_text; }. call Echo {; input: text = passthrough_text; }. output {; }; }; ```. This would allow you to swap out the internal call to `Echo` for something else, or rename the call, or add another call after it, or replace the entire workflow itself with a single task, etc, etc... and nobody who's calling `Test2` needs to worry about your internal refactorings. They just `call Test2` and supply the input and are done. Now having said that, I've pretty much changed my mind about this being something to enforce rather than just something to encourage, and would ideally like to go down the route of saying ""best practices are to pass through inputs and outputs but it's not enforced because quite often it's super-annoying""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4079#issuecomment-420048870:352,Echo,Echo,352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4079#issuecomment-420048870,3,"['Echo', 'down']","['Echo', 'down']"
Availability,This was requested by @patmagee - I agree that it's a good idea. Find a way to detect if a liquibase migration is pending if Cromwell starts. Add a config option (defaulting to a safe mode) such that if this option is enabled and a liquibase migration is required that the process will exit with an error message stating:. - That a migration is necessary; - Encouragement to the user to backup their database and/or do further testing if in a production environment; - Describe how to override (including via command line) the setting to allow Cromwell to start properly.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2429:299,error,error,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2429,1,['error'],['error']
Availability,"This wdl throws an error:. ```; scatter (unmapped_bam in flowcell_unmapped_bams) {; String base_name = sub(sub(unmapped_bam, ""gs://.*/"",""""), "".unmapped.bam$"", """"); call CollectQualityYieldMetrics {; input:; input_bam = unmapped_bam,; metrics_filename = base_name + "".unmapped.quality_yield_metrics"",; disk_size = flowcell_small_disk; }; }; ```. ```; ""Workflow has invalid declarations: Invalid parameters for engine function sub: (Failure(java.lang.IllegalArgumentException: Invalid parameters for engine function sub: (Failure(wdl4s.WdlExpressionException: Could not find a value for unmapped_bam),Success(WdlString(gs://.*/)),Success(WdlString())).),Success(WdlString(.unmapped.bam$)),Success(WdlString())).""; ```. But if used inside the task call it is successful:. ```; scatter (unmapped_bam in flowcell_unmapped_bams) {; call CollectQualityYieldMetrics {; input:; input_bam = unmapped_bam,; metrics_filename = sub(sub(unmapped_bam, ""gs://.*/"",""""), "".unmapped.bam$"", """") + "".unmapped.quality_yield_metrics"",; disk_size = flowcell_small_disk; }; }; ```. This just makes it very inconvenient if there are multiple tasks that need the value from the sub function as an input.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/687:19,error,error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/687,3,"['Failure', 'error']","['Failure', 'error']"
Availability,"This will add to the instrumentation metric path the http return code in case of failure, giving better insight as to the reason of the failures",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3506:81,failure,failure,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3506,2,['failure'],"['failure', 'failures']"
Availability,"This will always mean that Cromwell doesn't believe the workflow exists, yet the error message is that there's no collction for the workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3224:81,error,error,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3224,1,['error'],['error']
Availability,"This workflow `20d61ca9-0cc8-4dc1-9c20-85dbf9146de4`, running in the mint-dev Cromwell instance (version 35-fe8c4bf-SNAP) had a failed task where there is no start time in the metadata, but there is an end time. Was this task technically not started yet? If that's the case, then why would there be an end time?. ```; ""Adapter10xCount.inputs_for_submit.other_inputs"": [; {; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""shardIndex"": -1,; ""failures"": [; {; ""causedBy"": [],; ""message"": ""Failed to evaluate 'Adapter10xCount.inputs_for_submit.other_inputs' (reason 1 of 1): Evaluating [{\""name\"": \""sample_id\"", \""value\"": GetInputs.sample_id}, {\""name\"": \""reference_name\"", \""value\"": reference_name}, {\""name\"": \""transcriptome_tar_gz\"", \""value\"": transcriptome_tar_gz}, {\""name\"": \""expect_cells\"", \""value\"": expect_cells}] failed: :\nFailed to coerce one or more keys or values for creating a Map[String, Int?]:\nList(java.lang.NumberFormatException: For input string: \""expect_cells\""\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:301)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:301)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:29)\n\tat wom.types.WomIntegerType$$anonfun$coercion$1.applyOrElse(WomIntegerType.scala:20)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34)\n\tat wom.types.WomType.$anonfun$coerceRawValue$2(WomType.scala:37)\n\tat scala.util.Try$.apply(Try.scala:209)\n\tat wom.types.WomType.coerceRawValue(WomType.scala:37)\n\tat wom.types.WomType.coerceRawValue$(WomType.scala:27)\n\tat wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9)\n\tat wom.values.WomOptionalValue.coerceAndSetNestingLevel(WomOptionalValue.scala:127)\n\tat wom.types.WomOptionalType$$anonfun$co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4128:452,failure,failures,452,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4128,1,['failure'],['failures']
Availability,"This workflow run on JES will cause a JES failure with `the local copy message must have path set`. This is because the value for `a` is passed to JES as an input but because it's empty the submission fails. We should catch this before it gets to JES and fail with a better error message. ```; task t {; File a; command {; cat ${a}; }; output {; String out = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow w {; call t; }; ```. ```; {; ""w.t.a"": """"; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2367:42,failure,failure,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2367,2,"['error', 'failure']","['error', 'failure']"
Availability,"This workflow runs fine on ""29-2caec7d"" and fails with this stack trace on ""30-a3ea825-SNAP"" using the Pipelines API backend. ```; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""This workflow contains a cyclic dependency on SomaticPairedEndSingleSampleWorkflow.$scatter_2""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted(Scope.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.Scope.childGraphNodesSorted$(Scope.scala:43)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted$lzycompute(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.childGraphNodesSorted(WdlWorkflow.scala:63)""; },; {; causedBy: [ ],; message: ""wdl.WdlGraphNode$.buildWomGraph(WdlGraphNode.scala:140)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow$.womWorkflowDefinition(WdlWorkflow.scala:52)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition$lzycompute(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlWorkflow.womDefinition(WdlWorkflow.scala:73)""; },; {; causedBy: [ ],; message: ""wdl.WdlInputParsing$.buildWomExecutable(WdlInputParsing.scala:27)""; },; {; causedBy: [ ],; message: ""wdl.WdlNamespaceWithWorkflow.womExecutable(WdlNamespace.scala:98)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$15(MaterializeWorkflowDescriptorActor.scala:493)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; messa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143:131,failure,failures,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143,1,['failure'],['failures']
Availability,This works for me. The test failures are unrelated to this change. :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/226#issuecomment-146205665:28,failure,failures,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/226#issuecomment-146205665,1,['failure'],['failures']
Availability,"This would be extremely useful for us. We're currently having to deal with several problems that would be helped by an automated retry ability. . The first problems is what David said, we have tools that can fail sometimes due to GCS issues and being able to restart when that happens would be useful. We're working on making our code more robust to that, but it's difficult to completely fix the problem. Having to restart a workflow with 10s of thousands of jobs because 2 failed is pretty annoying. The second problem is out of memory issues. We have thousands of jobs, and most will run with a small amount of memory, but some of them will need more. It's difficult to predict ahead of time which shards will need more since it's a function of the data rather than of the file size. Having a way to automatically retry these shards with increased memory would be really valuable since it would let us provision for the average shard rather than the worst case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-315406584:340,robust,robust,340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-315406584,1,['robust'],['robust']
Availability,"This would greatly reduce my need to modify WDL scripts to start where I have data already processed. For example, if a script goes BAM-->coverage-->CNVs, if I have already collected coverage on my BAMs, I would like to be able to provide `coverage` to the same script and have Cromwell skip the tasks involving the BAM and run the remaining steps in the workflow, e.g. coverage-->CNVs. . I run WDLs using gcloud, within a VM and locally. I don't use FireCloud so my runs do not use call-caching. I want to take the boilerplate WDL scripts the GATK4 repo makes available to run processes. I am specifically looking at the latest somatic CNV workflow. If I have alreaded padded my intervals and/or collected counts on the BAMs, I'd like to still use the rest of the steps in the workflow by specifying in the INPUTS JSON an intermediate file. If the script is thus:; ```; call CNVTasks.PreprocessIntervals {; input:; intervals = intervals,; ref_fasta_dict = ref_fasta_dict,; gatk4_jar_override = gatk4_jar_override,; gatk_docker = gatk_docker; }. if (select_first([do_explicit_gc_correction, false])) {; call CNVTasks.AnnotateIntervals {; input:; intervals = PreprocessIntervals.preprocessed_intervals,; ref_fasta = ref_fasta,; ref_fasta_fai = ref_fasta_fai,; ref_fasta_dict = ref_fasta_dict,; gatk4_jar_override = gatk4_jar_override,; gatk_docker = gatk_docker; }; ```. In the inputs, instead of defining:; ```; ""CNVSomaticPanelWorkflow.intervals"": ""File"",; ```; I would like to be able to instead provide:; ```; ""CNVSomaticPanelWorkflow.PreprocessIntervals.preprocessed_intervals"": ""File"",; ```. And not have the run error due to the lack of the `CNVSomaticPanelWorkflow.intervals` file. . I would really appreciate such a feature as it saves me the time of having to rewrite WDL scripts for each tweaked subset workflow. Thanks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2949:561,avail,available,561,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2949,2,"['avail', 'error']","['available', 'error']"
Availability,This would work if the failure mode is passed through workflow options but not if it's set in the config.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2239:23,failure,failure,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2239,1,['failure'],['failure']
Availability,"Those requests are probably a red herring, but I suggest reaching out to us (DSP AppSec) on Slack for those ;). Re the dedicated SA, there're a couple issues with your config:. 1) We typically don't recommend downloading a SA key to a GCP VM, since all GCP VMs normally have a SA associated with them (when you start them). Cromwell will just pick them up automatically ""from the environment"". So please don't download a SA key to it and instead use this as the recommended option, per [Cromwell docs](https://cromwell.readthedocs.io/en/latest/backends/Google/):; ```hcl; {; name = ""application-default""; scheme = ""application_default""; },; ```. I can provide more details from the config I used previously, if this doesn't work. 2) Which SA is `MY-GOOGLE-PROJECT-############.json` for? From your earlier `gcloud projects add-iam-policy-binding` command, it seems like that was for `MY-NUMBER-compute@developer.gserviceaccount.com`, which is the so-called ""Default Compute Service Account"" in your project. Using it is not recommended, since it has pretty wide permissions from the get-go. So I'd recommend creating a separate SA and granting it those roles instead, and then assigning that SA to the Cromwell VM before you start it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686615315:209,down,downloading,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686615315,2,['down'],"['download', 'downloading']"
Availability,"Those two failures are probably unrelated test flakiness (the curl exit 6 is a ""Couldn't resolve host"" error). I've restarted the two builds, hopefully the errors will go away.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504405803:10,failure,failures,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504405803,3,"['error', 'failure']","['error', 'errors', 'failures']"
Availability,"Thought I would check out the new GCPBATCH support in the latest cromwell. Used the [example config](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/GCPBATCH.conf) for the newly supported GCPBATCH with my projects settings added and it errored because of multiple zones were configured:. ```; default-runtime-attributes {; ...; zones: [""us-central1-a"", ""us-central1-b""]; }; ```; I changed it to just `[""us-central1-a""]` and worked fine and ran until completion so I guess it is how the parser is combining these zones into whatever underlying batch command needs to be executed. (I also tried the value ""us-central1-a us-central1-b"" as a string instead of an array cause I thought I read somewhere that was supported, but that didn't work either). Here is the error thrown by cromwell: . ![Screenshot 2023-10-04 at 13 11 01](https://github.com/broadinstitute/cromwell/assets/40811287/9c9cc8d8-8e08-44a3-826d-a0300fc95278)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7232:270,error,errored,270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7232,2,['error'],"['error', 'errored']"
Availability,"Thoughts for a Monday Tech Talk™️:. Say we run a workflow with a 100-wide scatter and the floating Docker tags are resolved to hashes during workflow initialization. 90 of the jobs launch, but then the server goes down. The server comes up some time later and recovers the first 90 running jobs, but in the meantime the floating Docker tag has moved to a new version. We rerun the workflow initialization and calculate a different hash for the remaining 10 shards of the scatter. . It seems the hash for a Docker tag for a particular workflow should be persisted to be able to handle this case. I also wonder per @cjllanwarne if we shouldn't keep this activity in the `JobPreparationActor` to avoid knowingly creating a system that we'll have to replace when we implement dynamic dispatch. Keeping this in `JobPreparationActor` would also give us greater ability to resolve expressions for Docker tags than if we do this earlier in the workflow lifecycle.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497:214,down,down,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048#issuecomment-289168497,2,"['down', 'recover']","['down', 'recovers']"
Availability,Thx for the pointer to the outdated docs. Removed that section since that error should no longer occur in IntelliJ.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4476#issuecomment-446070591:74,error,error,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4476#issuecomment-446070591,1,['error'],['error']
Availability,"Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] Aborting all running workflows.; [2019-02-11 10:13:36,34] [info] WorkflowStoreActor stopped; [2019-02-11 10:13:36,34] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-11 10:13:36,34] [info] JobExecutionTokenDispenser stopped; [2019-02-11 10:13:36,34] [info] WorkflowLogCopyRouter stopped; [2019-02-11 10:13:36,35] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor All workflows finished; [2019-02-11 10:13:36,35] [info] WorkflowManagerActor stopped; [2019-02-11 10:13:36,78] [info] Connection pools shut down; [2019-02-11 10:13:36,78] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] SubWorkflowStoreActor stopped; [2019-02-11 10:13:36,78] [info] JobStoreActor stopped; [2019-02-11 10:13:36,78] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,78] [info] CallCacheWriteActor stopped; [2019-02-11 10:13:36,78] [info] IoProxy stopped; [2019-02-11 10:13:36,79] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,79] [info] DockerHashActor stopped; [2019-02-11 10:13:36,79] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-11 10:13:36,80] [info] ServiceRegistryActor stopped; [2019-02-11 10:13:36,80] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2019-02-11 10:13:36,80]",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:15565,down,down,15565,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,13,['down'],['down']
Availability,Timeout persisting runtime attributes causes workflow failure.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/737:54,failure,failure,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/737,1,['failure'],['failure']
Availability,"Timothy DeFreitas <notifications@github.com. > wrote:; > ; > @eddiebroad https://github.com/eddiebroad But those are all quoted; > strings, and don't look the same as a WDL comment. From an implementation; > perspective, doesn't cromwell pipe the command bock to /bin/bash anyway?; > And following bash rules unquoted # characters start a comment, so maybe; > WDL just has to follow the same comment parsing rules as bash?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200510863. ## . Edward A. Salinas; Senior Software Engineer - Getz Lab; Broad Institute of MIT and Harvard; 75 Ames Street, Rm 4013; Cambridge, MA 02142; Ph: 617-714-7905; Cell: 210-274-3172. ---. @scottfrazer commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200514792). I'm hesitant to try to follow the same rules as Bash for parsing a command because I worry that it'll be error prone and difficult to pin down the details. It feels like we'd essentially reimplement a Bash parser in WDL. It'd be difficult and hard to get the nuances of something like this:. ```; command {; python <<CODE; print(""#octothorps!!#""); CODE; }; ```. Maybe something as simple as syntax highlighting could help too... if `#`s are not highlighted differently in the command section then that could also be a hint. Granted, I know syntax highlighters won't always be used. ---. @eddiebroad commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200525363). If it's clear to WDL users how # is interpreted (or not interpreted) in the; command-block maybe that would be the best thing? Would a sentence/blurb; in the docs address this? Possibly as mentioned earlier good error; messages?. -eddie. On Wed, Mar 23, 2016 at 3:43 PM, Scott Frazer notifications@github.com; wrote:. > I'm hesitant to try to follow the same rules as Bas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2870:6866,error,error,6866,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2870,2,"['down', 'error']","['down', 'error']"
Availability,"To add some more context, not having this ability makes it difficult for us to use the On Hold status for queuing in Mint without a lot of overhead. In more detail:. We have a new service called Falcon that periodically queries our workflow collection in CaaS and starts the oldest On Hold workflow. Right now we have no choice but to query for *all* on hold workflows in each cycle, which for scale testing and production will be thousands of workflows -- even though we just want the oldest one or a few of the oldest ones. We could try to paginate and use multiple requests to skip to the last page, but when several workflows per second are being submitted we can't reliably find the oldest on hold workflow that way. It would be much more efficient if we could ask Cromwell to reverse the order and get just the first page of say the 10 oldest On Hold workflows.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3947#issuecomment-410034522:670,reliab,reliably,670,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3947#issuecomment-410034522,1,['reliab'],['reliably']
Availability,"To clear up my previous comment, it does indeed look like coverage was just an `sbt` test failure issue. Though I'm not clear on why the tests were failing. 🤷🤔:insert_flaky_test_emoji:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308:90,failure,failure,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6144#issuecomment-756278308,1,['failure'],['failure']
Availability,"To correct the test case once this is fixed:. Add this task:; ```; task defined_in_task {; File? f; Boolean is_defined = defined(f); command {; ${true=""cat"" false=""echo no file"" is_defined} ${f}; }; runtime {; docker: ""ubuntu:latest""; }; output {; String out = read_string(stdout()); }; }; ```; Call it from inside the scatter:; ```; scatter (p in masked_indices) {; ...; call defined_in_task { input: f = mk_file.f }; }; ```; Add the output:; ```; Array[String] dit_out = defined_in_task.out; ```; Add expectations",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1982#issuecomment-280668311:164,echo,echo,164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1982#issuecomment-280668311,1,['echo'],['echo']
Availability,"To echo @cjllanwarne - my interpretation of #972 isn't ""call level only"" but the whole enchilada",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1014#issuecomment-226783657:3,echo,echo,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1014#issuecomment-226783657,1,['echo'],['echo']
Availability,"To effectively use ""Retry with More Memory"" for JVM jobs running on Papi one has to use `MEM_SIZE` and `MEM_UNIT` to run with the correct memory settings. But those variables were not available on any other backends. Therefore one ended up having to use other creative options for [authoring multi-backend WDL](https://github.com/broadinstitute/warp/issues/481) such as using [`free`](https://github.com/broadinstitute/warp/pull/197/files/ae14bee73c07684b97dad22b8f3de53ff6404afe..f0692505010baa576f9fe09578fa001661a55145#r735883251). This PR exposes those two environment variables to the `command` block on any Cromwell ""standard"" backend that supports the `memory` runtime attribute. Using the environment variables also helps with call caching java jobs. One can use something along the lines of `-Xmx${MEM_SIZE%.*}${MEM_UNIT%?}` in a version 1.0+ WDL and the command block will stay the same even if the memory needs to be increased. <hr/>. Side note: If anyone comes across this PR and wonders why the default `Local` backend doesn't support `MEM_SIZE` and `MEM_UNIT` it's because the Local backend does not use `memory` (nor `cpu` at the moment). The `memory` runtime attribute would need to be added into the [runtime attributes](https://github.com/broadinstitute/cromwell/blob/79/core/src/main/resources/reference_local_provider_config.inc.conf#L9-L12) with something like:. ```hocon; runtime-attributes = """"""; String? docker; String? docker_user; Int memory_mb = 2048; """"""; ```. And then inside [`submit-docker`](https://github.com/broadinstitute/cromwell/blob/79/core/src/main/resources/reference_local_provider_config.inc.conf#L14-L34) use `--memory=${memory_mb}m`. Then the changes in this PR will generate `MEM_SIZE` and `MEM_UNIT` for the Local backend too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430:184,avail,available,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6766#issuecomment-1133753430,1,['avail'],['available']
Availability,"To fix the ""unmatched case"" error in #4651 :. * Make states a strict ADT rather than string-based; * Hopefully made the `pollStatus` logic a little easier to follow. Closes #4651",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4654:28,error,error,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4654,1,['error'],['error']
Availability,"To prevent deadlocks, we have to acquire locks in the same order in the heartbeat writing batches and the workflow starting. Workflows currently start by ascending submission_time, so we need to order the updates in the same order. . Unfortunately, at the time we are making the sequence of DBIO actions we don't have that information so it needs to be propagated along to that point and then use that information to sort that collection of actions",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4240:72,heartbeat,heartbeat,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4240,1,['heartbeat'],['heartbeat']
Availability,"To put some concrete numbers on this, I'm using `ab` to submit workflows using 32 threads. . Using `develop` I get a sustained ~450 requests per second for as long as my patience lasts (I've not gone beyond 200k submissions). CPU utilization remains steady throughout. Using my changes for small bursts I get ~2500 rps, if I run 200k submits I get ~1400 rps in aggregate. It starts going down pretty rapidly after a few minutes and not long after that is down to roughly the rate of develop (if that) - by 400k submissions it gets too slow for me to wait around. You can see the impact in the CPU utilization of both the JVM and MySQL as well. Using my changes and removing the metadata write I get a sustained 4200 requests per second up until at least 800k workflows. CPU utilization remains steady throughout.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269375525:388,down,down,388,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1810#issuecomment-269375525,2,['down'],['down']
Availability,"To run on each repo, cd to whatever directory you have your git repositories checked out to, say `~/src`, and then paste:. ```bash; (; gitprefix=""git@github.com:broadinstitute/""; ; update-git-secrets() {; grep -q ""git secrets"" .git/hooks/commit-msg || git-secrets --install; git-secrets --add 'private_key'; git-secrets --add 'private_key_id'; git-secrets --add --allowed '""private_key_id"": ""OMITTED""'; git-secrets --add --allowed '""private_key"": ""-----BEGIN PRIVATE KEY-----\\nBASE64 ENCODED KEY WITH \\n TO REPRESENT NEWLINES\\n-----END PRIVATE KEY-----\\n""'; git-secrets --add --allowed '""client_id"": ""22377410244549202395""'; git-secrets --add --allowed '`private_key` portion needs'; git-secrets --add --allowed '.Data.service_account.private_key'; }; ; for dir in */; do; (; cd $dir; if [[ -d .git ]] && \; [[ $(git remote get-url origin 2> /dev/null) == ${gitprefix}* ]]; then; echo updating $dir; update-git-secrets; fi; ); done; ); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-318097599:884,echo,echo,884,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2479#issuecomment-318097599,1,['echo'],['echo']
Availability,"To summarize, these will be the spec mandated minimums. There'll also be configuration parameters in Cromwell to tune these higher if one wants. Cromwell will attempt to check file size *prior* to reading it or pulling it across the network for cloud filesystems. Error messages should be very clear and checked past @katevoss . `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K ; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096:264,Error,Error,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-300349096,1,['Error'],['Error']
Availability,"To wire up the File output, either by running the file delocalisation in the execution script, or by copying the required files to a known mounted volume. Success criteria: we can run a WDL such as:. ```; task fileOut {; command { echo ""hello, amazon"" > myFile }; output { File outFile = ""myFile"" }; }. workflow {; call fileOut; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1569:231,echo,echo,231,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1569,1,['echo'],['echo']
Availability,"ToL: I'm fine if these actors hang as children off the initialization actor or some other _actor_, but not as singletons-in-the-system-jvm-referenced-from-the-non-actor-blaf. Relatedly, the name blaf is a slight misnomer, at it actually creates props, not actor refs, so maybe it should be the blpf? Pinging #1377 to record some of this as food for later thought.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1411#issuecomment-246895349:300,Ping,Pinging,300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1411#issuecomment-246895349,1,['Ping'],['Pinging']
Availability,"ToL:. It'd probably be best to slim down and refactor the old engine `cromwell.CromwellTestkitSpec` to a `cromwell.core.CromwellTestKitSpec` and `cromwell.engine.WorkflowTestKitSpec`. Also, the actor system created in the current `CromwellTestkitSpec` uses a custom configuration. As it doesn't fall back to `ConfigFactory.load()`, it doesn't seem to be support modifying [`akka.test.timefactor`](https://github.com/akka/akka/blob/v2.3.12/akka-testkit/src/main/scala/akka/testkit/TestKit.scala#L712-L714) on the sbt command line.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/906#issuecomment-222010926:36,down,down,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/906#issuecomment-222010926,1,['down'],['down']
Availability,"Today -- there seems to be a lot of failures on both the v1 and v2 backend related to the caching tests. Many call caching tests are structured in a way that a workflow runs one time --and second run of the workflow caches to the results of the first run. The issue here is that if Cromwell restarts during the first run --then its possible the first workflow caches to itself, which causes the test to fail. . AC: To prevent this from happening, we should change test expectations such that the first time a call caching test is run --read_from_cache is set to false so regardless of when the Cromwell restart occurs, the jobs of the first workflow aren't call cached but forced to run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4046:36,failure,failures,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4046,1,['failure'],['failures']
Availability,"Today when using Cromwell to copy cache hits to an RP enabled bucket (as the execution directory), there are issues where the the copying of outputs times out:. ```; ""hitFailures"":[{""e831b105-209d-44a4-b550-92ac77c2076e:test.hello:-1"":[{""message"":""The Cache hit copying actor timed out waiting for a response to copy gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/92241ad8-4f27-4112-826c-8c5230d9a2e0/test/e831b105-209d-44a4-b550-92ac77c2076e/call-hello/stdout to gs://fc-ec2179fb-ab57-4867-b548-b6728060cdef/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/test/102e99b1-26b2-4bf4-80ec-fcc02c32136d/call-hello/stdout"",""causedBy"":[]}]; ```. AC: For the functions used to copy cached outputs, incorporate retry logic when the action fails due to a `400 UserProjectMissing error`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4771:757,error,error,757,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4771,1,['error'],['error']
Availability,"Today, during Job preparation (prior to localization), when Cromwell calls Martha to get information on a dos url, there is a schema (defined in the Cromwell config) which statically extracts an element from an array, which is no longer guaranteed to be a GS url. Ideally, when Martha returns URL info, Cromwell sorts through the array of urls to *extract the first gcs url*, and when there is no GCS url, the job fails with an error message explaining how the DOS url couldn't be resolved into a GCS url. IN addition, Cromwell can also the print the dos url + actual urls associated to the dos url for a user -- so they can easily take both pieces of info for further debugging.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4118:428,error,error,428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4118,1,['error'],['error']
Availability,"Today, when using Cromwell, only 2 GPU types are allowed:. https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L26-L27. However, there's many more gpu types that are actually available:; https://cloud.google.com/compute/docs/gpus/. AC: ; - Cromwell shouldn't whitelist GPU types that are allowed, and instead make it possible to request a GPU machine type of choice.; - If someone asks for a gpu type that's a bad string, record the error returned by Piplines API and add it to our docs to explain that an error that looks like ""..."" means an invalid GPU type, and add a link to the list of available GPU types to the Cromwell docs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4691:313,avail,available,313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4691,4,"['avail', 'error']","['available', 'error']"
Availability,Tolerate optional output files not being present.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3456:0,Toler,Tolerate,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3456,1,['Toler'],['Tolerate']
Availability,"Totally valid workflow fails in the last stage in the latest development version of cromwell because of:; ```; Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types; ```; error.; I enclose both wdl and input. ; [quantification.zip](https://github.com/broadinstitute/cromwell/files/2761544/quantification.zip). The error is the following:. ```json; Workflow failed. WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:1:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2000), WomString(characteristics) -> WomString(number of donors -> 1;age -> 26 years old;tissue -> Kidney;vendor -> Biochain;isolate -> Lot no.: B106007;gender -> Male), WomString(series) -> WomString(GSE69360), WomString(organism) -> WomString(Homo sapiens), WomString(run) -> WomString(SRR2014240), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014240), WomString(name) -> WomString(Biochain_Adult_Kidney), WomString(gsm) -> WomString(GSM1698570), WomString(title) -> WomString(Biochain_Adult_Kidney))), WomString(run) -> WomString(SRR2014240), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/cal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555:188,error,error,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555,3,"['error', 'failure']","['error', 'failure']"
Availability,"Tracking production issue that occurred on May 30th at 6pm. Cromwell had 65K files open, reboot resolved the issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3716:89,reboot,reboot,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3716,1,['reboot'],['reboot']
Availability,"Transient ""failure"" in metadata during Abort",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4484:11,failure,failure,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4484,1,['failure'],['failure']
Availability,Transient error occasionally causes cromwell to have an error even though the workflow completed successfully,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2079:10,error,error,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2079,2,['error'],['error']
Availability,Transient failures uploading auth file,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2009:10,failure,failures,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2009,1,['failure'],['failures']
Availability,"Travis passed but Github is hanging getting the status back, possibly due to earlier github downtime (overheard in Slack)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4730#issuecomment-472146747:92,downtime,downtime,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4730#issuecomment-472146747,1,['downtime'],['downtime']
Availability,"Tried to pass string to wdl input of type ""File"", got no error and workflow failed.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1561:57,error,error,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1561,1,['error'],['error']
Availability,"Tried to pass string to wdl input of type ""File"", got no error and workflow failed. . I ran this in firecloud, and my expression for ""id"" evaluated as a string. Here's my wdl:. ```; task get_mutations_across_intervals {; File id; File orignal_maf; Array[File] mafs. command {; python /opt/make_intervals_v2.py ${id} ${orignal_maf} ${sep="" "" mafs}; }. runtime {; docker: ""gcr.io/broad-firecloud-itools/pysam""; }. output {; File interval_maf = ""${id}.forecallready.maf""; }; }. workflow get_mutations_across_intervals_wkfl {; call get_mutations_across_intervals; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1561:57,error,error,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1561,1,['error'],['error']
Availability,"Tried to re-run the 10K JG workflow with CC on, the workflow failed almost immediately with multiple errors like; ```; [ERROR] [04/18/2017 21:11:44.685] [cromwell-system-akka.dispatchers.service-dispatcher-86] [akka://cromwell-system/user/cromwell-service/ServiceRegistryActor/MetadataService/metadata-summary-actor] Failed to summarize metadata; java.util.concurrent.RejectedExecutionException: Task slick.basic.BasicBackend$DatabaseDef$$anon$2@64919660 rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@1dce40e4[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 800]; 	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047); 	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823); 	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369); 	at slick.util.AsyncExecutor$$anon$2$$anon$3.execute(AsyncExecutor.scala:120); 	at slick.basic.BasicBackend$DatabaseDef$class.runSynchronousDatabaseAction(BasicBackend.scala:233); 	at slick.jdbc.JdbcBackend$DatabaseDef.runSynchronousDatabaseAction(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.runInContext(BasicBackend.scala:210); 	at slick.jdbc.JdbcBackend$DatabaseDef.runInContext(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.run$1(BasicBackend.scala:153); 	at slick.basic.BasicBackend$DatabaseDef$class.runInContext(BasicBackend.scala:157); 	at slick.jdbc.JdbcBackend$DatabaseDef.runInContext(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.runInContext(BasicBackend.scala:179); 	at slick.jdbc.JdbcBackend$DatabaseDef.runInContext(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.runInternal(BasicBackend.scala:78); 	at slick.jdbc.JdbcBackend$DatabaseDef.runInternal(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$class.run(BasicBackend.scala:75); 	at slick.jdbc.JdbcBackend$DatabaseDef.run(JdbcBackend.scala:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182:101,error,errors,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182,2,"['ERROR', 'error']","['ERROR', 'errors']"
Availability,"Trivially happened again, same error... Output of `Ctl-\`:. ```; ""shutdownHook1"" #44 prio=5 os_prio=0 tid=0x00007fdbcc9ce000 nid=0x10a8 waiting on condition [0x00007fd9ccfce000]; java.lang.Thread.State: TIMED_WAITING (sleeping); at java.lang.Thread.sleep(Native Method); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$addShutdownHook$1.apply$mcV$sp(WorkflowManagerActor.scala:125); at scala.sys.ShutdownHookThread$$anon$1.run(ShutdownHookThread.scala:34). ""pool-1-thread-20"" #95 prio=5 os_prio=0 tid=0x00007fdaa80c0000 nid=0xa56 waiting on condition [0x00007fda90575000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""pool-1-thread-19"" #94 prio=5 os_prio=0 tid=0x00007fdaa80be800 nid=0xa55 waiting on condition [0x00007fda90676000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b9a2568> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039); at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.ja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:31,error,error,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['error'],['error']
Availability,"Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:47); 	at cromwell.CommandLineParser$.runCromwell(CommandLineParser.scala:95); 	at cromwell.CommandLineParser$.delayedEndpoint$cromwell$CommandLineParser$1(CommandLineParser.scala:105); 	at cromwell.CommandLineParser$delayedInit$body.apply(CommandLineParser.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CommandLineParser$.main(CommandLineParser.scala:8); 	at cromwell.CommandLineParser.main(CommandLineParser.scala); Caused by: java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 5004ms.; 	at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145); 	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:18); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:439); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:47); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:218); 	at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:217); 	at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:38); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:239); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:2613,avail,available,2613,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,1,['avail'],['available']
Availability,Trying to fix a CI error by merging with the main development branch.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-497016193:19,error,error,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4977#issuecomment-497016193,1,['error'],['error']
Availability,"Trying to follow example here: http://gatkforums.broadinstitute.org/wdl/discussion/7221/2-howto-write-a-simple-multi-step-workflow. I editted the json file (simpleVariantSelection_inputs.json) with the full paths on my local machine and replacing the ""File"" and ""String"" placeholders. However when I run I am getting the following error:. [2017-05-25 12:18:24,85] [info] WorkflowManagerActor Successfully started WorkflowActor-e52409b4-c85a-4285-9453-f47c6b0ae86c; [2017-05-25 12:18:24,85] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-05-25 12:18:24,93] [error] WorkflowManagerActor Workflow e52409b4-c85a-4285-9453-f47c6b0ae86c failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Unable to load namespace from workflow: ERROR: Finished parsing without consuming all tokens. {; ^. cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3$$anon$1: Workflow input processing failed:; Unable to load namespace from workflow: ERROR: Finished parsing without consuming all tokens. {; ^. 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:137); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anonfun$3.applyOrElse(MaterializeWorkflowDescriptorActor.scala:129); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); 	at akka.actor.FSM$class.processEvent(FSM.scala:663); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.akka$actor$LoggingFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:116); 	at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); 	at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:116); 	at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); 	at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); 	at akka.actor.Actor$c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2296:331,error,error,331,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2296,4,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Trying to print stdout and stderr to files so that I can see what my command is doing. I broke it down to the simple example shown in the docs. . ```task echo {; command <<< ; echo ""hello world"" ; echo ""another world""; >&2 echo ""hello world""; >>>; output {; File message = stdout(); File message2 = stderr(); }; }; ```. Both stdout and stderr return empty. My goal is to capture all stdout and stderr from whatever is in the command section. . On real workflows, I also get a bunch of nonsense in the stdout stderr logs. If my run fails I want to know what caused it. Using cromwell version 72. Any help?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6686:98,down,down,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6686,5,"['down', 'echo']","['down', 'echo']"
Availability,"Trying to read an object that has the same name as a variable causes Cromwell to abort not just the workflow, but the entire Cromwell instance; WDL file:; ```; task TestTask {; 	; 	command {; 		echo ""Hello World!"" > hello_world.txt; 	}. 	output {; 		File exists = ""hello_world.txt""; 	}. 	runtime {; 		docker: will_fail.docker; 		memory: will_fail.memory; 		disks: ""local-disk "" + will_fail.small_disk + "" HDD""; 	}; }. workflow KillsCromwell {; 	String test_string. # This here kills Cromwell; 	Object runtime_params = read_object(runtime_params). 	call TestTask ; }; ```. Inputs File:; ```; {; 	""KillsCromwell.test_string"": ""This is a string"",; 	""KillsCromwell.runtime_params"": {; 		""genomes_cloud_image"": ""broadinstitute/genomes-in-the-cloud:2.2.4-1469632282"",; 		""small_disk"": 100,; 		""medium_disk"": 200,; 		""large_disk"": 300,; 		""x_large_disk"": 400,; 		""preemptible_tries"": 3; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1946:194,echo,echo,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1946,1,['echo'],['echo']
Availability,Trying to recover space to prevent us hitting the 10 TB limit.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4880:10,recover,recover,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4880,1,['recover'],['recover']
Availability,"TumorAlignment.known_indels_sites_VCFs' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_mu' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_alt' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_bwt' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.known_indels_sites_indices' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_dict' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_bed' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_fasta' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.dbSNP_vcf_index' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.dbSNP_vcf' not specified.""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. But once I filled these out in my inputs json I then got this error. ```; [; {; causedBy: [; {; causedBy: [ ],; message: ""Missing inputs for subworkflow call SomaticRoot.TumorAlignment at index None: read_length, ref_fasta, agg_preemptible_tries, ref_dict, haplotype_database_file, ref_alt, ref_ann, known_indels_sites_indices, dbSNP_vcf, ref_sa, dbSNP_vcf_index, unmapped_bam_suffix, ref_amb, contamination_sites_ud, contamination_sites_bed, ref_bwt, ref_fasta_index, increase_disk_size, fingerprint_genotypes_file, preemptible_tries, known_indels_sites_VCFs, contamination_sites_mu, wgs_coverage_interval_list, ref_pac.""; }; ],; message: ""Couldn't resolve all inputs for SomaticRoot.TumorAlignment at index None.""; }; ],; ```. I think I'm passing in everything correctly but it could be an error on my part as well.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2912:3077,error,error,3077,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2912,2,['error'],['error']
Availability,"Two days ago I successfully ran my first wdl on Cromwell using the Google Pipelines API. Then I tried to change my service account and it broke. I'm not able to get it running anymore at all. Stacktrace can be seen below, the error is ""Scopes not configured for service account."". **stdout**. ```; ...; tsv_string += '\n' + ""unmapped"". with open(""sequence_grouping_with_unmapped.txt"",""w"") as tsv_file_with_unmapped:; tsv_file_with_unmapped.write(tsv_string); tsv_file_with_unmapped.close(); CODE`; 2018-05-25 12:55:24,629 cromwell-system-akka.dispatchers.backend-dispatcher-137 ERROR - Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; java.io.IOException: Scopes not configured for service account. Scoped should be specified by calling createScoped or passing scopes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:342); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:160); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:146); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:96); 	at cromwell.cloudsupport.gcp.genomics.GenomicsFactory$$anon$1.initialize(GenomicsFactory.scala:18); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:93); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:300); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:277); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$PAPIRunCreationRequest.httpRequest$lzycompute(JesApiQueryManager.scala:293); 	at cromwell.backend.impl.jes.statuspolling.JesApiQueryManager$PAPIRunCreationRequest.httpRequest(JesApiQueryManager.scala:293); 	at cromwell.backend.impl.jes.statuspo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3690:226,error,error,226,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3690,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Two errors on the testing:; ```; - should successfully run curl *** FAILED *** (1 minute, 37 seconds); centaur.test.CentaurTestException: Unexpected terminal status Failed but was waiting for Succeeded (workflow ID: bb88b541-3f1a-490c-9121-7685b4ab54b3). Metadata 'failures' content: [; {; ""causedBy"" : [; {; ""causedBy"" : [; ],; ""message"" : ""Job curl_wf.newsgrab:NA:1 exited with return code 6 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.""; }; ],; ""message"" : ""Workflow failed""; }; ]; ```. ```; info] PubSubMetadataServiceActorSpec:; [info] A PubSubMetadataActor with a subscription should ; [info] - should create the requested subscription *** FAILED *** (17 milliseconds); [info] java.lang.AssertionError: received 1 excess messages on InfoFilter(None,Left(Creating subscription baz),true); [info] at akka.testkit.EventFilter.intercept(TestEventListener.scala:116); [info] at cromwell.services.metadata.impl.pubsub.PubSubMetadataServiceActorSpec.$anonfun$new$9(PubSubMetadataServiceActorSpec.scala:40); [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); [info] at org.scalatest.Transformer.apply(Transformer.scala:22); [info] at org.scalatest.Transformer.apply(Transformer.scala:20); [info] at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); [info] at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); [info] at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); ```; I don't see how these are caused by this PR. I would gladly fix them if I would know how.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504347025:4,error,errors,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-504347025,2,"['error', 'failure']","['errors', 'failures']"
Availability,"Two independent ideas. a) do less work in each heartbeat transaction (say just one workflow I’d) and then commit so you’re holding fewer locks. B) rather that updating a single table with the latest heartbeat, how about an append only table with the heartbeats. Then there are no locks. When looking for workflows; To start just join to that table and get max(heartbeat time). > On Aug 20, 2018, at 4:25 PM, Adam Nichols <notifications@github.com> wrote:; > ; > Good idea, it's always possible it could still deadlock on an index; > ; > —; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub, or mute the thread.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4022#issuecomment-414458430:47,heartbeat,heartbeat,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022#issuecomment-414458430,4,['heartbeat'],"['heartbeat', 'heartbeats']"
Availability,"Two new centaur tests (test types actually):. * Simulates the transition from PAPI v1 to PAPI v2. Start with an FC-like config where PAPI v1 is the default `Papi` backend and PAPI v2 is available in a `Papiv2` backend. Launch a workflow into this system with two sequential calls. Shut down Cromwell once the second call starts. Tweak the Cromwell config to add `name-for-call-caching-purposes = ""Papi""` to the `Papiv2` backend. Bring Cromwell back up and let the previously-running workflow finish. Then launch a second workflow identical to the first except that its options request `{ ""backend"": ""Papiv2""}`. Let this workflow finish and confirm it call cached both calls from the first workflow despite running on different versions of PAPI. * Simulates the new steady-state after the v2 transition when new workflows are submitted with `{""backend"": ""Papiv2""}`. Run a workflow, shut down Cromwell in the middle of its execution, then bring Cromwell back up. Confirm this workflow completes successfully. Launch a second copy of the workflow also requesting `{""backend"": ""Papiv2""}` and confirm it completes successfully and caches to all the calls of the first run and that both executed on PAPI v2.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4490:186,avail,available,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4490,3,"['avail', 'down']","['available', 'down']"
Availability,"Typically, I do that through the Cloud Logging Console, instead of fetching the entire log (which could be huge, and expensive) ;); There, you can set up filters to narrow down on particular log entries. `iam.serviceAccountUser` is mostly about granting one `iam.serviceAccounts.actAs` permission on a service account. Not sure why it doesn't show up here, but this permission is required for the Cromwell server to be able to run a pipeline with a Compute SA. BTW `iam.serviceAccountUser` **should** be granted on a per-service-account level, not at the project level (not sure if you've set it up this way, just wanted to confirm). First make sure you don't have that permission granted at the project level, and then if you remove it from the service-account level, it should be able to be seen in the logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686056109:172,down,down,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-686056109,1,['down'],['down']
Availability,UPDATE: If we can make this logic retry all 500 errors but leave any other new `IOException`s un-retried I think that's the best way to go for now. Does that sounds feasible? Thanks!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521284310:48,error,errors,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521284310,1,['error'],['errors']
Availability,"URL:; https://github.com/broadinstitute/cromwell/releases/download/28/cromwell-28.jar. Expected SHA-256: 25c6c30fe062fb4a8384ef82aa5313ad5a5ee05eae62a2c831219d7c6c756347. Actual SHA-256:; c9ce762df236588ded042ceaf099848c0c1685d34788442ee251615ff13b5190. The expected checksum is what what we had in Homebrew for the SHA-256 when the formula was upgraded to version 28 on Fri Jun 30 12:44:57 2017 -0700. See https://github.com/Homebrew/homebrew-core/pull/15166. But currently downloading the file gives a different checksum. I wanted to make sure you weren't hacked, and ask what the reason(s) for the changes were.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2463:58,down,download,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2463,2,['down'],"['download', 'downloading']"
Availability,"UUID(dfefc8c0)]: Parsing workflow as CWL v1.0; 2018-12-10 13:27:22,372 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/main-prealign.cwl; 2018-12-10 13:31:56,222 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/organize_noalign.cwl; 2018-12-10 13:32:14,196 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/prep_samples_to_rec.cwl; 2018-12-10 13:32:32,071 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/prep_samples.cwl; 2018-12-10 13:32:49,793 INFO - Pre-Processing https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/postprocess_alignment_to_rec.cwl; 2018-12-10 13:33:07,284 cromwell-system-akka.dispatchers.engine-dispatcher-34 ERROR - WorkflowManagerActor Workflow dfefc8c0-c3a1-449c-a747-13147bf8b980 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Failed to run cwltool on file https://raw.githubusercontent.com/bcbio/test_bcbio_cwl/master/prealign/prealign-workflow/steps/postprocess_alignment_to_rec.cwl (reason 1 of 1): Traceback (most recent call last):; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/heterodon/__init__.py"", line 24, in apply; File ""<string>"", line 1, in <module>; File ""<string>"", line 11, in cwltool_salad; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/cwltool/load_tool.py"", line 113, in fetch_document; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 933, in fetch; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/schema_salad/ref_resolver.py"", line 933, in fetch; File ""/app/cromwell-37-416c665-SNAP.jar/Lib/ruamel/yaml/main.py"", line 948, in round_trip_load; File ""/app/cromwell-37-416c665-SNAP.ja",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-445825939:1183,ERROR,ERROR,1183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4235#issuecomment-445825939,1,['ERROR'],['ERROR']
Availability,"Uh, if this is causing test failures, then we have something very wrong in our tests",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/718#issuecomment-212404457:28,failure,failures,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/718#issuecomment-212404457,1,['failure'],['failures']
Availability,"Uh, isn't gsa4 supposed to be reserved for GATK automated test suites and release machinery? Please don't use it as an experimental pod racer or anything like that. If you take it down it affects user-facing systems.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277984572:180,down,down,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277984572,1,['down'],['down']
Availability,Unable to complete PAPI request due to system or connection error (Unknown Error.),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7482:60,error,error,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7482,2,"['Error', 'error']","['Error', 'error']"
Availability,Unable to recover running jobs in AWS backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4857:10,recover,recover,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4857,1,['recover'],['recover']
Availability,Unable to run multiple cromwell jobs in run mode due to connection not being available,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6208:77,avail,available,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6208,1,['avail'],['available']
Availability,Unclear error when dealing with File evaluates to empty string,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4158:8,error,error,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4158,1,['error'],['error']
Availability,Unhelpful Error Message,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4570:10,Error,Error,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4570,1,['Error'],['Error']
Availability,Unify RP error check so it also applies to singleton files [BA-6235],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5406:9,error,error,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5406,1,['error'],['error']
Availability,Unit test failure was `WorkflowOutputsSpec` which is a known flake (should fix). Restarted.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1687168722:10,failure,failure,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7178#issuecomment-1687168722,1,['failure'],['failure']
Availability,"Unless I configured something improperly, all output from stdout() is doubled when running with SLURM. Example pipeline:. ```; version 1.0. # WORKFLOW DEFINITION; workflow WholeGenomeGermlineSingleSample {; call SumFloats; output {; Float out = SumFloats.total_size; }; }. task SumFloats {; input {; Array[Float] sizes = [1,2,3,4,5.0]; Int preemptible_tries=3; }. command <<<; python -c ""print ~{sep=""+"" sizes}""; >>>; output {; Float total_size = read_float(stdout()); }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/python:2.7""; preemptible: preemptible_tries; }; }; ```. The error raised with cromwell-53 is:; Failed to read_float(""/data/og/ted/cromwell-executions/WholeGenomeGermlineSingleSample/00090ef9-5211-4f18-9de9-daf3de791408/call-SumFloats/execution/stdout"") (reason 1 of 1): For input string: ""15.0; 15.0""; The stdout file truly contains this. Running with local backend returns no error.; Contents of conf file:. ```; backend {; default = ""SLURM""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; include required(classpath(""reference_local_provider_config.inc.conf"")); concurrent-job-limit = 30; }; }; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpu = 1; Int requested_memory_mb_per_core = 8000; Int memory_mb = 4000; String queue = ""short""; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpu} \; --mem ${memory_mb} \; --wrap ""/bin/bash ${script}""; """"""; submit-docker = """"""; docker pull ${docker}. sbatch -J ${job_name} -D ${cwd} -o ${cwd}/execution/stdout -e ${cwd}/execution/stderr -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpu} \; --mem ${memory_mb} \; --wrap ""docker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_cwd}/execution/script""; """""". kill = ""scancel ${job_id}""; check-alive = ""sc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5932:578,error,error,578,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5932,2,['error'],['error']
Availability,"Unspecified type (Unspecified version) workflow a15c46b7-5f93-46d6-94a2-28f656914866 submitted; ...; [2021-08-13 10:44:56,46] [info] Request manager PAPIQueryManager created new PAPI request worker PAPIQueryWorker-58e6b395-916e-4ba4-965a-0ec8f1c0760d with batch interval of 3333 milliseconds; ...; [2021-08-13 10:44:56,67] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Parsing workflow as WDL draft-2; [2021-08-13 10:44:58,79] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Call-to-Backend assignments: wf_hello.hello -> PAPIv2; [2021-08-13 10:45:00,31] [info] Not triggering log of token queue status. Effective log interval = None; [2021-08-13 10:45:01,35] [info] WorkflowExecutionActor-a15c46b7-5f93-46d6-94a2-28f656914866 [a15c46b7]: Starting wf_hello.hello; [2021-08-13 10:45:02,34] [info] Assigned new job execution tokens to the following groups: a15c46b7: 1; [2021-08-13 10:45:04,75] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: echo ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; [2021-08-13 10:45:05,68] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); [2021-08-13 10:45:07,36] [error] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedRe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:2621,echo,echo,2621,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['echo'],['echo']
Availability,"Until we work this out, we shouldn't risk accidentally CCing, or spamming awful error messages.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3125:80,error,error,80,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3125,1,['error'],['error']
Availability,"Unwired the `DockerHashLookupWorkerActor`.; Removed the unused docker registry client.; Cleaned up spray dependencies now that the spray-client is no longer used.; Refactored places that were sending spray classes down into the business logic.; Turned back on deprecation warnings.; Fixed scalaz flatMap deprecation warning by adding Y.A. implicit import.; Undeprecated `TerminalUtil` used by the single workflow runner's pretty printer.; Removed empty files from git.; Bumped timeout for `SharedFileSystemJobExecutionActorSpec.recoverSpec` up to 10 seconds dilated.; Increased sleep for `WorkflowExecutionActorSpec.""retry a job 2 times""` up to 3 seconds dilated.; Updated scalatest to 3.0.0.; Removed leftover bits of `DontUseMainSpecTest`.; Printing test times, minimal stack traces, and resummarizing tests failures, via http://www.scalatest.org/user_guide/running_your_tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1338:214,down,down,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1338,3,"['down', 'failure', 'recover']","['down', 'failures', 'recoverSpec']"
Availability,Unzip CWL Dependencies and make available to pre-processing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2777:32,avail,available,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2777,2,['avail'],['available']
Availability,Up to 15 total failures,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/826#issuecomment-228104876:15,failure,failures,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/826#issuecomment-228104876,1,['failure'],['failures']
Availability,Update requester-pays error detection [BT-574],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6689:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6689,1,['error'],['error']
Availability,Updated GCR integration test to check for additional errors.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/366:53,error,errors,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/366,1,['error'],['errors']
Availability,"Updated to be way more specific in targeting Denis-reported errors. I've found this area can get difficult to reason about when we throw in ranges and very broad regexes, so I chose to be incredibly specific at the cost of more code & slightly less functionality (but Denis's functionality is there).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-566633999:60,error,errors,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5321#issuecomment-566633999,1,['error'],['errors']
Availability,"Updates ; * [ch.qos.logback:logback-access](https://github.com/qos-ch/logback); * [ch.qos.logback:logback-classic](https://github.com/qos-ch/logback); * [ch.qos.logback:logback-core](https://github.com/qos-ch/logback). from 1.2.11 to 1.4.5. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""ch.qos.logback"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""ch.qos.logback"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7026:945,down,down,945,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7026,1,['down'],['down']
Availability,"Updates ; * [com.dimafeng:testcontainers-scala-mariadb](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-mysql](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-postgresql](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-scalatest](https://github.com/testcontainers/testcontainers-scala). from 0.40.10 to 0.40.12.; [GitHub Release Notes](https://github.com/testcontainers/testcontainers-scala/releases/tag/v0.40.12) - [Version Diff](https://github.com/testcontainers/testcontainers-scala/compare/v0.40.10...v0.40.12). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.dimafeng"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.dimafeng"" }; }]; ```; </details>. labels: test-library-update, early-semver-minor, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7031:1356,down,down,1356,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7031,1,['down'],['down']
Availability,"Updates ; * [com.dimafeng:testcontainers-scala-mariadb](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-mysql](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-postgresql](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-scalatest](https://github.com/testcontainers/testcontainers-scala). from 0.40.2 to 0.40.10.; [GitHub Release Notes](https://github.com/testcontainers/testcontainers-scala/releases/tag/v0.40.10) - [Version Diff](https://github.com/testcontainers/testcontainers-scala/compare/v0.40.2...v0.40.10). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.dimafeng"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.dimafeng"" }; }]; ```; </details>. labels: test-library-update, early-semver-minor, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6849:1354,down,down,1354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6849,1,['down'],['down']
Availability,"Updates ; * [com.google.api-client:google-api-client-jackson2](https://github.com/googleapis/google-api-java-client); * [com.google.api-client:google-api-client-java6](https://github.com/googleapis/google-api-java-client). from 1.33.2 to 1.33.4.; [GitHub Release Notes](https://github.com/googleapis/google-api-java-client/releases/tag/v1.33.4) - [Version Diff](https://github.com/googleapis/google-api-java-client/compare/v1.33.2...v1.33.4). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api-client"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.api-client"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6853:1154,down,down,1154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6853,1,['down'],['down']
Availability,"Updates ; * [com.typesafe.akka:akka-http](https://github.com/akka/akka-http); * [com.typesafe.akka:akka-http-spray-json](https://github.com/akka/akka-http); * [com.typesafe.akka:akka-http-testkit](https://github.com/akka/akka-http). from 10.1.15 to 10.2.9.; [GitHub Release Notes](https://github.com/akka/akka-http/releases/tag/v10.2.9) - [Version Diff](https://github.com/akka/akka-http/compare/v10.1.15...v10.2.9). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Applied Scalafix Migrations</summary>. * com.typesafe.akka:akka-http.*:10.2.0 (created no change); * dependency:MigrateToServerBuilder@com.typesafe.akka:akka-http-scalafix-rules:10.2.0; * Documentation: https://doc.akka.io/docs/akka-http/10.2/migration-guide/migration-guide-10.2.x.html#akka-http-10-1-x-10-2-0; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.typesafe.akka"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.typesafe.akka"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, scalafix-migrations, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6869:1468,down,down,1468,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6869,1,['down'],['down']
Availability,"Updates ; * [com.typesafe.slick:slick](https://github.com/slick/slick); * [com.typesafe.slick:slick-hikaricp](https://github.com/slick/slick). from 3.4.0-M1 to 3.4.0.; [GitHub Release Notes](https://github.com/slick/slick/releases/tag/v3.4.0) - [Version Diff](https://github.com/slick/slick/compare/v3.4.0-M1...v3.4.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.typesafe.slick"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.typesafe.slick"" }; }]; ```; </details>. labels: library-update, early-semver-pre-release, semver-spec-pre-release, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6872:1028,down,down,1028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6872,1,['down'],['down']
Availability,"Updates ; * [com.typesafe.slick:slick](https://github.com/slick/slick); * [com.typesafe.slick:slick-hikaricp](https://github.com/slick/slick). from 3.4.0-M1 to 3.4.1.; [GitHub Release Notes](https://github.com/slick/slick/releases/tag/v3.4.1) - [Version Diff](https://github.com/slick/slick/compare/v3.4.0-M1...v3.4.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.typesafe.slick"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.typesafe.slick"" }; }]; ```; </details>. labels: library-update, early-semver-pre-release, semver-spec-pre-release, version-scheme:pvp, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7045:1028,down,down,1028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7045,1,['down'],['down']
Availability,"Updates ; * [io.circe:circe-core](https://github.com/circe/circe); * [io.circe:circe-generic](https://github.com/circe/circe); * [io.circe:circe-generic-extras](https://github.com/circe/circe-generic-extras); * [io.circe:circe-literal](https://github.com/circe/circe); * [io.circe:circe-parser](https://github.com/circe/circe); * [io.circe:circe-refined](https://github.com/circe/circe); * [io.circe:circe-shapes](https://github.com/circe/circe). from 0.14.1 to 0.14.2.; [GitHub Release Notes](https://github.com/circe/circe/releases/tag/v0.14.2) - [Version Diff](https://github.com/circe/circe/compare/v0.14.1...v0.14.2). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""io.circe"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6874:1321,down,down,1321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6874,1,['down'],['down']
Availability,"Updates ; * [io.circe:circe-core](https://github.com/circe/circe); * [io.circe:circe-generic](https://github.com/circe/circe); * [io.circe:circe-literal](https://github.com/circe/circe); * [io.circe:circe-parser](https://github.com/circe/circe); * [io.circe:circe-refined](https://github.com/circe/circe); * [io.circe:circe-shapes](https://github.com/circe/circe). from 0.14.1 to 0.14.4.; [GitHub Release Notes](https://github.com/circe/circe/releases/tag/v0.14.4) - [Version Diff](https://github.com/circe/circe/compare/v0.14.1...v0.14.4). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, version-scheme:early-semver, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7048:1239,down,down,1239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7048,1,['down'],['down']
Availability,"Updates ; * [io.github.jbwheatley:pact4s-circe](https://github.com/jbwheatley/pact4s); * [io.github.jbwheatley:pact4s-scalatest](https://github.com/jbwheatley/pact4s). from 0.7.0 to 0.8.0.; [GitHub Release Notes](https://github.com/jbwheatley/pact4s/releases/tag/v0.8.0) - [Version Diff](https://github.com/jbwheatley/pact4s/compare/v0.7.0...v0.8.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.github.jbwheatley"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.github.jbwheatley"" }; }]; ```; </details>. labels: library-update, early-semver-major, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7051:1061,down,down,1061,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7051,1,['down'],['down']
Availability,"Updates ; * [org.http4s:http4s-blaze-client](https://github.com/http4s/http4s); * [org.http4s:http4s-circe](https://github.com/http4s/http4s); * [org.http4s:http4s-dsl](https://github.com/http4s/http4s). from 0.21.31 to 0.21.33.; [GitHub Release Notes](https://github.com/http4s/http4s/releases/tag/v0.21.33) - [Version Diff](https://github.com/http4s/http4s/compare/v0.21.31...v0.21.33). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.http4s"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.http4s"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6885:1089,down,down,1089,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6885,1,['down'],['down']
Availability,"Updates ; * [org.http4s:http4s-blaze-client](https://github.com/http4s/http4s); * [org.http4s:http4s-circe](https://github.com/http4s/http4s); * [org.http4s:http4s-dsl](https://github.com/http4s/http4s). from 0.21.31 to 0.21.34.; [GitHub Release Notes](https://github.com/http4s/http4s/releases/tag/v0.21.34) - [Version Diff](https://github.com/http4s/http4s/compare/v0.21.31...v0.21.34). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.http4s"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.http4s"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, version-scheme:early-semver, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7067:1089,down,down,1089,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7067,1,['down'],['down']
Availability,"Updates ; * [org.typelevel:alleycats-core](https://github.com/typelevel/cats); * [org.typelevel:cats-core](https://github.com/typelevel/cats). from 2.7.0 to 2.8.0.; [GitHub Release Notes](https://github.com/typelevel/cats/releases/tag/v2.8.0) - [Version Diff](https://github.com/typelevel/cats/compare/v2.7.0...v2.8.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (2.7.0).; You might want to review and update them manually.; ```; services/src/test/scala/cromwell/services/database/QueryTimeoutSpec.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.typelevel"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6897:1319,down,down,1319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6897,1,['down'],['down']
Availability,"Updates ; * [org.typelevel:alleycats-core](https://github.com/typelevel/cats); * [org.typelevel:cats-core](https://github.com/typelevel/cats). from 2.7.0 to 2.9.0.; [GitHub Release Notes](https://github.com/typelevel/cats/releases/tag/v2.9.0) - [Version Diff](https://github.com/typelevel/cats/compare/v2.7.0...v2.9.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (2.7.0).; You might want to review and update them manually.; ```; services/src/test/scala/cromwell/services/database/QueryTimeoutSpec.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.typelevel"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7077:1319,down,down,1319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7077,1,['down'],['down']
Availability,"Updates ; * ch.qos.logback:logback-access; * ch.qos.logback:logback-classic; * ch.qos.logback:logback-core. from 1.2.10 to 1.2.11. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""ch.qos.logback"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""ch.qos.logback"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6842:835,down,down,835,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6842,1,['down'],['down']
Availability,"Updates ; * com.fasterxml.jackson.core:jackson-annotations; * [com.fasterxml.jackson.core:jackson-core](https://github.com/FasterXML/jackson-core); * com.fasterxml.jackson.core:jackson-databind. from 2.13.3 to 2.13.5. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.fasterxml.jackson.core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.fasterxml.jackson.core"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7033:934,down,down,934,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7033,1,['down'],['down']
Availability,"Updates ; * org.slf4j:jcl-over-slf4j; * org.slf4j:jul-to-slf4j; * org.slf4j:log4j-over-slf4j; * org.slf4j:slf4j-api. from 1.7.32 to 1.7.36. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.slf4j"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.slf4j"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6896:839,down,down,839,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6896,1,['down'],['down']
Availability,"Updates ; * software.amazon.awssdk:batch; * software.amazon.awssdk:cloudwatchlogs; * software.amazon.awssdk:core; * software.amazon.awssdk:s3; * software.amazon.awssdk:sts. from 2.17.194 to 2.17.265. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6901:912,down,down,912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6901,1,['down'],['down']
Availability,"Updates ; * software.amazon.awssdk:batch; * software.amazon.awssdk:cloudwatchlogs; * software.amazon.awssdk:core; * software.amazon.awssdk:s3; * software.amazon.awssdk:sts. from 2.17.265 to 2.17.295. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""software.amazon.awssdk"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7082:912,down,down,912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7082,1,['down'],['down']
Availability,"Updates ; * software.amazon.awssdk:core; * software.amazon.awssdk:s3; * software.amazon.awssdk:sts. from 2.17.152 to 2.17.172. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/e5600937e537c0b06266c2860dfad5605a4de5ef/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6734:839,down,down,839,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6734,1,['down'],['down']
Availability,"Updates ; * software.amazon.awssdk:s3; * software.amazon.awssdk:sts. from 2.17.152 to 2.17.173. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/5ba079e5e6b075ded705306e58f3111e16796466/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6738:808,down,down,808,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6738,1,['down'],['down']
Availability,"Updates Cromwell with the latest changes from wdl4s; I was able to unignore half of the unit tests that were previously ignored, and re-enable 60 centaur tests (on local and TES).; I haven't looked at the JES failures yet, so this is not be merged until they're fixed, but I though I would make a PR a bit early as this re-wires a decent amount of stuff that was broken due to missing FQNs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2680:209,failure,failures,209,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2680,1,['failure'],['failures']
Availability,"Updates [com.azure.resourcemanager:azure-resourcemanager](https://github.com/Azure/azure-sdk-for-java) from 2.17.0 to 2.18.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure.resourcemanager"", artifactId = ""azure-resourcemanager"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.azure.resourcemanager"", artifactId = ""azure-resourcemanager"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6847:879,down,down,879,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6847,1,['down'],['down']
Availability,"Updates [com.azure.resourcemanager:azure-resourcemanager](https://github.com/Azure/azure-sdk-for-java) from 2.18.0 to 2.24.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure.resourcemanager"", artifactId = ""azure-resourcemanager"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure.resourcemanager"", artifactId = ""azure-resourcemanager"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7029:879,down,down,879,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7029,1,['down'],['down']
Availability,"Updates [com.azure:azure-core-management](https://github.com/Azure/azure-sdk-for-java) from 1.7.0 to 1.7.1. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core-management"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core-management"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6843:845,down,down,845,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6843,1,['down'],['down']
Availability,"Updates [com.azure:azure-core-management](https://github.com/Azure/azure-sdk-for-java) from 1.7.1 to 1.10.1. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.7.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core-management"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core-management"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7027:1095,down,down,1095,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7027,1,['down'],['down']
Availability,"Updates [com.azure:azure-identity](https://github.com/Azure/azure-sdk-for-java) from 1.4.2 to 1.4.6. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-identity"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-identity"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6844:831,down,down,831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6844,1,['down'],['down']
Availability,"Updates [com.azure:azure-identity](https://github.com/Azure/azure-sdk-for-java) from 1.4.6 to 1.8.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-identity"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-identity"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7028:831,down,down,831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7028,1,['down'],['down']
Availability,"Updates [com.azure:azure-security-keyvault-secrets](https://github.com/Azure/azure-sdk-for-java) from 4.3.7 to 4.3.8. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-security-keyvault-secrets"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-security-keyvault-secrets"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6845:865,down,down,865,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6845,1,['down'],['down']
Availability,"Updates [com.azure:azure-storage-blob-nio](https://github.com/Azure/azure-sdk-for-java) from 12.0.0-beta.18 to 12.0.0-beta.19. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-storage-blob-nio"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-storage-blob-nio"" }; }]; ```; </details>. labels: library-update, early-semver-pre-release, semver-spec-pre-release, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6846:865,down,down,865,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6846,1,['down'],['down']
Availability,"Updates [com.chuusai:shapeless](https://github.com/milessabin/shapeless) from 2.3.7 to 2.3.9.; [GitHub Release Notes](https://github.com/milessabin/shapeless/releases/tag/v2.3.9) - [Version Diff](https://github.com/milessabin/shapeless/compare/v2.3.7...v2.3.9). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.chuusai"", artifactId = ""shapeless"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.chuusai"", artifactId = ""shapeless"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6848:989,down,down,989,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6848,1,['down'],['down']
Availability,"Updates [com.chuusai:shapeless](https://github.com/milessabin/shapeless) from 2.3.9 to 2.3.10.; [GitHub Release Notes](https://github.com/milessabin/shapeless/releases/tag/v2.3.10) - [Version Diff](https://github.com/milessabin/shapeless/compare/v2.3.9...v2.3.10). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.chuusai"", artifactId = ""shapeless"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.chuusai"", artifactId = ""shapeless"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, version-scheme:pvp, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7030:992,down,down,992,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7030,1,['down'],['down']
Availability,"Updates [com.eed3si9n:sbt-assembly](https://github.com/sbt/sbt-assembly) from 1.1.1 to 1.2.0.; [GitHub Release Notes](https://github.com/sbt/sbt-assembly/releases/tag/v1.2.0) - [Version Diff](https://github.com/sbt/sbt-assembly/compare/v1.1.1...v1.2.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.1.1).; You might want to review and update them manually.; ```; womtool/src/test/resources/validate/wdl_draft3/valid/arrays_v1/arrays_v1.inputs.json; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" }; }]; ```; </details>. labels: sbt-plugin-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7032:1292,down,down,1292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7032,1,['down'],['down']
Availability,"Updates [com.github.pathikrit:better-files](https://x-access-token@github.com/pathikrit/better-files) from 3.9.1 to 3.9.2.; [GitHub Release Notes](https://x-access-token@github.com/pathikrit/better-files/releases/tag/v3.9.2) - [Changelog](https://x-access-token@github.com/pathikrit/better-files/blob/master/CHANGES.md) - [Version Diff](https://x-access-token@github.com/pathikrit/better-files/compare/v3.9.1...v3.9.2). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.github.pathikrit"", artifactId = ""better-files"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.github.pathikrit"", artifactId = ""better-files"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7034:1159,down,down,1159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7034,1,['down'],['down']
Availability,"Updates [com.github.sbt:sbt-git](https://github.com/sbt/sbt-git) from 2.0.0 to 2.0.1.; [GitHub Release Notes](https://github.com/sbt/sbt-git/releases/tag/v2.0.1) - [Version Diff](https://github.com/sbt/sbt-git/compare/v2.0.0...v2.0.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (2.0.0).; You might want to review and update them manually.; ```; CromwellRefdiskManifestCreator/pom.xml; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.github.sbt"", artifactId = ""sbt-git"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.github.sbt"", artifactId = ""sbt-git"" }; }]; ```; </details>. labels: sbt-plugin-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7035:1253,down,down,1253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7035,1,['down'],['down']
Availability,"Updates [com.github.scopt:scopt](https://github.com/scopt/scopt) from 4.0.1 to 4.1.0.; [GitHub Release Notes](https://github.com/scopt/scopt/releases/tag/v4.1.0) - [Version Diff](https://github.com/scopt/scopt/compare/v4.0.1...v4.1.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.github.scopt"", artifactId = ""scopt"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.github.scopt"", artifactId = ""scopt"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6851:964,down,down,964,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6851,1,['down'],['down']
Availability,"Updates [com.google.api:gax-grpc](https://github.com/googleapis/gax-java) from 2.12.2 to 2.19.0.; [GitHub Release Notes](https://github.com/googleapis/gax-java/releases/tag/v2.19.0) - [Version Diff](https://github.com/googleapis/gax-java/compare/v2.12.2...v2.19.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api"", artifactId = ""gax-grpc"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.api"", artifactId = ""gax-grpc"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6852:995,down,down,995,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6852,1,['down'],['down']
Availability,"Updates [com.google.api:gax-grpc](https://github.com/googleapis/gax-java) from 2.19.0 to 2.19.6.; [GitHub Release Notes](https://github.com/googleapis/gax-java/releases/tag/v2.19.6) - [Version Diff](https://github.com/googleapis/gax-java/compare/v2.19.0...v2.19.6). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api"", artifactId = ""gax-grpc"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.api"", artifactId = ""gax-grpc"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7036:995,down,down,995,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7036,1,['down'],['down']
Availability,"Updates [com.google.auth:google-auth-library-oauth2-http](https://github.com/googleapis/google-auth-library-java) from 1.5.3 to 1.10.0.; [GitHub Release Notes](https://github.com/googleapis/google-auth-library-java/releases/tag/v1.10.0) - [Version Diff](https://github.com/googleapis/google-auth-library-java/compare/v1.5.3...v1.10.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.5.3).; You might want to review and update them manually.; ```; cwl/src/test/resources/cwl/ontology/EDAM.owl; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.auth"", artifactId = ""google-auth-library-oauth2-http"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.auth"", artifactId = ""google-auth-library-oauth2-http"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6857:1356,down,down,1356,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6857,1,['down'],['down']
Availability,"Updates [com.google.auth:google-auth-library-oauth2-http](https://github.com/googleapis/google-auth-library-java) from 1.5.3 to 1.16.0.; [GitHub Release Notes](https://github.com/googleapis/google-auth-library-java/releases/tag/v1.16.0) - [Version Diff](https://github.com/googleapis/google-auth-library-java/compare/v1.5.3...v1.16.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.auth"", artifactId = ""google-auth-library-oauth2-http"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.auth"", artifactId = ""google-auth-library-oauth2-http"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7040:1089,down,down,1089,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7040,1,['down'],['down']
Availability,"Updates [com.google.cloud:google-cloud-monitoring](https://github.com/googleapis/java-monitoring) from 3.2.5 to 3.2.10.; [GitHub Release Notes](https://github.com/googleapis/java-monitoring/releases/tag/v3.2.10) - [Version Diff](https://github.com/googleapis/java-monitoring/compare/v3.2.5...v3.2.10). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7041:1048,down,down,1048,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7041,1,['down'],['down']
Availability,"Updates [com.google.cloud:google-cloud-nio](https://github.com/googleapis/java-storage-nio) from 0.124.8 to 0.124.14.; [GitHub Release Notes](https://github.com/googleapis/java-storage-nio/releases/tag/v0.124.14) - [Version Diff](https://github.com/googleapis/java-storage-nio/compare/v0.124.8...v0.124.14). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.124.8).; You might want to review and update them manually.; ```; CHANGELOG.md; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-nio"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-nio"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6860:1284,down,down,1284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6860,1,['down'],['down']
Availability,"Updates [com.google.cloud:google-cloud-nio](https://github.com/googleapis/java-storage-nio) from 0.124.8 to 0.124.21.; [GitHub Release Notes](https://github.com/googleapis/java-storage-nio/releases/tag/v0.124.21) - [Version Diff](https://github.com/googleapis/java-storage-nio/compare/v0.124.8...v0.124.21). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.124.8).; You might want to review and update them manually.; ```; CHANGELOG.md; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-nio"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-nio"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7042:1284,down,down,1284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7042,1,['down'],['down']
Availability,"Updates [com.google.cloud:google-cloud-resourcemanager](https://github.com/googleapis/java-resourcemanager) from 1.2.5 to 1.2.11.; [GitHub Release Notes](https://github.com/googleapis/java-resourcemanager/releases/tag/v1.2.11) - [Version Diff](https://github.com/googleapis/java-resourcemanager/compare/v1.2.5...v1.2.11). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.2.5).; You might want to review and update them manually.; ```; cwl/src/test/resources/cwl/ontology/EDAM.owl; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-resourcemanager"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-resourcemanager"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6861:1340,down,down,1340,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6861,1,['down'],['down']
Availability,"Updates [com.google.cloud:google-cloud-resourcemanager](https://github.com/googleapis/java-resourcemanager) from 1.2.5 to 1.2.12.; [GitHub Release Notes](https://github.com/googleapis/java-resourcemanager/releases/tag/v1.2.12) - [Version Diff](https://github.com/googleapis/java-resourcemanager/compare/v1.2.5...v1.2.12). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-resourcemanager"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-resourcemanager"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7043:1073,down,down,1073,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7043,1,['down'],['down']
Availability,"Updates [com.google.guava:guava](https://github.com/google/guava) from 31.0.1-jre to 31.1-jre. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.guava"", artifactId = ""guava"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.guava"", artifactId = ""guava"" }; }]; ```; </details>. labels: library-update, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6863:823,down,down,823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6863,1,['down'],['down']
Availability,"Updates [com.google.oauth-client:google-oauth-client](https://github.com/googleapis/google-oauth-java-client) from 1.33.1 to 1.33.3.; [GitHub Release Notes](https://github.com/googleapis/google-oauth-java-client/releases/tag/v1.33.3) - [Version Diff](https://github.com/googleapis/google-oauth-java-client/compare/v1.33.1...v1.33.3). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.oauth-client"", artifactId = ""google-oauth-client"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.oauth-client"", artifactId = ""google-oauth-client"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6864:1083,down,down,1083,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6864,1,['down'],['down']
Availability,"Updates [com.iheart:ficus](https://github.com/iheartradio/ficus) from 1.5.1 to 1.5.2.; [GitHub Release Notes](https://github.com/iheartradio/ficus/releases/tag/v1.5.2) - [Version Diff](https://github.com/iheartradio/ficus/compare/v1.5.1...v1.5.2). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.5.1).; You might want to review and update them manually.; ```; cwl/src/test/resources/cwl/ontology/EDAM.owl; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.iheart"", artifactId = ""ficus"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.iheart"", artifactId = ""ficus"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6865:1237,down,down,1237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6865,1,['down'],['down']
Availability,"Updates [com.lihaoyi:pprint](https://github.com/com-lihaoyi/PPrint) from 0.7.1 to 0.7.3.; [GitHub Release Notes](https://github.com/com-lihaoyi/PPrint/releases/tag/0.7.3) - [Version Diff](https://github.com/com-lihaoyi/PPrint/compare/0.7.1...0.7.3). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.lihaoyi"", artifactId = ""pprint"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.lihaoyi"", artifactId = ""pprint"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6866:974,down,down,974,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6866,1,['down'],['down']
Availability,"Updates [com.lihaoyi:pprint](https://github.com/com-lihaoyi/PPrint) from 0.7.3 to 0.8.1.; [GitHub Release Notes](https://github.com/com-lihaoyi/PPrint/releases/tag/0.8.1) - [Version Diff](https://github.com/com-lihaoyi/PPrint/compare/0.7.3...0.8.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.lihaoyi"", artifactId = ""pprint"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.lihaoyi"", artifactId = ""pprint"" }; }]; ```; </details>. labels: library-update, early-semver-major, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7044:974,down,down,974,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7044,1,['down'],['down']
Availability,"Updates [com.storm-enroute:scalameter](http://scalameter.github.io/) from 0.19 to 0.21. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.19).; You might want to review and update them manually.; ```; CHANGELOG.md; database/migration/src/main/resources/changesets/restart_and_recover_migration.xml; docs/developers/bitesize/workflowParsing/wdlToWdlom_hermes.svg; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.storm-enroute"", artifactId = ""scalameter"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.storm-enroute"", artifactId = ""scalameter"" }; }]; ```; </details>. labels: library-update, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6867:1444,down,down,1444,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6867,1,['down'],['down']
Availability,"Updates [com.typesafe.scala-logging:scala-logging](https://github.com/lightbend/scala-logging) from 3.9.4 to 3.9.5.; [GitHub Release Notes](https://github.com/lightbend/scala-logging/releases/tag/v3.9.5) - [Version Diff](https://github.com/lightbend/scala-logging/compare/v3.9.4...v3.9.5). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.typesafe.scala-logging"", artifactId = ""scala-logging"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.typesafe.scala-logging"", artifactId = ""scala-logging"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6871:1036,down,down,1036,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6871,1,['down'],['down']
Availability,"Updates [com.typesafe:config](https://github.com/lightbend/config) from 1.4.1 to 1.4.2.; [GitHub Release Notes](https://github.com/lightbend/config/releases/tag/v1.4.2) - [Version Diff](https://github.com/lightbend/config/compare/v1.4.1...v1.4.2). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.typesafe"", artifactId = ""config"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.typesafe"", artifactId = ""config"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6868:973,down,down,973,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6868,1,['down'],['down']
Availability,"Updates [commons-net:commons-net](https://commons.apache.org/proper/commons-net/) from 3.8.0 to 3.9.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""commons-net"", artifactId = ""commons-net"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""commons-net"", artifactId = ""commons-net"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7046:832,down,down,832,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7046,1,['down'],['down']
Availability,"Updates [eu.timepit:refined](https://github.com/fthomas/refined) from 0.9.28 to 0.9.29.; [GitHub Release Notes](https://github.com/fthomas/refined/releases/tag/v0.9.29) - [Version Diff](https://github.com/fthomas/refined/compare/v0.9.28...v0.9.29). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""eu.timepit"", artifactId = ""refined"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""eu.timepit"", artifactId = ""refined"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6873:973,down,down,973,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6873,1,['down'],['down']
Availability,"Updates [io.circe:circe-config](https://github.com/circe/circe-config) from 0.8.0 to 0.10.0.; [GitHub Release Notes](https://github.com/circe/circe-config/releases/tag/v0.10.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"", artifactId = ""circe-config"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"", artifactId = ""circe-config"" }; }]; ```; </details>. labels: library-update, early-semver-major, semver-spec-minor, version-scheme:early-semver, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7047:905,down,down,905,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7047,1,['down'],['down']
Availability,"Updates [io.circe:circe-generic-extras](https://github.com/circe/circe-generic-extras) from 0.14.1 to 0.14.2.; [GitHub Release Notes](https://github.com/circe/circe-generic-extras/releases/tag/v0.14.2) - [Version Diff](https://github.com/circe/circe-generic-extras/compare/v0.14.1...v0.14.2). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/b38f25673d5484107dba15f3ace47a65d20c9952/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.14.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"", artifactId = ""circe-generic-extras"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""io.circe"", artifactId = ""circe-generic-extras"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6908:1278,down,down,1278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6908,1,['down'],['down']
Availability,"Updates [io.circe:circe-generic-extras](https://github.com/circe/circe-generic-extras) from 0.14.1 to 0.14.3.; [GitHub Release Notes](https://github.com/circe/circe-generic-extras/releases/tag/v0.14.3) - [Version Diff](https://github.com/circe/circe-generic-extras/compare/v0.14.1...v0.14.3). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.14.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"", artifactId = ""circe-generic-extras"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"", artifactId = ""circe-generic-extras"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, version-scheme:early-semver, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7049:1278,down,down,1278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7049,1,['down'],['down']
Availability,"Updates [io.circe:circe-yaml](https://github.com/circe/circe-yaml) from 0.14.1 to 0.14.2.; [GitHub Release Notes](https://github.com/circe/circe-yaml/releases/tag/v0.14.2) - [Version Diff](https://github.com/circe/circe-yaml/compare/v0.14.1...v0.14.2). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.14.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"", artifactId = ""circe-yaml"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"", artifactId = ""circe-yaml"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7050:1228,down,down,1228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7050,1,['down'],['down']
Availability,"Updates [io.grpc:grpc-core](https://github.com/grpc/grpc-java) from 1.45.0 to 1.45.1.; [GitHub Release Notes](https://github.com/grpc/grpc-java/releases/tag/v1.45.1) - [Version Diff](https://github.com/grpc/grpc-java/compare/v1.45.0...v1.45.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.grpc"", artifactId = ""grpc-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""io.grpc"", artifactId = ""grpc-core"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6876:968,down,down,968,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6876,1,['down'],['down']
Availability,"Updates [io.grpc:grpc-core](https://github.com/grpc/grpc-java) from 1.45.1 to 1.45.4.; [GitHub Release Notes](https://github.com/grpc/grpc-java/releases/tag/v1.45.4) - [Version Diff](https://github.com/grpc/grpc-java/compare/v1.45.1...v1.45.4). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.grpc"", artifactId = ""grpc-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.grpc"", artifactId = ""grpc-core"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7052:968,down,down,968,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7052,1,['down'],['down']
Availability,"Updates [io.sentry:sentry-logback](https://github.com/getsentry/sentry-java) from 5.2.4 to 5.7.4.; [GitHub Release Notes](https://github.com/getsentry/sentry-java/releases/tag/5.7.4) - [Version Diff](https://github.com/getsentry/sentry-java/compare/5.2.4...5.7.4). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.sentry"", artifactId = ""sentry-logback"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""io.sentry"", artifactId = ""sentry-logback"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6877:995,down,down,995,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6877,1,['down'],['down']
Availability,"Updates [io.sentry:sentry-logback](https://github.com/getsentry/sentry-java) from 5.7.4 to 6.14.0.; [GitHub Release Notes](https://github.com/getsentry/sentry-java/releases/tag/6.14.0) - [Version Diff](https://github.com/getsentry/sentry-java/compare/5.7.4...6.14.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.sentry"", artifactId = ""sentry-logback"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.sentry"", artifactId = ""sentry-logback"" }; }]; ```; </details>. labels: library-update, early-semver-major, semver-spec-major, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7053:998,down,down,998,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7053,1,['down'],['down']
Availability,"Updates [io.swagger:swagger-parser](https://github.com/swagger-api/swagger-parser) from 1.0.56 to 1.0.61.; [GitHub Release Notes](https://github.com/swagger-api/swagger-parser/releases/tag/v1.0.61) - [Version Diff](https://github.com/swagger-api/swagger-parser/compare/v1.0.56...v1.0.61). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.swagger"", artifactId = ""swagger-parser"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""io.swagger"", artifactId = ""swagger-parser"" }; }]; ```; </details>. labels: test-library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6878:1020,down,down,1020,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6878,1,['down'],['down']
Availability,"Updates [io.swagger:swagger-parser](https://github.com/swagger-api/swagger-parser) from 1.0.56 to 1.0.64.; [GitHub Release Notes](https://github.com/swagger-api/swagger-parser/releases/tag/v1.0.64) - [Version Diff](https://github.com/swagger-api/swagger-parser/compare/v1.0.56...v1.0.64). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.swagger"", artifactId = ""swagger-parser"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.swagger"", artifactId = ""swagger-parser"" }; }]; ```; </details>. labels: test-library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7054:1020,down,down,1020,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7054,1,['down'],['down']
Availability,"Updates [jakarta.annotation:jakarta.annotation-api](https://projects.eclipse.org/projects/ee4j.ca) from 1.3.5 to 2.1.1. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""jakarta.annotation"", artifactId = ""jakarta.annotation-api"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""jakarta.annotation"", artifactId = ""jakarta.annotation-api"" }; }]; ```; </details>. labels: library-update, early-semver-major, semver-spec-major, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7056:867,down,down,867,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7056,1,['down'],['down']
Availability,"Updates [mysql:mysql-connector-java](https://github.com/mysql/mysql-connector-j) from 8.0.28 to 8.0.30.; [GitHub Release Notes](https://github.com/mysql/mysql-connector-j/releases/tag/8.0.30) - [Version Diff](https://github.com/mysql/mysql-connector-j/compare/8.0.28...8.0.30). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""mysql"", artifactId = ""mysql-connector-java"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""mysql"", artifactId = ""mysql-connector-java"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6879:1010,down,down,1010,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6879,1,['down'],['down']
Availability,"Updates [net.sourceforge.owlapi:owlapi-distribution](https://github.com/owlcs/owlapi) from 5.1.19 to 5.1.20. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""net.sourceforge.owlapi"", artifactId = ""owlapi-distribution"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""net.sourceforge.owlapi"", artifactId = ""owlapi-distribution"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6880:857,down,down,857,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6880,1,['down'],['down']
Availability,"Updates [nl.grons:metrics4-scala](https://github.com/erikvanoosten/metrics-scala) from 4.2.8 to 4.2.9.; [GitHub Release Notes](https://github.com/erikvanoosten/metrics-scala/releases/tag/v4.2.9) - [Changelog](https://github.com/erikvanoosten/metrics-scala/blob/master/CHANGELOG.md) - [Version Diff](https://github.com/erikvanoosten/metrics-scala/compare/v4.2.8...v4.2.9). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""nl.grons"", artifactId = ""metrics4-scala"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""nl.grons"", artifactId = ""metrics4-scala"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6881:1101,down,down,1101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6881,1,['down'],['down']
Availability,"Updates [org.apache.commons:commons-csv](https://gitbox.apache.org/repos/asf?p=commons-csv.git) from 1.9.0 to 1.10.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.9.0).; You might want to review and update them manually.; ```; project/plugins.sbt; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.commons"", artifactId = ""commons-csv"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.apache.commons"", artifactId = ""commons-csv"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7059:1096,down,down,1096,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7059,1,['down'],['down']
Availability,"Updates [org.apache.tika:tika-core](https://tika.apache.org/) from 2.3.0 to 2.4.1. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (2.3.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.aws.inputs.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; cwl/src/test/resources/cwl/lodash.js; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.tika"", artifactId = ""tika-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.apache.tika"", artifactId = ""tika-core"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6882:1359,down,down,1359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6882,1,['down'],['down']
Availability,"Updates [org.apache.tika:tika-core](https://tika.apache.org/) from 2.3.0 to 2.7.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (2.3.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.aws.inputs.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.tika"", artifactId = ""tika-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.apache.tika"", artifactId = ""tika-core"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7061:1321,down,down,1321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7061,1,['down'],['down']
Availability,"Updates [org.codehaus.janino:janino](https://github.com/janino-compiler/janino) from 3.1.6 to 3.1.7.; [GitHub Release Notes](https://github.com/janino-compiler/janino/releases/tag/v3.1.7) - [Version Diff](https://github.com/janino-compiler/janino/compare/v3.1.6...v3.1.7) - [Version Diff](https://github.com/janino-compiler/janino/compare/3.1.6...3.1.7). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (3.1.6).; You might want to review and update them manually.; ```; cwl/src/test/resources/cwl/ontology/EDAM.owl; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.codehaus.janino"", artifactId = ""janino"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.codehaus.janino"", artifactId = ""janino"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6883:1354,down,down,1354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6883,1,['down'],['down']
Availability,"Updates [org.codehaus.janino:janino](https://github.com/janino-compiler/janino) from 3.1.7 to 3.1.9.; [GitHub Release Notes](https://github.com/janino-compiler/janino/releases/tag/v3.1.9) - [Version Diff](https://github.com/janino-compiler/janino/compare/v3.1.7...v3.1.9). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.codehaus.janino"", artifactId = ""janino"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.codehaus.janino"", artifactId = ""janino"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7063:1005,down,down,1005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7063,1,['down'],['down']
Availability,"Updates [org.glassfish.jersey.inject:jersey-hk2](https://github.com/eclipse-ee4j/jersey) from 2.32 to 2.39.; [GitHub Release Notes](https://github.com/eclipse-ee4j/jersey/releases/tag/2.39) - [Version Diff](https://github.com/eclipse-ee4j/jersey/compare/2.32...2.39). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (2.32).; You might want to review and update them manually.; ```; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.glassfish.jersey.inject"", artifactId = ""jersey-hk2"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.glassfish.jersey.inject"", artifactId = ""jersey-hk2"" }; }]; ```; </details>. labels: library-update, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7064:1500,down,down,1500,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7064,1,['down'],['down']
Availability,"Updates [org.gnieh:diffson-spray-json](https://github.com/gnieh/diffson) from 4.1.1 to 4.3.0.; [GitHub Release Notes](https://github.com/gnieh/diffson/releases/tag/v4.3.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.gnieh"", artifactId = ""diffson-spray-json"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.gnieh"", artifactId = ""diffson-spray-json"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7065:907,down,down,907,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7065,1,['down'],['down']
Availability,"Updates [org.hsqldb:hsqldb](http://hsqldb.org) from 2.6.1 to 2.7.1. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (2.6.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.hsqldb"", artifactId = ""hsqldb"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.hsqldb"", artifactId = ""hsqldb"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7066:1040,down,down,1040,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7066,1,['down'],['down']
Availability,"Updates [org.mariadb.jdbc:mariadb-java-client](https://github.com/mariadb-corporation/mariadb-connector-j) from 2.7.4 to 2.7.6.; [GitHub Release Notes](https://github.com/mariadb-corporation/mariadb-connector-j/releases/tag/2.7.6) - [Changelog](https://github.com/mariadb-corporation/mariadb-connector-j/blob/master/CHANGELOG.md) - [Version Diff](https://github.com/mariadb-corporation/mariadb-connector-j/compare/2.7.4...2.7.6). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mariadb.jdbc"", artifactId = ""mariadb-java-client"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.mariadb.jdbc"", artifactId = ""mariadb-java-client"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6887:1172,down,down,1172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6887,1,['down'],['down']
Availability,"Updates [org.mariadb.jdbc:mariadb-java-client](https://github.com/mariadb-corporation/mariadb-connector-j) from 2.7.4 to 2.7.8.; [GitHub Release Notes](https://github.com/mariadb-corporation/mariadb-connector-j/releases/tag/2.7.8) - [Changelog](https://github.com/mariadb-corporation/mariadb-connector-j/blob/master/CHANGELOG.md) - [Version Diff](https://github.com/mariadb-corporation/mariadb-connector-j/compare/2.7.4...2.7.8). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mariadb.jdbc"", artifactId = ""mariadb-java-client"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.mariadb.jdbc"", artifactId = ""mariadb-java-client"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7069:1172,down,down,1172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7069,1,['down'],['down']
Availability,"Updates [org.mock-server:mockserver-netty](https://www.mock-server.com) from 5.11.2 to 5.14.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mock-server"", artifactId = ""mockserver-netty"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.mock-server"", artifactId = ""mockserver-netty"" }; }]; ```; </details>. labels: test-library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6888:833,down,down,833,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6888,1,['down'],['down']
Availability,"Updates [org.mock-server:mockserver-netty](https://www.mock-server.com) from 5.14.0 to 5.15.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mock-server"", artifactId = ""mockserver-netty"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.mock-server"", artifactId = ""mockserver-netty"" }; }]; ```; </details>. labels: test-library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7070:833,down,down,833,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7070,1,['down'],['down']
Availability,"Updates [org.mockftpserver:MockFtpServer](https://mockftpserver.org) from 3.0.0 to 3.1.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (3.0.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/green/arrays/arrays.options; database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mockftpserver"", artifactId = ""MockFtpServer"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.mockftpserver"", artifactId = ""MockFtpServer"" }; }]; ```; </details>. labels: test-library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7071:1198,down,down,1198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7071,1,['down'],['down']
Availability,"Updates [org.mockito:mockito-core](https://github.com/mockito/mockito) from 3.12.4 to 5.1.1.; [GitHub Release Notes](https://github.com/mockito/mockito/releases/tag/v5.1.1) - [Version Diff](https://github.com/mockito/mockito/compare/v3.12.4...v5.1.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mockito"", artifactId = ""mockito-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.mockito"", artifactId = ""mockito-core"" }; }]; ```; </details>. labels: library-update, early-semver-major, semver-spec-major, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7072:982,down,down,982,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7072,1,['down'],['down']
Availability,"Updates [org.mozilla:rhino](https://mozilla.github.io/rhino/) from 1.7.13 to 1.7.14. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mozilla"", artifactId = ""rhino"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.mozilla"", artifactId = ""rhino"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6890:808,down,down,808,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6890,1,['down'],['down']
Availability,"Updates [org.postgresql:postgresql](https://github.com/pgjdbc/pgjdbc) from 42.3.3 to 42.3.6.; [Changelog](https://github.com/pgjdbc/pgjdbc/blob/master/CHANGELOG.md). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.postgresql"", artifactId = ""postgresql"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.postgresql"", artifactId = ""postgresql"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6891:897,down,down,897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6891,1,['down'],['down']
Availability,"Updates [org.postgresql:postgresql](https://github.com/pgjdbc/pgjdbc) from 42.4.1 to 42.4.3.; [Changelog](https://github.com/pgjdbc/pgjdbc/blob/master/CHANGELOG.md). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.postgresql"", artifactId = ""postgresql"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.postgresql"", artifactId = ""postgresql"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7073:897,down,down,897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7073,1,['down'],['down']
Availability,"Updates [org.scala-graph:graph-core](https://github.com/scala-graph/scala-graph) from 1.13.1 to 1.13.5.; [GitHub Release Notes](https://github.com/scala-graph/scala-graph/releases/tag/1.13.5). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scala-graph"", artifactId = ""graph-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.scala-graph"", artifactId = ""graph-core"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6892:925,down,down,925,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6892,1,['down'],['down']
Availability,"Updates [org.scala-lang:scala-library](https://github.com/scala/scala) from 2.13.9 to 2.13.10.; [GitHub Release Notes](https://github.com/scala/scala/releases/tag/v2.13.10) - [Version Diff](https://github.com/scala/scala/compare/v2.13.9...v2.13.10). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scala-lang"", artifactId = ""scala-library"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scala-lang"", artifactId = ""scala-library"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7074:984,down,down,984,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7074,1,['down'],['down']
Availability,"Updates [org.scalactic:scalactic](https://github.com/scalatest/scalatest) from 3.2.10 to 3.2.13.; [GitHub Release Notes](https://github.com/scalatest/scalatest/releases/tag/release-3.2.13) - [Version Diff](https://github.com/scalatest/scalatest/compare/release-3.2.10...release-3.2.13). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (3.2.10).; You might want to review and update them manually.; ```; cwl/src/test/resources/cwl/ontology/EDAM.owl; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scalactic"", artifactId = ""scalactic"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.scalactic"", artifactId = ""scalactic"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6893:1312,down,down,1312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6893,1,['down'],['down']
Availability,"Updates [org.scalatest:scalatest](https://github.com/scalatest/scalatest) from 3.2.10 to 3.2.13.; [GitHub Release Notes](https://github.com/scalatest/scalatest/releases/tag/release-3.2.13) - [Version Diff](https://github.com/scalatest/scalatest/compare/release-3.2.10...release-3.2.13). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (3.2.10).; You might want to review and update them manually.; ```; cwl/src/test/resources/cwl/ontology/EDAM.owl; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scalatest"", artifactId = ""scalatest"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.scalatest"", artifactId = ""scalatest"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6894:1312,down,down,1312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6894,1,['down'],['down']
Availability,"Updates [org.scalatest:scalatest](https://github.com/scalatest/scalatest) from 3.2.10 to 3.2.15.; [GitHub Release Notes](https://github.com/scalatest/scalatest/releases/tag/release-3.2.15) - [Version Diff](https://github.com/scalatest/scalatest/compare/release-3.2.10...release-3.2.15). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scalatest"", artifactId = ""scalatest"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scalatest"", artifactId = ""scalatest"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7075:1016,down,down,1016,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7075,1,['down'],['down']
Availability,"Updates [org.scoverage:sbt-scoverage](https://github.com/scoverage/sbt-scoverage) from 1.9.3 to 2.0.2.; [GitHub Release Notes](https://github.com/scoverage/sbt-scoverage/releases/tag/v2.0.2) - [Version Diff](https://github.com/scoverage/sbt-scoverage/compare/v1.9.3...v2.0.2). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" }; }]; ```; </details>. labels: sbt-plugin-update, early-semver-major, semver-spec-major, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6895:1010,down,down,1010,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6895,1,['down'],['down']
Availability,"Updates [org.scoverage:sbt-scoverage](https://github.com/scoverage/sbt-scoverage) from 2.0.4 to 2.0.7.; [GitHub Release Notes](https://github.com/scoverage/sbt-scoverage/releases/tag/v2.0.7) - [Version Diff](https://github.com/scoverage/sbt-scoverage/compare/v2.0.4...v2.0.7). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" }; }]; ```; </details>. labels: sbt-plugin-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7076:1010,down,down,1010,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7076,1,['down'],['down']
Availability,"Updates [org.typelevel:kittens](https://github.com/typelevel/kittens) from 2.3.2 to 3.0.0.; [GitHub Release Notes](https://github.com/typelevel/kittens/releases/tag/v3.0.0) - [Version Diff](https://github.com/typelevel/kittens/compare/v2.3.2...v3.0.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (2.3.2).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"", artifactId = ""kittens"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.typelevel"", artifactId = ""kittens"" }; }]; ```; </details>. labels: library-update, early-semver-major, semver-spec-major, version-scheme:early-semver, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7078:1229,down,down,1229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7078,1,['down'],['down']
Availability,"Updates [org.typelevel:mouse](https://github.com/typelevel/mouse) from 1.0.10 to 1.0.11.; [GitHub Release Notes](https://github.com/typelevel/mouse/releases/tag/v1.0.11) - [Version Diff](https://github.com/typelevel/mouse/compare/v1.0.10...v1.0.11). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"", artifactId = ""mouse"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.typelevel"", artifactId = ""mouse"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6898:975,down,down,975,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6898,1,['down'],['down']
Availability,"Updates [org.typelevel:mouse](https://github.com/typelevel/mouse) from 1.0.11 to 1.2.1.; [GitHub Release Notes](https://github.com/typelevel/mouse/releases/tag/v1.2.1) - [Version Diff](https://github.com/typelevel/mouse/compare/v1.0.11...v1.2.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.0.11).; You might want to review and update them manually.; ```; .sdkmanrc; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"", artifactId = ""mouse"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.typelevel"", artifactId = ""mouse"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7079:1205,down,down,1205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7079,1,['down'],['down']
Availability,"Updates bio.terra:workspace-manager-client from 0.254.452-SNAPSHOT to 0.254.586-SNAPSHOT. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.254.452-SNAPSHOT).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""bio.terra"", artifactId = ""workspace-manager-client"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""bio.terra"", artifactId = ""workspace-manager-client"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7025:1092,down,down,1092,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7025,1,['down'],['down']
Availability,"Updates com.google.apis:google-api-services-cloudkms from v1-rev20220104-1.32.1 to v1-rev20220819-2.0.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-cloudkms"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-cloudkms"" }; }]; ```; </details>. labels: library-update, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6854:855,down,down,855,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6854,1,['down'],['down']
Availability,"Updates com.google.apis:google-api-services-cloudkms from v1-rev20220104-1.32.1 to v1-rev20230127-2.0.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-cloudkms"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-cloudkms"" }; }]; ```; </details>. labels: library-update, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7037:855,down,down,855,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7037,1,['down'],['down']
Availability,"Updates com.google.apis:google-api-services-genomics from v2alpha1-rev20210811-1.32.1 to v2alpha1-rev20220328-2.0.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-genomics"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-genomics"" }; }]; ```; </details>. labels: library-update, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6855:867,down,down,867,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6855,1,['down'],['down']
Availability,"Updates com.google.apis:google-api-services-genomics from v2alpha1-rev20210811-1.32.1 to v2alpha1-rev20220913-2.0.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-genomics"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-genomics"" }; }]; ```; </details>. labels: library-update, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7038:867,down,down,867,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7038,1,['down'],['down']
Availability,"Updates com.google.apis:google-api-services-lifesciences from v2beta-rev20210813-1.32.1 to v2beta-rev20220401-2.0.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-lifesciences"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-lifesciences"" }; }]; ```; </details>. labels: library-update, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6856:871,down,down,871,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6856,1,['down'],['down']
Availability,"Updates com.google.apis:google-api-services-lifesciences from v2beta-rev20210813-1.32.1 to v2beta-rev20220916-2.0.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-lifesciences"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-lifesciences"" }; }]; ```; </details>. labels: library-update, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7039:871,down,down,871,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7039,1,['down'],['down']
Availability,"Updates com.google.cloud:google-cloud-bigquery from 2.10.0 to 2.10.10. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-bigquery"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-bigquery"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6858:815,down,down,815,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6858,1,['down'],['down']
Availability,"Updates com.google.cloud:google-cloud-monitoring from 3.2.5 to 3.2.9. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (3.2.5).; You might want to review and update them manually.; ```; cwl/src/test/resources/cwl/ontology/EDAM.owl; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6859:1083,down,down,1083,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6859,1,['down'],['down']
Availability,"Updates com.google.cloud:google-cloud-storage from 2.9.2 to 2.9.3. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-storage"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-storage"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6862:810,down,down,810,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6862,1,['down'],['down']
Availability,"Updates com.typesafe.sbt:sbt-git from 1.0.2 to 2.0.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.0.2).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.wdl; src/ci/bin/test.inc.sh; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.typesafe.sbt"", artifactId = ""sbt-git"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.typesafe.sbt"", artifactId = ""sbt-git"" }; }]; ```; </details>. labels: sbt-plugin-update, early-semver-major, semver-spec-major, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6870:1106,down,down,1106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6870,1,['down'],['down']
Availability,"Updates io.github.swagger2markup:swagger2markup from 1.3.3 to 1.3.4. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.3.3).; You might want to review and update them manually.; ```; cwl/src/test/resources/cwl/ontology/EDAM.owl; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.github.swagger2markup"", artifactId = ""swagger2markup"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""io.github.swagger2markup"", artifactId = ""swagger2markup"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6875:1081,down,down,1081,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6875,1,['down'],['down']
Availability,"Updates jakarta.activation:jakarta.activation-api from 1.2.1 to 1.2.2. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.2.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; project/GenerateRestApiDocs.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""jakarta.activation"", artifactId = ""jakarta.activation-api"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""jakarta.activation"", artifactId = ""jakarta.activation-api"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7055:1102,down,down,1102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7055,1,['down'],['down']
Availability,"Updates jakarta.xml.bind:jakarta.xml.bind-api from 2.3.2 to 2.3.3. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (2.3.2).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""jakarta.xml.bind"", artifactId = ""jakarta.xml.bind-api"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""jakarta.xml.bind"", artifactId = ""jakarta.xml.bind-api"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7057:1059,down,down,1059,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7057,1,['down'],['down']
Availability,"Updates mysql:mysql-connector-java from 8.0.28 to 8.0.32. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""mysql"", artifactId = ""mysql-connector-java"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""mysql"", artifactId = ""mysql-connector-java"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7058:790,down,down,790,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7058,1,['down'],['down']
Availability,"Updates org.apache.httpcomponents:httpclient from 4.5.13 to 4.5.14. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.httpcomponents"", artifactId = ""httpclient"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.apache.httpcomponents"", artifactId = ""httpclient"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7060:810,down,down,810,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7060,1,['down'],['down']
Availability,"Updates org.broadinstitute.dsde.workbench:workbench-google from 0.21-5c9c4f6 to 0.23-629b08c. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" }; }]; ```; </details>. labels: library-update, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7062:850,down,down,850,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7062,1,['down'],['down']
Availability,"Updates org.hsqldb:hsqldb from 2.6.1 to 2.7.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.hsqldb"", artifactId = ""hsqldb"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.hsqldb"", artifactId = ""hsqldb"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6884:770,down,down,770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6884,1,['down'],['down']
Availability,"Updates org.liquibase:liquibase-core from 4.8.0 to 4.15.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.liquibase"", artifactId = ""liquibase-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.liquibase"", artifactId = ""liquibase-core"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6886:793,down,down,793,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6886,1,['down'],['down']
Availability,"Updates org.liquibase:liquibase-core from 4.8.0 to 4.19.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.liquibase"", artifactId = ""liquibase-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.liquibase"", artifactId = ""liquibase-core"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7068:793,down,down,793,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7068,1,['down'],['down']
Availability,"Updates org.mockito:mockito-core from 3.11.2 to 3.12.4. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mockito"", artifactId = ""mockito-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.mockito"", artifactId = ""mockito-core"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6889:786,down,down,786,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6889,1,['down'],['down']
Availability,"Updates org.webjars:swagger-ui from 4.5.0 to 4.5.2. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.webjars"", artifactId = ""swagger-ui"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.webjars"", artifactId = ""swagger-ui"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6899:780,down,down,780,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6899,1,['down'],['down']
Availability,"Updates org.webjars:swagger-ui from 4.5.2 to 4.15.5. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.webjars"", artifactId = ""swagger-ui"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.webjars"", artifactId = ""swagger-ui"" }; }]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7080:781,down,down,781,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7080,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:batch from 2.17.152 to 2.17.179. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/a73dbf8f0f456857ba8dd425f017fa09baa6c7e4/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""batch"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""batch"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6746:798,down,down,798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6746,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:batch from 2.17.152 to 2.17.189. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/0a4d1a09b47760b51c89870db3ba25430aae8ad2/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""batch"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""batch"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6759:798,down,down,798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6759,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:cloudwatchlogs from 2.17.152 to 2.17.180. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/a73dbf8f0f456857ba8dd425f017fa09baa6c7e4/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""cloudwatchlogs"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""cloudwatchlogs"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6750:816,down,down,816,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6750,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:cloudwatchlogs from 2.17.152 to 2.17.190. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/c2be23ff17cde28ee3c44ff6474bb4621d41dbbd/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""cloudwatchlogs"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""cloudwatchlogs"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6760:816,down,down,816,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6760,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:core from 2.17.152 to 2.17.181. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/ba26ba03fb0bdd22cfa55b5595686b14ad894174/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""core"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6751:796,down,down,796,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6751,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:core from 2.17.152 to 2.17.191. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/c2be23ff17cde28ee3c44ff6474bb4621d41dbbd/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""core"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6761:796,down,down,796,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6761,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:s3 from 2.17.152 to 2.17.182. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9599889c173a765f84ef9138c01ded7a1a8d561c/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""s3"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""s3"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6753:792,down,down,792,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6753,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:s3 from 2.17.152 to 2.17.193. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2504ff1bee3ef0eeb3c13a8d1917caf18e1aef5f/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""s3"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""s3"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6764:792,down,down,792,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6764,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:sts from 2.17.152 to 2.17.174. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/5ba079e5e6b075ded705306e58f3111e16796466/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""sts"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""sts"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6740:794,down,down,794,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6740,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:sts from 2.17.152 to 2.17.184. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/4e12175cb40eccba16c6030fae30ac4fab719481/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""sts"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""sts"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6754:794,down,down,794,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6754,1,['down'],['down']
Availability,"Updates software.amazon.awssdk:sts from 2.17.152 to 2.17.194. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/ac359164689493f9b5c048982a362475b9a4129b/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""software.amazon.awssdk"", artifactId = ""sts"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequest = { frequency = ""@monthly"" },; dependency = { groupId = ""software.amazon.awssdk"", artifactId = ""sts"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6765:794,down,down,794,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6765,1,['down'],['down']
Availability,Updates to fix coercion errors. DSDEEPB-727,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/84:24,error,errors,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/84,1,['error'],['errors']
Availability,Updating to say that I ran across the other day and it was only a vague memory of this issue which led me to not go down a giant rabbit hole of WTF,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3073#issuecomment-516557267:116,down,down,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3073#issuecomment-516557267,1,['down'],['down']
Availability,"Upon further reflection I think I might want to work on this a bit more before merging. This addresses the problems of the list limit being too low and not configurable, but it doesn't yet address the problem that when the list limit is hit the code silently truncates. I'm not sure if GCS provides an API to say how many object have a given prefix, but I'd like to see if something like that is available. If the number of objects that would be returned by list() exceeds our limit, I think the code should fail noisily and not silently.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/710#issuecomment-210461439:396,avail,available,396,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/710#issuecomment-210461439,1,['avail'],['available']
Availability,"Ups, looks like I've made type error, closing this",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2413#issuecomment-313128986:31,error,error,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2413#issuecomment-313128986,1,['error'],['error']
Availability,"Use ""read committed"" transaction isolation levels for workflow starting and heartbeat writing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4238:76,heartbeat,heartbeat,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4238,1,['heartbeat'],['heartbeat']
Availability,"Use of `wdlSource` is meant to be deprecated, not a hard error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2497:57,error,error,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2497,1,['error'],['error']
Availability,Use reference files from mounted disk instead of downloading them from the `gcp-public-data--broad-references` bucket [BA-6482],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5746:49,down,downloading,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5746,1,['down'],['downloading']
Availability,Use the right failureMode value,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2239:14,failure,failureMode,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2239,1,['failure'],['failureMode']
Availability,"Useful tidbit to note, [this is how you signal failure due to unimplemented functionality](https://github.com/common-workflow-language/common-workflow-language/pull/278/files#diff-ee814a9c027fc9750beb075c283a973cR49) - which I believe means that one can still be ""green"" on CI",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2590#issuecomment-333315542:47,failure,failure,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2590#issuecomment-333315542,1,['failure'],['failure']
Availability,"User @gavvvr reports:. If i run cromwell with -Dworkflow-options.workflow-failure-mode=""ContinueWhilePossible"" it does not work. Also using -Dconfig.file=application.conf does not work. Only workflow_failure_mode option in JSON config works.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1380:74,failure,failure-mode,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1380,1,['failure'],['failure-mode']
Availability,"User reported issue: https://gatkforums.broadinstitute.org/gatk/discussion/12759/womtool-should-tell-me-to-not-use-the-reserved-keyword-output-upon-validate. AC: Ensure that Womtool returns the true cause for an invalid workflow (in this case its using the ""output"" keyword as a variable name instead of a misleading/unrelated(?) error. Testing Criteria: Add a test for certain reserved keywords, such as `input`, `output`, `command` and possibly others to ensure that using them in unexpected ways in a WDL yields the appropriate error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4031:330,error,error,330,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4031,2,['error'],['error']
Availability,"Using `better.files` for metadata writing, and removed heaviest duplicated `FileUtil` similarity.; Refactored `Main` to avoid test issues with `DelayedInit`, `System.exit`, consistent error generation, etc.; Removed blocking code from `SingleWorkflowRunnerActor`, and passing `Shutdown` as message now, instead of killing system directly.; Reused / refactored some of the test methods for actors, and removed bitrotted `DefaultWorkflowManagerActor`.; Left comments (warnings?) about possible bugs / improvements to workflow `Actor` messaging.; Replaced use of Java `SystemProperties` with Scala `sys.props`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/265:184,error,error,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/265,1,['error'],['error']
Availability,"Using cromwell v36, not all log output appears to be captured even if log level set to DEBUG. For example, using the HaplotypeCallerGvcf_GATK4 reference on GitHub, if an non-existent path is provided for `input_bam` in the inputs file, this line will cause its workflow to fail:; ```; Int disk_size = ceil(((size(input_bam, ""GB"") + 30) / hc_scatter) + ref_size) + 20; ```. The resulting 'file not found' error is printed (50x, one per shard) on cromwell's stdout, but not captured anywhere in the logs written to disk. The workflow execution directory in `cromwell-executions/HaplotypeCallerGvcf_GATK4/{guid}` is just an empty folder and its workflow log (if preserved) indicates the workflow was started but no errors:; ```; ...; 2018-10-24 13:29:31,409 INFO - WorkflowExecutionActor-28a80dbf-a2ae-4fc8-96e6-0279c32a0e13 [UUID(28a80dbf)]: Starting HaplotypeCallerGvcf_GATK4.HaplotypeCaller (50 shards); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4310:404,error,error,404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4310,2,['error'],"['error', 'errors']"
Availability,"Using release 0.21.; There seems to be a discrepancy between [what the docs say should happen](https://github.com/broadinstitute/cromwell#error-handling) if you submit badly-formed WDL inputs to REST API, and what actually happens: which is that the inputs are accepted (get back a response with ""Submitted"" status), but later fail. Or am I misunderstanding the docs?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1515:138,error,error-handling,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1515,1,['error'],['error-handling']
Availability,"Using the AWS backend with Cromwell 85, if the S3 key of the input contains spaces, the generated commands for localizing the inputs seem to be incorrect, with the command line argument to the the `aws s3 cp` command not being quoted. ### Steps to reproduce. Run a workflow using the AWS backend where the S3 uri of an input contains the space character. Here is a minimal WDL for reproducing; basically any task that uses a File input will do.; ```; version 1.0; task repro_task {; input {; File in; }; command <<<; cat '~{in}'; >>>; output {; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""debian:bullseye-slim""; }; }; ```. And here is an example inputs JSON:; ```; {; ""in"": ""s3://bucket-name/test/somewhere containing spaces/filename.txt""; }; ```. ### Observed result. Here is a line from the script that is generated, a bit below the line that reads `echo '*** LOCALIZING INPUTS ***'`. ```; /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://bucket-name/test/somewhere containing spaces/filename.txt /tmp/scratch/bucket-name/test/somewhere%20containing%20spaces/filename.txt; ```. This leads to `aws s3 cp` to fail with an error message like this:; ```; Unknown options: spaces/filename.txt,/tmp/scratch/bucket-name/test/somewhere%20containing%20spaces/filename.txt; ```. ### Expected result. I think that in the script the argument to `aws s3 cp` should be quoted, for example like this:. ```; /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress 's3://bucket-name/test/somewhere containing spaces/filename.txt' /tmp/scratch/bucket-name/test/somewhere%20containing%20spaces/filename.txt; ```. ### Additional information. The location in the code where the command line is created would seem to be at [AwsBatchJob.scala:132](https://github.com/broadinstitute/cromwell/blob/85/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/AwsBatchJob.scala#L132).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7102:873,echo,echo,873,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7102,2,"['echo', 'error']","['echo', 'error']"
Availability,"Using this wdl, call caching takes a very long time for each task in the scatter to go from success to done. This is labeled ""cromwell final overhead"" in the timing diagram. This task scatters 100 wide and outputs 2 arrays of 901 elements each. It is also more error prone with call caching on than off. ```; task FileSpam {; String sample_name = ""DeliciousFileSpam""; Int index. command <<<; mkdir ${sample_name}_${index}; for i in `seq 0 900`; do; echo $i > ${sample_name}_${index}_$i.txt; echo $i > ${sample_name}_${index}/$i.txt; done; >>>; runtime {; docker: ""ubuntu:latest""; memory: ""3 GB""; cpu: ""1""; disks: ""local-disk 5 HDD""; }; output {; Array[File] outer = glob(""${sample_name}_${index}_*""); Array[File] inner = glob(""${sample_name}_${index}/*""); }; }. workflow DeliciousFileSpam {; Array[Int] indexing_list = [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,; 11, 12, 13, 14, 15, 16, 17, 18, 19, 20,; 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,; 31, 32, 33, 34, 35, 36, 37, 38, 39, 40,; 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,; 51, 52, 53, 54, 55, 56, 57, 58, 59, 60,; 61, 62, 63, 64, 65, 66, 67, 68, 69, 70,; 71, 72, 73, 74, 75, 76, 77, 78, 79, 80,; 81, 82, 83, 84, 85, 86, 87, 88, 89, 90,; 91, 92, 93, 94, 95, 96, 97, 98, 99, 100 ]. scatter (idx in indexing_list) {; call FileSpam { input: index = idx }; }; output {; None; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/901:261,error,error,261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/901,3,"['echo', 'error']","['echo', 'error']"
Availability,"Using wom-tool-30.2, the following wdl validates using `java -jar womtool-30.2.jar validate test.wdl` and shows no inputs with `java -jar womtool-30.2.jar inputs test.wdl`. I think this should fail validation because the variable `to_echo` is used without being declared. . ```; workflow test_validation{; # missing variable; #String to_echo; 	call example{ input:; 		to_echo = to_echo; 	}; }. task example{; 	String to_echo; 	; 	command{; 		echo ""${to_echo}""; 	}; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3381:442,echo,echo,442,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3381,1,['echo'],['echo']
Availability,"VE=toolchain.tar.xz; + TOOLCHAIN_ENV_FILENAME=toolchain_env; + CHROMIUMOS_SDK_GCS=https://storage.googleapis.com/chromiumos-sdk; + ROOT_OS_RELEASE=/root/etc/os-release; + KERNEL_SRC_DIR=/build/usr/src/linux; + NVIDIA_DRIVER_VERSION=418.40.04; + NVIDIA_DRIVER_MD5SUM=; + NVIDIA_INSTALL_DIR_HOST=/var/lib/nvidia; + NVIDIA_INSTALL_DIR_CONTAINER=/usr/local/nvidia; + ROOT_MOUNT_DIR=/root; + CACHE_FILE=/usr/local/nvidia/.cache; + LOCK_FILE=/root/tmp/cos_gpu_installer_lock; + LOCK_FILE_FD=20; + set +x; [INFO 2020-08-04 23:40:07 UTC] Checking if this is the only cos-gpu-installer that is running.; [INFO 2020-08-04 23:40:07 UTC] Running on COS build id 12871.1174.0; [INFO 2020-08-04 23:40:07 UTC] Checking if third party kernel modules can be installed; [INFO 2020-08-04 23:40:07 UTC] Checking cached version; [INFO 2020-08-04 23:40:07 UTC] Cache file /usr/local/nvidia/.cache not found.; [INFO 2020-08-04 23:40:07 UTC] Did not find cached version, building the drivers...; [INFO 2020-08-04 23:40:07 UTC] Downloading GPU installer ...; [INFO 2020-08-04 23:40:09 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/tesla/418.40.04/NVIDIA-Linux-x86_64-418.40.04.run; ls: cannot access '/build/usr/src/linux': No such file or directory; [INFO 2020-08-04 23:40:11 UTC] Kernel sources not found locally, downloading; [INFO 2020-08-04 23:40:11 UTC] Kernel source archive download URL: https://storage.googleapis.com/cos-tools/12871.1174.0/kernel-src.tar.gz. real	0m2.220s; user	0m0.183s; sys	0m0.338s; [INFO 2020-08-04 23:40:18 UTC] Setting up compilation environment; [INFO 2020-08-04 23:40:18 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain_env. real	0m0.126s; user	0m0.014s; sys	0m0.001s; [INFO 2020-08-04 23:40:18 UTC] Downloading toolchain from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain.tar.xz. real	0m11.907s; user	0m0.428s; sys	0m1.039s; [INFO 2020-08-04 23:41:17 UTC] Configuring environment varia",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:3678,Down,Downloading,3678,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,1,['Down'],['Downloading']
Availability,"VSCode uses the bloop build server, and with the current version of SCoverage, there is an anticipated binary conflict for the scala-xml library due to the major version difference, which does not seem to be causing an issue. This change forces this to be ignored rather than throwing an error.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7124:288,error,error,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7124,1,['error'],['error']
Availability,Vague error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4032:6,error,error,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4032,1,['error'],['error']
Availability,Validate checksum for AWS S3 signed URL downloads during localization [BT-257],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6496:40,down,downloads,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6496,1,['down'],['downloads']
Availability,"Validates `cpuMin`, `cpuMax`, `ramMin` and `ramMax` as valid cpu / memory runtime attributes; Maps `cpuMin` and `ramMin` to `cpu` and `memory` (respectively); All the resource requirements are then available in the runtime attributes map for the backend to play with if it wants to / can.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3077:198,avail,available,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3077,1,['avail'],['available']
Availability,"Validation config validated once per running application and it will be crushed with errors before any workflow runs, if config values are invalid.; CallCaching variable's names changed at MaterializeWorkflowDescriptorActor and MaterializeWorkflowDescriptorActorSpec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5217:85,error,errors,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5217,1,['error'],['errors']
Availability,"Variables in string interpolations do not undergo existence checks, so the following will validate despite `t` not existing:. ```; task foo {; # Shouldn't validate, no 't' defined:; String s = ""I like to drink ${t}""; command {; echo ${s}; }; output {; String out = read_string(stdout()); }; }; ```. EDIT: Simplified the example",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2209:228,echo,echo,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2209,1,['echo'],['echo']
Availability,"Version 1.67 is vulnerable and versions 1.69+ are safe. Upgraded to latest version 1.70. `sbt assembly` succeeds. The line; ```; addDependencyTreePlugin; ```; enables a handy new command; ```; whatDependsOn org.bouncycastle bcprov-jdk15on 1.67; ```; shows what chain of artifacts uses a leaf-node dependency – an inversion of the traditional dependency tree:; ```; [info] org.bouncycastle:bcprov-jdk15on:1.67; [info] +-org.bouncycastle:bcpkix-jdk15on:1.67; [info] +-io.grpc:grpc-xds:1.46.0; [info] +-io.grpc:grpc-googleapis:1.46.0; [info] +-com.google.api:gax-grpc:2.18.1; [info] +-com.google.cloud:google-cloud-resourcemanager:1.4.0; [info] +-org.broadinstitute:cloud-nio-spi_2.13:80-d24645a-SNAP [S]; [info] +-org.broadinstitute:cloud-nio-util_2.13:80-d24645a-SNAP [S]; ```. With this PR's fix in place, the output is a gratifying ""this isn't used anywhere"" error:; ```; root(aen_bw_1227)> | 80> whatDependsOn org.bouncycastle bcprov-jdk15on 1.67; [error] Expected 'org.broadinstitute'; [error] Expected '1.70'; [error] whatDependsOn org.bouncycastle bcprov-jdk15on 1.67; [error] ^; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6775:860,error,error,860,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6775,5,['error'],['error']
Availability,"Version: Cromwell v38; Backend: PAPI v2. Running a hello world workflow on Pipelines API v2 against inputs originating from a Requester Pays bucket. The job finishes running, generates a 0 return code and detritus logs (stdout/stderr). However the workflow failures with an error message (full message attached).; [failed-workflow-metadata.txt](https://github.com/broadinstitute/cromwell/files/3008597/failed-workflow-metadata.txt). ```Unable to complete JES Api Request. Caught NPE while processing operation projects/dvoet-prod-test-20190305-3/operations/11506961050484846699```. Searched the code and found the error here:; https://github.com/broadinstitute/cromwell/blob/2d8ff3b0962bbe84828445fd3ac77d2379e499c2/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/request/GetRequestHandler.scala#L49-L83. Operation metadata attached as well.; [operation-metadata.yaml.txt](https://github.com/broadinstitute/cromwell/files/3008623/operation-metadata.yaml.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4772:257,failure,failures,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4772,3,"['error', 'failure']","['error', 'failures']"
Availability,WAAS will have different `/status` requirements than the reader or writer Cromwells. This could involve (depending on time available) either:; * Generalizing the current status implementation to allow callers of `/status` to opt-in/opt-out of status information they do or don't care about; * Creating a new specialist health implementation which knows what WAAS would care about.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4737:123,avail,available,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4737,1,['avail'],['available']
Availability,"WDL 1.0 and up [require a dedicated `inputs` section](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#workflow-inputs). You can easily find such errors when editing WDLs in IntelliJ (it will automatically suggest to install a WDL helper plugin). . <img width=""929"" alt=""Screen Shot 2022-05-23 at 1 39 42 PM"" src=""https://user-images.githubusercontent.com/1087943/169876581-2b2e91f3-16fe-4dcf-96bc-3af317eecbb5.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134959939:159,error,errors,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767#issuecomment-1134959939,1,['error'],['errors']
Availability,WDL syntax error in comment still reported as an error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2870:11,error,error,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2870,2,['error'],['error']
Availability,"WDL tasks that specify an output directory of ""."" (sometimes indirectly) will cause failures in downstream tasks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2844:84,failure,failures,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2844,2,"['down', 'failure']","['downstream', 'failures']"
Availability,WM-2428: Include full error context when failing to abort TES jobs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7354:22,error,error,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7354,1,['error'],['error']
Availability,WOMtool should specify what WDL (or subWDL) has the error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3055:52,error,error,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3055,1,['error'],['error']
Availability,WOMtool validate returns unhelpful fatal error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6767:41,error,error,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767,1,['error'],['error']
Availability,"WX-1126 Upgrade to modern Python, 3.8 not available in package repo anymore",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7164:42,avail,available,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7164,1,['avail'],['available']
Availability,WX-1225 Print TES error messages to job logger,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7220:18,error,error,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7220,1,['error'],['error']
Availability,WX-1595 Refactor the preemption error handling from GCP Batch backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7457:32,error,error,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7457,1,['error'],['error']
Availability,WX-1611 Fix GHA ordering error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7424:25,error,error,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7424,1,['error'],['error']
Availability,"WX-1625 Better handling of code 9 ""no available zones"" error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7431:38,avail,available,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7431,2,"['avail', 'error']","['available', 'error']"
Availability,WX-1672 Fix Docker-related CI failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7450:30,failure,failures,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7450,1,['failure'],['failures']
Availability,WX-757 Fix workflow stuck in aborting after WDL type error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385:53,error,error,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385,1,['error'],['error']
Availability,WX-843 Workflow failure reason should accurately indicate issues opening blob filesystem,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6965:16,failure,failure,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6965,1,['failure'],['failure']
Availability,WX-876 Surface TES System Logs to Cromwell when TES backend returns task error status,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6979:73,error,error,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6979,2,['error'],['error']
Availability,"WX-927 GCPBATCH retry with more memory and error message fixes, Centaur tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7494:43,error,error,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7494,1,['error'],['error']
Availability,WX-927 Proper GCP Batch failure on noAddress,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7542:24,failure,failure,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7542,1,['failure'],['failure']
Availability,"Waaaaay back when @yfarjoun asked for a ""dry run"" feature which was similar. We got lost in the weeds as I was involved. I remember at the time noting that this wouldn't be particularly useful since you only check things which were fully resolvable at submission time but that was based on what I now believe to be a faulty assumption, that typically only the entrypoint calls would have resolvable inputs at submission time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293587582:317,fault,faulty,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163#issuecomment-293587582,1,['fault'],['faulty']
Availability,"Warn on async failures, we're not logging debug and this is a bad state.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/376:14,failure,failures,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/376,1,['failure'],['failures']
Availability,"Was meaning to push this before heading out for the week: . A work-in-progress Job Store writer which skirts the problem of databases by not actually ever using one. Instead, every Job has a known filesystem location and we just write to and (not yet) read back from disk. Currently has all of the hooks and wiring needed to write jobs the JobStore and clear them out on workflow completion. All that should be left is to update the JobStoreReader to read back the JSON from an appropriate file (but the tests are there for the JSON implicits and they seems good). NB I went down the JSON route because I anticipate an eventual DB schema more like the metadata, to avoid having to store multiple MBs or GBs in a single cell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1155:575,down,down,575,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1155,1,['down'],['down']
Availability,"Was there a solution to this? I am also encountering this using the broad institute mutect2 implementation here:; https://github.com/broadinstitute/gatk/blob/master/scripts/mutect2_wdl/mutect2.wdl. When implemented using slurm/singularity on my institute's HPC. Specifically my error is below and I think has to do like people above have said to do the 50 scatter I am currently using. `[INFO] [06/04/2024 06:56:15.291] [cromwell-system-akka.dispatchers.engine-dispatcher-32] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Workflow ccfe50af-5661-4bcd-b351-9da287c5affb failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'M2.tumor_pileups': Future timed out after [60 seconds]; Bad output 'M2.normal_pileups': Future timed out after [60 seconds]; Bad output 'M2.tumor_sample': Failed to read_string(""tumor_name.txt"") (reason 1 of 1): Future timed out after [60 seconds]; Bad output 'M2.normal_sample': Failed to read_string(""normal_name.txt"") (reason 1 of 1): Future timed out after [60 seconds]; 	at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:990); 	at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2147918932:278,error,error,278,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2147918932,1,['error'],['error']
Availability,We add the [workflow ID as a label.](https://github.com/broadinstitute/cromwell/blob/b29d8005e33aadd4e9e57178101bc3ef9d0ca9bc/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/api/GcpBatchRequestFactoryImpl.scala#L139) Are task and shared generated by Cromwell and would they be available from the parameters or somewhere else?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286994667:305,avail,available,305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7491#issuecomment-2286994667,1,['avail'],['available']
Availability,"We are investigating ways we can store additional information with a cromwell run that is useful for our downstream applications. The two ways to pass associated metadata with a workflow are the Labels attribute through the runtimeOptions, or the `meta` object within cromwell itself. Exposing either of these as parsable json in the metadata objet would achieve what we are looking for; ; ## Meta; If I Have a workflow with a meta object:. ```; workflow a {; call b. meta {; some_key: ""some_value""; }; }; ```; The only way to retrieve the meta values from the /metadata endpoint would be to re-parse the workflow string contained in metadata object ; ```; ""workflow"": "" .....""; ```; Since they are simple KV pairs it would be nice to have an output that looks like the following, embedded in the `metadata` object:; ```; {; ""meta"": {; ""key"":""value"",; ""key2"":""value2""; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2421:105,down,downstream,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2421,1,['down'],['downstream']
Availability,"We are running Cromwell with the Google Genomics (aka Google Pipelines API, aka Google Life Sciences API) backend plugin. . There's a known issue on Google's side (unfortunately no public link) that causes ssh-server to fail to start up (`tcp4 0.0.0.0:22: bind: address already in use`). This causes the entire workflow to fail. A change in [SSHAccessAction](https://github.com/broadinstitute/cromwell/blob/a69d12ec71bd453bf50be563f0666e7fb65c6874/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/SSHAccessAction.scala#L24) will allow the worker to ignore the error:; ```scala; setIgnoreExitStatus(true); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6771:613,error,error,613,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6771,1,['error'],['error']
Availability,"We are running a wld pipeline on GCP. Requested 16 CPSs. we have; runtime {; cpuPlatform: ""Intel Cascade Lake""; }; set in the .conf file. Nevertheless, the task was given an e2-standard-16 machine, and took 9 hours to turn a bam into a cram, and another hour to upload the 51 gb ram file. I ran the code again, only change was I used an n2-standard-16; The entire task, including the upload, finished in less than 30 minutes. How do we make it so we never, ever, get an ""e"" machine? I downloaded the cromwell code and looked through it, trying to figure out where it is you pick the machine type on GCP, but never found anything that looked like it would pick an e machine. What am I missing?. The cromwell that assigned the e2-standard-16 was cromwell-87.jar, if that matters. Thank you",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474:485,down,downloaded,485,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474,1,['down'],['downloaded']
Availability,"We are seeing infrequent situations where JES says a call is done but cromwell thinks it is still running. I see the call starting in the logs:; ```; 2017-02-08 18:55:58,500 cromwell-system-akka.dispatchers.engine-dispatcher-963 INFO - WorkflowExecutionActor-fa7e25a2-f51f-4763-9f8a-5a2e5cd1c954 [UUID(fa7e25a2)]: Starting calls: pon_gatk_workflow.PadTargets:NA:1; ```. Then the only suspicious things I see later in the logs are these messages (which could be completely unrelated however they do start to appear 2.5 minutes after the job completes on JES):; ```; 2017-02-08 19:17:07,588 cromwell-system-akka.dispatchers.backend-dispatcher-424 INFO - The JES polling actor Actor[akka://cromwell-system/user/cromwell-service/$b/$a/$Enn#762671444] unexpectedly terminated while conducting 100 polls. Making a new one...; java.lang.NullPointerException: null; 2017-02-08 19:17:07,588 cromwell-system-akka.dispatchers.backend-dispatcher-246 ERROR - null; ```. For future reference by a FireCloud admin the operations id is ELSl1PihKxjdhp-6gvr3weYBILma7PWMHyoPcHJvZHVjdGlvblF1ZXVl and the workflow id is fa7e25a2-f51f-4763-9f8a-5a2e5cd1c954 and the workflow was aborted 3PM Feb 8.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1965:938,ERROR,ERROR,938,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1965,1,['ERROR'],['ERROR']
Availability,"We are trying to abort a job in FireCloud using cromwell .21. The workflow appears stuck in submitted state. It appears to have no calls. Is there are race condition that is we abort too soon the job is stuck?. ```; ~/projects/rawls [develop*] $ curl -H ""Authorization: Bearer `gcloud auth print-access-token`"" https://cromwell2.dsde-alpha.broadinstitute.org/api/workflows/v1/b29c0ef3-e988-4d70-8093-bdd1a039170d/status; {; ""status"": ""Submitted"",; ""id"": ""b29c0ef3-e988-4d70-8093-bdd1a039170d""; }(dvoet@wm163-585) 14:55; ~/projects/rawls [develop*] $ curl -H ""Authorization: Bearer `gcloud auth print-access-token`"" https://cromwell2.dsde-alpha.broadinstitute.org/api/workflows/v1/b29c0ef3-e988-4d70-8093-bdd1a039170d/metadata; {; ""submittedFiles"": {; ""inputs"": ""{\""test.hello.name\"":\""subject_HCC1143\""}"",; ""workflow"": ""task hello {\n String? name\n\n command {\n echo 'hello ${name}!'\n }\n output {\n File response = stdout()\n }\n runtime {\n docker: \""ubuntu\""\n }\n}\n\nworkflow test {\n call hello\n}"",; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""zones\"": \""us-central1-c us-central1-f\""\n },\n \""google_project\"": \""broad-dsde-alpha\"",\n \""auth_bucket\"": \""gs://cromwell-auth-broad-dsde-alpha\"",\n \""refresh_token\"": \""cleared\"",\n \""final_workflow_log_dir\"": \""gs://fc-ceb841f8-e512-4f70-831b-eb2c43af9b42/d938c20b-916d-4bd5-a132-533c72a84eb0/workflow.logs\"",\n \""account_name\"": \""test.firec@gmail.com\"",\n \""jes_gcs_root\"": \""gs://fc-ceb841f8-e512-4f70-831b-eb2c43af9b42/d938c20b-916d-4bd5-a132-533c72a84eb0\""\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""b29c0ef3-e988-4d70-8093-bdd1a039170d"",; ""inputs"": {. },; ""submission"": ""2017-01-20T16:37:31.589Z""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1885:864,echo,echo,864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1885,1,['echo'],['echo']
Availability,"We are trying to be 100% heads down on cwl work for probably another couple of weeks. If this is a raging fire we can divert attention from that but that not without cost towards that and the ripple effect on other goals. So is this “this sucks please fix soon” or “OMG we’re blocked, at the risk of cheesing off other users this needs to be fixed right this second”",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358518084:31,down,down,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156#issuecomment-358518084,1,['down'],['down']
Availability,"We are trying to create a reference disk manifest as mentioned here:; https://github.com/broadinstitute/cromwell/blob/928ad9616bb81cd8948077a1d0319ade6127e521/src/ci/resources/papi_v2_reference_image_manifest.conf. While running the create_image.sh with an input.tsv reflecting our storage (; https://github.com/broadinstitute/cromwell/blob/develop/scripts/reference_disks/create_images.sh ); I get this error:; ```; Checking for existing resources...; Creating images...; DRY RUN to manifest: refdata-disk-image-public-2023-12-19.manifest.conf; {; imageIdentifier: ""projects/xxxxxxx/global/images/refdata-disk-image-public-2023-12-19""; diskSizeGb: 1310; files: [; curl: (22) The requested URL returned error: 401 Unauthorized; create_images.sh: line 128: 16#: invalid integer constant (error token is ""16#""); ```. I am wondering if I can get any help to resolve this. We are trying to get PAPIv2 working, and our workflows expect that a data directory get passed as input to the workflows and bound to the docker image so that the tools have access to them. . Example: ; `rqcfilter2.sh -Xmx${default=""60G"" memory} -da threads=${jvm_threads} ${chastityfilter} jni=t in=${input_files} path=filtered rna=f trimfragadapter=t qtrim=r trimq=0 maxns=3 maq=3 minlen=51 mlf=0.33 phix=t removehuman=t removedog=t removecat=t removemouse=t khist=t removemicrobes=t sketch kapa=t clumpify=t tmpdir= barcodefilter=f trimpolyg=5 usejni=f rqcfilterdata=${database}/RQCFilterData > >(tee -a ${filename_outlog}) 2> >(tee -a ${filename_errlog} >&2)`. Where ${database} would be the base directory containing all the reference data.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7348:404,error,error,404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7348,3,['error'],['error']
Availability,"We are trying to expose workflow failure messages to the user in Firecloud. However, the failures field seems to have an inconsistent format:. Compare the failures sections for the following:. ```; {; ""workflowName"": ""echo_strings"",; ""submittedFiles"": {; ""inputs"": ""{...},; ""calls"": {; ""echo_strings.echo_files"": [{; ""preemptible"": false,; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files/echo_files-stdout.log"",; ""backendStatus"": ""Failed"",; ""shardIndex"": -1,; ""jes"": {; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""machineType"": ""us-central1-c/n1-standard-1"",; ""googleProject"": ""broad-dsde-dev"",; ""executionBucket"": ""gs://cromwell-dev/cromwell-executions"",; ""zone"": ""us-central1-c"",; ""instanceName"": ""ggp-3462354720519617596""; },; ""runtimeAttributes"": {...},; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""CallCachingOff"",; ""inputs"": {...; },; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }],; ""jobId"": ""operations/EJiq_oWfKxi8-N-X4qiwhjAgw7vetLsXKg9wcm9kdWN0aW9uUXVldWU"",; ""backend"": ""JES"",; ""end"": ""2017-01-30T19:14:19.708Z"",; ""stderr"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files/echo_files-stderr.log"",; ""callRoot"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files"",; ""at",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:33,failure,failure,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,3,['failure'],"['failure', 'failures']"
Availability,"We are using cromwell server to run workflows in a shared HPC environment. As it is a shared environment and we are dealing with sensitive data, we have purposely set cromwell to use umask 0007 so that others can't access our files. However, cromwell seems to override the umask and gives rwx world permissions on all directories it creates. . As far as I can tell, the override is programmed here:; cromwell/core/src/main/scala/cromwell/core/path/EvenBetterPathMethods.scala. Beyond it being non-ideal from a security perspective, this ends up causing downstream ""access denied"" problems for us when cromwell creates directories downstream of a linux access control list (ACL). It seems like what is happening in this case is (1) the directory is created via cromwell (2) via the OS, the ACL is applied to the folder, giving user & group privileges (3) the world privileges are modified via cromwell, which disrupts the ACL, thus the user and group lose all privileges (4) access denied error when cromwell tries to use the folder",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3333:553,down,downstream,553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3333,3,"['down', 'error']","['downstream', 'error']"
Availability,"We can not get memory retry to work. Have not not found anywhere a complete example showing it working, including what should be int eh .conf file. If such an example exists, please point us to it,. Command:; nohup java -Dconfig.file=My.conf -jar cromwell-87-5448b85-SNAP-pre-edits.jar run ~/MemoryRetryTest.wdl 2>&1 > nohup.out. MemoryRetryTest.wdl:; workflow MemoryRetryTest {; 	String message = ""Killed""; 	; 	call TestOutOfMemoryRetry {}; 	call TestBadCommandRetry {}; }. task TestOutOfMemoryRetry {; 	command <<<; 		free -h; 		df -h; 		cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		tail /dev/zero; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; 	; }. task TestBadCommandRetry {; 	command <<<; free -h; df -h; cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		bedtools intersect nothing with nothing; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; }. My.conf:. include required(classpath(""application"")). system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"". system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; config {; project = ""$my_project""; root = ""$my_bucket""; name-for-call-caching-purposes: PAPI; slow-job-warning-time: 24 hours; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600. # Setup GCP to give more memory with each retry; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; ; # Number of workers to assign to PAPI requests; request-workers = 3. virtual-private-cloud {; network-label-key = ""network-key""; network-name = ""network-name""; subnetwork-name = ""subnetwork-name""; auth = ""auth""; }; pipeline-timeout = 7 days; genomics {; auth = ""auth""; comp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7451:561,echo,echo,561,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451,2,['echo'],['echo']
Availability,"We currently support dockerhub, google container registry and quay as far as looking up hashes is concerned. With the way things are currently, if one of those services goes down or has a high error rate forcing us to retry a lot, all lookups will potentially be slowed down even if they target a service that is doing fine.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-332941637:174,down,down,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2329#issuecomment-332941637,3,"['down', 'error']","['down', 'error']"
Availability,We do a lot of files of filenames and I think we'd get the same error in the case of an empty line. Right?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-344351642:64,error,error,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1954#issuecomment-344351642,1,['error'],['error']
Availability,"We do this for preemptible VMs, when a job fails with certain error codes (13/14 I believe) which indicate that a VM shut down unexpectedly, we retry a certain number of times which is user specified via the WDL task definition. @dvoet has requested this same feature for non-preemptible VMs for FireCloud",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1042:62,error,error,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1042,2,"['down', 'error']","['down', 'error']"
Availability,"We don't typically patch releases, but we do make the latest version from `develop` available in a Docker image. Folks who want the latest changes between major releases are advised to use these development versions, ex. `87-225ea5a`, which are named with the next major version and short hash of the merge commit. https://hub.docker.com/r/broadinstitute/cromwell/tags",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885342414:84,avail,available,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885342414,1,['avail'],['available']
Availability,"We feel this is another in a long series of weird concurrency issues which we see from time to time involving shared filesystems and fs synching. We've never been able to reliably reproduce these, but if someone can provide something here we can reopen and look at it",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959#issuecomment-519647059:171,reliab,reliably,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959#issuecomment-519647059,1,['reliab'],['reliably']
Availability,"We get 'fail to delocalize' as the error message over a bunch of failure types. In particular, when the task failed with return code 0. This makes failures hard to debug in FireCloud, as users have to dig down into logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2774#issuecomment-342886864:35,error,error,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2774#issuecomment-342886864,4,"['down', 'error', 'failure']","['down', 'error', 'failure', 'failures']"
Availability,"We get noisy messages in our logs that seem to come from [this line of code](https://github.com/Azure/azure-sdk-for-java/blob/main/sdk/core/azure-core/src/main/java/com/azure/core/util/serializer/SerializerEncoding.java#L62) in the Azure SDK.; ```; 2023-02-17 16:30:13 reactor-http-nio-2 WARN - 'Content-Type' not found. Returning default encoding: JSON; ```; Searching for the error, I found [an issue](https://github.com/Azure/azure-sdk-for-java/issues/32250) and [fix PR](https://github.com/Azure/azure-sdk-for-java/pull/32833) mentioning it (in rhyming but not identical circumstances). I confirmed that going from `1.7.1` to `1.10.2` brings in the improved handling by examining the copy of `HttpResponseBodyDecoder.java` that SBT downloads.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7088:378,error,error,378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7088,2,"['down', 'error']","['downloads', 'error']"
Availability,"We had a situation where develop was broken (grab commit 0ff86c409d2e5dac4b766fceb89f47ba3c304f99). If you run ""sbt test"" it fails with a compilation error about HtCondorInitializationActorSpec.scala. However, Travis for this is green. Travis is running:. sbt -Dbackend.providers.Local.config.filesystems.local.localization.0=copy clean coverage nointegration:test coverageReport && sbt coverageAggregate && sbt assembly. Which if you run it locally yields a successful test. First thought was that it was because this test was flagged as ""nointegration"" but that's not even the case. However even if it were, we should be checking that things compile even if we don't run a certain class of tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/888:150,error,error,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/888,1,['error'],['error']
Availability,"We had two workflows fail with the following message:. ```; ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable; { ; ""code"" : 503,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Backend Error"",; ""reason"" : ""backendError""; } ],; ""message"" : ""Backend Error""; }""; ```. JES indicated that the rate of unavailability is below what they consider a problem, and suggested retries. I showed the stacktrace below to Miguel, who says there is a retry around this call, but that it is fairly short. ```; {;   ""code"" : 503,;   ""errors"" : [ {;     ""domain"" : ""global"",;     ""message"" : ""Backend Error"",;     ""reason"" : ""backendError"";   } ],;   ""message"" : ""Backend Error""; };   at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19];   at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19];   at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19];   at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19];   at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19];   at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19];   at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19];   at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19];   at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(GcsFileSystemProvider.scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/810:173,error,errors,173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/810,6,"['Error', 'error']","['Error', 'errors']"
Availability,"We have a cromwell task that failed with the unhelpful error message: `Unexpected failure in EJEA (root cause not captured).` After investigation we found that the task was being killed by the google cloud compute system:; ```; ""error"": {; ""code"": 1,; ""message"": ""Operation canceled at 2017-07-23T01:01:39-07:00 because it is older than 6 days""; },; ```; It would be better if cromwell was able to report this error directly, so we don't have to look at the gcloud operations properties. Looking at the cromwell logs I see this sequence:; ```; 2017-07-23 08:09:42 [cromwell-system-akka.actor.default-dispatcher-3449] WARN c.e.w.l.e.WorkflowExecutionActor - WorkflowExecutionActor-f15009ab-b5bc-47ac-b832-3d0ae2f15888 [UUID(f15009ab)]: WorkflowExecutionActor [UUID(f15009ab)] received an unhandled message: AbortedResponse(PairedEndSingleSampleWorkflow.MarkDuplicates:-1:1) in state: WorkflowExecutionInProgressState; 2017-07-23 08:09:47 [cromwell-system-akka.actor.default-dispatcher-3375] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f15009ab-b5bc-47ac-b832-3d0ae2f15888 failed (during ExecutingWorkflowState): Unexpected failure in EJEA (root cause not captured).; ```. In this case the google operations ID is `EOjNtPvUKxiOgeqc763--TEgn6KQ6Z4NKg9wcm9kdWN0aW9uUXVldWU`. I can provide more operations IDs if necessary.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496:55,error,error,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496,6,"['ERROR', 'error', 'failure']","['ERROR', 'error', 'failure']"
Availability,"We have a very similar use case. We'd like to be able to run a different annotator that has a massive pile of data sources ~20gb. We want an easy way to package different sets of test files and make them available for people to use with our docker image, without having to make a 20gb docker image.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-349763129:204,avail,available,204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2190#issuecomment-349763129,1,['avail'],['available']
Availability,"We have a workflow that scatters 95 wide as its first step. When running with `read_from_cache=false` the workflow chugs along normally. When we allow the same workflow to read from the cache, Cromwell seems to lock up after the first handful of cache hits(~30). Cromwell will stop responding to api requests and after some time with logs being written the workflow that was getting the cache hits will hit 503 and timeout errors. When running the workflow with `read_from_cache=false` we run into none of these errors. Timeout Error. ```; 2016-05-05 17:37:02,285 cromwell-system-akka.actor.default-dispatcher-25 WARN - Configured registration timeout of 1 second expired, stoppingw; ```. 503 Error. ```; Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/ccba2c79-c998-4f03-b736-af097391db66/call-SplitGvcf/shard-50 to gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/7164dc88-af61-4ea6-8a73-f0b79594ae9a/call-SplitGvcf/shard-50. com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable. {. ""code"" : 503,. ""errors"" : [ {. ""domain"" : ""global"",. ""message"" : ""Backend Error"",. ""reason"" : ""backendError"". } ],. ""message"" : ""Backend Error"". }. at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]. at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]. at com.google.api.client.google",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/794:423,error,errors,423,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/794,4,"['Error', 'error']","['Error', 'errors']"
Availability,"We have also seen the `address already in use` error. Are you saying that the error is false and we should ignore it?. If it is a real error, then it seems like we would want to continue seeing it, and have the workaround be turning off the SSH enablement option `enable_ssh_access` [0]. [0] https://cromwell.readthedocs.io/en/stable/wf_options/Google/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1138741894:47,error,error,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6771#issuecomment-1138741894,3,['error'],['error']
Availability,"We have the same issue; we use an OGE back-end that implements hard limits on resource utilization which will terminate jobs that exceed these limits without allowing them to create an rc file. Currently we are running local mode cromwell instances with a workaround of putting a time-limit on the cromwell task of 48 hours, but this is extremely wasteful of our back end resources (cromwell itself consumes a large amount of memory on our cluster nodes). Our current workarounds involve putting a soft-limit on each job, attempting to trap SIGUSR1 in advance of the job being killed by the queue manager, at which point they create an rc file with a non-zero error code, but there is no guarantee that we can catch every instance of this. . Ideally we would like for cromwell to query running jobs at a user configurable interval (optionally never, but not as often as the file system is pinged for rc files so as not to burden the scheduler) against its list of jobs that are both not finished, and running and at minimum trigger the equivalent error state of a non-zero rc file return code. . We experience this problem frequently (and expect it to increase as we move more pipelines to cromwell) because our pipelines can not reliably estimate the amount of memory they need for tasks apriori.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150:660,error,error,660,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150,4,"['error', 'ping', 'reliab']","['error', 'pinged', 'reliably']"
Availability,"We lose this feature for local backend yes, but it's not the only one (we also lose support for the ability to use files in calls that were not explicitly set as inputs), which was the point of the ticket because that is what JES is doing.; In the end this was a downgrade of Local/SGE Backends to match JES. Now we can re-upgrade but it should be done on all backends at the same time IMO.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/249#issuecomment-151876098:263,down,downgrade,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/249#issuecomment-151876098,1,['down'],['downgrade']
Availability,"We need to re-figure out our hashing strategy for docker images, especially in GCR. In the meantime, any GCR image that does not have a `v2+json` hash available will run, but will always fail call caching. We currently request a ""v2+json"" Docker-Content-Digest for all GCR images. Recently, GCR [""correctly""](https://enterprise.google.com/supportcenter/managecases#Case/0016000000MNGDy/U-13919143) returns 404 when it doesn't actually have ""v2+json"" for the images. There does appear to be an alternative ""v1+prettyjws"" Docker-Content-Digest available for GCR images. (FYI there are a [number of search hits](https://www.google.com/search?q=Docker-Content-Digest) re: ""Docker-Content-Digest"", especially v1 vs. v2, etc.) It's likely that Google's ""correction"" mentioned above was to fix the server from returning the wrong ""v1"" hash when ""v2"" was requested-but-not-available. While cromwell only uses the headers via HTTP HEAD, the hashes in the headers and the contents of an HTTP GET _are_ different between v1 and v2. Google tech support have also mentioned that clients may request either hash type, submitting both `Accept` headers, and that one can read the `Content-Type` from the server to determine which `Docker-Content-Digest` was returned. Bodies reformatted by jq for readability:; ```bash; $ curl -i -s -H 'Accept: application/vnd.docker.distribution.manifest.v1+prettyjws' https://gcr.io/v2/google-containers/ubuntu-slim/manifests/0.14; HTTP/1.1 200 OK; Docker-Distribution-API-Version: registry/2.0; Content-Type: application/vnd.docker.distribution.manifest.v1+prettyjws; Content-Length: 2647; Docker-Content-Digest: sha256:781290a693dc805993b19b7b4c5be40f7688f595312646b926abe2baae2fa9ff; Date: Mon, 06 Nov 2017 16:15:32 GMT; Server: Docker Registry; X-XSS-Protection: 1; mode=block; X-Frame-Options: SAMEORIGIN; Alt-Svc: quic="":443""; ma=2592000; v=""41,39,38,37,35"". {; ""name"": ""unused"",; ""tag"": ""unused"",; ""architecture"": ""amd64"",; ""fsLayers"": [; {; ""blobSum"": ""sha256:a3ed95caeb02f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2826:151,avail,available,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2826,3,['avail'],['available']
Availability,"We noticed this in FC alpha -- no status field. Rebooting Cromwell (well over an hour after submission) fixed it. ```; {; ""submittedFiles"": {; ""inputs"": ""{\""test.hello.name\"":\""subject_HCC1143\""}"",; ""workflow"": ""task hello {\n String? name\n\n command {\n echo 'hello ${name}!'\n }\n output {\n File response = stdout()\n }\n runtime {\n docker: \""ubuntu\""\n }\n}\n\nworkflow test {\n call hello\n}"",; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""zones\"": \""us-central1-b us-central1-c us-central1-f\""\n },\n \""google_project\"": \""broad-dsde-alpha\"",\n \""auth_bucket\"": \""gs://cromwell-auth-broad-dsde-alpha\"",\n \""refresh_token\"": \""cleared\"",\n \""final_workflow_log_dir\"": \""gs://fc-07b4785f-2cc2-4147-bd6f-67cf8a4049ba/fe75135f-3637-4ead-9bc7-26573cfc50cc/workflow.logs\"",\n \""account_name\"": \""test.firec@gmail.com\"",\n \""jes_gcs_root\"": \""gs://fc-07b4785f-2cc2-4147-bd6f-67cf8a4049ba/fe75135f-3637-4ead-9bc7-26573cfc50cc\""\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""bea0383d-6ca8-4392-b048-03914e87444d"",; ""inputs"": {. },; ""submission"": ""2017-02-11T01:27:49.935Z""; }; ```. Cromwell version: 24-489f66b; hostname: gce-cromwell-alpha102; workflow id: 3d01da76-98f9-4751-a3c0-efc61ef67030",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978:48,Reboot,Rebooting,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978,2,"['Reboot', 'echo']","['Rebooting', 'echo']"
Availability,"We now serenade users with the delightful error. ```; ""failures"": [; {; ""causedBy"": [; {; ""message"": ""Failed to evaluate 's' (reason 1 of 1):; Evaluating read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") failed:; Failed to read_string(\""https://sa1314b2aa9c1b89e6b409.blob.core.windows.net/sc-1314b2aa-2f7a-4524-9aba-9c1b89e6b409/test-data/inputFile.txt\"") (reason 1 of 1):; [Attempted 1 time(s)] - ApiException: ; <!DOCTYPE HTML PUBLIC \""-//IETF//DTD HTML 2.0//EN\"">\n<html><head><script src=\""https://us.jsagent.tcell.insight.rapid7.com/tcellagent.min.js\"" tcellappid=\""FCNonprod-NaVu9\"" tcellapikey=\""AQQBBAFLGLOxL7VE9IF9ESlLvCxD5Ykr_7xkQKq_rgn_P58IWjOhOzIh6p3aI4pTWaprlUw\"" tcellbaseurl=\""https://us.agent.tcell.insight.rapid7.com/api/v1\""></script>\n<title>401 Unauthorized</title>\n</head><body>\n<h1>Unauthorized</h1>\n<p>This server could not verify that you\nare authorized to access the document\nrequested. Either you supplied the wrong\ncredentials (e.g., bad password), or your\nbrowser doesn't understand how to supply\nthe credentials required.</p>\n</body></html>\n"",; ""causedBy"": []; }; ],; ""message"": ""Workflow failed""; }; ]; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357:42,error,error,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6965#issuecomment-1341933357,2,"['error', 'failure']","['error', 'failures']"
Availability,"We probably also want to say where to copy it to, e.g. ; ```; When using Spark backend, copy the Spark configuration in reference.conf (available under core/src/main/resources) into the main application.conf (in src/main/resources):; ```. I think we probably also don't want to have a copy/pasted version in this file, since it's unlikely to be updated if reference.conf is changed? Up to you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-283980646:136,avail,available,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2043#issuecomment-283980646,1,['avail'],['available']
Availability,"We randomly receive PKIX and RPC deadline errors on a small subset of GCP LS API and Batch runs, which always crash the main Cromwell java process. We have added local certs to our java install launching cromwell - this has reduced, but not eliminated PKIX errors. We have no remedy for these issues at the moment besides submitting again. Is there anything we can do as users to make cromwell more fault-tolerant of cloud-related issues like these?. PKIX error example:. Failed to query job status (projects/XXXXX/jobs/job-7638b0fb-XXXX-XXXX-81ca-9f609da4c664) from GCP; com.google.api.gax.rpc.UnavailableException: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception; Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]; at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:112); at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:41); at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:86); at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:66); at com.google.api.gax.grpc.GrpcExceptionCallable$ExceptionTransformingFuture.onFailure(GrpcExceptionCallable.java:97); at com.google.api.core.ApiFutures$1.onFailure(ApiFutures.java:84); at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1133); at com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:31); at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:1277); at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:1038); at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:808); at io.grpc.stub.ClientCalls$GrpcFuture.setException(ClientCalls.java:574); at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:544); at io.grpc.PartialForw",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:42,error,errors,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,4,"['error', 'fault']","['error', 'errors', 'fault-tolerant']"
Availability,"We recently adjusted this timeout, I would try again with the latest `develop` build. The errors are likely happening because Cromwell is putting work into its IO queue faster than it can finish it. There's a backpressure system that aims to prevent this by stopping Cromwell picking up new work when IO operations aren't getting finished fast enough, but in this case it isn't responsive enough to keep you out of trouble. If you continue seeing these errors, you can tune down `system.io.command-backpressure-staleness` to make the engine more sensitive to long IO queues.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912:90,error,errors,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-2296872912,3,"['down', 'error']","['down', 'errors']"
Availability,We redundantly re-verified the absence of the problem class [0] by [unzipping](https://docs.oracle.com/javase/tutorial/deployment/jar/unpack.html) the shipping Cromwell JAR and manually checking that the path is empty. [0] `org/apache/logging/log4j/core/lookup/JndiLookup.class`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-996997802:3,redundant,redundantly,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6588#issuecomment-996997802,1,['redundant'],['redundantly']
Availability,"We run custom WDL genomics pipelines and also parts of the BioWDL somaticcalling pipeline (https://biowdl.github.io/) using the broadinstitute/cromwell:64 cromwell container with a gridengine backend. Occasionally we see errors like the below:. `; cromwell_1 | 2023-03-20 14:56:42,590 cromwell-system-akka.dispatchers.engine-dispatcher-193 INFO - WorkflowManagerActor: Workflow 3f4aa8de-d1a9-419a-b9b4-10f9ed0a9d53 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; cromwell_1 | Bad output 'scatterList.scatters': Failed to read_lines(""/home/devarea/karl/PathoCromwell/cromwell-executions/Agilent_Exome_Single/3f4aa8de-d1a9-419a-b9b4-10f9ed0a9d53/call-SomaticVariants/SomaticVariantcalling/0c6cefa3-4c84-497f-8003-b0d7221bedbc/call-mutect2/Mutect2/fb66e417-4c06-4f15-8607-da8261e16448/call-scatterList/execution/stdout"") (reason 1 of 1): [Attempted 1 time(s)] - IOException: Could not read from /home/devarea/karl/PathoCromwell/cromwell-executions/Agilent_Exome_Single/3f4aa8de-d1a9-419a-b9b4-10f9ed0a9d53/call-SomaticVariants/SomaticVariantcalling/0c6cefa3-4c84-497f-8003-b0d7221bedbc/call-mutect2/Mutect2/fb66e417-4c06-4f15-8607-da8261e16448/call-scatterList/execution/stdout: /home/devarea/karl/PathoCromwell/cromwell-executions/Agilent_Exome_Single/3f4aa8de-d1a9-419a-b9b4-10f9ed0a9d53/call-SomaticVariants/SomaticVariantcalling/0c6cefa3-4c84-497f-8003-b0d7221bedbc/call-mutect2/Mutect2/fb66e417-4c06-4f15-8607-da8261e16448/call-scatterList/execution/stdout; cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:973); cromwell_1 | at scala.util.Success.$anonfun$map$1(Try.scala:255) cromwell_1 | at scala.util.Success.map(Try.scala:213); cromwell_1 | at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) ; cromwell_1 | at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) cromwell_1 | at scala.concurrent.imp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094:221,error,errors,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094,1,['error'],['errors']
Availability,We should be able to grab the principal ID from the HTTP header via the apache proxy. Get this info from DevOps and demonstrate capability to grab this in the HTTP code. I suspect the easiest path here would be to talk to some folks in firecloud on how to handle this for testing and such when the proxy is not available,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2132:311,avail,available,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2132,1,['avail'],['available']
Availability,"We should leave this open. This is basically the same thing @danbills has been poking at for Firecloud but we weren't able to reproduce it. For their side of things we discovered that they weren't taking advantage of metadata batching, which they're going to change. It likely won't *solve* the issue but should make it robust enough that they don't see it anymore. However the underlying problem is still lurking.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-313531180:320,robust,robust,320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-313531180,1,['robust'],['robust']
Availability,"We track collection via a label injection. If a workflow dies early on, eg MWDA, the label doesn't appear to be persisted and thus the user can't look up errors for their WF",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3225:154,error,errors,154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3225,1,['error'],['errors']
Availability,"We typically have workflow inputs that fall into different categories based on how often they vary.; For example there are inputs that vary according to:; Sample (every time); Genome build (almost never); Plexity (rarely, but sometimes). It would be nice if we could pass multiple workflow jsons to the cromwell server. That would allow us to maintain a static, per-genome build inputs file as well as more dynamic per-run inputs files. The former points to public data and could be distributed among our collaborators when we share our method. We get around the absence of this feature by munging multiple jsons together on every submission. But this makes things harder to share with others. It's also error prone. Priority is low. This is a convenience and community-adoption feature- not a blocker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/982:704,error,error,704,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/982,1,['error'],['error']
Availability,"We want to run green workflows with slimmed down inputs to make sure we haven't broken any of the features they currently use with our changes. First try will be with using Travis and probably moving to Jenkins if it can't be done. This test could be run nightly/weekly, discuss with the team once things are set up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2336:44,down,down,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2336,1,['down'],['down']
Availability,"We want to run green workflows with slimmed down inputs to make sure we haven't broken any of the features they currently use with our changes. The easiest first step would be to make this a part of the Tyburn daily test we already have on Jenkins. This test should probably run weekly, discuss with the team once things are set up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2337:44,down,down,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337,1,['down'],['down']
Availability,"We were recently told that FireCloud has updated it's Cromwell version to 0.24. For our local testing, we had previously been using Cromwell 0.21, so I was trying to upgrade our build system to use 0.24 as well. However, v0.24 seems to have some regressions with respect to our WDLs. In addition after encountering the error cromwell is unkillable without `kill -9`. To reproduce (Note: I am using OSX 10.10.5) :. 1. Download and unzip [cromwell_24_bug.zip](https://github.com/broadinstitute/cromwell/files/745711/cromwell_24_bug.zip). 2. Get v0.21 of cromwell, to verify that the existing WDL runs; ```bash; $ cd cromwell_24_bug; $ curl -L -o cromwell-21.jar https://github.com/broadinstitute/cromwell/releases/download/0.21/cromwell-0.21.jar; $ java -jar cromwell-21.jar run tool_icomut.wdl inputs.json; ```; The workflow should succeed.; ```; ...; [2017-02-01 13:18:32,36] [info] WorkflowManagerActor WorkflowActor-5e2fd288-37a5-44d2-ab8a-a65b2fb5179d is in a terminal state: WorkflowSucceededState; {; ""outputs"": {; ""tool_icomut_workflow.tool_icomut.iCoMut_table"": ""/Users/timdef/tmp/cromwell_24_bug/cromwell-executions/tool_icomut_workflow/5e2fd288-37a5-44d2-ab8a-a65b2fb5179d/call-tool_icomut/execution/TCGA-ACC.coMut_table.txt""; },; ""id"": ""5e2fd288-37a5-44d2-ab8a-a65b2fb5179d""; }; [2017-02-01 13:18:34,64] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; ```. 3. Now download v0.24, and retry:; ```bash; $ curl -L -o cromwell-24.jar https://github.com/broadinstitute/cromwell/releases/download/24/cromwell-24.jar; $ java -jar cromwell-24.jar run tool_icomut.wdl inputs.json; ```. The workflow now fails:; ```; ...; [2017-02-01 13:08:17,13] [error] BackgroundConfigAsyncJobExecutionActor [c290b1fftool_icomut_workflow.tool_icomut:NA:1]: Error attempting to Execute; java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlFileType,None); 	at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:48); 	at wdl4s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1937:319,error,error,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1937,3,"['Down', 'down', 'error']","['Download', 'download', 'error']"
Availability,We will likely need to upgrade liquibase at some point but for the moment PR #4619 does a temporary downgrade. Issue #4618 tracks that MariaDB needs to be CI tested to make sure this doesn't re-occur. Thanks for the report!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605#issuecomment-461564197:100,down,downgrade,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605#issuecomment-461564197,1,['down'],['downgrade']
Availability,"We would like to handle certain JES VM errors in a manner which allows for some automatic retries prior to failing the job. There will be two categories, each handled slightly differently from the other:. - Preemption for preemptible VMs (i.e. Error 14); - JES error codes other than preemption which we believe to be transient (e.g. Error 13). For both situations, the following statements will be true:. - When one of these events happens we will determine if the job is retryable (see below for criteria); - If the job is not retryable it will be failed; - If the job is retryable; -- An event will be sent to the metadata service; -- The retry will be logged; -- The attempt number shall be incremented ; -- The job will be attempted again. In terms of visibility to the end user, preemption shall be handled exactly as it is now. For the other error codes we retry up to 2 times, and there will be a server level configuration option to toggle this behavior on and off. There will be a subtle implementation change for these compared to how preemption is handled now. In the JES backend if a job fails for one of these two reasons and we decide the job is retryable (either because it was preempted and the remaining preemption count is > 0 or non-preemption and the master retry count is > 0) a signal will be sent back to the engine that the job failed but is to be retried. The difference here from the current behavior is that the JES backend is the one tracking the retry counts and merely telling the backend to retry the job instead of the engine tracking the retry count(s).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1910:39,error,errors,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1910,5,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"We would like to handle certain JES VM errors in a manner which allows for some automatic retries prior to failing the job. There will be two categories, each handled slightly differently from the other:. Preemption for preemptible VMs (i.e. Error 14); JES error codes other than preemption which we believe to be transient (e.g. Error 13); For both situations, the following statements will be true:. When one of these events happens we will determine if the job is retryable (see below for criteria); If the job is not retryable it will be failed; If the job is retryable; -- An event will be sent to the metadata service; -- The retry will be logged; -- The attempt number shall be incremented; -- The job will be attempted again; In terms of visibility to the end user, preemption shall be handled exactly as it is now. For the other error codes we retry up to 2 times, and there will be a server level configuration option to toggle this behavior on and off. There will be a subtle implementation change for these compared to how preemption is handled now. In the JES backend if a job fails for one of these two reasons and we decide the job is retryable (either because it was preempted and the remaining preemption count is > 0 or non-preemption and the master retry count is > 0) a signal will be sent back to the engine that the job failed but is to be retried. The difference here from the current behavior is that the JES backend is the one tracking the retry counts and merely telling the backend to retry the job instead of the engine tracking the retry count(s).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1912:39,error,errors,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1912,5,"['Error', 'error']","['Error', 'error', 'errors']"
Availability,"We'd need a way to detect that a job was timed out rather than genuinely preempted (either another error code or by analyzing the total run time).; We'd also need a special case in the ""start this as a preemptible VM?"" logic to not start the subsequent job preemptibly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-293705422:99,error,error,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2168#issuecomment-293705422,1,['error'],['error']
Availability,"We're beginning to get warning during our build because both wdl4s and cromwell are specifying different lenthall versions:. ```scala; [warn] There may be incompatibilities among your library dependencies.; [warn] Here are some of the libraries that were evicted:; [warn] 	* org.broadinstitute:lenthall_2.11:0.20 -> 0.21-e1b7822-SNAP; [warn] Run 'evicted' to see detailed eviction warnings; ```. Options:; 1. wdl4s leaves the lenthall dependency as a compile dependency, the default. cromwell removes its explicit dependency on lenthall, and receives lethall as a transitive dependency through wdl4s. Anytime cromwell wants to update lenthall, wdl4s will need to also be updated and regression tested.; 2. wdl4s marks the lenthall dependency as provided. It must be included in cromwell, wdltool, rawls(?), and other downstream build.sbt's. This option may not catch lenthall version incompatibilities between cromwell and wdl4s until runtime, but will not cause version conflicts in build.sbt.; 3. We ignore the problem, and continue to have sbt output warnings whenever cromwell and wdl4s disagee on a lenthall version. Has similar potential runtime errors as option 2.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1955:817,down,downstream,817,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1955,2,"['down', 'error']","['downstream', 'errors']"
Availability,"We're currently evaluating Cromwell for use automating some fairly large and complicated workflows, and this feature would definitely make automated handoff to downstream users easier. @katevoss, to complete your user story:. As a production pipeline runner, I want to write all output files in one directory (rather than hierarchical), so that I can more easily and automatically locate and pass on those files to the next stage/team in the pipeline (who may not be running Cromwell).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-451557132:160,down,downstream,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1641#issuecomment-451557132,1,['down'],['downstream']
Availability,"We're trying to run on HPC cluster and would prefer to lower the load on the filesystem as much as possible. If we use any of the hashing based caching mechanisms, it hits the filesystem hard which tends to slow everything down. Our production is currently running with ""fingerprint"" and hardlink with singularity containers. The samba mounts on the nodes can do 2Gbps and my cromwell server instance maxes it out pretty much right away. On top of that, doing that much IO over a GPFS mount lead to an increase in GPFS buffer size which balooned enough to kill cromwell server process. We'd like to use ""path+modtime"", so we'd prefer a softlink option. We tested this internally and it works as long as the target location is mounted within the singularity containers at the same location. We also think that cromwell should let the users softlink if they so choose, perhaps with a warning if they're running containers.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040380663:223,down,down,223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040380663,1,['down'],['down']
Availability,"We've now seen a new ""flavor"" of this failure mode, where this same ""Communications link failure"" error is cropping up but it's following some type of HttpRequest failure: ; 2016-05-16 23:42:39,080 cromwell-system-akka.actor.default-dispatcher-18 INFO - WorkflowActor [UUID(e7e4c6d2)]: Collection complete for Scattered Call CollectQualityYieldMetrics.; 2016-05-16 23:42:39,365 cromwell-system-akka.actor.default-dispatcher-10 INFO - WorkflowActor [UUID(e7e4c6d2)]: persisting status of CollectQualityYieldMetrics to Done.; 2016-05-16 23:42:45,957 cromwell-system-akka.actor.default-dispatcher-21 INFO - JES Run [UUID(303ad2dd):ScatterIntervalList]: Status change from Running to Success; 2016-05-16 23:42:50,399 cromwell-system-akka.actor.default-dispatcher-18 INFO - WorkflowActor [UUID(303ad2dd)]: persisting status of ScatterIntervalList to Done.; 2016-05-16 23:42:51,977 cromwell-system-akka.actor.default-dispatcher-10 INFO - WorkflowActor [UUID(303ad2dd)]: persisting status of $scatter_2 to Starting.; 2016-05-16 23:42:55,593 cromwell-system-akka.actor.default-dispatcher-10 INFO - WorkflowActor [UUID(303ad2dd)]: persisting status of $scatter_2 to Done.; 2016-05-16 23:43:15,790 cromwell-system-akka.actor.default-dispatcher-17 INFO - JES Run [UUID(303ad2dd):CollectQualityYieldMetrics:0:2]: Status change from Initializing to Running; 2016-05-16 23:43:18,436 cromwell-system-akka.actor.default-dispatcher-4 INFO - JES Run [UUID(7bbc0491):HaplotypeCaller:35]: Status change from Running to Success; 2016-05-16 23:43:19,178 cromwell-system-akka.actor.default-dispatcher-17 INFO - WorkflowActor [UUID(7bbc0491)]: persisting status of HaplotypeCaller:35 to Done.; 2016-05-16 23:43:29,519 cromwell-system-akka.actor.default-dispatcher-21 ERROR - Error during processing of request HttpRequest(GET,http://app:8000/api/workflows/v1/9c68fe34-7a9e-434a-b958-aa4d91339da9/status,List(Connection: Keep-Alive, X-Forwarded-Server: cromwell.gotc-prod.broadinstitute.org, X-Forwarded-Host: cromwell.gotc-pr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/742#issuecomment-221909650:38,failure,failure,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/742#issuecomment-221909650,4,"['error', 'failure']","['error', 'failure']"
Availability,We've seen a few times the IO error `Failed to evaluate job outputs [...] Futures timed out after [10 seconds]` in tests. I hypothesize that file read times from buckets may suffer from occasional outliers due to 🌩. I know this is the right timeout to change thanks to [this branch](https://github.com/broadinstitute/cromwell/compare/aen_make_it_timeout?expand=1) where I induced error `Failed to evaluate job outputs [...] Futures timed out after [10 microseconds]`,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4036:30,error,error,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4036,2,['error'],['error']
Availability,We've seen in centaur local testing that we fail at times because cromwell is reading an output file before all of its contents has been written and we have no way to detect this (except via the artificial expectation management of centaur). There was some handwaving about filesystems and putting in sleeps but both of these seem like the wrong path. @kcibul this seems like this could go under the reliability umbrella?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1868:400,reliab,reliability,400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1868,1,['reliab'],['reliability']
Availability,"We've seen several workflows fail with this error:. 2016-05-10 11:38:08,737 cromwell-system-akka.actor.default-dispatcher-3 ERROR - WorkflowActor [UUID(972b838f)]: Completion work failed for call CollectUnsortedReadgroupBamQualityMetrics:10.; java.net.SocketTimeoutException: Read timed out;   at java.net.SocketInputStream.socketRead0(Native Method) ~[na:1.8.0_72];   at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) ~[na:1.8.0_72];   at java.net.SocketInputStream.read(SocketInputStream.java:170) ~[na:1.8.0_72];   at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_72];   at sun.security.ssl.InputRecord.readFully(InputRecord.java:465) ~[na:1.8.0_72];   at sun.security.ssl.InputRecord.read(InputRecord.java:503) ~[na:1.8.0_72];   at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72];   at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) ~[na:1.8.0_72];   at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) ~[na:1.8.0_72];   at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_72];   at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_72];   at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_72];   at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704) ~[na:1.8.0_72];   at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647) ~[na:1.8.0_72];   at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536) ~[na:1.8.0_72];   at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441) ~[na:1.8.0_72];   at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) ~[na:1.8.0_72];   at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338) ~[na:1.8.0_72];   at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37) ~[cromwell.jar:0.19];   at ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/826:44,error,error,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/826,2,"['ERROR', 'error']","['ERROR', 'error']"
Availability,"Weird, and definitely a bug! Thanks for reporting... It looks to me like maybe at graph construction time the temporary (anonymous) expression nodes (dashed lines, used to generate input values for calls) are accidentally being added into the pool of ""generally available values"" for expression lookup (but they shouldn't be - they should be one-time-only values for the call expression they're feeding)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3999#issuecomment-418386831:262,avail,available,262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3999#issuecomment-418386831,1,['avail'],['available']
Availability,"Well if it's doing it only when the job is finished I would even make it `warn` instead of `info`. > Right now a workflow is not robust enough to run it on a/our HPC, see also on gitte:; > Peter van 't Hof @ffinfo Aug 26 19:05; > ye I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesn’t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesn’t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that'",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:129,robust,robust,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348,1,['robust'],['robust']
Availability,Well that's what happens when we design something in a way where that's semi-intentional :) We should sit down and figure out how to work all of this in a way which doesn't tie up the whole system (i.e. the reason we went down this path in the first place),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297419330:106,down,down,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2182#issuecomment-297419330,2,['down'],['down']
