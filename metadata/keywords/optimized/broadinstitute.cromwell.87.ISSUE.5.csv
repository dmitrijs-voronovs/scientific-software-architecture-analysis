quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Deployability,```The code passed to eventually never returned normally. Attempted 51 times over 3.8950992149999997 seconds. Last failure message: HighLoad was not equal to NormalLoad.```. `https://broadinstitute.atlassian.net/browse/GAWB-3876`. ```tc: LoadControllerServiceActor should update global load level periodically (each time with same test case)```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4237:272,update,update,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4237,1,['update'],['update']
Deployability,"`centaurCwlConformancePAPI` doesn't have the same credential check as `centaurJes`, and so is failing when users external users try to contribute patches. - Example [checking for creds](https://github.com/broadinstitute/cromwell/blob/e7b0833d53a305039af7224879cd26ceab1f1881/src/bin/travis/testCentaurJes.sh#L3-L12) and NOP passing: ; https://travis-ci.org/broadinstitute/cromwell/jobs/324545050; - Example [missing](https://github.com/broadinstitute/cromwell/blob/develop/src/bin/travis/testCentaurCwlConformancePAPI.sh) the above check and failing CI: ; https://travis-ci.org/broadinstitute/cromwell/jobs/324545054",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3102:146,patch,patches,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3102,1,['patch'],['patches']
Deployability,`mkdocs serve` if you want to try this out locally (`brew install mkdocs` may be required).,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6581:58,install,install,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6581,1,['install'],['install']
Deployability,"`pipeline.getResources.getVirtualMachine.getPreemptible` is intended to be `null` if there was no preemption. Aaron says,; >Client code should be updated to properly access these fields even if they are absent. Closes https://github.com/broadinstitute/cromwell/issues/4772",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4773:1,pipeline,pipeline,1,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4773,2,"['pipeline', 'update']","['pipeline', 'updated']"
Deployability,"`sbt doc` was erroring out, preventing `sbt publish` from running and pushing artifacts to artifactory. Now `sbt doc` is fixed and continuously tested.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1661#issuecomment-259710497:131,continuous,continuously,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1661#issuecomment-259710497,1,['continuous'],['continuously']
Deployability,"`womgraph` produced vastly too much output for my (apparently) complex pipeline. `dot` couldn't render it in png at all and produced a corrupted pdf file. I think that an option that produces ""simple"" output that non-developers can look at and mostly comprehend, like the old `graph` option, is still very desirable.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561411295:71,pipeline,pipeline,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234#issuecomment-561411295,1,['pipeline'],['pipeline']
Deployability,`womtool graph germline.wdl`; I got an error:; ![image](https://github.com/broadinstitute/cromwell/assets/61043072/03a9e638-6f63-45b7-ab74-a81d599d9de9). wdl and version:; germline.wdl(https://github.com/biowdl/germline-DNA/releases/download/v5.0.0/germline_v5.0.0.wdl); womtool:; ```shell; # Name Version Build Channel; _libgcc_mutex 0.1 conda_forge conda-forge; _openmp_mutex 4.5 2_gnu conda-forge; argcomplete 1.12.3 pyhd8ed1ab_0 conda-forge; atomicwrites 1.4.1 pyhd8ed1ab_0 conda-forge; attrs 23.1.0 pyh71513ae_1 conda-forge; bcrypt 3.2.2 py310h5764c6d_1 conda-forge; brotli-python 1.0.9 py310hd8f1fbe_9 conda-forge; bullet-python 2.2.0 py310hff52083_6 conda-forge; bzip2 1.0.8 h7f98852_4 conda-forge; c-ares 1.19.1 hd590300_0 conda-forge; ca-certificates 2023.7.22 hbcca054_0 conda-forge; certifi 2023.7.22 pyhd8ed1ab_0 conda-forge; cffi 1.15.1 py310h255011f_3 conda-forge; charset-normalizer 3.2.0 pyhd8ed1ab_0 conda-forge; colorama 0.4.6 pyhd8ed1ab_0 conda-forge; coloredlogs 15.0.1 pyhd8ed1ab_3 conda-forge; cromwell 0.40 1 bioconda; cryptography 41.0.2 py310h75e40e8_0 conda-forge; docker-py 6.1.3 pyhd8ed1ab_0 conda-forge; exceptiongroup 1.1.2 pyhd8ed1ab_0 conda-forge; expat 2.5.0 hcb278e6_1 conda-forge; findutils 4.6.0 h166bdaf_1001 conda-forge; font-ttf-dejavu-sans-mono 2.37 hab24e00_0 conda-forge; fontconfig 2.14.2 h14ed4e7_0 conda-forge; freetype 2.12.1 hca18f0e_1 conda-forge; gettext 0.21.1 h27087fc_0 conda-forge; humanfriendly 10.0 py310hff52083_4 conda-forge; idna 3.4 pyhd8ed1ab_0 conda-forge; importlib-metadata 6.8.0 pyha770c72_0 conda-forge; importlib_metadata 6.8.0 hd8ed1ab_0 conda-forge; importlib_resources 6.0.0 pyhd8ed1ab_1 conda-forge; iniconfig 2.0.0 pyhd8ed1ab_0 conda-forge; jinja2 3.1.2 pyhd8ed1ab_1 conda-forge; js2py 0.74 pyhd8ed1ab_0 conda-forge; jsonschema 4.18.4 pyhd8ed1ab_0 conda-forge; jsonschema-specifications 2023.7.1 pyhd8ed1ab_0 conda-forge; keyutils 1.6.1 h166bdaf_0 conda-forge; krb5 1.21.1 h659d440_0 conda-forge; lark 1.1.7 pyhd8ed1ab_0 conda-for,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7212:224,release,releases,224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7212,1,['release'],['releases']
Deployability,"a) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the mo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1512,release,releases,1512,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,1,['release'],['releases']
Deployability,"a27"" && echo ""/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/tmp.bf92fa27""); chmod 777 ""$tmpDir""; export _JAVA_OPTIONS=-Djava.io.tmpdir=""$tmpDir""; export TMPDIR=""$tmpDir""; export HOME=""$HOME""; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. ); outed746149=""${tmpDir}/out.$$"" erred746149=""${tmpDir}/err.$$""; mkfifo ""$outed746149"" ""$erred746149""; trap 'rm ""$outed746149"" ""$erred746149""' EXIT; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stdout.log' < ""$outed746149"" &; tee '/gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-stderr.log' < ""$erred746149"" >&2 &; (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicate_CAATGAGC-TATCGCAC.merged_R2.fq.gz .; ) > ""$outed746149"" 2> ""$erred746149""; echo $? > /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; sync. ); mv /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5421:2069,pipeline,pipeline,2069,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421,1,['pipeline'],['pipeline']
Deployability,"a818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-20/SR00c.NA19001.txt.gz /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-20/SR00c.NA19001.txt.gz; > /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-43/SR00c.NA20509.txt.gz /tmp/scratch/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-43/SR00c.NA20509.txt.gz; > /usr/local/aws-cli/v2/current/bin/aws s3 cp --no-progress s3://aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-32/SR00c.NA19678.txt.gz /tmp/scratch/s3:/aaaaaaa-gwf-core/cromwell-execution/Module00c/ab433d98-4b83-4bc5-bb21-ec8f057d81af/call-EvidenceMerging/EvidenceMerging/f4fa818c-5f46-4891-a26a-0f0368fef639/call-SetSampleIdSR/shard-32/SR00c.NA19678.txt.gz; >; > {code}; >; > (sorry for the long log); > As you can see, in the last line, the output path of s3 copy is malformed,; > there is an 's3:/' lost there. This causes the whole pipeline to fail. I; > already tried several times and sometimes it happens, sometimes don't. Also; > when it happens, it's not always in the same shard. Do you have any ideia; > why this is happening?; > Thanks in advance; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6106>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ELH2MI44ZN2D3LYJZTSSOGSVANCNFSM4UHQIFCA>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6106#issuecomment-738953857:15462,pipeline,pipeline,15462,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6106#issuecomment-738953857,1,['pipeline'],['pipeline']
Deployability,"a9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:6:1]: Status change from Running to Success; 2019-01-18 18:43:33,255 cromwell-system-akka.dispatchers.engine-dispatcher-5 ERROR - WorkflowManagerActor Workflow dba9b85f-e9ea-4e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.Cal",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:1792,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1792,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,a:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(Strea,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:3944,pipeline,pipeline,3944,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,1,['pipeline'],['pipeline']
Deployability,a:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipeli,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:4674,pipeline,pipeline,4674,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,3,['pipeline'],['pipeline']
Deployability,"abase was created with version 0.8.2) Consider calling the analyze() method of this object.; ""method of this object."" % self.version); Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 58, in process; out = fn(fnargs); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/utils.py"", line 52, in wrapper; return apply(f, *args, **kwargs); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/multitasks.py"", line 208, in pipeline_summary; return qcsummary.pipeline_summary(*args); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/pipeline/qcsummary.py"", line 70, in pipeline_summary; data[""summary""] = _run_qc_tools(work_bam, work_data); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/pipeline/qcsummary.py"", line 162, in _run_qc_tools; out = qc_fn(bam_file, data, cur_qc_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/qc/qualimap.py"", line 347, in run_rnaseq; metrics = _parse_metrics(metrics); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/qc/qualimap.py"", line 210, in _parse_metrics; out.update({name: float(metrics[name])}); TypeError: float() argument must be a string or a number; ```. This is what the command Cromwell generated looks like:. ```; 'bcbio_nextgen.py' 'runfn' 'pipeline_summary' 'cwl' 'sentinel_runtime=cores,2,ram,4096' 'sentinel_parallel=multi-parallel' 'sentinel_outputs=qcout_rec:summary__qc;summary__metrics;resources;description;reference__fasta__base;config__algorithm__coverage_interval;genome_build;genome_resources__rnaseq__transcripts;config__algorithm__tools_off;config__algorithm__qc;analysis;config__algorithm__tools_on;align_bam' 'sentinel_inputs=qc_rec:record' 'run_number=",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:5513,pipeline,pipeline,5513,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['pipeline'],['pipeline']
Deployability,"ached. By default hsqldb uses in memory tables. ; # Setting this to cache for improved memory usage.; # hsqldb.result_max_memory_rows=10000 . Limits the amount of rows in memory for temp tables; # hsqldb.tx=mvcc cromwell default. Not changing it. Not clear what this does. http://hsqldb.org/doc/guide/sessions-chapt.html#snc_tx_mvcc; # hsqldb.large_data=true. Cromwell creates huge DBs that need to be opened.; # hsqldb.applog=1. Log errors.; # hsqldb.lob_compressed=true. Compress lobs. This saves a lot of space.; # hsqldb.script_format=3. Compress script. (uses gzip internally).; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; # Override the cromwell default of only 3 seconds (3000 milliseconds) and allow for 300s to read the database file.; connectionTimeout = 300000; numThreads = 1; }; }; ```; Please note the `connectionTimeout = 300000` where we set the connection timeout to 5 minutes. This works for most cases. On a side note: HSQLDB has got to be the worst performing embedded database designed in the history of mankind. When running a decent-sized WDL workflow it can get 30 GB in memory! When using the file-based database it still needs 2 GB in memory (on top of the 1 GB that cromwell needs) is very slow, and creates a multiple GB file database. (EDIT: I checked my multiple run 100 sample RNA-seq pipeline that has run multiple times, using call-caching and sometimes with slightly different settings: **85 GB** in files for the database.); MySQL performs much better with cromwell, but is infeasible to use in a per-project, per-user fashion. [I am working on solving the problem using SQLite](https://github.com/broadinstitute/cromwell/issues/5490) but there is NO ETA, and I don't know if I will ever get it to work. Still it is worth trying.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757:1985,pipeline,pipeline,1985,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-624458757,1,['pipeline'],['pipeline']
Deployability,"ackend.scala:46); at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef.acquireSession(BasicBackend.scala:250); at slick.basic.BasicBackend$DatabaseDef.acquireSession$(BasicBackend.scala:249); at slick.jdbc.JdbcBackend$DatabaseDef.acquireSession(JdbcBackend.scala:37); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:275); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: org.postgresql.util.PSQLException: FATAL: sorry, too many clients already; at org.postgresql.core.v3.ConnectionFactoryImpl.doAuthentication(ConnectionFactoryImpl.java:514); at org.postgresql.core.v3.ConnectionFactoryImpl.tryConnect(ConnectionFactoryImpl.java:141); at org.postgresql.core.v3.ConnectionFactoryImpl.openConnectionImpl(ConnectionFactoryImpl.java:192); at org.postgresql.core.ConnectionFactory.openConnection(ConnectionFactory.java:49); at org.postgresql.jdbc.PgConnection.<init>(PgConnection.java:195); at org.postgresql.Driver.makeConnection(Driver.java:454); at org.postgresql.Driver.connect(Driver.java:256); at com.zaxxer.hikari.util.DriverDataSource.getConnection(DriverDataSource.java:136); at com.zaxxer.hikari.pool.PoolBase.newConnection(PoolBase.java:369); at com.zaxxer.hikari.pool.PoolBase.newPoolEntry(PoolBase.java:198); at com.zaxxer.hikari.pool.HikariPool.createPoolEntry(HikariPool.java:467); at com.zaxxer.hikari.pool.HikariPool.access$100(HikariPool.java:71); at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:706); at com.zaxxer.hikari.pool.HikariPool$PoolEntryCreator.call(HikariPool.java:692); at java.util.concurrent.FutureTask.run(FutureTask.java:266); ... 3 common frames omitted; ```. </details>. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6208:5130,configurat,configuration,5130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6208,1,['configurat'],['configuration']
Deployability,"ackends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cro",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2084,pipeline,pipelinesRunner,2084,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,1,['pipeline'],['pipelinesRunner']
Deployability,actor.default-dispatcher-561] ERROR c.e.workflow.WorkflowManagerActor - WorkflowManagerActor Workflow f0000000-0000-0000-0000-000000000000 failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.TumorCramToBam:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. The zone 'projects/crashy/zones/us-west1-a' does not have enough resources available to fulfill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExec,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4920:1026,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1026,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,add CromIAM endpoint to update collection for workflow(s),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2839:24,update,update,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2839,1,['update'],['update']
Deployability,add install instructions for OS X,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/335:4,install,install,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/335,1,['install'],['install']
Deployability,add integration test capability,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2375:4,integrat,integration,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2375,1,['integrat'],['integration']
Deployability,added support for a FileRoller logback configuration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1692:39,configurat,configuration,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1692,1,['configurat'],['configuration']
Deployability,added support for a FileRoller logback configuration (#1692),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1710:39,configurat,configuration,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1710,1,['configurat'],['configuration']
Deployability,"adding polling async. Runs successfully, but does not update status in cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6974:54,update,update,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6974,1,['update'],['update']
Deployability,"additional tests](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519) (and something entirely different) that it made me step back. I had this growing feeling that started to solidify that there are too many layers. I am developing things and I **still** don't understand (or think Singularity is ready yet) to be any kind of backend. I'm forcing a dog into a cat shaped hole just because this is the hole I'm supposed to fill. Is that a good idea? I've lost sight of what the tool is trying to do. Cromwell is trying to make it easy to run a Singularity container. But if that's the case, then why has this command:. ```bash; singularity run shub://vsoch/hello-world; ```. turned into needing Cromwell (java and the jar), an inputs json file, a wdl specification, a backend configuration, and a runtime command that I can't seem to remember, and then the entire thing takes much longer than an instance to echo a tiny Rawwwwr! If this is the goal we are going for, is this making life easier for the scientist? If I'm a programmer person, and this is the minimum I am allowed for this to just run a simple container, what happens when it gets harder? I realized that without a proper services API, singularity is no more special than python, bash, samtools, it's just a binary. . And I realize also that it's easy to get caught up in details like ""Should we use Travis or Circle?"" Does it work on Amazon with this kind of input? And there will always be bugs! But I think the forest is being a bit lost for the trees. . ## Question 4: What is the direction to go in?. You can probably take what I'm saying with a grain of salt because I'm new to this entire universe, and there is so much invested there is no turning back or rethinking. But all of this seems too complicated, and too hard. What is needed is a solution that is just really stupid and simple. You have a container that understands its data. You point the container at a dataset and run it. You outsource the w",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:9784,configurat,configuration,9784,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,1,['configurat'],['configuration']
Deployability,"ailed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$Bloc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:1460,pipeline,pipelines,1460,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,1,['pipeline'],['pipelines']
Deployability,"ailing with an error because the `--read` argument is missing (even though you can see it's not, in the above log):; ```; cromwell_1 | java.lang.Exception: Task germline_variant_calling.fastqc:0:1 failed. The job was stopped before the command finished. PAPI error code 10. 11: Docker run failed: command failed: usage: fastqc_docker.py [-h] -r READ -o OUTPUT_DIR [-c CONTAMINANTS]; cromwell_1 | [-a ADAPTERS] [-l LIMITS] [-f FORMAT] [-n NO_GROUP]; cromwell_1 | [-e EXTRA_OPTIONS]; cromwell_1 | fastqc_docker.py: error: argument -r/--read is required; cromwell_1 | . See logs at gs://genovic-cromwell/cromwell-execution/trio/f5454139-c51d-4d04-ae0a-9b9d4ce650aa/call-germline_variant_calling/shard-0/germline_variant_calling/5d4c4459-a91c-4d3b-8ca4-b98457134750/call-fastqc/shard-0/; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); cromwell_1 | at scala.concurrent.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:1863,pipeline,pipelines,1863,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['pipeline'],['pipelines']
Deployability,"airedSingleSampleWf](https://github.com/broadinstitute/wdl/blob/develop/scripts/broad_pipelines/PublicPairedSingleSampleWf_160927.wdl) using Cromwell 0.19.3 and it was consistently failing with the following:. ```; 2016-10-06 21:32:38,239 cromwell-system-akka.actor.default-dispatcher-41 INFO - JES Run [UUID(65d59b8b):SamToFastqAndBwaMem:1]: Status change from Running to Success; 2016-10-06 21:32:38,586 cromwell-system-akka.actor.default-dispatcher-4 ERROR -WorkflowActor [UUID(65d59b8b)]: Completion work failed for call SamToFastqAndBwaMem:1.; java.sql.SQLDataException: data exception: string data, right truncation; table: EXECUTION_EVENT column: DESCRIPTION; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCPreparedStatement.fetchResult(Unknown Source) ~[cromwell.jar:0.19]; at org.hsqldb.jdbc.JDBCPreparedStatement.executeUpdate(Unknown Source) ~[cromwell.jar:0.19]; <snip>; ```. I had run the previous version of the PublicPairedSingleSampleWf successfully. I finally realized that it had nothing to do with the workflow, but rather because I changed the GCS path for Cromwell's `baseExecutionBucket` to something much longer. Instead of my previous:. ```; gs://my-bucket/gatk-test/; ```. I had changed to:. ```; gs://my-bucket/broad_pipelines/PublicPairedSingleSampleWf_160927/; ```. When I went back and shortened the path, everything worked again. Along the way, I ran a similar test where I changed the task name for my test [vcf_chr_count.wdl](https://github.com/googlegenomics/pipelines-api-examples/blob/master/wdl_runner/workflows/vcf_chr_count/vcf_chr_count.wdl) from:. ```; task vcf_split; ```. to . ```; task vcf_split_what_if_i_have_a_really_long_task_is_that_the_problem; ```. This also generated the same exception:. ```; java.sql.SQLDataException: data exception: string data, right truncation; table: EXECUTION_EVENT column: DESCRIPTION; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1555:1658,pipeline,pipelines-api-examples,1658,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1555,1,['pipeline'],['pipelines-api-examples']
Deployability,akka version update,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1188:13,update,update,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1188,1,['update'],['update']
Deployability,"akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861:1317,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"al file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initially:. ```HOCON; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text&journal_mode=truncate""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:?foreign_keys=true&date_class=text""; numThreads=1; }; }; }; ```; This limits the amount of IO operations to the bare minimum to get call-caching working. With this configuration cromwell was able to rerun the callcached 1000+ job workflow in ~25 minutes. However it crashed when it needed to write jobs failing with a SQLite error reporting no such table: `METADATA_ENTRY`. EDIT3: I found that using the SQLite in-memory database allows Cromwell to exceed limits that are set using `-Xmx=1G` for instance. I think this is acceptable, as SQLite does not create very large databases and it allows for using Cromwell with the same setting everywhere, no matter whether the workflow is large or small. It is a gotcha to be aware of, however.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:2712,configurat,configuration,2712,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906,1,['configurat'],['configuration']
Deployability,al run.; The initial run produces this command:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/257197fe-a034-4c51-addc-3dc47564b967/call-sample/shard-0/sample/0e4e2e8d-d042-4907-9fb5-1d4f894f8366/call-library/shard-0/library/d995de1f-ecc6-4034-b35b-a860506821ff/call-starAlignment/AlignStar/d339740c-c1be-44ae-b4dd-8db2b47237c7/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The rerun uses this:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The second argument changes from the intended path to a path inside of the execution folder. It looks like the output from the preceding mapping job gets linked to in the execution fold,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717:1206,pipeline,pipeline,1206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717,2,['pipeline'],['pipeline']
Deployability,ala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.e,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:3833,pipeline,pipeline,3833,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,1,['pipeline'],['pipeline']
Deployability,"ala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:239); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:237); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```; The Cromwell configuration is:; ```; system {; workflow-restart = true; }; call-caching {; enabled = true; }. database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:file:/n/groups/bcbio/cwl/test_bcbio_cwl/somatic/cromwell_work/persist/metadata;shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 20000; }; }. backend {; providers {; Local {; config {; runtime-attributes = """"""; Int? cpu; Int? memory_mb; """"""; submit-docker: """". filesystems {; local {; caching {; duplication-strategy: [""soft-link""]; hashing-strategy: ""path""; }; }; }. }; }. }; }; ```; Does this provide enough information to identify what might be happening? Thanks for any thoughts or clues about avoid this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3607:5911,configurat,configuration,5911,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607,1,['configurat'],['configuration']
Deployability,"all_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadP",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:35571,update,update,35571,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,1,['update'],['update']
Deployability,"also, once it's looking good, we'll want a hotfix edition of this changeset",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5509#issuecomment-624746718:43,hotfix,hotfix,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5509#issuecomment-624746718,1,['hotfix'],['hotfix']
Deployability,"alue_2_give"": ""String (optional, default = \""default value\"")""; }; ```; We see that we are able to provide a value for ""overwrite_given_value"" of the SubTask. . I have tried to add the key to this anyway to the MainWorkflow input but womtools won't accept it:; ```; $ cat MainWorkflow_inputs.json; {; ""MainWorkflow.SubWorkflow.SubTask.overwrite_given_value"": ""test""; }; $ java -jar womtool-84.jar validate MainWorkflow.wdl -i MainWorkflow_inputs.json; WARNING: Unexpected input provided: MainWorkflow.SubWorkflow.SubTask.overwrite_given_value (expected inputs: [MainWorkflow.MainTask.overwrite_given_value, MainWorkflow.SubWorkflow.overwrite_value_2_give, MainWorkflow.value_2_give]); ```. **Conclusion**; This let me to the conclusion that it is not possible to provide values for tasks called in subworkflows. I'm not sure if this is a bug or a missing feature but it would really be helpful if it is possible to implement especially given that it is already possible for the main workflows. **Motivation**; I run and develop genomics pipelines that are using WDL+Cromwell with a HPC Slurm back-end. For each task we require variables that tells the back-end how much resources it need in order to complete it. By default these values are calculated or given by the workflow which is calling the task however it can happen that some tasks need a little bit more to complete. As you cannot overwrite variables that are given by the workflow that calls them, we decided to use optional task variables and the select_first function as a workaround. However given that my pipeline are using subworkflows a lot we cannot do so unless we move those variables up to the input of the workflow which results in a huge list of optional variables. It would be save a lot of work if this could be fixed/added. **End Note**; Thanks for reading my issue and I hope this can be easily be solved. ; _Disclaimer: I do not often write issues so feel free to give feedback and let me know if I missed a convention. ðŸ˜‡_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6841:3385,pipeline,pipelines,3385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6841,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"aml/snakeyaml/src/master/CHANGELOG.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.33).; You might want to review and update them manually.; ```; core/src/test/resources/hello_goodbye_scattered_papiv2.json; docs/developers/bitesize/ci/Cromwell_Deployment_Strategies.svg; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/vers",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7081:2155,update,update,2155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7081,1,['update'],['update']
Deployability,"aml/snakeyaml/src/master/CHANGELOG.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/00809e6249b134635f71919c17c1c81603beb22d/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.29).; You might want to review and update them manually.; ```; centaur/src/test/resources/centaur/test/metadata/failingInSeveralWaysMetadata.json; core/src/test/resources/hello_goodbye_scattered_papiv2.json; docs/developers/bitesize/ci/Cromwell_Deployment_Strategies.svg; docs/developers/bitesize/workflowParsing/forkjoin_graph.svg; docs/developers/bitesize/workflowParsing/wdlToWdlom_hermes.svg; scripts/metadata_comparison/test/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6635:2156,update,update,2156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6635,1,['update'],['update']
Deployability,"aml/snakeyaml/src/master/CHANGELOG.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.30).; You might want to review and update them manually.; ```; docs/developers/bitesize/workflowParsing/wdlToWdlom_hermes.svg; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; ```; </details>; <details>; <summary>Adjust future updates</sum",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6900:2156,update,update,2156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6900,1,['update'],['update']
Deployability,"and"": ""/opt/gridss/gridss.sh"",; ""arguments"": [; {; ""prefix"": ""--threads"",; ""valueFrom"": ""$get_threads_val(inputs)""; }; ],; ""inputs"": [; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""Optional - location of the GRIDSS assembly BAM. This file will be created by GRIDSS.\n"",; ""inputBinding"": {; ""prefix"": ""--assembly""; },; ""default"": "".assembly.bam"",; ""id"": ""#gridss-2.9.4.cwl/assembly""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional - BED file containing regions to ignore\n"",; ""inputBinding"": {; ""prefix"": ""--blacklist""; },; ""id"": ""#gridss-2.9.4.cwl/blacklist""; },; {; ""type"": ""string"",; ""doc"": ""portion of 6 sigma read pairs distribution considered concordantly mapped. Default: 0.995\n"",; ""inputBinding"": {; ""prefix"": ""--concordantreadpairdistribution""; },; ""default"": ""0.995"",; ""id"": ""#gridss-2.9.4.cwl/concordantreadpairdistribution""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional - configuration file use to override default GRIDSS settings.\n"",; ""inputBinding"": {; ""prefix"": ""--configuration""; },; ""id"": ""#gridss-2.9.4.cwl/configuration""; },; {; ""type"": [; ""null"",; ""boolean""; ],; ""doc"": ""Optional - use the system version of bwa instead of the in-process version packaged with GRIDSS\n"",; ""inputBinding"": {; ""prefix"": ""--externalaligner""; },; ""default"": false,; ""id"": ""#gridss-2.9.4.cwl/externalaligner""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""Optional - location of GRIDSS jar\n"",; ""inputBinding"": {; ""prefix"": ""--jar""; },; ""default"": ""/opt/gridss/gridss-2.9.4-gridss-jar-with-dependencies.jar"",; ""id"": ""#gridss-2.9.4.cwl/jar""; },; {; ""type"": ""boolean"",; ""doc"": ""zero-based assembly job index (only required when performing parallel assembly across multiple computers)\n"",; ""inputBinding"": {; ""prefix"": ""--jobindex""; },; ""default"": false,; ""id"": ""#gridss-2.9.4.cwl/jobindex""; },; {; ""type"": ""boolean"",; ""doc"": ""total number of assembly jobs (only required when performing parallel assembly across multiple computers). Note than an assembly jobs is required after all",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:11419,configurat,configuration,11419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['configurat'],['configuration']
Deployability,"and:; nohup java -Dconfig.file=My.conf -jar cromwell-87-5448b85-SNAP-pre-edits.jar run ~/MemoryRetryTest.wdl 2>&1 > nohup.out. MemoryRetryTest.wdl:; workflow MemoryRetryTest {; 	String message = ""Killed""; 	; 	call TestOutOfMemoryRetry {}; 	call TestBadCommandRetry {}; }. task TestOutOfMemoryRetry {; 	command <<<; 		free -h; 		df -h; 		cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		tail /dev/zero; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; 	; }. task TestBadCommandRetry {; 	command <<<; free -h; df -h; cat /proc/cpuinfo. 		echo ""Killed"" >&2; 		bedtools intersect nothing with nothing; 	>>>; 	; 	runtime {; 		cpu: ""1""; 		memory: ""1 GB""; 		maxRetries: 4; 		continueOnReturnCode: 0; 	}; }. My.conf:. include required(classpath(""application"")). system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }. backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory"". system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; config {; project = ""$my_project""; root = ""$my_bucket""; name-for-call-caching-purposes: PAPI; slow-job-warning-time: 24 hours; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600. # Setup GCP to give more memory with each retry; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; ; # Number of workers to assign to PAPI requests; request-workers = 3. virtual-private-cloud {; network-label-key = ""network-key""; network-name = ""network-name""; subnetwork-name = ""subnetwork-name""; auth = ""auth""; }; pipeline-timeout = 7 days; genomics {; auth = ""auth""; compute-service-account = ""$my_account""; endpoint-url = ""https://lifesciences.googleapis.com/""; location = ""us-central1""; restrict-metadata-access = false; localization-attempts = 3; parallel-composite-uploa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7451:1186,Pipeline,PipelinesApiLifecycleActorFactory,1186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451,1,['Pipeline'],['PipelinesApiLifecycleActorFactory']
Deployability,"aning up the copying resources. If the restart solves the problem then you may want to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the workflow bucket? If not, are they in the same region?; > [â€¦](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf â€¦ <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1268,release,release,1268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,2,"['configurat', 'release']","['configuration', 'release']"
Deployability,any update on this?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-583565273:4,update,update,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4579#issuecomment-583565273,1,['update'],['update']
Deployability,any updates?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1288984564:4,update,updates,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6221#issuecomment-1288984564,1,['update'],['updates']
Deployability,"aplotypecaller.HC_GVCF:1:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr10.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:6258,Pipeline,Pipeline,6258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,1,['Pipeline'],['Pipeline']
Deployability,"aplotypecaller.HC_GVCF:6:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr15.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] Submitting job to AWS Batch; [2019-05-22 19:19:19,34] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignandmolvar:1.3.2; [2019-05-22 19:19:19,34] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4; [2019-05-22 19:19:19,34] [info] taskId: Haplotypecaller.HC_GVCF-Some(1)-1; [2019-05-22 19:19:19,34] [info] hostpath root: hc.Haplotypecaller/hc.HC_GVCF/755021ae-948b-47f9-94a8-66b486bda47d/Some(1)/1; ...; [2019-05-22 19:19:19,34] [info] Submitting job to AWS Batch; [2019-05-22 19:19:19,34] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignandmolvar:1.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:7635,Pipeline,Pipeline,7635,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,1,['Pipeline'],['Pipeline']
Deployability,"ard error: ""Error response from daemon: pull access denied for gcr.io/broad-cumulus/cellranger, repository does not exist or may require 'docker login': denied: Permission denied for \""sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"" from request \""/v2/broad-cumulus/cellranger/manifests/sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"".\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:91); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:812); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:95); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1340); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1336); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:14059,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,14059,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"ard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3732:1847,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1847,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,ardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at crom,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:2141,pipeline,pipelines,2141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['pipeline'],['pipelines']
Deployability,ardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:2059,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2059,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"arne failed. Error: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 2019-01-31 19:38:58,618 INFO - changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Successfully released change log lock; 2019-01-31 19:38:58,637 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.ru",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:1438,Update,UpdateVisitor,1438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['Update'],['UpdateVisitor']
Deployability,"as per http://akka.io/blog/news/2017/05/03/akka-http-10.0.6-released#compatibility-notes, to use akka-actor and akka-http together; #2579",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2580:60,release,released,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2580,1,['release'],['released']
Deployability,"aster/Changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.33).; You might want to review and update them manually.; ```; core/src/test/resources/hello_goodbye_scattered_papiv2.json; docs/developers/bitesize/ci/Cromwell_Deployment_Strategies.svg; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; src/ci/resources/papi_v2_reference_image_manifest.conf; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.yaml"", artifactId = ""snakeyaml"" }; }]; ```; </details>. labels: test-library-update, old-version-remains, com",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7081:2788,update,update,2788,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7081,1,['update'],['update']
Deployability,"at [`gsutil` must use JSON API (not CLI)](https://cloud.google.com/storage/docs/requester-pays) for ""requester pays"" buckets. Is there any plan to support ""requester pays"" buckets for JES backend? . ```; [2017-11-18 16:01:09,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: python $(which encode_bowtie2.py) \; /cromwell_root/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/d57a5f97-8542-4fcc-89c4-b7c487957dea/call-trim_adapter/shard-0/glob-019a547c7b0dda79121d0398158a07d0/ENCFF439VSY.trim.merged.R1.fastq.gz \; \; --multimapping 4 \; \; --nth 4; [2017-11-18 16:01:09,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: python $(which encode_bowtie2.py) \; /cromwell_root/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/d57a5f97-8542-4fcc-89c4-b7c487957dea/call-trim_adapter/shard-1/glob-019a547c7b0dda79121d0398158a07d0/ENCFF463QCX.trim.merged.R1.fastq.gz \; \; --multimapping 4 \; \; --nth 4; [2017-11-18 16:01:18,97] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: job id: operations/EJSf0Yz9Kxjs8__E9aKWivQBILWN-vrbGyoPcHJvZHVjdGlvblF1ZXVl; [2017-11-18 16:01:18,97] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: job id: operations/EJWg0Yz9KxiXpeC4gsSenC4gtY36-tsbKg9wcm9kdWN0aW9uUXVldWU; [2017-11-18 16:01:30,30] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: Status change from - to Initializing; [2017-11-18 16:01:30,30] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: Status change from - to Initializing; [2017-11-18 17:44:21,09] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: Status change from Initializing to Running; [2017-11-18 19:30:04,06] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: Status change fro",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916:1434,pipeline,pipeline-workflows,1434,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916,1,['pipeline'],['pipeline-workflows']
Deployability,"at is specified for the task is in a private repo to which the Centaur service account has been granted access. This test passes on PAPI v2 but on GCP Batch jobs fail with messages like the following visible in `gcloud batch jobs describe`:. ```; Job state is set from RUNNING to FAILED for job projects/1005074806481/locations/us-central1/jobs/job-27607753-d2d5-404d-89af-a786da8ad383.Job; failed due to task failure. Specifically, task with index 0 failed due to the; following task event: ""Task state is updated from RUNNING to FAILED on zones/us-central1-b/instances/8098872438472929780; with exit code 125."". ```. Exit code 125 being a typical ""[something's wrong with that Docker invocation](https://stackoverflow.com/questions/53640424/exit-code-125-from-docker-when-trying-to-run-container-programmatically)"" error. in Cloud Logging I see the following, including what looks like a plaintext password which I have x'd out below:. ```; Executing runnable container:{image_uri:""broadinstitute/cloud-cromwell@sha256:0d51f90e1dd6a449d4587004c945e43f2a7bbf615151308cff40c15998cc3ad4"" commands:""/mnt/disks/cromwell_root/script"" entrypoint:""/bin/bash"" volumes:""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root"" username:""firecloud"" password:""xxxxx""} labels:{key:""tag"" value:""UserRunnable""} for Task task/job-27607753-d2d5-132dc052-df92-4db100-group0-0/0/0 in TaskGroup group0 of Job job-27607753-d2d5-132dc052-df92-4db100.; ```. So it looks like the GCP Batch backend has acquired and plumbed through the required Docker credentials, but the login to Docker Hub doesn't seem to have happened. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515:1982,Release,Release,1982,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686:1587,pipeline,pipeline,1587,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686,1,['pipeline'],['pipeline']
Deployability,"atabase; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pipelines/results/graywhale/transcriptome/diamond/blastp""; ```; when I run the workflow many times with input changes I get the following:; ```. name | status | stdout | stderr | cache | shard; -- | -- | -- | -- | -- | --; Diamond_Blast.copy_results | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stderr | Cache Hit: fdda40c0-a501-456c-a903-954aa52af83d:Diamond_Blast.copy_results:-1 | -1; Diamond_Blast.diamond_blast | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stderr | Cache Miss | -1; ```; that shows that the task diamond_blast recomputes while the copy task copies the cached result, in other words it sends the same file as previous time, totally ignoring the changes of output in the diamond_blast task from which it depends!!!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3044:2258,pipeline,pipelines,2258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044,4,['pipeline'],['pipelines']
Deployability,"ate directory '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': Permission denied; ï…; 04:26:11; /bin/bash: line 66: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe/cromwell_glob_control_file: No such file or directory; ï…; 04:26:11; ln: failed to access '/cromwell_root/cwl.output.json': No such file or directory; ï…; 04:26:11; /bin/bash: line 72: /cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe.list: Permission denied; ï…; 04:26:11; ls: cannot access '/cromwell_root/glob-b34dfc006a981a93d6da067cf50036fe': No such file or directory; ï…; 04:26:11; mv: cannot stat '/cromwell_root/bbmap-rc.txt.tmp': No such file or directory; ï…; 04:26:11; MIME-Version: 1.0; ï…; 04:26:11; Content-Type: multipart/alternative; boundary=278185423cec5467d351ab751807c36a; ï…; 04:26:11; --278185423cec5467d351ab751807c36a; ï…; 04:26:11; Content-Type: text/plain; ï…; 04:26:11; Content-Disposition: attachment; filename=rc.txt; ï…; 04:26:11; cat: /cromwell_root/bbmap-rc.txt: No such file or directory; ï…; 04:26:11; --278185423cec5467d351ab751807c36a; ï…; 04:26:11; Content-Type: text/plain; ï…; 04:26:11; Content-Disposition: attachment; filename=stdout.txt; ï…; 04:26:11; cat: /cromwell_root/bbmap-stdout.log: No such file or directory; ï…; 04:26:11; --278185423cec5467d351ab751807c36a; ï…; 04:26:11; Content-Type: text/plain; ï…; 04:26:11; Content-Disposition: attachment; filename=stderr.txt; ï…; 04:26:11; cat: /cromwell_root/DA0000317_WSU-DLCL.qcstats: No such file or directory; ï…; 04:26:11; --278185423cec5467d351ab751807c36a--; ï…; 04:26:11; cat: /cromwell_root/bbmap-rc.txt: No such file or directory; ï…; 04:26:11; rm: cannot remove '/out.1': No such file or directory; ï…; 04:26:11; rm: cannot remove '/err.1': No such file or directory; ```; I have also tried to login to the node, and explicitly specify 777 permissions to /cromwell-root but the result was the same.; Are there any specific considerations regarding the docker image or any additional configuration required? . Thanks in advance for your help",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4542:8017,configurat,configuration,8017,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4542,1,['configurat'],['configuration']
Deployability,"ate): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:1477,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1477,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"ationSchema=true |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:1522,configurat,configuration,1522,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,1,['configurat'],['configuration']
Deployability,"atley/pact4s/releases/tag/v0.10.1-java8) - [Version Diff](https://github.com/jbwheatley/pact4s/compare/v0.9.0...v0.10.1-java8). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.9.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Pair/cnv_somatic_pair_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Panel/cnv_somatic_panel_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.aws.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.inputs; centaur/src/main/resources/integrationTestCases/germline/haplotype-caller-workflow/HaplotypeCallerWF.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-production-workflow/PairedEndSingleSampleWf.options.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; centaur/src/main/resources/integrationTestCases/green/arrays/arrays.wdl; womtool/src/test/resources/validate/wdl_draft3/valid/HaplotypeCallerWF/HaplotypeCallerWF.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/cnv_somatic_pair_workflow/cnv_somatic_pair_workflow.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/joint-discovery-gatk/joint-discovery-gatk.inputs.j",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7294:1249,integrat,integrationTestCases,1249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7294,1,['integrat'],['integrationTestCases']
Deployability,"atus change from Running to Success; 2019-01-18 18:43:33,255 cromwell-system-akka.dispatchers.engine-dispatcher-5 ERROR - WorkflowManagerActor Workflow dba9b85f-e9ea-4e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$Ab",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:1873,pipeline,pipelines,1873,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['pipeline'],['pipelines']
Deployability,"ausedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 2019-01-31 20:10:51,492 INFO - changesets/failure_metadata.xml::causedByLists::cjllanwarne: Successfully released change log lock; 2019-01-31 20:10:51,531 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624);",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1556,update,update,1556,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['update'],['update']
Deployability,"aven't investigated, but I also have some concerns over whether this mechanism would require significant rework of the process input/output and return code. It looks like @delagoya may have considered this in the batch-task-runner repo, but I'm not clear on this after the limited time I've had reviewing the repo. So, with that in mind, I spent a bit of time researching the ""always on sidecar"" approach, which I'll reference as ""cromwell agent"" moving forward for clarity. I took a look at the limitations of the permissions issue I mentioned above, and I believe I have a workable solution. The high level process would work like this:. 0. Each host runs the cromwell agent container similar to the way the ecs-agent operates today; 1. The cromwell agent container listens to the system events as described above; 2. When a cromwell task is started, the cromwell agent container will pause the target container (cromwell task) immediately; 3. It can then inspect the container and use the ECS ""AWS_CONTAINER_CREDENTIALS_RELATIVE_URI"" in conjunction with the ecs-agent container credentials endpoint at 169.254.170.2 to fetch the **target container credentials**; 4. Using the target container credentials, we can localize inputs, then unpause the target container; 5. Upon completion of the task (we should see this from the system events stream), we can then delocalize outputs. This process feels workable with the following advantages:. * Minimal changes to the Cromwell code base/the agent can be developed and maintained separately; * Per-task IAM roles; * No changes needed to Cromwell task definitions or containers; * Cromwell task supervision stays within AWS Batch and Cromwell; * AWS Batch Job Definition configuration does not have to pass through an intermediary. Right now I have a POC to accomplish through step 3. Unless there are objections I'm going to continue to prototype to ensure 4 and 5 work as expected, then we can make a final call either on this thread or in a meeting.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068:3113,configurat,configuration,3113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405738068,1,['configurat'],['configuration']
Deployability,awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutT,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1643,pipeline,pipeline,1643,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,"ay/0bb77c74-4c5c-4314-8463-072e7055ee7c/call-t1/shard-4/execution/out.txt""]; },; ""id"": ""0bb77c74-4c5c-4314-8463-072e7055ee7c""; }; ```. But I got the following error when I tried with 36.; ```; $ java -jar ~/cromwell-36.jar run test_opt_array.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2018-10-25 21:17:04,83] [info] Running with database db.url = jdbc:hsqldb:mem:bb200ed8-7db5-49a0-a250-ca46b3332697;shutdown=false;hsqldb.tx=mvcc; [2018-10-25 21:17:12,03] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-10-25 21:17:12,04] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-10-25 21:17:12,13] [info] Running with database db.url = jdbc:hsqldb:mem:c7a7ec22-dec6-4fae-a53b-6c9933402fa9;shutdown=false;hsqldb.tx=mvcc; [2018-10-25 21:17:12,59] [info] Slf4jLogger started; [2018-10-25 21:17:12,88] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-f5ccf1c"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-10-25 21:17:12,90] [info] Metadata summary refreshing every 2 seconds.; [2018-10-25 21:17:12,98] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-25 21:17:12,98] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-10-25 21:17:12,98] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-10-25 21:17:13,79] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-10-25 21:17:13,80] [info] SingleWorkflowRunnerActor: Version 36; [2018-10-25 21:17:13,81] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-10-25 21:17:13,84] [info] Unspecified type (Unspecified version) workflow e22c6324-5aec-4694-8750-f62160e2ca81 submitted; [2018-10-25 21:17:13,85] [info] SingleWorkflowRunnerActor: Workflow submitted e22c6324-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4318:10309,configurat,configuration,10309,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4318,1,['configurat'],['configuration']
Deployability,"azon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: null; Status Code: 429; Request ID: cffe6e45-d66c-11e8-a1df-05402551b0ba)`. The specific case where this happens is in the `gatk3-data-processing` workflow, when running the `ApplyBQSR` task, which is run in parallel over some calculated intervals. The full error trace I get is:. ```; 2018-10-23 02:39:07,631 cromwell-system-akka.dispatchers.backend-dispatcher-53345 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(6d97fef4)GPPW.ApplyBQSR:15:1]: Error attempting to Execute; software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: null; Status Code: 429; Request ID: cfc6e34e-d66c-11e8-be0b-dd778498cf15); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:1152,pipeline,pipeline,1152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['pipeline'],['pipeline']
Deployability,"b was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecuto",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2204,pipeline,pipelines,2204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['pipeline'],['pipelines']
Deployability,"b.com/circe/circe/compare/v0.13.0...v0.14.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/92f43ce5f010fa03daeb2625a3f9c1b1be80cca5/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.13.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/rnaseq-workflow/steps/prepare_sample.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/process_alignment.cwl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6410:1619,integrat,integrationTestCases,1619,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6410,1,['integrat'],['integrationTestCases']
Deployability,"b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.9.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Pair/cnv_somatic_pair_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Panel/cnv_somatic_panel_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.aws.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.inputs; centaur/src/main/resources/integrationTestCases/germline/haplotype-caller-workflow/HaplotypeCallerWF.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-production-workflow/PairedEndSingleSampleWf.options.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; centaur/src/main/resources/integrationTestCases/green/arrays/arrays.wdl; womtool/src/test/resources/validate/wdl_draft3/valid/HaplotypeCallerWF/HaplotypeCallerWF.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/cnv_somatic_pair_workflow/cnv_somatic_pair_workflow.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/joint-discovery-gatk/joint-discovery-gatk.inputs.json; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.github.jbwheatley"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.github.jbwheatley"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, old-ve",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7294:1760,integrat,integrationTestCases,1760,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7294,1,['integrat'],['integrationTestCases']
Deployability,"bExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:40,200 cromwell-system-akka.dispatchers.backend-dispatcher-109 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:42,570 cromwell-system-akka.dispatchers.backend-dispatcher-40 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:42,897 cromwell-system-akka.dispatchers.backend-dispatcher-140 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:58,493 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Workflow drs_usa_jdr complete. Final Outputs:; ""drs_usa_jdr.path1"": ""/cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json"",; ""drs_usa_jdr.map1"": {; ""drs_usa_jdr.size1"": 18.0,; ""drs_usa_jdr.hash1"": ""faf12e94c25bef7df62e4a5eb62573f5"",; ""drs_usa_jdr.cloud1"": ""gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc""; - should successfully run drs_usa_jdr (7 minutes, 24 seconds); </pre>; </details>",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:8823,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,8823,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,batch label updates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3755:12,update,updates,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3755,1,['update'],['updates']
Deployability,"be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.Batc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3732:1782,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1782,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"bench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; Our backend: ; GCP PAPIv2 ; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory"". endpoint-url = ""https://genomics.googleapis.com/"". <!-- Paste/Attach your workflow if possible: -->; workflow runtime; runtime {; docker: ""us.gcr.io/cloudypipelines-com/til_segmentation:1.5""; bootDiskSizeGb: 70; disks: ""local-disk 70 SSD""; memory: ""52 GB""; cpu: ""8""; maxRetries: 1; gpuCount: 1; zones: ""us-east1-d us-east1-c us-central1-a us-central1-c us-west1-a us-west1-b""; ##gpuType: ""nvidia-tesla-k80""; gpuType: ""nvidia-tesla-t4""; nvidiaDriverVersion: ""418.40.04""; ##nvidiaDriverVersion: ""418.87.00""; ; }. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; #### Recently, Our All workflows with GPU failed under the same configurations which most of workflows used to work on Cromwell 48, we updated to the latest Cromwell 52, still had the same errors, see belowL. 2020-08-04 23:44:00,228 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - WorkflowManagerActor Workflow f1dca11c-ea29-48b1-9691-9f30c9e59",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:1085,Pipeline,PipelinesApiLifecycleActorFactory,1085,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,1,['Pipeline'],['PipelinesApiLifecycleActorFactory']
Deployability,"bff928bb92dafc8a8f9213c5e52be1de7924356\"" from request \""/v2/broad-cumulus/cellranger/manifests/sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"".\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:91); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:812); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:95); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1340); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1336); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:14288,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,14288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"big select query into 2 parts - first find the hog group with lowest actively running workflows and then fetch workflows for that hog group because the outer select query had `forUpdate` and inner select query with `groupBy` clause which is not allowed in Postgres DB and throws the below error.; ```; cromwell-system-akka.dispatchers.engine-dispatcher-27 ERROR - Error trying to fetch new workflows; org.postgresql.util.PSQLException: ERROR: FOR UPDATE is not allowed with GROUP BY clause; ```. SQL generated from Slick:; Query to find hog group with lowest count of actively running workflows; ```; select ; x2.x3 ; from ; (; select ; `HOG_GROUP` as x3, ; count(1) as x4 ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; not (`WORKFLOW_STATE` = 'On Hold'); ) ; and (; not (; `HOG_GROUP` in ('Zardoz'); ); ) ; group by ; `HOG_GROUP` ; order by ; count(1); ) x2, ; (; select ; `HOG_GROUP` as x5, ; count(1) as x6 ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; (; (`HEARTBEAT_TIMESTAMP` is null) ; or (; `HEARTBEAT_TIMESTAMP` < {ts '2022-02-16 10:26:31.39' }; ); ) ; and (; not (`WORKFLOW_STATE` = 'On Hold'); ); ) ; and (; not (; `HOG_GROUP` in ('Zardoz'); ); ) ; group by ; `HOG_GROUP` ; order by ; count(1); ) x7 ; where ; x2.x3 = x7.x5 ; order by ; (x2.x4 - x7.x6), ; x2.x3 ; limit ; 1; ```. Query to select startable workflows for above hog group; ```; select ; `WORKFLOW_EXECUTION_UUID`, ; `WORKFLOW_DEFINITION`, ; `WORKFLOW_URL`, ; `WORKFLOW_ROOT`, ; `WORKFLOW_TYPE`, ; `WORKFLOW_TYPE_VERSION`, ; `WORKFLOW_INPUTS`, ; `WORKFLOW_OPTIONS`, ; `WORKFLOW_STATE`, ; `SUBMISSION_TIME`, ; `IMPORTS_ZIP`, ; `CUSTOM_LABELS`, ; `CROMWELL_ID`, ; `HEARTBEAT_TIMESTAMP`, ; `HOG_GROUP`, ; `WORKFLOW_STORE_ENTRY_ID` ; from ; `WORKFLOW_STORE_ENTRY` ; where ; (; (; (`HEARTBEAT_TIMESTAMP` is null) ; or (`HEARTBEAT_TIMESTAMP` < ?); ) ; and (; not (`WORKFLOW_STATE` = ?); ); ) ; and (`HOG_GROUP` = ?) ; order by ; `SUBMISSION_TIME` ; limit ; ? for ; update; ```. Closes https://broadworkbench.atlassian.net/browse/BW-568",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6680:2153,update,update,2153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6680,1,['update'],['update']
Deployability,"bio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/utils.py"", line 52, in wrapper; return apply(f, *args, **kwargs); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/multitasks.py"", line 208, in pipeline_summary; return qcsummary.pipeline_summary(*args); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/pipeline/qcsummary.py"", line 70, in pipeline_summary; data[""summary""] = _run_qc_tools(work_bam, work_data); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/pipeline/qcsummary.py"", line 162, in _run_qc_tools; out = qc_fn(bam_file, data, cur_qc_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/qc/qualimap.py"", line 347, in run_rnaseq; metrics = _parse_metrics(metrics); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/qc/qualimap.py"", line 210, in _parse_metrics; out.update({name: float(metrics[name])}); TypeError: float() argument must be a string or a number; ```. This is what the command Cromwell generated looks like:. ```; 'bcbio_nextgen.py' 'runfn' 'pipeline_summary' 'cwl' 'sentinel_runtime=cores,2,ram,4096' 'sentinel_parallel=multi-parallel' 'sentinel_outputs=qcout_rec:summary__qc;summary__metrics;resources;description;reference__fasta__base;config__algorithm__coverage_interval;genome_build;genome_resources__rnaseq__transcripts;config__algorithm__tools_off;config__algorithm__qc;analysis;config__algorithm__tools_on;align_bam' 'sentinel_inputs=qc_rec:record' 'run_number=0'; ```. And the `cwl.inputs.json`:. ```; {; ""qc_rec"": {; ""genome_build"": ""hg19"",; ""config__algorithm__tools_on"": [],; ""align_bam"": {; ""nameext"": "".bam"",; ""location"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec/call-process_alignment/shard-0/align/Test1/Test1-sort.bam"",; ""path"": ""/cromwell_root/tj-bcbio-papi/main-rnaseq.cwl/6c75cc7c-5515-45e0-9e5b-9a1b9e6fd2e1/call-qc_to_rec",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:5893,update,update,5893,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['update'],['update']
Deployability,"ble, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ; Jobs which required gpuType: ""nvidia-tesla-t4"", nvidiaDriverVersion: ""418.40.04"", failed. Our pipeline backend is Google : genomics.googleapis.com; ""jes"": {; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""zone"": ""us-central1-f"",; ....; },. job runtimeAttributes:; ...; ""preemptible"": ""1"",; ""gpuCount"": ""1"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""70"",; ""disks"": ""local-disk 70 SSD"",; ""continueOnReturnCode"": ""0"",; ""gpuType"": ""nvidia-tesla-t4"",; ""nvidiaDriverVersion"": ""418.40.04"",; ""maxRetries"": ""0"",; ""cpu"": ""8"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zone"": ""us-central1-f"",; ""memoryMin"": ""2 GB"",; ""memory"": ""64 GB"". Jobs failed with following message:; ""Task wf_quip_lymphocyte_segmentation_incep_v01052021.quip_lymphocyte_segmentation:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: generic::unknown: installing drivers: container exited with unexpected exit code 1: + COS_KERNEL_INFO_FILENAME=kernel_info\n+ COS_KERNEL_SRC_ARCHIVE=kernel-src.tar.gz\n+ COS_KERNEL_SRC_HEADER=kernel-headers.tgz\n+ TOOLCHAIN_URL_FILENAME=toolchain_url\n+ TOOLCHAIN_ARCHIVE=toolchain.tar.xz\n+ TOOLCHAIN_ENV_FILENAME=toolchain_env\n+ TOOLCHAIN_PKG_DIR=/build/cos-tools\n+ CHROMIUMOS_SDK_GCS=https://storage.googleapis.com/chromiumos-sdk\n+ ROOT_OS_RELEASE=/root/etc/os-release\n+ KERNEL_SRC_DIR=/build/usr/src/linux\n+ KERNEL_SRC_HEADER=/build/usr/src/linux-headers\n+ NVIDIA_DRIVER_VERSION=450.51.06\n+ NVIDIA_DRIVER_MD5SUM=\n+ NVIDIA_INSTALL_DIR_HOST=/var/lib/nvidia\n+ NVIDIA_INSTALL_DIR_CONTAINER=/usr/local/nvidia\n+ ROOT_MOUNT_DIR=/root\n+ CACHE_FILE=/usr/local/nvidia/.cache\n+ LOCK_FILE=/root/tmp/cos_gpu_installer_lock\n+ LOCK_FILE_FD=20\n+ set +x\n[INFO 2021-02-22 23:09:17 UTC] PRELOAD: false\n[INFO 2021-02-22 23:09:17 UTC] Running on COS build id 13310.1209.10\n[INFO 2021-02-22 23:09:17 UTC] Data dependencies (e.g. kernel source) will be fetched from https:/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:2033,install,installing,2033,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['install'],['installing']
Deployability,"ble/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2225,Pipeline,Pipelines,2225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,1,['Pipeline'],['Pipelines']
Deployability,bleStage$RetryExecutor.execute(RetryableStage.java:105); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); at software.amazon.awssdk.core.client.Syn,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:2899,pipeline,pipeline,2899,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['pipeline'],['pipeline']
Deployability,"bmitted; ...; [2021-08-13 10:44:56,46] [info] Request manager PAPIQueryManager created new PAPI request worker PAPIQueryWorker-58e6b395-916e-4ba4-965a-0ec8f1c0760d with batch interval of 3333 milliseconds; ...; [2021-08-13 10:44:56,67] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Parsing workflow as WDL draft-2; [2021-08-13 10:44:58,79] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Call-to-Backend assignments: wf_hello.hello -> PAPIv2; [2021-08-13 10:45:00,31] [info] Not triggering log of token queue status. Effective log interval = None; [2021-08-13 10:45:01,35] [info] WorkflowExecutionActor-a15c46b7-5f93-46d6-94a2-28f656914866 [a15c46b7]: Starting wf_hello.hello; [2021-08-13 10:45:02,34] [info] Assigned new job execution tokens to the following groups: a15c46b7: 1; [2021-08-13 10:45:04,75] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: echo ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; [2021-08-13 10:45:05,68] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); [2021-08-13 10:45:07,36] [error] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.goog",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:2717,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2717,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,brew distribution is still pointing to release version 37,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4713:39,release,release,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4713,1,['release'],['release']
Deployability,build updates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/189:6,update,updates,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/189,1,['update'],['updates']
Deployability,"c-7c69317ba0a2/call-PairedFastQsToUnmappedBAM/inputs/-2135135022/S000021_S7367Nr1.2.fastq.gz --OUTPUT S7367Nr1.unmapped.bam --READ_GROUP_NAME S7367Nr1 --SAMPLE_NAME S4431Nr1 --LIBRARY_NAME TwistCore+RefSeq+Mito-Panel --PLATFORM_UNIT platform_unit --PLATFORM Illumina --SEQUENCING_CENTER CeGaT --RUN_DATE 2021-10-10T06:00:00+0000 --USE_SEQUENTIAL_FASTQS false --SORT_ORDER queryname --MIN_Q 0 --MAX_Q 93 --STRIP_UNPAIRED_MATE_NUMBER false --ALLOW_AND_IGNORE_EMPTY_LINES false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; 2023-02-03 12:38:34 [Fri Feb 03 09:38:34 GMT 2023] Executing as root@d65fc5b7d470 on Linux 5.15.49-linuxkit amd64; OpenJDK 64-Bit Server VM 1.8.0_242-8u242-b08-0ubuntu3~18.04-b08; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.3.0.0; 2023-02-03 12:38:35 INFO 2023-02-03 09:38:35 FastqToSam Auto-detected quality format as: Standard.; 2023-02-03 12:39:08 INFO 2023-02-03 09:39:08 FastqToSam Processed 1,000,000 records. Elapsed time: 00:00:32s. Time for last 1,000,000: 32s. Last read position: */*`. I tried via Java 18.0.1.1 JDK and also later with 1.8.0_202 JDK. I also tried with the conda installation where Java dependency of OpenJDK 11.0.15 is automatically installed. I also tried combinations with Cromwell 69, 80 and 84. None of them works. They all have the same problem. It only works if I use Cromwell version 55 along with Java 1.8.0_202 JDK. It would be amazing if you look into this, as we would love to use the latest Cromwell versions and benefit from the conda environment. Thanks!. Machine info: `Darwin Ibrahims-MacBook-Pro.local 22.2.0 Darwin Kernel Version 22.2.0: Fri Nov 11 02:04:44 PST 2022; root:xnu-8792.61.2~4/RELEASE_ARM64_T8103 arm64`. MacOS = Ventura 13.1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6998:3081,install,installation,3081,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6998,2,['install'],"['installation', 'installed']"
Deployability,"can I close this ticket and we can make a separate one for Hotfix? I'm also unsure of why the ""closes #issue"" doesn't seem to be working properly for most tickets.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/856#issuecomment-221252219:59,Hotfix,Hotfix,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/856#issuecomment-221252219,1,['Hotfix'],['Hotfix']
Deployability,"cb8-bb22-c8a753f58c49/myco/10fa31a8-acbe-4ab7-a96a-6550ec08df12/call-pull/shard-0/ SAMEA104027315_pull_results.txt; 2023/04/18 21:55:04 Delocalizing output /cromwell_root/SAMEA104027315.tar -> gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submissions/93bf6971-bfa1-4cb8-bb22-c8a753f58c49/myco/10fa31a8-acbe-4ab7-a96a-6550ec08df12/call-pull/shard-0/SAMEA104027315.tar; 2023/04/18 21:55:04 Delocalization script execution complete.; 2023/04/18 21:55:05 Done delocalization.; ```. In Job Manager, an error with the outputs can be seen. <img width=""1115"" alt=""job outputs"" src=""https://user-images.githubusercontent.com/27784612/232939192-8823373b-c21e-4586-8c1b-516770a212e3.png"">. Because Job Manager breaks on large scatters, and to save money on compute credits, I decided to stop the workflow early rather than let it keep going to find out if the workflow log would eventually show an errors. So far, it seems to have considered everything a success. ```; 2023-04-18 21:59:54,599 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:108:1]: Status change from Running to Success; 2023-04-18 22:00:09,060 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:107:1]: Status change from Running to Success; 2023-04-18 22:00:18,464 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:106:1]: Status change from Running to Success; 2023-04-18 22:01:20,604 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:111:1]: Status change from Running to Success; 2023-04-18 22:14:47,728 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: Aborting workflow; 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:262:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/9178938377659283430); 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBacken",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:4079,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,4079,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"ccessfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HA",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:35806,update,updateSchema,35806,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,1,['update'],['updateSchema']
Deployability,"cd36fb1c.yml --type CWL --workflow-root main; [2018-09-14 13:19:10,55] [info] Running with database db.url = jdbc:hsqldb:mem:d07a09a8-8d20-4095-967b-c6f375a3f309;shutdown=false;hsqldb.tx=mvcc; [2018-09-14 13:19:19,53] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-09-14 13:19:19,55] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-09-14 13:19:19,67] [info] Running with database db.url = jdbc:hsqldb:mem:e41fe9de-508c-4f49-aeaa-ce7474d7c1e2;shutdown=false;hsqldb.tx=mvcc; [2018-09-14 13:19:20,18] [info] Slf4jLogger started; [2018-09-14 13:19:20,25] [info] Pre Processing Workflow...; [2018-09-14 13:19:20,65] [info] Pre-Processing /home/jeremiah/gdc-dnaseq-cwl/workflows/bamfastq_align/transform_pack.cwl; [2018-09-14 13:19:54,70] [info] Pre Processing Inputs...; [2018-09-14 13:19:54,94] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-89ab52b"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-09-14 13:19:55,04] [info] Metadata summary refreshing every 2 seconds.; [2018-09-14 13:19:55,31] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-09-14 13:19:55,32] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-09-14 13:19:56,83] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-09-14 13:19:56,88] [info] SingleWorkflowRunnerActor: Version 35-fd560e9-SNAP; [2018-09-14 13:19:56,91] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-09-14 13:19:57,89] [info] CWL (Unspecified version) workflow caab4283-a3d4-4966-85ba-56d0992c8f00 submitted; [2018-09-14 13:19:57,90] [info] SingleWorkflowRunnerActor: Workflow submitted caab4283-a3d4-4966-85ba-56d0992c8f00; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103:1339,configurat,configuration,1339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103,1,['configurat'],['configuration']
Deployability,"cessMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$GoogleJsonException: Request contains an invalid argument.; ... 21 more. [2021-08-13 10:45:10,13] [info] WorkflowManagerActor: Workflow actor for a15c46b7-5f93-46d6-94a2-28f656914866 completed with status 'Failed'. The workflow will be removed from the workflow store.; [2021-08-13 10:45:13,98] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2021-08-13 10:45:15,05] [info] Workflow polling stopped; [2021-08-13 10:45:15,07] [info] 0 workflows released by cromid-de31b6d; [2021-08-13 10:45:15,07] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; ...; ```. Contents of hello.wdl:; ```; task hello {; String addressee; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. Contents of hello.inputs:; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```; Contents of cromwell.BROADexamples.v4.conf:; ```; # This is a ""default"" Cromwell example that is intended for you you to start with; # and edit for your needs. Specifically, you will be interested to customize; # the configuration based on your preferred backend (see the backends section; # below in the file). For backend-specific examples for you to copy paste here,; # please see the cromwell.backend.examples folder in the repository. The files; # there also include links to online doc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:8339,release,released,8339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['release'],['released']
Deployability,"ch size 10000 and process rate 2 minutes.; [2018-08-27 02:04:08,17] [info] MaterializeWorkflowDescriptorActor [967af8b6]: Parsing workflow as WDL draft-2; [2018-08-27 02:04:08,86] [info] MaterializeWorkflowDescriptorActor [967af8b6]: Call-to-Backend assignments: wgbs.flatten_ -> singularity; [2018-08-27 02:04:12,30] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Starting wgbs.flatten_; [2018-08-27 02:04:13,48] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: ; mkdir -p mapping; cat /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/write_lines_8f61fd340a04ccd930e243709dfb1bed.tmp | xargs -I % ln -s % mapping; ls mapping; [2018-08-27 02:04:13,50] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: executing: chmod u+x /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script && \; singularity \; exec \; gemBS.simg \; /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script &; echo $?; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: job id: 0; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: Status change from - to Done; [2018-08-27 02:04:19,50] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Workflow wgbs complete. Final Outputs:; {. }; [2018-08-27 02:04:19,53] [info] WorkflowManagerActor WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468 is in a terminal state: WorkflowSucceededState; [2018-08-27 02:04:22,18] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {. },; ""id"": ""967af8b6-0d68-44c4-b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:4853,pipeline,pipelines,4853,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"cher-38 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.CreateSequenceGroupingTSV:NA:1 [UUID(0123c178)]: Call cache hit process had 0 total hit failures before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > [â€¦](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba02308",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2341,release,release,2341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,2,"['configurat', 'release']","['configuration', 'release']"
Deployability,"cho ~{bam}; cp ~{bam} ~{name}.bam; cp /cromwell_root/~{name}/~{name} ~{name}.bam. samtools index -b ~{name}.bam; cp ~{name}.bam.bai ~{name}.bai; }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.3-1513176735""; memory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + "" HDD""; }. output {; File output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; enable-fuse = true; }; filesystems {; sra {}; gc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:3060,pipeline,pipelines,3060,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161,1,['pipeline'],['pipelines']
Deployability,"cificity_oncotate_oncotated_target_seg_gt_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_oncotate/SM-74ND9.per_target.oncotated.txt"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_small_sens_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.small_segs.tsv"",; ""crsp_validation_workflow_specificity_run_sensitivity_precision_del_sens_prec_file"": ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/020aa0e3-d12f-4085-b8a7-1de06c8df598/call-specificity_run_sensitivity_precision/SM-74ND9.sens_prec.del.tsv""; },; ""id"": ""020aa0e3-d12f-4085-b8a7-1de06c8df598""; }; [INFO] [12/08/2016 21:02:52.660] [cromwell-system-akka.actor.default-dispatcher-3] [akka://cromwell-system/user/SingleWorkflowRunnerActor] SingleWorkflowRunnerActor writing metadata to /home/lichtens/test_eval/crsp_validation_input_files/crsp_validation_from_cromwell.json.metadata.json; [INFO] [12/08/2016 21:02:52.719] [shutdownHook1] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor: Received shutdown signal.; [INFO] [12/08/2016 21:02:52.720] [cromwell-system-akka.actor.default-dispatcher-34] [akka://cromwell-system/deadLetters] Message [cromwell.engine.workflow.WorkflowManagerActor$AbortAllWorkflowsCommand$] from Actor[akka://cromwell-system/deadLetters] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Capturing latest dir...; 7d822ec4-ca21-4e05-94ad-9d16acd5e534; lichtens@lichtens-big:~/test_eval$; ```. Do you see it? Look at that last line! It's a prompt! Cromwell exited successfully!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352:3531,configurat,configuration,3531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-266019352,1,['configurat'],['configuration']
Deployability,"ckend-dispatcher-33 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: `echo ""Hello Cromwell! Welcome to Cromwell . . . on AWS!""`; 2021-09-27 13:48:33,376 cromwell-system-akka.dispatchers.backend-dispatcher-33 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Adjusting boot disk size to 16 GB: 10 GB (runtime attributes) + 5 GB (user command image) + 1 GB (Cromwell support images); 2021-09-27 13:48:38,987 cromwell-system-akka.dispatchers.backend-dispatcher-33 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: job id: projects/gred-cumulus-sb-01-991a49c4/operations/15427360049616748078; 2021-09-27 13:49:07,692 cromwell-system-akka.dispatchers.backend-dispatcher-35 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from - to Running; 2021-09-27 13:50:48,340 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from Running to Failed; 2021-09-27 13:50:49,875 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Workflow 075e0cf3-194b-4f53-a43d-d31f0b370f79 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Execution failed: generic::permission_denied: pulling image: docker pull: running [""docker"" ""pull"" ""gcr.io/broad-cumulus/cellranger@sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356""]: exit status 1 (standard error: ""Error response from daemon: pull access denied for gcr.io/broad-cumulus/cellranger, repository does not exist or may require 'docker login': denied: Permission denied for \""sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"" from request \""/v2/broad-cumulus/cellranger/manifests/sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:12422,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,12422,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"ckendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001:1249,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,closing -- sounds like this is a documentation update (which is being handled). Please re-open if there is still a bug here,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-233072259:47,update,update,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-233072259,1,['update'],['update']
Deployability,"cluded in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the probl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1416,release,releases,1416,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,1,['release'],['releases']
Deployability,color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>Thatâ€™s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>Thatâ€™s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917:3942,pipeline,pipelines,3942,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917,6,"['Pipeline', 'pipeline']","['PipelinesApiRequestWorker', 'pipelines']"
Deployability,"column::tjeandet: ChangeSet changesets/workflow_store_workflow_root_column.xml::workflow-store-workflow-root-column::tjeandet ran successfully in 0ms; 2018-06-07 12:16:10,992 INFO - changelog.xml: changesets/workflow_store_horizontal_db.xml::workflow-store-horizontal-db::mcovarr: Column WORKFLOW_STORE_ENTRY.RESTARTED dropped; 2018-06-07 12:16:10,992 INFO - changelog.xml: changesets/workflow_store_horizontal_db.xml::workflow-store-horizontal-db::mcovarr: Columns CROMWELL_ID(VARCHAR(100)) added to WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,993 INFO - changelog.xml: changesets/workflow_store_horizontal_db.xml::workflow-store-horizontal-db::mcovarr: Columns HEARTBEAT_TIMESTAMP(TIMESTAMP) added to WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,994 INFO - changelog.xml: changesets/workflow_store_horizontal_db.xml::workflow-store-horizontal-db::mcovarr: ChangeSet changesets/workflow_store_horizontal_db.xml::workflow-store-horizontal-db::mcovarr ran successfully in 3ms; 2018-06-07 12:16:10,997 INFO - Successfully released change log lock; 2018-06-07 12:16:11,007 INFO - Running with database db.url = jdbc:hsqldb:mem:78e2c868-f948-49e1-b7ba-840a9b54f3aa;shutdown=false;hsqldb.tx=mvcc; 2018-06-07 12:16:11,051 INFO - Successfully acquired change log lock; 2018-06-07 12:16:11,069 INFO - Creating database history table with name: PUBLIC.SQLMETADATADATABASECHANGELOG; 2018-06-07 12:16:11,071 INFO - Reading from PUBLIC.SQLMETADATADATABASECHANGELOG; 2018-06-07 12:16:11,080 INFO - sql_metadata_changelog.xml: metadata_changesets/move_sql_metadata_changelog.xml::move_metadata_changelog::kshakir: Table CUSTOM_LABEL_ENTRY created; 2018-06-07 12:16:11,081 INFO - sql_metadata_changelog.xml: metadata_changesets/move_sql_metadata_changelog.xml::move_metadata_changelog::kshakir: Table METADATA_ENTRY created; 2018-06-07 12:16:11,081 INFO - sql_metadata_changelog.xml: metadata_changesets/move_sql_metadata_changelog.xml::move_metadata_changelog::kshakir: Table SUMMARY_STATUS_ENTRY created; 2018-06-07 12",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:90822,release,released,90822,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['release'],['released']
Deployability,"com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf) to solve the problem, but maybe it is better to have a `post-docker` configuration which is added to the pipeline similar to the `script-epilogue`. This would make easier the configuration of docker runs, separating submission and checks. By now, I will use the following local configuration to continue my work with the cromwell runner:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 15; # set the root directory to the run; filesystems.local {; ## do not allow copy (huge files); localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. By the way, it looks like the configuration of the local backend in the docs is still under development (http://cromwell.readthedocs.io/en/develop/tutorials/LocalBackendIntro/). I think that this kind of things can be part of the docs if not included as default in the source code - let me know if I can do something to help documenting the local end, which I am using as my default one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526:2364,configurat,configuration,2364,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526,1,['configurat'],['configuration']
Deployability,configuration for slurm,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7086:0,configurat,configuration,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7086,1,['configurat'],['configuration']
Deployability,"copying over from hotfix branch,. fix for looking for ""config"" in the papi provider config",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3729:18,hotfix,hotfix,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3729,1,['hotfix'],['hotfix']
Deployability,core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCal,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1770,pipeline,pipeline,1770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,correct typo in Slurm configuration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3959:22,configurat,configuration,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3959,1,['configurat'],['configuration']
Deployability,"cp`, as @aednichols suggested (in my case Cromwell runs with service account `30148356615-compute@developer.gserviceaccount.com`):; ```; $ gcloud config set account giulio@broadinstitute.org; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; * giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/. Copying gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai...; / [1 files][143.2 KiB/143.2 KiB]; Operation completed over 1 objects/143.2 KiB.; $ gcloud config set account 30148356615-compute@developer.gserviceaccount.com; Updated property [core/account].; $ gcloud auth list; Credentialed Accounts; ACTIVE ACCOUNT; * 30148356615-compute@developer.gserviceaccount.com; giulio.genovese@gmail.com; giulio@broadinstitute.org. To set the active account, run:; $ gcloud config set account `ACCOUNT`. $ gsutil cp gs://fc-118c254f-010a-4ee6-b149-6f0bb5abaa77/GeneticNeuroscience_McCarroll_CIRM_GRU_Exome_9qCN-LOH_PDO-21129/RP-1875/Exome/CW60141_P13_MT_1-19-18/v1/CW60141_P13_MT_1-19-18.cram.crai /tmp/; AccessDeniedException: 403 30148356615-compute@developer.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```. So in this case the more appropriate questions would be:; 1) How do I get to have my service account `30148356615-compute@developer.gserviceaccount.com` have the same permissions as my personal account `giulio@broadinstitute.org`?; 2) How do I get Cromwell to run with my personal account `giulio@broadinstitute.org` instead of my service account?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782:1059,Update,Updated,1059,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665434782,1,['Update'],['Updated']
Deployability,"cs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.9.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Pair/cnv_somatic_pair_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Panel/cnv_somatic_panel_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.aws.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.inputs; centaur/src/main/resources/integrationTestCases/germline/haplotype-caller-workflow/HaplotypeCallerWF.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-production-workflow/PairedEndSingleSampleWf.options.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; centaur/src/main/resources/integrationTestCases/green/arrays/arrays.wdl; womtool/src/test/resources/validate/wdl_draft3/valid/HaplotypeCallerWF/HaplotypeCallerWF.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/cnv_somatic_pair_workflow/cnv_somatic_pair_workflow.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/joint-discovery-gatk/joint-discovery-gatk.inputs.json; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.github.jbwheatley"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.github.jbwheatley"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7294:1901,integrat,integrationTestCases,1901,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7294,6,"['integrat', 'update']","['integrationTestCases', 'update', 'updates']"
Deployability,ctor [UUID(6d97fef4)GPPW.ApplyBQSR:15:1]: Error attempting to Execute; software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: null; Status Code: 429; Request ID: cfc6e34e-d66c-11e8-be0b-dd778498cf15); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); at software.amazon.awssd,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:1638,pipeline,pipeline,1638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['pipeline'],['pipeline']
Deployability,"ctor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-12-05 20:11:24,83] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2017-12-05 20:11:24,84] [info] Using noop to send events.; [2017-12-05 20:11:24,85] [info] WorkflowManagerActor WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324 is in a terminal state: WorkflowFailedState; [2017-12-05 20:11:32,22] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-12-05 20:11:32,25] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-12-05 20:11:32,26] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow 159210e6-fa6a-4a99-b386-5931ae245324 transitioned to state Failed; [2017-12-05 20:11:32,30] [info] Automatic shutdown of the async connection; [2017-12-05 20:11:32,30] [info] Gracefully shutdown sentry threads.; [2017-12-05 20:11:32,30] [info] Shutdown finished.; ```; This code worked with `cromwell-29.jar`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3007:6297,configurat,configuration,6297,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007,2,['configurat'],['configuration']
Deployability,"ctor.scala:136); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-12-05 20:11:24,83] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2017-12-05 20:11:24,84] [info] Using noop to send events.; [2017-12-05 20:11:24,85] [info] WorkflowManagerActor WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324 is in a terminal state: WorkflowFailedState; [2017-12-05 20:11:32,22] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-12-05 20:11:32,25] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-12-05 20:11:32,26] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow 159210e6-fa6a-4a99-b386-5931ae245324 transitioned to state Failed; [2017-12-05 20:11:32,30] [info] Automatic shutdown of the async connection; [2017-12-05 20:11:32,30] [info] Gracefully shutdown sentry threads.; [2017-12-05 20:11:32,30] [info] Shutdown finished.; ```; This code worked with `cromwell-29.jar`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406:6210,configurat,configuration,6210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406,2,['configurat'],['configuration']
Deployability,"curl -X POST ""http://172.20.1.67:8001/api/workflows/v1"" -H ""accept: application/json"" -F ""workflowSource=@three_task_sequence.wdl"" -F ""workflowOptions=@workflow_options.json""; `; Where workflow_options.json content is:; ```; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686:1224,pipeline,pipeline,1224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686,1,['pipeline'],['pipeline']
Deployability,currently it just returns the updated ones. this is needed for job manager. https://www.youtube.com/watch?v=1Isjgc0oX0s,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2953:30,update,updated,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2953,1,['update'],['updated']
Deployability,"cution complete.; 2023/04/18 21:55:05 Done delocalization.; ```. In Job Manager, an error with the outputs can be seen. <img width=""1115"" alt=""job outputs"" src=""https://user-images.githubusercontent.com/27784612/232939192-8823373b-c21e-4586-8c1b-516770a212e3.png"">. Because Job Manager breaks on large scatters, and to save money on compute credits, I decided to stop the workflow early rather than let it keep going to find out if the workflow log would eventually show an errors. So far, it seems to have considered everything a success. ```; 2023-04-18 21:59:54,599 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:108:1]: Status change from Running to Success; 2023-04-18 22:00:09,060 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:107:1]: Status change from Running to Success; 2023-04-18 22:00:18,464 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:106:1]: Status change from Running to Success; 2023-04-18 22:01:20,604 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:111:1]: Status change from Running to Success; 2023-04-18 22:14:47,728 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: Aborting workflow; 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:262:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/9178938377659283430); 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:112:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/8559201934542591362); 2023-04-18 22:14:48,295 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Successfully requested cancellation of projects/16371921765/locations/us-centr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:4514,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,4514,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"cution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorCo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3732:2015,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2015,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,cutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:3722,pipeline,pipeline,3722,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,1,['pipeline'],['pipeline']
Deployability,"d change log lock; 2019-01-31 20:10:51,531 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SE",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1739,update,updateSchema,1739,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['update'],['updateSchema']
Deployability,d cursor: DownField(manifestFormatVersion); at cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$.apply(PipelinesApiConfigurationAttributes.scala:307); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.defaultBuildAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:32); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.$anonfun$papiAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:34); at scala.util.Try$.apply(Try.scala:210); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.cromwell$backend$google$pipelines$common$PipelinesApiBackendLifecycleActorFactory$$build$1(PipelinesApiBackendLifecycleActorFactory.scala:109); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.robustBuildAttributes(PipelinesApiBackendLifecycleActorFactory.scala:120); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.<init>(PipelinesApiBackendLifecycleActorFactory.scala:34); at cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:10); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at cromwell.engine.backend.BackendConfigurationEntry.$anonfun$asBackendLifecycleActorFactory$1(BackendConfiguration.scala:13); at scala.util.Try$.apply(Try.scala:210); at cromwell.engine.backend.BackendConfigurationEntry.asBackendLifecycleActorFactory(BackendConfiguration.scala:14); at cromwell.engine.backend.CromwellBackends.$anonfun$backendLifecycleActorFactories$1(CromwellBa,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:1720,Pipeline,PipelinesApiBackendLifecycleActorFactory,1720,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,1,['Pipeline'],['PipelinesApiBackendLifecycleActorFactory']
Deployability,"d outputs forever. However we have a large volume of intermediate files which end up in our cromwell-executions bucket. We love caching. It works great. A fully cached workflow runs in about 5 minutes at next to no cost. Fresh workflows (no cache hits) cost on the order of $0.50 for typical examples, and run for a few hours. Object storage has been eating us up, though. We've worked out that for a single one of these workflows the break even point at which it's cheaper to rerun it than to save it and cache it is about a week. If you take into account that we re-run workflows only a small part of the time, it probably doesn't even pay to keep the execution folders at all (except in the intangible wall clock time). [And nearline / coldline makes no sense at all. Each cached file is accessed multiple times which makes cached runs way way more expensive than fresh runs.]. Weâ€™ve examined the pipeline, and we see that we could reduce the size of intermediate outputs, from 126G to 40G by combining separate tasks, which obviates the need to make the large file an output of the first task and input to the second. This leads me to a question for the deep thinkers in Cromwell caching. I want to ask if something makes sense in theory, for the purpose of making caching more feasible for us. Suppose I took the two tasks I spoke of, one of which â€œpassesâ€ a large file to the second, and made them into a sub-workflow. And I mark the large files as â€œtoo big to keepâ€ so they Cromwell would strip them out of the execution folder after the run completed. If caching were to work by looking at the inputs and outputs of the sub-workflow, and not at each task one by one, then it would be possible to cache the entire sub-workflow. Right?. Letâ€™s say this sounds theoretically possible. Wouldnâ€™t it be possible then, to skip making an actual sub-workflow at all to bracket the trashed intermediates, but just have clever Cromwell analyse the execution tree and the presence of â€œtoo big to keepâ€ vaca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4064:2314,pipeline,pipeline,2314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064,1,['pipeline'],['pipeline']
Deployability,"d-dispatcher-666 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(656ddc45)PairedEndSingleSampleWorkflow.ApplyBQSR:13:1]:; Status change from Running to Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:1138,pipeline,pipelines,1138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,1,['pipeline'],['pipelines']
Deployability,"d. PAPI error code 5. 8: Failed to pull image broadinstitut; e/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71: ""docker pull broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71"" failed: exit status 1: sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71: Pulling from broadinstitute/gatk; cromwell_1 | ae79f2514705: Pulling fs layer; cromwell_1 | 5ad56d5fc149: Pulling fs layer; cromwell_1 | 170e558760e8: Pulling fs layer; cromwell_1 | 395460e233f5: Pulling fs layer; cromwell_1 | 6f01dc62e444: Pulling fs layer; cromwell_1 | 98db058f41f6: Pulling fs layer; [...]; cromwell_1 | failed to register layer: Error processing tar file(exit status 1): write /root/.cache/pip/http/5/1/d/8/2/51d82969228464b761a16257d5eefe8e2b3dde3c1ad733721353e785: no space left on device; cromwell_1 |; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); cromwell_1 | at scala.concurrent.impl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337:1876,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1876,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"d18a728dfc5 /cromwell-executions/HelloWorldWithDocker/3d200d74-77eb-45d8-8fe6-b41b14aab6f6/call-WriteGreeting/execution/script; Hello Docker; +++ cat /home/jaruga/git/dockstore-cli-docker-test/cromwell-executions/HelloWorldWithDocker/3d200d74-77eb-45d8-8fe6-b41b14aab6f6/call-WriteGreeting/execution/docker_cid; ++ docker wait 56df02a84010fae2573793c374e4ae3c506dc5c9a759169d09c378d0dcdfaa0d; + rc=0; ++ cat /home/jaruga/git/dockstore-cli-docker-test/cromwell-executions/HelloWorldWithDocker/3d200d74-77eb-45d8-8fe6-b41b14aab6f6/call-WriteGreeting/execution/docker_cid; + docker rm 56df02a84010fae2573793c374e4ae3c506dc5c9a759169d09c378d0dcdfaa0d; 56df02a84010fae2573793c374e4ae3c506dc5c9a759169d09c378d0dcdfaa0d; + exit 0; ```. ## Workaround. I was able to suppress this error by disabling SE Linux on Fedora 36. But this is not an ideal way. Because I think SE Linux is enabled (Current mode: enforcing) as a default on RPM-based distributions such as Fedora, RHEL, CentOS stream and etc for security reasons. ```; $ sestatus ; SELinux status: enabled; SELinuxfs mount: /sys/fs/selinux; SELinux root directory: /etc/selinux; Loaded policy name: targeted; Current mode: enforcing; Mode from config file: enforcing; Policy MLS status: enabled; Policy deny_unknown status: allowed; Memory protection checking: actual (secure); Max kernel policy version: 33; ```. Disabled like this. ```; $ sudo setenforce 0. $ sestatus; SELinux status: enabled; SELinuxfs mount: /sys/fs/selinux; SELinux root directory: /etc/selinux; Loaded policy name: targeted; Current mode: permissive; Mode from config file: enforcing; Policy MLS status: enabled; Policy deny_unknown status: allowed; Memory protection checking: actual (secure); Max kernel policy version: 33; ```. I think you can try [SE Linux on Ubuntu](https://wiki.ubuntu.com/SELinux) to reproduce this error if you only have Debian-based Linux. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6905:8657,configurat,configuration,8657,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6905,1,['configurat'],['configuration']
Deployability,"d935e7cd5632; [2017-12-01 20:01:02,95] [info] 1 new workflows fetched; [2017-12-01 20:01:02,95] [info] WorkflowManagerActor Starting workflow 132d7527-a0af-4f08-8291-d935e7cd5632; [2017-12-01 20:01:02,96] [info] WorkflowManagerActor Successfully started WorkflowActor-132d7527-a0af-4f08-8291-d935e7cd5632; [2017-12-01 20:01:02,96] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-01 20:01:03,27] [info] MaterializeWorkflowDescriptorActor [132d7527]: Call-to-Backend assignments: test.t1 -> Local; [2017-12-01 20:01:04,64] [info] WorkflowExecutionActor-132d7527-a0af-4f08-8291-d935e7cd5632 [132d7527]: Starting calls: test.t1:NA:1; [2017-12-01 20:01:04,82] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: echo test1 > test1.txt; echo test2 > test2.txt; [2017-12-01 20:01:04,86] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: executing: /bin/bash /users/leepc12/code/atac-seq-pipeline/test/cromwell-executions/test/132d7527-a0af-4f08-8291-d935e7cd5632/call-t1/execution/script; [2017-12-01 20:01:04,91] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: job id: 9836; [2017-12-01 20:01:04,92] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from - to WaitingForReturnCodeFile; [2017-12-01 20:01:06,50] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2017-12-01 20:01:06,61] [error] WorkflowManagerActor Workflow 132d7527-a0af-4f08-8291-d935e7cd5632 failed (during ExecutingWorkflowState): Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; java.lang.RuntimeException: Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:189); at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:188); at scala.runtime.AbstractPartialFunction",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2972:2703,pipeline,pipeline,2703,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2972,1,['pipeline'],['pipeline']
Deployability,"d; [2022-12-15 21:28:53,46] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2022-12-15 21:28:53,46] [info] Aborting all running workflows.; [2022-12-15 21:28:53,46] [info] 0 workflows released by cromid-b254006; [2022-12-15 21:28:53,47] [info] WorkflowStoreActor stopped; [2022-12-15 21:28:53,47] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2022-12-15 21:28:53,47] [info] WorkflowLogCopyRouter stopped; [2022-12-15 21:28:53,47] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2022-12-15 21:28:53,47] [info] JobExecutionTokenDispenser stopped; [2022-12-15 21:28:53,47] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2022-12-15 21:28:53,47] [info] WorkflowManagerActor: All workflows finished; [2022-12-15 21:28:53,47] [info] WorkflowManagerActor stopped; [2022-12-15 21:28:53,71] [info] Connection pools shut down; [2022-12-15 21:28:53,71] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2022-12-15 21:28:53,71] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2022-12-15 21:28:53,72] [info] SubWorkflowStoreActor stopped; [2022-12-15 21:28:53,72] [info] JobStoreActor stopped; [2022-12-15 21:28:53,72] [info] CallCacheWriteActor stopped; [2022-12-15 21:28:53,72] [info] IoProxy stopped; [2022-12-15 21:28:53,74] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=0 waitQueue.size=0 maxWaitQueueLimit=256 closed=false; [2022-12-15 21:28:53,74] [info] Shutting down connection pool: curAllocated=0 idleQueues.size=",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971:49296,release,released,49296,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971,1,['release'],['released']
Deployability,"d; [2023-02-08 16:32:11,69] [info] checkpointClose end; [2023-02-08 16:32:11,69] [info] Checkpoint end - txts: 5342; [2023-02-08 16:32:21,70] [info] Checkpoint start; [2023-02-08 16:32:21,70] [info] checkpointClose start; [2023-02-08 16:32:21,70] [info] checkpointClose synched; [2023-02-08 16:32:21,74] [info] checkpointClose script done; [2023-02-08 16:32:21,74] [info] dataFileCache commit start; [2023-02-08 16:32:21,76] [info] dataFileCache commit end; [2023-02-08 16:32:21,82] [info] checkpointClose end; [2023-02-08 16:32:21,82] [info] Checkpoint end - txts: 5348; [2023-02-08 16:32:21,89] [error] Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.LockException: Could not acquire change log lock. Currently locked by fdb0:cafe:d0d0:ceb4:ba59:9fff:fec3:33de%p1p1 (fdb0:cafe:d0d0:ceb4:ba59:9fff:fec3:33de%p1p1) since 2/8/23, 4:23 PM; 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:270); 	at liquibase.Liquibase.lambda$update$1(Liquibase.java:214); 	at liquibase.Scope.lambda$child$0(Scope.java:180); 	at liquibase.Scope.child(Scope.java:189); 	at liquibase.Scope.child(Scope.java:179); 	at liquibase.Scope.child(Scope.java:158); 	at liquibase.Liquibase.runInScope(Liquibase.java:2405); 	at liquibase.Liquibase.update(Liquibase.java:211); 	at liquibase.Liquibase.update(Liquibase.java:197); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:74); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:46); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7009:3702,update,update,3702,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7009,1,['update'],['update']
Deployability,dValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at crom,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:1966,pipeline,pipelines,1966,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['pipeline'],['pipelines']
Deployability,"d](https://github.com/circe/circe); * [io.circe:circe-shapes](https://github.com/circe/circe). from 0.13.0 to 0.14.1.; [GitHub Release Notes](https://github.com/circe/circe/releases/tag/v0.14.1) - [Version Diff](https://github.com/circe/circe/compare/v0.13.0...v0.14.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/92f43ce5f010fa03daeb2625a3f9c1b1be80cca5/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.13.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/rnaseq-workflow/steps/prepare_sample.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/variantcall_batch_region.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6410:1398,integrat,integrationTestCases,1398,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6410,1,['integrat'],['integrationTestCases']
Deployability,"date this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.9.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Pair/cnv_somatic_pair_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Panel/cnv_somatic_panel_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.aws.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.inputs; centaur/src/main/resources/integrationTestCases/germline/haplotype-caller-workflow/HaplotypeCallerWF.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-production-workflow/PairedEndSingleSampleWf.options.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; centaur/src/main/resources/integrationTestCases/green/arrays/arrays.wdl; womtool/src/test/resources/validate/wdl_draft3/valid/HaplotypeCallerWF/HaplotypeCallerWF.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/cnv_somatic_pair_workflow/cnv_somatic_pair_workflow.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/joint-discovery-gatk/joint-discovery-gatk.inputs.json; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7294:1444,integrat,integrationTestCases,1444,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7294,1,['integrat'],['integrationTestCases']
Deployability,"dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861:1415,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"de 10 was able to get a virtual machine, though. Maybe there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunct",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1191,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"de to manipulate internal data structures in ways unintended by the authors. Malformed input may lead to crashes (with a buffer overflow or other memory corruption) or other unspecified behaviors. This crosses a privilege boundary in, for example, certain web-hosting environments in which a Control Panel allows an unprivileged user account to create subaccounts.; https://security-tracker.debian.org/tracker/CVE-2017-12424; -----------------------------------------; CVE-2018-13347: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; mpatch.c in Mercurial before 4.6.1 mishandles integer addition and subtraction, aka OVE-20180430-0002.; https://security-tracker.debian.org/tracker/CVE-2018-13347; -----------------------------------------; CVE-2017-17458: [High] ; Found in: mercurial [4.0-1+deb9u1]; Fixed By: ; In Mercurial before 4.4.1, it is possible that a specially malformed repository can cause Git subrepositories to run arbitrary code in the form of a .git/hooks/post-update script checked into the repository. Typical use of Mercurial prevents construction of such repositories, but they can be created programmatically.; https://security-tracker.debian.org/tracker/CVE-2017-17458; -----------------------------------------; CVE-2017-12562: [High] ; Found in: libsndfile [1.0.27-3]; Fixed By: ; Heap-based Buffer Overflow in the psf_binheader_writef function in common.c in libsndfile through 1.0.28 allows remote attackers to cause a denial of service (application crash) or possibly have unspecified other impact.; https://security-tracker.debian.org/tracker/CVE-2017-12562; -----------------------------------------; CVE-2018-1000001: [High] ; Found in: glibc [2.24-11+deb9u4]; Fixed By: ; In glibc 2.26 and earlier there is confusion in the usage of getcwd() by realpath() which can be used to write before the destination buffer leading to a buffer underflow and potential code execution.; https://security-tracker.debian.org/tracker/CVE-2018-1000001; ------------------------",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4979:1843,update,update,1843,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4979,1,['update'],['update']
Deployability,"de7924356""]: exit status 1 (standard error: ""Error response from daemon: pull access denied for gcr.io/broad-cumulus/cellranger, repository does not exist or may require 'docker login': denied: Permission denied for \""sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"" from request \""/v2/broad-cumulus/cellranger/manifests/sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"".\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:91); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:812); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:95); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1340); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1336); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.j",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:14042,pipeline,pipelines,14042,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['pipeline'],['pipelines']
Deployability,"de: 0; memory: ""2 GB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""us-central1-a"", ""us-central1-b""]; }. include ""papi_v2_reference_image_manifest.conf""; }; }; }; }; ```. WDL:. ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. input. ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. Gcloud log (edited):. ```; done: true; metadata:; '@type': type.googleapis.com/google.cloud.lifesciences.v2beta.Metadata; createTime: '2021-08-03T15:21:55.984657Z'; endTime: '2021-08-03T15:24:03.533702405Z'; events:; - description: Worker released; timestamp: '2021-08-03T15:24:03.533702405Z'; workerReleased:; instance: google-pipelines-worker-xxxxxx; zone: us-central1-b; - containerStopped:; actionId: 19; description: Stopped running ""-c python -c 'import base64; print(base64.b64decode(\""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh""; timestamp: '2021-08-03T15:24:02.823519462Z'; - containerStarted:; actionId: 19; description: Started running ""-c python -c 'import base64; print(base64.b64decode(\""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh""; timestamp: '2021-08-03T15:23:57.785552960Z'; - containerStopped:; actionId: 18; description: Stopped running ""-c python -c 'import base64; print(base64.b64decode(\""xxxxxx""));'; > /tmp/1xxxxxx.sh && chmod u+x /tmp/1xxxxxx.sh; && sh /tmp/1xxxxxx.sh""; timestamp: '2021-08-03T15:23:57.673915859Z'; - containerStarted:; actionId: 18; description: Started running ""-c python -c 'import base64; print(base64.b64decode(\""xxxxxx""));'; > /tmp/1xxxxxx.sh && chmod u+x /tmp/1xxxxxx.sh; && sh /tmp/1xxxxxx.sh""; timestamp: '2021-08-03T15:23:55.116803722Z'; - containerStoppe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:6770,pipeline,pipelines-worker-xxxxxx,6770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['pipeline'],['pipelines-worker-xxxxxx']
Deployability,del.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686:1841,pipeline,pipeline,1841,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686,2,['pipeline'],['pipeline']
Deployability,deliverable: google doc / updated scaladocs on Metadata Service API,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/783:26,update,updated,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/783,1,['update'],['updated']
Deployability,deliverable: updated design document; deliverable: case classes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/782:13,update,updated,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/782,1,['update'],['updated']
Deployability,deploy hook for (bio)conda?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2258:0,deploy,deploy,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2258,1,['deploy'],['deploy']
Deployability,"dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. I followed this format but got this error; ```; [2022-11-20 18:17:16,88] [warn] Failed to build PipelinesApiConfigurationAttributes on attempt 1 of 3, retrying.; cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$$anon$1: Google Pipelines API configuration is not valid: Errors:; Attempt to decode value on failed cursor: DownField(manifestFormatVersion); at cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$.apply(PipelinesApiConfigurationAttributes.scala:307); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.defaultBuildAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:32); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.$anonfun$papiAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:34); at scala.util.Try$.apply(Try.scala:210); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.cromwell$backend$google$pipelines$common$PipelinesApiBackendLifecycleActorFactory$$build$1(PipelinesApiBackendLifecycleActorFactory.scala:109); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.robustBuildAttributes(PipelinesApiBackendLifecycleActorFactory.scala:120); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.<init>(PipelinesApiBackendLifecycleActorFactory.scala:34); at cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:10); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Con",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:1312,pipeline,pipelines,1312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,1,['pipeline'],['pipelines']
Deployability,"dinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. Running on a Local backend with `java -jar $CROMWELL_JAR run -i input.json wf.wdl`. <!-- Paste/Attach your workflow if possible: -->; ```; task hello {; String outfilename; String ? name. command {; echo ""Hello ${default='world' name}"" > ${outfilename}; }; output {; File out = ""${outfilename}""; }; }. workflow test1 {; String name. call hello {; input: outfilename=""${name}.txt"", name = ""${name}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 se",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:1181,configurat,configuration,1181,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,1,['configurat'],['configuration']
Deployability,dk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); at software.amazon.awssdk.services.batch.DefaultBatchClient.registerJobDefinition(DefaultBatchClient.java:644); at cromwell.backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:3179,pipeline,pipeline,3179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['pipeline'],['pipeline']
Deployability,dk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awss,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:2617,pipeline,pipeline,2617,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,doc updates forthcoming,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6590#issuecomment-994016118:4,update,updates,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6590#issuecomment-994016118,1,['update'],['updates']
Deployability,"docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above tha",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:1877,deploy,deploy,1877,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,1,['deploy'],['deploy']
Deployability,"docs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. If you see I've configured root to be root = ""/fast/gdr/uat/cromwell-executions"". but randomly sometime workflows when I check cromwell api metadata it is pointing to old root which was /g/cromwell/cromwell-executions. . Note I'm running cromwell in server mode with mariadb. I've cleaned and deleted all tables from mariadb. restarted the server as well. Can't find any other config/cache file where it has saved old address. Sometime workflows are fine pointing to new root but sometime not. <!-- Which backend are you running? -->; SLURM on cromwell 36. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. backend {; # Override the default backend.; default = ""PhoenixSLURM"". # The list of providers.; providers {. PhoenixSLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; String userid; String partitions; String memory_per_node; Int nodes; Int cores; String time; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). exit-code-timeout-seconds = 600. submit = """"""; chmod 770 -R ${cwd}; sudo change-files.sh ${userid} ${cwd}; phoenix_home_cwd=""/home/${userid}""; phoenix_home_out=""/home/${userid}/stdout""; phoenix_home_err=""/home/${userid}/stderr"". phoenix_script=${script}_phonix; cat ${script} ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4404:1225,configurat,configuration,1225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4404,1,['configurat'],['configuration']
Deployability,"docs.io/en/stable/backends/Google/; The format for the json file should be; ```; {; ""biocontainers/samtools:1.3.1"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-biocontainers-samtools-1-3-1"",; ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. I followed this format but got this error; ```; [2022-11-20 18:17:16,88] [warn] Failed to build PipelinesApiConfigurationAttributes on attempt 1 of 3, retrying.; cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$$anon$1: Google Pipelines API configuration is not valid: Errors:; Attempt to decode value on failed cursor: DownField(manifestFormatVersion); at cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$.apply(PipelinesApiConfigurationAttributes.scala:307); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.defaultBuildAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:32); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.$anonfun$papiAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:34); at scala.util.Try$.apply(Try.scala:210); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.cromwell$backend$google$pipelines$common$PipelinesApiBackendLifecycleActorFactory$$build$1(PipelinesApiBackendLifecycleActorFactory.scala:109); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.robustBuildAttributes(PipelinesApiBackendLifecycleActorFactory.scala:120); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.<init>(PipelinesApiBackendLifecycleActorFactory.scala:34); at cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:10); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Met",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:1029,Pipeline,PipelinesApiBackendLifecycleActorFactory,1029,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,1,['Pipeline'],['PipelinesApiBackendLifecycleActorFactory']
Deployability,documentation on GPU driver update BA-5951,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5137:28,update,update,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5137,1,['update'],['update']
Deployability,"don't know if this is ""how it's supposed to look"" but here is what a test run spits out for me:. ```bash; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; [2018-08-27 02:03:57,30] [info] Running with database db.url = jdbc:hsqldb:mem:3cd4a928-7743-4252-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-27 02:04:06,71] [info] Metadata summary refreshing every 2 seconds.; [2018-08-27 02:04:06,81] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-27 02:04:07,85] [info] JobExecutionTokenDispenser - Distribution rate: 50 pe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:1566,pipeline,pipelines,1566,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,1,['pipeline'],['pipelines']
Deployability,"duler$$anon$7.run(Scheduler.scala:117); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:599); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:597); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467); at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375); at java.lang.Thread.run(Thread.java:745); [2015-12-18 08:43:19,174] [info] Message [cromwell.engine.workflow.WorkflowManagerActor$RestartWorkflows] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,180] [info] Message [akka.actor.Status$Failure] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,182] [error] WorkflowManagerActor: Workflow failed submission: cannot create children while terminating or terminated; java.lang.IllegalStateException: cannot create children while terminating or terminated; at akka.actor.dungeon.Children$class.makeChild(Children.scala:199); at akka.actor.dungeon.Children$class.actorOf(Children.scala:37); at akka.actor.ActorCell.actorOf(ActorCell.scala:369); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$11.apply(WorkflowManagerActor.scala:246); at c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/334:2399,configurat,configuration,2399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/334,1,['configurat'],['configuration']
Deployability,"e / negative. Docker runtime attributes with docker hashes do not need any additional processing. All logic in this ticket applies to docker runtime attributes with a ""floating"" tag, which will be referred as ""tag"" in this issue. In all cases, if Cromwell fails to retrieve the docker hash for a task, for any reason, the corresponding call(s) will NOT be eligible for call caching, neither read nor write, regardless of the call caching configuration in effect. **When to get the hashes and what to do with them:**. 1. Cromwell will lookup the hashes corresponding to docker tags, for all docker attributes in all tasks in a workflow and its subworkflows, at Materialization time.; If the runtime attribute value can't be determined, the task in question will be ineligible for call caching. The only case when that should be true is if the attribute is an expression with variables depending on previous tasks being run. 2. If the hash lookup succeed, Cromwell will use that hash to perform any call cache read / write according to the call caching configuration in effect. It will also provide that hash, along with the original floating tag, to the backend when the job gets dispatched. 3. Backends will choose wether to use the hash or the floating tag. They will report to the engine which one they used, so that the engine can send this information to the metadata. **How to get the hash:**. 1. How to get the hash depends on the backend. Which means, at this time, that only workflows for which the backend is known statically at workflow submission time will be supported. 2. If the task is expected to run on the **Local Backend**, Cromwell will attempt to find the hash corresponding to the tag on the machine where it's being run. This first attempt must be done without executing a `pull` to avoid overriding the current local image, if it exits, with the remote repository version.; If the image is not present locally, cromwell will attempt to `pull` the image locally, and use the has",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2048:1173,configurat,configuration,1173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2048,1,['configurat'],['configuration']
Deployability,"e check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. Running on a Local backend with `java -jar $CROMWELL_JAR run -i input.json wf.wdl`. <!-- Paste/Attach your workflow if possible: -->; ```; task hello {; String outfilename; String ? name. command {; echo ""Hello ${default='world' name}"" > ${outfilename}; }; output {; File out = ""${outfilename}""; }; }. workflow test1 {; String name. call hello {; input: outfilename=""${name}.txt"", name = ""${name}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:1069,configurat,configuration,1069,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,1,['configurat'],['configuration']
Deployability,"e conditional on failure of the Python section; if [[ ""${debug_dump_flag}"" == ""always"" || ( ""${debug_dump_flag}"" == ""onfail"" && $exit_code -ne 0 ) ]]; then; echo ""Creating debug bundle""; # tar up the output directory; touch debug_bundle.tar.gz; tar cfz debug_bundle.tar.gz --exclude=debug_bundle.tar.gz .; else; touch debug_bundle.tar.gz; fi; /opt/src/algutil/monitor_stop.py. # exit statement must be the last line in the command block; exit $exit_code. }; output {; File out_gvcf = ""${out_gvcf_fn}""; File out_gvcf_index = ""${out_gvcf_fn}.tbi""; File monitor_start=""monitor_start.log""; File monitor_stop=""monitor_stop.log""; File dstat=""dstat.log""; File debug_bundle=""debug_bundle.tar.gz""; } runtime {; docker : ""gcr.io/btl-dockers/btl_gatk:1""; memory: ""${ram_gb}GB""; cpu: ""${cpu_cores}""; disks: ""local-disk ${output_disk_gb} HDD""; bootDiskSizeGb: ""${boot_disk_gb}""; preemptible: ""${preemptible}""; }; parameter_meta {. }. }. application.conf. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. system.new-workflow-poll-rate=1; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; }; ]; }. engine {; filesystems {; gcs {; }; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. backend {; default = ""Jes""; providers {; Jes {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""gcid-cromwell"". // Base bucket for workflow executions. // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3905:4276,configurat,configuration,4276,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3905,1,['configurat'],['configuration']
Deployability,"e cromwell in server mode with a mysql (`mariadb-10.3.12-5.fc30.x86_64`) backend, I get a series of Exceptions, each of which is fixed by changing double quotes in a `.sql` file to single quotes. The first is; ```; 2019-01-31 19:14:34,340 INFO - changelog.xml: changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet: ChangeSet changesets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet ran successfully in 117ms; 2019-01-31 19:14:34,435 ERROR - changelog.xml: changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Change Set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlD",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606:999,release,released,999,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606,1,['release'],['released']
Deployability,"e entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and more. It is starting to impact our through-put for our production pipeline processing. > ------------------------------- ; > kemp@google.com <kemp@google.com> #6 Jan 17, 2018 10:44AM ; > Nothing has changed in Pipelines API in this regard. I suspect either a GCE preemption policy change or some other resourcing issue. Mike, can you reach out to the GCE team on this?; > ; > Garret, let's look at some of the operations in #1 and see if we can see any differences that point to the 13/14 error codes. > ------------------------------- ; > maltarace@google.com <maltarace@google.com> #7 Jan 17, 2018 11:45AM ; > Henry, can I get a project name and recent most time this issue occurred?. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #8 Jan 17, 2018 12:12PM ; > Mike,; > ; > Will get some more recent ones - but here is the project, start, end times, instance name, zone/machinetype for the opids listed in ticket. As you can see diff projects, diff region/zones, diff machine types; > ; > broad-wgs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:5271,pipeline,pipeline,5271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['pipeline'],['pipeline']
Deployability,"e there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContex",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1256,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"e to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""us-central1"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses fro",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:3528,Pipeline,Pipelines,3528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['Pipeline'],['Pipelines']
Deployability,"e tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Reporter.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestFailed.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestCanceled.html; - https://github.com/scalatest/scalatest/blob/3.0.1/scalatest/src/main/resources/org/scalatest/ScalaTestBundle.properties#L664; - https://github.com/scalatest/scalatest/b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:3348,configurat,configuration,3348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['configurat'],['configuration']
Deployability,e validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:1562,pipeline,pipelines,1562,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['pipeline'],['pipelines']
Deployability,"e were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any kind of backend. I'm forcing a dog into a cat; > shaped hole just because this is the hole I'm supposed to fill. Is that a; > good idea? I've lost sight of what the tool is trying to do. Cromwell is; > trying to make it easy to run a Singularity container. But if that's the; > case, then why has this command:; >; > singularity run shub://vsoch/hello-world; >; > turned into needing Cromwell (java and the jar), an inputs json file, a; > wdl specification, a backend configuration, and a runtime command that I; > can't seem to remember, and then the entire thing takes much longer than an; > instance to echo a tiny Rawwwwr! If this is the goal we are going for, is; > this making life easier for the scientist? If I'm a programmer person, and; > this is the minimum I am allowed for this to just run a simple container,; > what happens when it gets harder? I realized that without a proper services; > API, singularity is no more special than python, bash, samtools, it's just; > a binary.; >; > And I realize also that it's easy to get caught up in details like ""Should; > we use Travis or Circle?"" Does it work on Amazon with this kind of input?; > And there will always be bugs! But I think the forest is being a bit lost; > for the trees.; > Question 4: What is the direction to go in?; >; > You can probably take what I'm saying with a grain of salt because I'm new; > to this entire universe, and there is so much invested there is no turning; > back or rethinking. But all of this seems too complicated, and too hard.; > What is needed is a solution that is just really stupid and simple. You; > have a container t",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:12174,configurat,configuration,12174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,1,['configurat'],['configuration']
Deployability,"e#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | [...ll/engine/workflow/WorkflowDockerLookupActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS93b3JrZmxvdy9Xb3JrZmxvd0RvY2tlckxvb2t1cEFjdG9yLnNjYWxh) | `95.34% <0%> (+1.16%)` | :arrow_up: |; | [...cle/execution/callcaching/CallCacheDiffActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS93b3JrZmxvdy9saWZlY3ljbGUvZXhlY3V0aW9uL2NhbGxjYWNoaW5nL0NhbGxDYWNoZURpZmZBY3Rvci5zY2FsYQ==) | `96.38% <0%> (+1.2%)` | :arrow_up: |; | [...scala/cromwell/languages/util/ImportResolver.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-bGFuZ3VhZ2VGYWN0b3JpZXMvbGFuZ3VhZ2UtZmFjdG9yeS1jb3JlL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2xhbmd1YWdlcy91dGlsL0ltcG9ydFJlc29sdmVyLnNjYWxh) | `98.7% <0%> (+1.29%)` | :arrow_up: |; | [...src/main/scala/wdl/draft2/model/WdlNamespace.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL1dkbE5hbWVzcGFjZS5zY2FsYQ==) | `92.37% <0%> (+1.34%)` | :arrow_up: |; | ... and [402 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=footer). Last update [8055dad...803ebd8](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251:4537,update,update,4537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251,1,['update'],['update']
Deployability,"e, I changed the `backend.providers.Local.config.submit-docker` script for the following:. ```bash; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}. # get the return code (working even if the container was detached); rc=$(docker wait `cat ${docker_cid}`). # remove the container after waiting; docker rm `cat ${docker_cid}`. # return exit code; exit $rc; ```. Maybe this could be the default value in the [reference configuration file](https://github.com/broadinstitute/cromwell/blob/develop/core/src/main/resources/reference.conf) to solve the problem, but maybe it is better to have a `post-docker` configuration which is added to the pipeline similar to the `script-epilogue`. This would make easier the configuration of docker runs, separating submission and checks. By now, I will use the following local configuration to continue my work with the cromwell runner:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 15; # set the root directory to the run; filesystems.local {; ## do not allow copy (huge files); localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; # custom submit-docker to workaround detached container due to timeout in the virtual machine; # first, we do not remove the container until it really finishes (no --rm flag); # if the docker run command fails, then it runs docker wait to wait until it finishes and store the return code; # if the docker run command fails, then it runs docker wait to return the real exit code even if detached; # once it finishes, removes the docker container with docker rm; # finally, returns the ""real return code"" stored; submit-docker = """"""; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526:1080,configurat,configuration,1080,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370#issuecomment-371448526,1,['configurat'],['configuration']
Deployability,"e/cromwell/files/539674/local_application.conf.txt). conf file attached in case I got something wrong... Stack trace where a lot of time seems to be spent... ```; ""cromwell-system-akka.actor.default-dispatcher-16"" #50 prio=5 os_prio=0 tid=0x00007f2fb0054800 nid=0x956 runnable [0x00007f301befc000]; java.lang.Thread.State: RUNNABLE; at sun.nio.ch.FileDispatcherImpl.read0(Native Method); at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46); at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223); at sun.nio.ch.IOUtil.read(IOUtil.java:197); at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159); - locked <0x00000006c54b2e78> (a java.lang.Object); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109); at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103); - locked <0x00000006c54b2ec8> (a sun.nio.ch.ChannelInputStream); at org.apache.commons.codec.digest.DigestUtils.updateDigest(DigestUtils.java:798); at org.apache.commons.codec.digest.DigestUtils.digest(DigestUtils.java:50); at org.apache.commons.codec.digest.DigestUtils.md5(DigestUtils.java:274); at org.apache.commons.codec.digest.DigestUtils.md5Hex(DigestUtils.java:310); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.HashFileStrategy$$anonfun$hash$3.apply(ConfigHashingStrategy.scala:70); at cromwell.util.TryWithResource$$anonfun$tryWithResource$1.apply(TryWithResource.scala:16); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryWithResource$.tryWithResource(TryWithResource.scala:10); at cromwell.backend.impl.sfs.config.HashFileStrategy.hash(ConfigHashingStrategy.scala:70); at cromwell.backend.impl.sfs.config.ConfigHashingStrategy.getHash(ConfigHashingStrategy.scala:47); at cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory$$anonfun$fileHashingFunction$1.apply(ConfigBackendLifecycleAct",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1597:1049,update,updateDigest,1049,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1597,1,['update'],['updateDigest']
Deployability,"e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.jav",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2040,pipeline,pipelines,2040,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['pipeline'],['pipelines']
Deployability,"e: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:5689,configurat,configuration,5689,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['configurat'],['configuration']
Deployability,"e:NA:1]: Status change from Running to Succeeded; [2019-05-22 19:19:18,44] [info] 755021ae-948b-47f9-94a8-66b486bda47d-SubWorkflowActor-SubWorkflow-Haplotypecaller:0:1 [755021ae]: Starting Haplotypecaller.HC_GVCF (23 shards); [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr10.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:5965,Pipeline,Pipeline,5965,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,1,['Pipeline'],['Pipeline']
Deployability,e; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:51); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:34); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:20); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); at akka.actor.ActorCell.invoke(ActorCell.scala:583); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.r,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:4041,pipeline,pipelines,4041,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['pipeline'],['pipelines']
Deployability,"eJavascriptRequirement""; },; {; ""coresMin"": 8,; ""ramMin"": 32000,; ""class"": ""ResourceRequirement""; }; ],; ""baseCommand"": ""/opt/gridss/gridss.sh"",; ""arguments"": [; {; ""prefix"": ""--threads"",; ""valueFrom"": ""$get_threads_val(inputs)""; }; ],; ""inputs"": [; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""Optional - location of the GRIDSS assembly BAM. This file will be created by GRIDSS.\n"",; ""inputBinding"": {; ""prefix"": ""--assembly""; },; ""default"": "".assembly.bam"",; ""id"": ""#gridss-2.9.4.cwl/assembly""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional - BED file containing regions to ignore\n"",; ""inputBinding"": {; ""prefix"": ""--blacklist""; },; ""id"": ""#gridss-2.9.4.cwl/blacklist""; },; {; ""type"": ""string"",; ""doc"": ""portion of 6 sigma read pairs distribution considered concordantly mapped. Default: 0.995\n"",; ""inputBinding"": {; ""prefix"": ""--concordantreadpairdistribution""; },; ""default"": ""0.995"",; ""id"": ""#gridss-2.9.4.cwl/concordantreadpairdistribution""; },; {; ""type"": [; ""null"",; ""File""; ],; ""doc"": ""Optional - configuration file use to override default GRIDSS settings.\n"",; ""inputBinding"": {; ""prefix"": ""--configuration""; },; ""id"": ""#gridss-2.9.4.cwl/configuration""; },; {; ""type"": [; ""null"",; ""boolean""; ],; ""doc"": ""Optional - use the system version of bwa instead of the in-process version packaged with GRIDSS\n"",; ""inputBinding"": {; ""prefix"": ""--externalaligner""; },; ""default"": false,; ""id"": ""#gridss-2.9.4.cwl/externalaligner""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""Optional - location of GRIDSS jar\n"",; ""inputBinding"": {; ""prefix"": ""--jar""; },; ""default"": ""/opt/gridss/gridss-2.9.4-gridss-jar-with-dependencies.jar"",; ""id"": ""#gridss-2.9.4.cwl/jar""; },; {; ""type"": ""boolean"",; ""doc"": ""zero-based assembly job index (only required when performing parallel assembly across multiple computers)\n"",; ""inputBinding"": {; ""prefix"": ""--jobindex""; },; ""default"": false,; ""id"": ""#gridss-2.9.4.cwl/jobindex""; },; {; ""type"": ""boolean"",; ""doc"": ""total number of assembly jobs (only requ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:11322,configurat,configuration,11322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['configurat'],['configuration']
Deployability,"eSet changesets/add_hog_group_in_workflow_store.xml::add_hog_group_in_workflow_store::cjllanwarne ran successfully in 10ms; 2019-07-21 23:07:19,332 INFO - INSERT INTO public.databasechangelog (ID, AUTHOR, FILENAME, DATEEXECUTED, ORDEREXECUTED, MD5SUM, DESCRIPTION, COMMENTS, EXECTYPE, CONTEXTS, LABELS, LIQUIBASE, DEPLOYMENT_ID) VALUES ('add_hog_group_in_workflow_store', 'cjllanwarne', 'changesets/add_hog_group_in_workflow_store.xml', NOW(), 32, '8:618f223b37b310ec4ba7a1a89eb37e09', 'addColumn tableName=WORKFLOW_STORE_ENTRY', '', 'EXECUTED', NULL, NULL, '3.6.3', '3750437988'); 2019-07-21 23:07:19,335 INFO - alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint; 2019-07-21 23:07:19,336 ERROR - Change Set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir failed. Error: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 2019-07-21 23:07:19,372 INFO - Successfully released change log lock; 2019-07-21 23:07:19,386 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:34831,release,released,34831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,1,['release'],['released']
Deployability,"eSlickDatabase.scala#L75) by removing transaction semantics from the heartbeat write query. This way, the second query no longer locks multiple rows at once. I am using Slick's [`withPinnedSession`](http://slick.lightbend.com/doc/3.2.0/dbio.html#transactions-and-pinned-sessions) to preserve the efficiency gain of having all the queries in a single session. The MySQL query log shows that `transactionally` and `withPinnedSession` both cause queries to execute in a single session, as evidenced by the setting of session variable `autocommit`:. - `database.run(action.transactionally)`:; ```; Query SET autocommit=0; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c8482924-ef9e-4b3f-930c-ab5f023eeb78'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'e79a1ee7-dd21-4a55-b52d-03f50031b75e'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query SET autocommit=1; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4022:1359,update,update,1359,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022,1,['update'],['update']
Deployability,eStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stage,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:4775,pipeline,pipeline,4775,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,3,['pipeline'],['pipeline']
Deployability,eStage.java:66); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.j,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:3039,pipeline,pipeline,3039,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['pipeline'],['pipeline']
Deployability,"echo""; }; ```; This was run with: `java -jar cromwell-36.jar run works.json --inputs inputs.json`. There are two issues:; - clearly the `name` key is being ignored. Since it is not required (see next item), this is by itself quite minor.; - a `name` key is *not* required per the CWL spec (https://www.commonwl.org/v1.0/CommandLineTool.html#InputRecordSchema). As mentioned, ignoring the `name` parameter is probably acceptable, BUT if I remove that parameter, the execution fails. The failing example is the same, but with ` ""name"": ""SOME JUNK VALUE"",` removed:; ```; $ diff works.json fails.json ; 9d8; < ""name"": ""SOME JUNK VALUE"",; ```; The stack trace reports:; ```; [2018-10-30 21:46:32,22] [error] WorkflowManagerActor Workflow de935a6c-85a6-476f-845f-cf5360bbef03 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; error when parsing file /tmp/cwl_temp_dir_9897655526044348367/cwl_temp_file_de935a6c-85a6-476f-845f-cf5360bbef03.cwl; DecodingFailure at .inputs[0].type: DecodingFailure at .inputs[0].type: DecodingFailure at .inputs[0].type: String; ``` ; <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4338:3202,configurat,configuration,3202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4338,1,['configurat'],['configuration']
Deployability,"ecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Im using FireCloud (workspace: broad-firecloud-dsde/dsde-methods-sv-dev); <!-- Paste/Attach your workflow if possible: -->; The WDL can be found in GATK's repo: [cnv_germline_cohort_workflow.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/cnv_germline_cohort_workflow.wdl) that imports [cnv_common_tasks.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/cnv_common_tasks.wdl). This is the graph that ```wdltools``` output for that WDL; [graph.pdf](https://github.com/broadinstitute/cromwell/files/2406647/graph.pdf); ![graph](https://user-images.githubusercontent.com/791104/45901323-88187c80-bdb0-11e8-91df-c9a61a12a96a.png). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. As you can see in the monitor's ""Failure"" [report ](https://portal.firecloud.org/#workspaces/broad-firecloud-dsde/dsde-methods-sv-dev/monitor/88f444ae-0898-4b5e-af0c-ede98216641d/6d980272-4aa7-4d32-ab90-84880a0723b2)```GermlineCNVCallerCohortMode``` scatter task never get calls before the dependent ```PostprocessGermineCNVCalls```.; <img width=""788"" alt=""screen shot 2018-09-21 at 3 21 43 pm"" src=""https://user-images.githubusercontent.com/791104/45901815-47b9fe00-bdb2-11e8-9043-9f771ee8bd9e.png"">. The log confirms this if one searches for ""Starting"":; ```; 2018-09-20 22:45:12,561 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.PreprocessIntervals; 2018-09-20 23:03:42,454 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.CollectCounts (95 shards), CNVGermlineCohortWorkflow.ScatterInterva",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4136:1341,configurat,configuration,1341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136,1,['configurat'],['configuration']
Deployability,"ecloud/tasks/aggregate_mafs/tests/TCGA-OR-A5J5-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-OR-A5J6-01A-31D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-OR-A5J7-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-OR-A5J8-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-OR-A5J9-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; /Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/cromwell-executions/aggregate_mafs_workflow/814c47aa-9d11-4c81-a08c-f2b77c002b46/call-aggregate_mafs/inputs/Users/dheiman/Documents/workspace/gdac-firecloud/tasks/aggregate_mafs/tests/TCGA-OR-A5JA-01A-11D-A29I-10.ff872fc4-bd1c-4975-85c8-3655ccd199a2.maf.txt; ```. Apologies if this was corrected by #1857, I originally included this in #1875 last week, but had not heard back about this part of that issue. ---- . Update: `write_lines()` was improved for Cromwell 27 but some issues appear to remain.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1906:20897,Update,Update,20897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1906,1,['Update'],['Update']
Deployability,"ecover(StandardAsyncExecutionActor.scala:942); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.aroundReceive(AwsBatchAsyncBackendJobExecutionActor.scala:74); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); ... 5 more; ```. Basically, it seems that the AWS parser for the disk specification doesn't understand specs in the form `'local-disk 100 HDD'`. This needs to be fixed, since the Broad pipelines won't run without heavy modification otherwise. I've tracked down the file spec parsing code to https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/aws/src/main/scala/cromwell/backend/impl/aws/io/AwsBatchVolume.scala#L52-L74, but I'm not informed enough about Scala to write a PR for this.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4274:3630,pipeline,pipelines,3630,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4274,1,['pipeline'],['pipelines']
Deployability,"ecutingWorkflowState): ; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.nio.file.NoSuchFileException: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; ...; [2019-05-22 19:42:10,31] [info] WorkflowManagerActor WorkflowActor-3997371c-9513-4386-a579-a72639c6e960 is in a terminal state: WorkflowFailedState; [2019-05-22 19:42:59,50] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; ...; Workflow 3997371c-9513-4386-a579-a72639c6e960 transitioned to state Failed; ```. Pulling the actual AWS Batch Job parameters for the ""failed"" job (7c2d29c2-f04e-4b3f-8579-915a6fbc9033) I see the following:; ```; {""jobs"": [{; ""status"": ""SUCCEEDED"", ; ""container"": {; ""mountPoints"": [{",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:11446,Pipeline,Pipeline,11446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,1,['Pipeline'],['Pipeline']
Deployability,"ecycleActorFactory"". system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; config {; project = ""$my_project""; root = ""$my_bucket""; name-for-call-caching-purposes: PAPI; slow-job-warning-time: 24 hours; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600. # Setup GCP to give more memory with each retry; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; ; # Number of workers to assign to PAPI requests; request-workers = 3. virtual-private-cloud {; network-label-key = ""network-key""; network-name = ""network-name""; subnetwork-name = ""subnetwork-name""; auth = ""auth""; }; pipeline-timeout = 7 days; genomics {; auth = ""auth""; compute-service-account = ""$my_account""; endpoint-url = ""https://lifesciences.googleapis.com/""; location = ""us-central1""; restrict-metadata-access = false; localization-attempts = 3; parallel-composite-upload-threshold=""150M""; }; filesystems {; gcs {; auth = ""auth""; project = ""$my_project""; caching {; duplication-strategy = ""copy""; }; }; }; system {; memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; }; runtime {; cpuPlatform: ""Intel Cascade Lake""; }; default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; disks: ""local-disk 375 SSD""; noAddress: true; preemptible: 1; maxRetries: 3; system.memory-retry-error-keys = [""OutOfMemory"", ""Killed"", ""Error:""]; memory_retry_multiplier = 4; zones: [""us-central1-a"", ""us-central1-b""]; }. include ""papi_v2_reference_image_manifest.conf""; }; }; }; }. gustily ls gs://cromwell-executions/MemoryRetryTest/d54a5a39-4d3b-4ac7-9bb1-97043d761b56/call-TestOutOfMemoryRetry; TestOutOfMemoryRetry.log; gcs_delocalization.sh; gcs_localization.sh; gcs_transfer.sh; rc; script; stderr; stdout; pipelines-logs. stderr:; Killed; /cromwell_root/script: line 32: 17 Killed tail /dev/zero. rc:; 137",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7451:3102,pipeline,pipelines-logs,3102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7451,1,['pipeline'],['pipelines-logs']
Deployability,"ed if this was user error. From the configuration:. ```; ...snip...; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; # Google project; project = ""broad-dsde-methods""; ; # Base bucket for workflow executions; root = ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/""; ; # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 1000; ; # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; ; # Optional Dockerhub Credentials. Can be used to access private docker images. REMOVED HERE; dockerhub {; account = ""user_manually_removed""; token = ""password_manually_removed""; }; ; genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default""; ; // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default""; ; # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }; ; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }. #AWS {; ...snip...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1748:1828,Pipeline,Pipelines,1828,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1748,1,['Pipeline'],['Pipelines']
Deployability,"ed. PAPI error code 7. Execution failed: generic::permission_denied: pulling image: docker pull: running [""docker"" ""pull"" ""gcr.io/broad-cumulus/cellranger@sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356""]: exit status 1 (standard error: ""Error response from daemon: pull access denied for gcr.io/broad-cumulus/cellranger, repository does not exist or may require 'docker login': denied: Permission denied for \""sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"" from request \""/v2/broad-cumulus/cellranger/manifests/sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"".\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:91); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:812); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:95); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1340); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1336); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Prom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:13825,pipeline,pipelines,13825,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['pipeline'],['pipelines']
Deployability,"edConfigAsyncJobExecutionActor.aroundReceive(ConfigAsyncJobExecutionActor.scala:208); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527); at akka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Optional value was not set and no 'default' attribute was provided; Optional value was not set and no 'default' attribute was provided; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:60); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:56); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:534); ... 39 common frames omitted. ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3927:6357,configurat,configuration,6357,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3927,1,['configurat'],['configuration']
Deployability,"edEndSingleSampleWorkflow.ApplyBQSR:13:1]:; Status change from Running to Success; 2018-06-07 08:24:07,064 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow 656ddc45-2d1d-4e24-a086-c47fa847c658 failed (during ExecutingWorkflowState): java.lang; .Exception: Task PairedEndSingleSampleWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$Blockable",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:1216,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1216,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"edSession`](http://slick.lightbend.com/doc/3.2.0/dbio.html#transactions-and-pinned-sessions) to preserve the efficiency gain of having all the queries in a single session. The MySQL query log shows that `transactionally` and `withPinnedSession` both cause queries to execute in a single session, as evidenced by the setting of session variable `autocommit`:. - `database.run(action.transactionally)`:; ```; Query SET autocommit=0; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c8482924-ef9e-4b3f-930c-ab5f023eeb78'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'e79a1ee7-dd21-4a55-b52d-03f50031b75e'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query SET autocommit=1; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4022:1546,update,update,1546,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022,1,['update'],['update']
Deployability,efreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withB,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1475,Pipeline,PipelinesApiRunCreationClient,1475,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['Pipeline'],['PipelinesApiRunCreationClient']
Deployability,"eg something like:; ```; $ cromwell quickstart; ## BACKEND ##; Would you like to run tasks [local], [sge] or [papi] (Google cloud)?; > [local] | papi; Installing gcloud......; Done! Would you like me to run [gcloud auth] for you to get default credentials set up?; > [yes] |; Running gcloud auth.......; Done!; Backend setup complete!. ## DATABASE ##; Would you like to keep a database of past runs (eg so that you can call-cache?); > [yes] | ; Would you like to use [mysql] or [hsqldb] file?; > [mysql] | ; MySQL detected. Will not reinstall.; Please enter a MySQL username:; > [root] | chris; Please enter the MySQL password:; > [] |; Database setup complete!. ## SETTING UP YOUR SYSTEM ##; Writing start/stop script to /usr/local/bin/cromwell.server...; Writing configuration file to /etc/cromwell/cromwell.conf... To run Cromwell you can now run:; # cromwell.server start; # cromwell submit <wdl> -i <inputs.json>. Good luck!; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2624:151,Install,Installing,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2624,2,"['Install', 'configurat']","['Installing', 'configuration']"
Deployability,"eive(MaterializeWorkflowDescriptorActor.scala:123); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2023-02-04 08:55:08,24] [info] WorkflowManagerActor WorkflowActor-48f62f22-25fe-4f0f-b5fe-21191f035abd is in a terminal state: WorkflowFailedState; [2023-02-04 08:55:08,24] [info] $a [[38;5;2m48f62f22[0m]: Copying workflow logs from /mnt/g/ELM-WES-pipeline/cromwell-workflow-logs/workflow.48f62f22-25fe-4f0f-b5fe-21191f035abd.log to /mnt/g/ELM-WES-pipeline/cromwell_wf_logs/workflow.48f62f22-25fe-4f0f-b5fe-21191f035abd.log; [2023-02-04 08:55:15,88] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2023-02-04 08:55:17,27] [info] Workflow polling stopped; [2023-02-04 08:55:17,29] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2023-02-04 08:55:17,30] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2023-02-04 08:55:17,31] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2023-02-04 08:55:17,31] [info] Aborting all running workflows.; [2023-02-04 08:55:17,31] [info] JobExecutionTokenDispenser stopped; [2023-02-04 08:55:17,31] [info] WorkflowStoreActor stopped; [2023-02-04 08:55:17,32] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2023-02-04 08:55:17,32] [info] WorkflowLogCopyRouter stopped; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor All workflows finished; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor stopped; [2023-02-0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:15741,pipeline,pipeline,15741,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,1,['pipeline'],['pipeline']
Deployability,"elism-max = 64; }; }; }. dispatchers {; # A dispatcher for actors performing blocking io operations; # Prevents the whole system from being slowed down when waiting for responses from external resources for instance; io-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; # Using the forkjoin defaults, this can be tuned if we wish; }. # A dispatcher for actors handling API operations; # Keeps the API responsive regardless of the load of workflows being run; api-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher for engine actors; # Because backends behaviour is unpredictable (potentially blocking, slow) the engine runs; # on its own dispatcher to prevent backends from affecting its performance.; engine-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # A dispatcher used by supported backend actors; backend-dispatcher {; type = Dispatcher; executor = ""fork-join-executor""; }. # Note that without further configuration, all other actors run on the default dispatcher; }; }. spray.can {; server {; request-timeout = 40s; }; client {; request-timeout = 40s; connecting-timeout = 40s; }; }. system {; // If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false. // Max number of retries per job that the engine will attempt in case of a retryable failure received from the backend; max-retries = 10. // If 'true' then when Cromwell starts up, it tries to restart incomplete workflows; workflow-restart = true. // Cromwell will cap the number of running workflows at N; max-concurrent-workflows = 5000. // Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-worker",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:84281,configurat,configuration,84281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['configurat'],['configuration']
Deployability,"ell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom attributes. runtime-attributes = """""". String? docker. String? docker_name. """""". . # Submit string when there is no ""docker"" runtime attribute. submit = ""/bin/bash ${script}"". . # Submit string when there is a ""docker"" runtime attribute. submit-docker = """""". chmod u+x ${cwd}/execution/script && \. docker run --rm \. -v ${cwd}:${docker_cwd} \. ${docker_name} /bin/bash -c ${script}. """""". . # Root directory where Cromwell writes job results. This directory must be. # visible and writeable by the Cromwell process as well as the jobs that Cromwell. # launches. root = ""cromwell-executions"". . # File system configuration. filesystems {. . # For SFS backends, the ""local"" configuration specifies how files are handled. local {. . # Try to hard link (ln), then soft-link (ln -s), and if both fail, then copy the files. localization: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Call caching strategies. caching {. # When copying a cached result, what type of file duplication should occur. Attempted in the order listed below:. duplication-strategy: [. ""hard-link"", ""soft-link"", ""copy"". ]. . # Possible values: file, path. # ""file"" will compute an md5 hash of the file content. # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",. # in order to allow for the original file path to be hashed. hashing-strategy: ""file"". . # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash. # If false or the md5 does not exist, will proceed with the above-defined hashing strategy. check-sibling-md5: false.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:2689,configurat,configuration,2689,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595,1,['configurat'],['configuration']
Deployability,ell_1 | 170e558760e8: Pulling fs layer; cromwell_1 | 395460e233f5: Pulling fs layer; cromwell_1 | 6f01dc62e444: Pulling fs layer; cromwell_1 | 98db058f41f6: Pulling fs layer; [...]; cromwell_1 | failed to register layer: Error processing tar file(exit status 1): write /root/.cache/pip/http/5/1/d/8/2/51d82969228464b761a16257d5eefe8e2b3dde3c1ad733721353e785: no space left on device; cromwell_1 |; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); cromwell_1 | at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); cromwell_1 | at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); cromwell_1 | at scala.concurrent.BlockContext$,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337:2339,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"elog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.33).; You might want to review and update them manually.; ```; core/src/test/resources/hello_goodbye_scattered_papiv2.json; docs/developers/bitesize/ci/Cromwell_Deployment_Strategies.svg; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; src/ci/resources/papi_v2_reference_image_manifest.conf; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.yaml"", artifactId = ""snakeyaml"" }; }]; ```; </details>. labels: test-library-update, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7081:3316,update,updates,3316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7081,5,['update'],"['update', 'updates']"
Deployability,"ence to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""service-account"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""europe-west4"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses fro",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:13174,Pipeline,Pipelines,13174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['Pipelines']
Deployability,"enied for gcr.io/broad-cumulus/cellranger, repository does not exist or may require 'docker login': denied: Permission denied for \""sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"" from request \""/v2/broad-cumulus/cellranger/manifests/sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"".\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:91); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:812); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:95); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1340); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1336); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockCont",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:14124,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,14124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,ent-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[google-http-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[google-api-client-1.20.0.jar:1.20.0]; at cromwell.engine.backend.jes.Run$.runPipeline$1(Run.scala:59) ~[classes/:na]; at cromwell.engine.backend.jes.Run$.apply(Run.scala:67) ~[classes/:na]; at cromwell.engine.backend.jes.Pipeline.run(Pipeline.scala:75) ~[classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1.cromwell$engine$backend$jes$JesBackend$$anonfun$$attemptToCreateJesRun$1(JesBackend.scala:409) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1$$anonfun$apply$10.apply(JesBackend.scala:412) ~[classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1$$anonfun$apply$10.apply(JesBackend.scala:412) ~[classes/:na]; at cromwell.util.TryUtil$$anonfun$4.apply(TryUtil.scala:64) ~[classes/:na]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.util.TryUtil$.retryBlock(TryUtil.scala:64) ~[classes/:na]; at cromwell.engine.backend.jes.JesBackend$.withRetry(JesBackend.scala:113) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesR,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486:2720,Pipeline,Pipeline,2720,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486,1,['Pipeline'],['Pipeline']
Deployability,entrypoint fix to default docker configuration for #2256,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2280:33,configurat,configuration,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2280,1,['configurat'],['configuration']
Deployability,equestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scal,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1756,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1756,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,equivalent update applied,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5620#issuecomment-670986809:11,update,update,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5620#issuecomment-670986809,8,['update'],['update']
Deployability,"er!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; Our backend: ; GCP PAPIv2 ; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory"". endpoint-url = ""https://genomics.googleapis.com/"". <!-- Paste/Attach your workflow if possible: -->; workflow runtime; runtime {; docker: ""us.gcr.io/cloudypipelines-com/til_segmentation:1.5""; bootDiskSizeGb: 70; disks: ""local-disk 70 SSD""; memory: ""52 GB""; cpu: ""8""; maxRetries: 1; gpuCount: 1; zones: ""us-east1-d us-east1-c us-central1-a us-central1-c us-west1-a us-west1-b""; ##gpuType: ""nvidia-tesla-k80""; gpuType: ""nvidia-tesla-t4""; nvidiaDriverVersion: ""418.40.04""; ##nvidiaDriverVersion: ""418.87.00""; ; }. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; #### Recently, Our All workflows with GPU failed under the same configurations which most of workflows used to work on Cromwell 48, we updated to the latest Cromwell 52, still had the same errors, see belowL. 2020-08-04 23:44:00,228 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - WorkflowManagerActor Workflow f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:1066,pipeline,pipelines,1066,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,1,['pipeline'],['pipelines']
Deployability,er-29 ERROR - No configuration setting found for key 'services'; akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor: exception during creation; 	at akka.actor.ActorInitializationException$.apply(Actor.scala:193); 	at akka.actor.ActorCell.create(ActorCell.scala:669); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283); 	at akka.dispatch.Mailbox.run(Mailbox.scala:224); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services'; 	at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156); 	at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188); 	at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268); 	at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41); 	at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35); 	at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63); 	at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65); 	at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25); 	at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87); 	at akka.actor.Props.newActor(Props.scala:212); 	at akka.actor.ActorCe,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881:1114,configurat,configuration,1114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467132881,1,['configurat'],['configuration']
Deployability,"er; â€‚â€‚; Future PR Plans:; â€‚â€‚In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).; â€‚â€‚This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation. â€‚â€‚In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily because hdfs is spark's natural file system.; â€‚â€‚Note that this doesn't actually prevent spark from writing to the hdfs, in order words, the spark application can write or read from the hdfs if given hdfs locations as arguments. Reason for restriction on environment:; â€‚â€‚In spark cluster mode, the assembly jar file containing the application has to exist in all the nodes of the cluster since the driver program can be started on any of the nodes in the cluster.; â€‚â€‚Known solution to this is to put the jar file in a shared file system like hdfs or a network file system, or a parallel distributed file system like lustre where all nodes in the cluster can access the file.; â€‚â€‚For this reason, we did all our testing using the lustre file system, though it works just fine on a local file system with replication.; â€‚â€‚; â€‚â€‚Also, note that as pointed out in the future PR plans section, the hadoop file system is not supported on this release. Supported File Systems:; â€‚â€‚Local File System; â€‚â€‚Network File System; â€‚â€‚Distributed file system. PS: Please find attached read me for examples on How to use Spark Backend; [readMe.md.zip](https://github.com/broadinstitute/cromwell/files/437890/readMe.md.zip). contributor: @iyanuobidele. Reviewers: @geoffjentry @francares",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1339:2930,release,release,2930,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339,1,['release'],['release']
Deployability,erPathMethods.createPermissionedDirectories$(EvenBetterPathMethods.scala:62); 	at cromwell.filesystems.s3.S3Path.createPermissionedDirectories(S3PathBuilder.scala:156); 	at cromwell.core.path.EvenBetterPathMethods.createPermissionedDirectories(EvenBetterPathMethods.scala:64); 	at cromwell.core.path.EvenBetterPathMethods.createPermissionedDirectories$(EvenBetterPathMethods.scala:62); 	at cromwell.filesystems.s3.S3Path.createPermissionedDirectories(S3PathBuilder.scala:156); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowLogsActor.copyLog(CopyWorkflowLogsActor.scala:36); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowLogsActor$$anonfun$copyLogsReceive$1.$anonfun$applyOrElse$1(CopyWorkflowLogsActor.scala:67); 	at scala.Option.foreach(Option.scala:257); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowLogsActor$$anonfun$copyLogsReceive$1.applyOrElse(CopyWorkflowLogsActor.scala:62); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.engine.workflow.lifecycle.finalization.CopyWorkflowLogsActor.aroundReceive(CopyWorkflowLogsActor.scala:30); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); ```. Is there additional configuration required to specify outputs on s3? . Thanks in advance for your help,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4541:4774,configurat,configuration,4774,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4541,1,['configurat'],['configuration']
Deployability,"er_count,; m2_extra_args = select_first([m2_extra_args, """"]) + "" --max-mnp-distance 0"",; gatk_override = gatk_override,; gatk_docker = gatk_docker,; preemptible = preemptible,; max_retries = max_retries,; pon = pon,; pon_idx = pon_idx,; gnomad = gnomad,; gnomad_idx = gnomad_idx; }; }. output {; Array[File] normal_calls = Mutect2.filtered_vcf; Array[File] normal_calls_idx = Mutect2.filtered_vcf_idx. }; }. ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; engine {; filesystems {; gcs {; auth = ""application-default""; }; }; }; backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600; // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }; filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 12800000; json = 12800000; tsv = 12800000; map = 12800000; object = 12800000; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5352:5914,Pipeline,Pipelines,5914,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352,1,['Pipeline'],['Pipelines']
Deployability,eredInputStream.read(BufferedInputStream.java:345); at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:735); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:3118,pipeline,pipelines,3118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['pipeline'],['pipelines']
Deployability,"erformed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initially:. ```HOCON; database {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite:cromwell.sqlite?foreign_keys=true&date_class=text&journal_mode=truncate""; numThreads=1; }; metadata {; profile = ""slick.jdbc.SQLiteProfile$""; db {; driver = ""org.sqlite.JDBC""; url = ""jdbc:sqlite::memory:?foreign_keys=true&date_class=text""; numThreads=1; }; }; }; ```; This limits the amount of IO operations to the bare minimum to get call-caching working. With this configuration cromwell was able to rerun the callcached 1000+ job workflow in ~25 minutes. However it crashed when it needed to write jobs failing with a SQLite error reporting no such table: `METADATA_ENTRY`. EDIT3: I found that using the SQLite in-memory database allows Cromwell to exceed limits that are set using `-Xmx=1G` for instance. I think this is acceptable, as SQLite does not create very large databases and it allows for using Cromwell with the same setting everywhere, no ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:2189,configurat,configuration,2189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906,1,['configurat'],['configuration']
Deployability,ermissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804:2014,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2014,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,ermissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2292,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"es before completing successfully; > 2020-11-07 17:54:51,674 cromwell-system-akka.dispatchers.engine-dispatcher-33 INFO - 0123c178-1d7e-4fbc-94bd-7fc147e5ccfb-EngineJobExecutionActor-GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1 [UUID(0123c178)]: Could not copy a suitable cache hit for 0123c178:GATK4_WGS_ALL_IN_ONE.N_SamToFastqAndBwaMem:0:1. EJEA attempted to copy 1 cache hits before failing. Of these 1 failed to copy and 0 were already blacklisted from previous attempts). Falling back to running job. As you can see, some small tasks worked but large tasks failed. > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > [â€¦](#); > On Sat, Oct 24, 2020 at 8:27 PM Luyu ***@***.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcessingForVaria",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807:2497,Install,Installing,2497,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-723478807,1,['Install'],['Installing']
Deployability,esApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.aroundReceive(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:3840,pipeline,pipelines,3840,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,3,"['Pipeline', 'pipeline']","['PipelinesApiAsyncBackendJobExecutionActor', 'pipelines']"
Deployability,"es](https://github.com/sbt/sbt-assembly/releases/tag/v1.1.1) - [Version Diff](https://github.com/sbt/sbt-assembly/compare/v1.1.0...v1.1.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.1.0).; You might want to review and update them manually.; ```; CHANGELOG.md; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/variantcall_batch_region.cwl; cloud-nio/cloud-nio-impl-drs/src/main/scala/cloud/nio/impl/drs/DrsCloudNioFileSystemProvider.scala; cwl/src/test/resources/cwl/lodash.js; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" };",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6850:1090,integrat,integrationTestCases,1090,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6850,1,['integrat'],['integrationTestCases']
Deployability,"es_task {; command {; echo ""Hello JES!""; }; runtime {; docker: ""ubuntu:latest""; memory: ""4G""; cpu: ""3""; zones: ""us-central1-c us-central1-a""; disks: ""/mnt/mnt1 3 SSD, /mnt/mnt2 500 HDD""; }; }; workflow jes_workflow {; call jes_task; }; ```. and the console output:. ```; [2016-04-28 15:35:51,218] [info] JesBackend [1cb9c1d2:jes_task]: echo ""Hello JES!""; Apr 28, 2016 3:35:51 PM com.google.api.client.googleapis.services.AbstractGoogleClient <init>; WARNING: Application name is not set. Call Builder#setApplicationName.; [2016-04-28 15:35:51,646] [info] JES Pipeline [1cb9c1d2:jes_task]: Inputs:; exec -> disk:local-disk relpath:exec.sh; [2016-04-28 15:35:51,647] [info] JES Pipeline [1cb9c1d2:jes_task]: Outputs:; jes_task-rc.txt -> disk:local-disk relpath:jes_task-rc.txt; [2016-04-28 15:35:51,648] [info] JES Pipeline [1cb9c1d2:jes_task]: Mounts:; c98942d68bf4c33728f1adef1bfd9ccc -> /mnt/mnt1 (3GB PERSISTENT_SSD); 4fd1d1e01455dfdd4eabcf02c1abaf55 -> /mnt/mnt2 (500GB PERSISTENT_HDD); local-disk -> /cromwell_root (10GB PERSISTENT_SSD); [2016-04-28 15:35:51,728] [warn] JesBackend [1cb9c1d2:jes_task]: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""status"" : ""INVALID_ARGUMENT""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""reason"" : ""badRequest"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757:1296,Pipeline,Pipeline,1296,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757,1,['Pipeline'],['Pipeline']
Deployability,esh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecut,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1573,Pipeline,PipelinesApiRunCreationClient,1573,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['Pipeline'],['PipelinesApiRunCreationClient']
Deployability,"est1-a"", ""us-west1-b"", ""us-west1-c""]; }; }; }; }; }. ```. However, when I tried to run a WDL workflow test which used ""gcr.io/broad-cumulus/cellranger:6.1.1"" docker image, the execution failed with the following log:. ```; 2021-09-27 13:47:50,363 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; Skipping auto-registration; 2021-09-27 13:47:55,753 WARN - Skipping auto-registration; 2021-09-27 13:47:55,833 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; Skipping auto-registration; 2021-09-27 13:47:56,493 WARN - Skipping auto-registration; 2021-09-27 13:47:57,524 INFO - Reference disks feature for PAPIv2 backend is not configured.; 2021-09-27 13:47:58,075 INFO - Slf4jLogger started; 2021-09-27 13:47:58,470 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-69bdc1a"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2021-09-27 13:47:58,845 cromwell-system-akka.dispatchers.service-dispatcher-15 INFO - Metadata summary refreshing every 1 second.; 2021-09-27 13:47:58,865 cromwell-system-akka.dispatchers.service-dispatcher-15 INFO - No metadata archiver defined in config; 2021-09-27 13:47:58,865 cromwell-system-akka.dispatchers.service-dispatcher-15 INFO - No metadata deleter defined in config; 2021-09-27 13:47:58,926 cromwell-system-akka.dispatchers.engine-dispatcher-27 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2021-09-27 13:47:58,929 cromwell-system-akka.dispatchers.service-dispatcher-14 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2021-09-27 13:47:58,935 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:5759,configurat,configuration,5759,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['configurat'],['configuration']
Deployability,"ete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: Unique constraint UC_CUSTOM_LABEL_ENTRY_CLK_CLV_WEU dropped from CUSTOM_LABEL_ENTRY; 2018-06-07 12:16:11,094 INFO - sql_metadata_changelog.xml: metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: Unique constraint added to CUSTOM_LABEL_ENTRY(CUSTOM_LABEL_KEY, WORKFLOW_EXECUTION_UUID); 2018-06-07 12:16:11,094 INFO - sql_metadata_changelog.xml: metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir: ChangeSet metadata_changesets/delete_duplicate_custom_labels.xml::delete_duplicate_custom_labels::kshakir ran successfully in 2ms; 2018-06-07 12:16:11,095 INFO - Successfully released change log lock; 2018-06-07 12:16:11,332 INFO - Slf4jLogger started; 2018-06-07 12:16:11,499 cromwell-system-akka.dispatchers.engine-dispatcher-4 INFO - Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-6c9b8d4"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; 2018-06-07 12:16:11,540 cromwell-system-akka.dispatchers.service-dispatcher-10 INFO - Metadata summary refreshing every 2 seconds.; 2018-06-07 12:16:11,574 cromwell-system-akka.dispatchers.service-dispatcher-8 INFO - WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.actor.default-dispatcher-2 INFO - KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; 2018-06-07 12:16:11,575 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - JobStoreWriterActor configured to flush with batch size 1000 and process rate 1 second.; 2018-06-07 12:16:11,576 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; 2018-06-07 12:16:12,232 cromwell-system-akka.dispatchers.engine-dispatcher-49 INFO - JobExecutionToken",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:96258,configurat,configuration,96258,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['configurat'],['configuration']
Deployability,"etect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_1dec3320bf1ad48ec05404d0a505d12b.tmp \; --adapters gs://atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_d3da014369f27e577cdffc1919be7d8e.tmp \; \; --auto-detect-adapter \; \; \; --nth 2; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032:1991,pipeline,pipeline-workflows,1991,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032,2,['pipeline'],['pipeline-workflows']
Deployability,etryableStage$RetryExecutor.doExecute(RetryableStage.java:139); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); at software.amazon.a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:2777,pipeline,pipeline,2777,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['pipeline'],['pipeline']
Deployability,"ets/add_attempt_in_call_caching_entry.xml::add_attempt_in_call_caching_entry::tjeandet ran successfully in 117ms; 2019-01-31 19:14:34,435 ERROR - changelog.xml: changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Change Set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(Str",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606:1378,UPDATE,UPDATE,1378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606,1,['UPDATE'],['UPDATE']
Deployability,"example from a failed CRON test:. ```; 2018-07-04 07:18:56,909 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunna",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861:991,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,991,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"expected exit status 1 was not ignored; 9606 Execution failed: action 14: unexpected exit status 1 was not ignored; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 962",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3732:1511,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"ext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2020-08-21 11:08:59,66] [info] WorkflowManagerActor WorkflowActor-dbd5cdc0-c79a-42cd-b929-56ddb1115467 is in a terminal state: WorkflowFailedState; [2020-08-21 11:09:00,66] [info] Not triggering log of token queue status. Effective log interval = None; ```. Here is the relevant part of the wdl:. ```; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }; filesystems {; gcs {; // A reference to a poten",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793:2845,pipeline,pipelines,2845,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793,1,['pipeline'],['pipelines']
Deployability,"ey are not being staged in the input directory folder path. Cromwell uses the empty input directory folder path as input to the tool which causes it to fail. . Example.cwl; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0; class: CommandLineTool. baseCommand: [""ls""]; arguments: [""$(inputs.dir)""]. requirements:; - class: DockerRequirement; dockerPull: ""ubuntu:xenial"". inputs:; dir:; type: Directory. outputs:; example_out:; type: stdout. stdout: output.txt; ```; Input.yaml:; ```; dir: ; class: Directory; listing:; - class: File; path: ./data/1.txt; - class: File; path: ./data/2.txt; ```. staged files:; ```; => find inputs/; inputs/; inputs/1465754395; inputs/1465754395/2.txt; inputs/1465754395/1.txt; inputs/-143808698; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d; inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d/.file; ```. And the generated Cromwell command; ```; [d14c14d1example.cwl:NA:1]: 'ls' '/cromwell-executions/example.cwl/d14c14d1-ce96-44fc-9315-d1c431011f83/call-example.cwl/inputs/-143808698/87e206a9-befc-4977-9a6e-c7a36832385d'; ```. Tested using Cromwell 35 and 37. Cwltool works as expected after adding a basename to the input directory in the yaml. . <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4670:2459,configurat,configuration,2459,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4670,1,['configurat'],['configuration']
Deployability,"f010fa03daeb2625a3f9c1b1be80cca5/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.13.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/rnaseq-workflow/steps/prepare_sample.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/variantcall_batch_region.cwl; ```; </details>; <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"" } ]; ```; </details>. labels: library-update, semver-minor, old-version-remains",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6410:2031,integrat,integrationTestCases,2031,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6410,11,"['integrat', 'update']","['integrationTestCases', 'update', 'updates']"
Deployability,fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecuto,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1338,pipeline,pipelines,1338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['pipeline'],['pipelines']
Deployability,"failed: generic::permission_denied: pulling image: docker pull: running [""docker"" ""pull"" ""gcr.io/broad-cumulus/cellranger@sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356""]: exit status 1 (standard error: ""Error response from daemon: pull access denied for gcr.io/broad-cumulus/cellranger, repository does not exist or may require 'docker login': denied: Permission denied for \""sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"" from request \""/v2/broad-cumulus/cellranger/manifests/sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"".\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:91); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:812); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:95); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1340); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1336); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:13842,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,13842,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"fair amount of work in google genomics before this, but this is my first use of cromwell/WDL. Some details first. I first noticed this error on 0.19.2, but went back to check 0.19 and HEAD of the develop branch. where it occurs as well. For completeness, here's my WDL file:. ```; cat ~/workflows/hello-jes.wdl ; task jes_task {; command {; echo ""Hello JES!""; }; runtime {; docker: ""ubuntu:latest""; memory: ""4G""; cpu: ""3""; zones: ""us-central1-c us-central1-a""; disks: ""/mnt/mnt1 3 SSD, /mnt/mnt2 500 HDD""; }; }; workflow jes_workflow {; call jes_task; }; ```. and the console output:. ```; [2016-04-28 15:35:51,218] [info] JesBackend [1cb9c1d2:jes_task]: echo ""Hello JES!""; Apr 28, 2016 3:35:51 PM com.google.api.client.googleapis.services.AbstractGoogleClient <init>; WARNING: Application name is not set. Call Builder#setApplicationName.; [2016-04-28 15:35:51,646] [info] JES Pipeline [1cb9c1d2:jes_task]: Inputs:; exec -> disk:local-disk relpath:exec.sh; [2016-04-28 15:35:51,647] [info] JES Pipeline [1cb9c1d2:jes_task]: Outputs:; jes_task-rc.txt -> disk:local-disk relpath:jes_task-rc.txt; [2016-04-28 15:35:51,648] [info] JES Pipeline [1cb9c1d2:jes_task]: Mounts:; c98942d68bf4c33728f1adef1bfd9ccc -> /mnt/mnt1 (3GB PERSISTENT_SSD); 4fd1d1e01455dfdd4eabcf02c1abaf55 -> /mnt/mnt2 (500GB PERSISTENT_HDD); local-disk -> /cromwell_root (10GB PERSISTENT_SSD); [2016-04-28 15:35:51,728] [warn] JesBackend [1cb9c1d2:jes_task]: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""status"" : ""INVALID_ARGUMENT""; }; com.google.api.client.googleapis.json.Googl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757:1159,Pipeline,Pipeline,1159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757,1,['Pipeline'],['Pipeline']
Deployability,"feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ; Jobs which required gpuType: ""nvidia-tesla-t4"", nvidiaDriverVersion: ""418.40.04"", failed. Our pipeline backend is Google : genomics.googleapis.com; ""jes"": {; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""zone"": ""us-central1-f"",; ....; },. job runtimeAttributes:; ...; ""preemptible"": ""1"",; ""gpuCount"": ""1"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""70"",; ""disks"": ""local-disk 70 SSD"",; ""continueOnReturnCode"": ""0"",; ""gpuType"": ""nvidia-tesla-t4"",; ""nvidiaDriverVersion"": ""418.40.04"",; ""maxRetries"": ""0"",; ""cpu"": ""8"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zone"": ""us-central1-f"",; ""memoryMin"": ""2 GB"",; ""memory"": ""64 GB"". Jobs failed with following message:; ""Task wf_quip_lymphocyte_segmentation_incep_v01052021.quip_lymphocyte_segmentation:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: generic::unknown: installing drivers: container exited with unexpected exit code 1: + COS_KERNEL_INFO_FILENAME=kernel_info\n+ COS_KERNEL_SRC_ARCHIVE=kernel-src.tar.gz\n+ COS_KERNEL_SRC_HEADER=kernel-headers.tgz\n+ TOOLCHAIN_URL_FILENAME=toolchain_url\n+ TOOLC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:1257,pipeline,pipeline,1257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['pipeline'],['pipeline']
Deployability,"fferent modes.; 2. Hyperlinks from the call caching section to other sections (and possibly elsewhere in the docs) are broken as it would appear there has been a change in the URL format - for example, on that same [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, ; > Call Caching can be enabled in your [Cromwell Configuration](https://cromwell.readthedocs.io/en/stable/cromwell_features/Configuring#call-caching). links to `/en/stable/cromwell_features/Configuring#call-caching`; when the current link path is `/en/stable/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4810:1544,Configurat,Configuration,1544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810,1,['Configurat'],['Configuration']
Deployability,"finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExec",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2221,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,fix disk name bug against 0.19 hotfix,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/780:31,hotfix,hotfix,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/780,1,['hotfix'],['hotfix']
Deployability,fix papi config hotfix,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3728:16,hotfix,hotfix,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3728,1,['hotfix'],['hotfix']
Deployability,fixed in https://github.com/broadinstitute/cromwell/releases/tag/34,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3878#issuecomment-407144286:52,release,releases,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3878#issuecomment-407144286,1,['release'],['releases']
Deployability,flowManagerActor - WorkflowManagerActor Workflow f0000000-0000-0000-0000-000000000000 failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.TumorCramToBam:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. The zone 'projects/crashy/zones/us-west1-a' does not have enough resources available to fulfill the request. '(resource type:compute)'.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:619); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:627); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1108); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1104); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(A,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4920:1091,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1091,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4920,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,for the integration test suggestion https://broadworkbench.atlassian.net/browse/BA-6526,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5584#issuecomment-664506839:8,integrat,integration,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5584#issuecomment-664506839,1,['integrat'],['integration']
Deployability,"from Aaron Kemp of Google pipelines, emphasis mine:. ""In the short term, I think the original option we discussed is the best: try without, if it fails, try with. **You will not be billed for the failed request**.""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401347492:26,pipeline,pipelines,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3799#issuecomment-401347492,1,['pipeline'],['pipelines']
Deployability,"from Aaron Kemp:. > Yes - I think you should. You would need to switch to using 'gcr.io/cloud-genomics-pipelines/io' instead of 'google/cloud-sdk:slim' and replace your existing script with something like:. ```; gsutil -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs 2> gsutil_output.txt; RC_GSUTIL=$?; ; if [ ""$RC_GSUTIL"" = ""1"" ]; then; grep ""Bucket is requester pays bucket but no user project provided."" gsutil_output.txt && echo ""Retrying with user project""; ; gsutil -u broad-pharma5-compute3 -h ""Content-Type: text/plain; charset=UTF-8"" -m rsync -r /google/logs gs://broad-pharma5-execution2/.../pipelines-logs;; fi; ```. since now `gsutil` will actually be running our magic retry script.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4640:103,pipeline,pipelines,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4640,3,['pipeline'],"['pipelines', 'pipelines-logs']"
Deployability,"ftfy the metadata sort by deleting it. Batching the summary updates didn't work out since the metadata summary ""last updated id"" scheme assumes rows are processed in batches where each batch has IDs greater than those of the preceding batch. AFAICT that wouldn't be possible without sorting all rows with IDs greater than the last processed row. Instead this just takes all committed rows with IDs greater than the last processed row and assumes that rows are being committed with monotonically increasing IDs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3324:60,update,updates,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3324,2,['update'],"['updated', 'updates']"
Deployability,fwiw - green team currently run their own cromwell instances - so they are not currently impacted by anything we do on WB prod for their pipelines. Plus they generally are not as aggressive at taking newer cromwell versions - so even if you disable (remove) that endpoint they would likely not see the results for quite a bit.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-395762351:137,pipeline,pipelines,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-395762351,1,['pipeline'],['pipelines']
Deployability,g from broadinstitute/gatk; cromwell_1 | ae79f2514705: Pulling fs layer; cromwell_1 | 5ad56d5fc149: Pulling fs layer; cromwell_1 | 170e558760e8: Pulling fs layer; cromwell_1 | 395460e233f5: Pulling fs layer; cromwell_1 | 6f01dc62e444: Pulling fs layer; cromwell_1 | 98db058f41f6: Pulling fs layer; [...]; cromwell_1 | failed to register layer: Error processing tar file(exit status 1): write /root/.cache/pip/http/5/1/d/8/2/51d82969228464b761a16257d5eefe8e2b3dde3c1ad733721353e785: no space left on device; cromwell_1 |; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); cromwell_1 | at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); cromwell_1 |,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337:2228,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"g image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(A",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2286,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"g on COS build id 13310.1209.10\n[INFO 2021-02-22 23:09:17 UTC] Data dependencies (e.g. kernel source) will be fetched from https://storage.googleapis.com/cos-tools/13310.1209.10\n[INFO 2021-02-22 23:09:17 UTC] Getting the kernel source repository path.\n[INFO 2021-02-22 23:09:17 UTC] Obtaining kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n[INFO 2021-02-22 23:09:19 UTC] Downloading kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n\nreal\t0m0.072s\nuser\t0m0.013s\nsys\t0m0.006s\n[INFO 2021-02-22 23:09:19 UTC] Checking if this is the only cos-gpu-installer that is running.\n[INFO 2021-02-22 23:09:19 UTC] Checking if third party kernel modules can be installed\n[INFO 2021-02-22 23:09:19 UTC] Checking cached version\n[INFO 2021-02-22 23:09:19 UTC] Cache file /usr/local/nvidia/.cache not found.\n[INFO 2021-02-22 23:09:19 UTC] Did not find cached version, building the drivers...\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer ... \n[INFO 2021-02-22 23:09:19 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n\nreal\t0m1.891s\nuser\t0m0.181s\nsys\t0m0.449s\n[INFO 2021-02-22 23:09:21 UTC] Setting up compilation environment\n[INFO 2021-02-22 23:09:21 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/13310.1209.10/toolchain_env\n[INFO 2021-02-22 23:09:21 UTC] Downloading toolchain_env file from https://storage.googleapis.com/cos-tools/13310.1209.10/toolchain_env\n\nreal\t0m0.042s\nuser\t0m0.014s\nsys\t0m0.003s\n[INFO 2021-02-22 23:09:21 UTC] Found toolchain path file locally\nls: cannot access '/build/cos-tools': No such ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:3974,install,installer,3974,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['install'],['installer']
Deployability,"g to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleCon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2567,configurat,configuration,2567,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,2,['configurat'],['configuration']
Deployability,"g19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr10.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr15.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:6760,Pipeline,Pipeline,6760,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,1,['Pipeline'],['Pipeline']
Deployability,gStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:120); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:73); 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); 	at software.amazon.awssdk.services.batch.DefaultBatchClient.listJobs(DefaultBatchClient.java:675); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.findJobsInStatus$1(OccasionalStatusPollingActor.scala:88); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$6(OccasionalStatusPollingActor.scala:105); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.immutable.List.flatMap(List.scala:355); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$5(OccasionalStatusPollingActor.scala:104); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.s,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:4428,update,updateStatuses,4428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['update'],['updateStatuses']
Deployability,"ge from Running to Success; 2023-04-18 22:00:18,464 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:106:1]: Status change from Running to Success; 2023-04-18 22:01:20,604 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:111:1]: Status change from Running to Success; 2023-04-18 22:14:47,728 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: Aborting workflow; 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:262:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/9178938377659283430); 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:112:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/8559201934542591362); 2023-04-18 22:14:48,295 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Successfully requested cancellation of projects/16371921765/locations/us-central1/operations/9178938377659283430; 2023-04-18 22:15:56,564 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: Status change from Running to Success; 2023-04-18 22:16:44,505 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Status change from Running to Cancelled; 2023-04-18 22:16:44,539 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: WorkflowExecutionActor [UUID(10fa31a8)] aborted: myco.pull:262:1; 2023-04-18 22:16:45,159 INFO - $f [UUID(10fa31a8)]: Copying workflow logs from /cromwell-workflow-logs/workflow.10fa31a8-acbe-4ab7-a96a-6550ec08df12.log to gs://fc-caa84e5a-8ef7-434e-af9c-feaf6366a042/submissions/93bf6971-bfa1-4cb8-bb22-c8a753f58c49/workflow.logs/workflow.10fa31a8-acbe-4ab7-a96a-6550ec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:5351,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,5351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"gh security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; auth = ""cromwell-service-account""; location: ""${region}""; compute-service-account = ""${compute_service_account}"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; auth = ""cromwell-service-account"". # For billing; project = ""${billing_project}"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }. }; http {}; }. # Important!! Some of the workflows take an excessive amount of time to run; batch-timeout = 28 days. default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2 GB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238:2754,configurat,configuration,2754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238,2,['configurat'],['configuration']
Deployability,gh there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExec,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1355,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,gle-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[google-http-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[google-api-client-1.20.0.jar:1.20.0]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[google-api-client-1.20.0.jar:1.20.0]; at cromwell.engine.backend.jes.Run$.runPipeline$1(Run.scala:59) ~[classes/:na]; at cromwell.engine.backend.jes.Run$.apply(Run.scala:67) ~[classes/:na]; at cromwell.engine.backend.jes.Pipeline.run(Pipeline.scala:75) ~[classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1.cromwell$engine$backend$jes$JesBackend$$anonfun$$attemptToCreateJesRun$1(JesBackend.scala:409) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1$$anonfun$apply$10.apply(JesBackend.scala:412) ~[classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1$$anonfun$apply$10.apply(JesBackend.scala:412) ~[classes/:na]; at cromwell.util.TryUtil$$anonfun$4.apply(TryUtil.scala:64) ~[classes/:na]; at scala.util.Try$.apply(Try.scala:192) ~[scala-library-2.11.7.jar:1.0.0-M1]; at cromwell.util.TryUtil$.retryBlock(TryUtil.scala:64) ~[classes/:na]; at cromwell.engine.backend.jes.JesBackend$.withRetry(JesBackend.scala:113) [classes/:na]; at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486:2707,Pipeline,Pipeline,2707,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/338#issuecomment-166392486,1,['Pipeline'],['Pipeline']
Deployability,"gleapis/google-auth-library-java) from 1.1.0 to 1.3.0.; [GitHub Release Notes](https://github.com/googleapis/google-auth-library-java/releases/tag/v1.3.0) - [Version Diff](https://github.com/googleapis/google-auth-library-java/compare/v1.1.0...v1.3.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/00809e6249b134635f71919c17c1c81603beb22d/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.1.0).; You might want to review and update them manually.; ```; CHANGELOG.md; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/variantcall_batch_region.cwl; cloud-nio/cloud-nio-impl-drs/src/main/scala/cloud/nio/impl/drs/DrsCloudNioFileSystemProvider.scala; cwl/src/test/resources/cwl/lodash.js; project/plugins.sbt; ```; </details>; <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.auth"", artifactId = ""google-auth-library-oauth2-http"" } ]; ```; </details>. labels: library-update, early-semver-mino",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6608:1055,integrat,integrationTestCases,1055,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6608,1,['integrat'],['integrationTestCases']
Deployability,glob mechanism updated from wdl4s,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2646:15,update,updated,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2646,1,['update'],['updated']
Deployability,"gnments: wgbs.flatten_ -> singularity; [2018-08-27 02:04:12,30] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Starting wgbs.flatten_; [2018-08-27 02:04:13,48] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: ; mkdir -p mapping; cat /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/write_lines_8f61fd340a04ccd930e243709dfb1bed.tmp | xargs -I % ln -s % mapping; ls mapping; [2018-08-27 02:04:13,50] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: executing: chmod u+x /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script && \; singularity \; exec \; gemBS.simg \; /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script &; echo $?; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: job id: 0; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: Status change from - to Done; [2018-08-27 02:04:19,50] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Workflow wgbs complete. Final Outputs:; {. }; [2018-08-27 02:04:19,53] [info] WorkflowManagerActor WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468 is in a terminal state: WorkflowSucceededState; [2018-08-27 02:04:22,18] [info] SingleWorkflowRunnerActor workflow finished with status 'Succeeded'.; {; ""outputs"": {. },; ""id"": ""967af8b6-0d68-44c4-b04e-204674333468""; }; [2018-08-27 02:04:26,91] [info] Workflow polling stopped; [2018-08-27 02:04:26,91] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2018-08-27 02:04:26,92] [info] Aborting all running workflows.; [2018-08-27 02:04:26,9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:5062,pipeline,pipelines,5062,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,"google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 25000. # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. #docker-image-cache-manifest-file = ""gs://xxxxx-xxxxx/xxxxx.json"". # Number of workers to assign to PAPI requests; request-workers = 3. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # pipeline-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""us-central1"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; #",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:2742,Pipeline,Pipelines,2742,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['Pipeline'],['Pipelines']
Deployability,gotc-prod; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/devstorage.read_write; - https://www.googleapis.com/auth/genomics; - https://www.googleapis.com/auth/compute; runtimeMetadata:; '@type': type.googleapis.com/google.genomics.v1alpha2.RuntimeMetadata; computeEngine:; diskNames:; - local-disk-15719508696139283496; instanceName: ggp-15719508696139283496; machineType: us-central1-c/n1-highmem-2; zone: us-central1-c; startTime: '2016-09-28T07:20:39Z'; name: operations/EIf5mP32Khio-K6o-ru6k9oBIJ-ikOmeDSoPcHJvZHVjdGlvblF1ZXVl; ```. from .21:. ```; resources:; bootDiskSizeGb: 10; disks:; - autoDelete: true; mountPoint: /cromwell_root; name: local-disk; sizeGb: 200; type: PERSISTENT_HDD; minimumCpuCores: 1; minimumRamGb: 7; preemptible: true; zones:; - us-central1-b; - us-central1-c; pipelineArgs:; clientId: ''; inputs:; exec: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/exec.sh; input_bam-0: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-SortAndFixReadGroupBam/shard-22/HTC3GCCXX.8.Pond-536132.sorted.bam; ref_dict-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.dict; ref_fasta-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta; ref_fasta_index-0: gs://broad-references/hg38/v0/Homo_sapiens_assembly38.fasta.fai; logging:; gcsPath: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4/call-ValidateReadGroupSamFile/shard-22/ValidateReadGroupSamFile-22.log; outputs:; HTC3GCCXX.8.Pond-536132.validation_report: gs://broad-gotc-int-cromwell-execution/PairedEndSingleSampleWorkflow/4d02d218-b473-4a88-8820-964da7e508c4,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1501:3220,pipeline,pipelineArgs,3220,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1501,1,['pipeline'],['pipelineArgs']
Deployability,"gotcha! I'll dig into this and try for a first shot, will send back update when I break I mean, dip in my toe a big more.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413025139:68,update,update,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413025139,1,['update'],['update']
Deployability,ground:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>Thatâ€™s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>Thatâ€™s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.r,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917:3681,Pipeline,PipelinesApiRequestWorker,3681,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917,1,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,"gure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.9.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Pair/cnv_somatic_pair_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Panel/cnv_somatic_panel_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.aws.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.inputs; centaur/src/main/resources/integrationTestCases/germline/haplotype-caller-workflow/HaplotypeCallerWF.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-production-workflow/PairedEndSingleSampleWf.options.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; centaur/src/main/resources/integrationTestCases/green/arrays/arrays.wdl; womtool/src/test/resources/validate/wdl_draft3/valid/HaplotypeCallerWF/HaplotypeCallerWF.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/cnv_somatic_pair_workflow/cnv_somatic_pair_workflow.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/joint-discovery-gatk/joint-discovery-gatk.inputs.json; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.github.jbwheatley"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { gr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7294:1631,integrat,integrationTestCases,1631,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7294,1,['integrat'],['integrationTestCases']
Deployability,"h the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and more. It is starting to impact our through-put for our production pipeline processing. > ------------------------------- ; > kemp@google.com <kemp@google.com> #6 Jan 17, 2018 10:44AM ; > Nothing has changed in Pipelines API in this regard. I suspect either a GCE preemption policy change or some other resourcing issue. Mike, can you reach out to the GCE team on this?; > ; > Garret, let's look at some of the operations in #1 and see if we can see any differences that point to the 13/14 error codes. > ------------------------------- ; > maltarace@google.com <maltarace@google.com> #7 Jan 17, 2018 11:45AM ; > Henry, can I get a project name and recent most time this issue occurred?. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #8 Jan 17, 2018 12:12PM ; > Mike,; > ; > Will get some more recent ones - but here is the project, start, end times, instance name, zone/machinetype for the opids listed in ticket. As you can see diff projects, diff region/zones, diff machine types; > ; > broad-wgs-prod2, 2018-01-02T03:00:06Z, 2018-01-02T03:53:16Z, ggp-17542369260334007071, us-east1-c/n1-standard-2; > broad-wgs-prod2, 2018-01-02T04:05:53Z, 2018-0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:5415,Pipeline,Pipelines,5415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['Pipeline'],['Pipelines']
Deployability,h.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1433,Pipeline,PipelinesApiRunCreationClient,1433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['Pipeline'],['PipelinesApiRunCreationClient']
Deployability,"hange; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table creation proceeds; as it undoes one of the changes in this commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-88f82a0-SNAP.jar server; 2019-01-31 18:42:55,062 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:4989,configurat,configuration,4989,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['configurat'],['configuration']
Deployability,"hangelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/00809e6249b134635f71919c17c1c81603beb22d/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.29).; You might want to review and update them manually.; ```; centaur/src/test/resources/centaur/test/metadata/failingInSeveralWaysMetadata.json; core/src/test/resources/hello_goodbye_scattered_papiv2.json; docs/developers/bitesize/ci/Cromwell_Deployment_Strategies.svg; docs/developers/bitesize/workflowParsing/forkjoin_graph.svg; docs/developers/bitesize/workflowParsing/wdlToWdlom_hermes.svg; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; ```; </details>; <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; </details>. labels: test-library-update, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6635:2789,update,update,2789,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6635,5,['update'],"['update', 'updates']"
Deployability,"hat making them optional like `String? memory_mb` and then using syntax like `${""--mem "" + round(memory_mb) + ""m""} \` in the submit script means that argument will only be added if `memory` is defined, and will be omitted if `memory` is not defined. I've followed the documentation as closely as I can. However, when I try to submit a test job without `cpu` and `memory` set as a runtime attribute, I get a failure with these exceptions:; ```; cromwell.core.CromwellAggregatedException: Initialization Failure:; Runtime validation failed:; 	Task myTask has an invalid runtime attribute cpu = !! NOT FOUND !!; 	Task myTask has an invalid runtime attribute memory = !! NOT FOUND !!; 	at cromwell.engine.workflow.WorkflowActor$$anonfun$3.applyOrElse(WorkflowActor.scala:356); 	at cromwell.engine.workflow.WorkflowActor$$anonfun$3.applyOrElse(WorkflowActor.scala:339); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35); 	at akka.actor.FSM.processEvent(FSM.scala:707); 	at akka.actor.FSM.processEvent$(FSM.scala:704); ```. Here is the test WDL I'm using:. ```; # Example workflow; # Declare WDL version 1.0 if working in Terra; version 1.0; workflow myWorkflow {; call myTask. }. task myTask {; command <<<; echo ""hello world""; >>>; output {; String out = read_string(stdout()); }; }; ```. And my complete configuration for this backend:; ```; backend {; default = slurm. providers {; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"" ; config {; runtime-attributes = """"""; Int? runtime_minutes; Int? cpu; Float? memory_mb; String? docker; String? partition; """""". submit = """"""; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; ${""-t "" + runtime_minutes} \; ${""-c "" + cpu} \; ${""--mem "" + round(memory_mb) + ""m""} \; ${""-p "" + partition} \; --wrap ""/bin/bash ${script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7455:2261,configurat,configuration,2261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7455,1,['configurat'],['configuration']
Deployability,"haven't updated the call caching or restart parts yet. migration doc is missing database migration instructions for 0.21. Also @ruchim wanted to take over the API documentation, so I did not update that as part of this PR.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1171:8,update,updated,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1171,2,['update'],"['update', 'updated']"
Deployability,"he Travis output, the build failure is currently being caused by:; ```; [0m[[0minfo[0m] [0m[31m*** 1 TEST FAILED ***[0m[0m; [0m[[0minfo[0m] [0m[31mWdlSubworkflowWomSpec:[0m[0m; [0m[[0minfo[0m] [0m[31mWdlNamespaces with subworkflows [0m[0m; [0m[[0minfo[0m] [0m[31m- should support WDL to WOM conversion of subworkflow calls *** FAILED *** (51 milliseconds)[0m[0m; [0m[[0minfo[0m] [0m[31m wdl4s.parser.WdlParser$SyntaxError: ERROR: out is declared as a Array[String] but the expression evaluates to a String:[0m[0m; [0m[[0minfo[0m] [0m[31m[0m[0m; [0m[[0minfo[0m] [0m[31m Array[String] out = inner.out[0m[0m; [0m[[0minfo[0m] [0m[31m ^[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.$anonfun$typeCheckDeclaration$1(WdlNamespace.scala:493)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.Option.flatMap(Option.scala:171)[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.typeCheckDeclaration(WdlNamespace.scala:488)[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.validateDeclaration(WdlNamespace.scala:466)[0m[0m; [0m[[0minfo[0m] [0m[31m at wdl.WdlNamespace$.$anonfun$apply$35(WdlNamespace.scala:381)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:241)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.Iterator.foreach(Iterator.scala:929)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.Iterator.foreach$(Iterator.scala:929)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.AbstractIterator.foreach(Iterator.scala:1417)[0m[0m; [0m[[0minfo[0m] [0m[31m at scala.collection.IterableLike.foreach(IterableLike.scala:71)[0m[0m; ```. I'm not sure whether you intended to roll back that change at the same time as rolling back the test case? I think we can argue to make the set of coercions explicit in draft 3 (and not include `X => Array[X]`), but IMO we shouldn't ""unsupport"" that Cromwell feature with this PR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-342278838:1840,rolling,rolling,1840,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-342278838,1,['rolling'],['rolling']
Deployability,"he script is generated by AwsBatchJob.scala; > [â€¦](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isnâ€™t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark â€¦ <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:1134,pipeline,pipeline,1134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383,1,['pipeline'],['pipeline']
Deployability,"her backend providers include SGE, SLURM, Docker, udocker, Singularity. etc.; # Don't forget you will need to customize them for your particular use case.; backend {; # Override the default backend.; default = slurm. # The list of providers.; providers {; # Copy paste the contents of a backend provider in this section; # Examples in cromwell.example.backends include:; # LocalExample: What you should use if you want to define a new backend provider; # AWS: Amazon Web Services; # BCS: Alibaba Cloud Batch Compute; # TES: protocol defined by GA4GH; # TESK: the same, with kubernetes support; # Google Pipelines, v2 (PAPIv2); # Docker; # Singularity: a container safe for HPC; # Singularity+Slurm: and an example on Slurm; # udocker: another rootless container solution; # udocker+slurm: also exemplified on slurm; # HtCondor: workload manager at UW-Madison; # LSF: the Platform Load Sharing Facility backend; # SGE: Sun Grid Engine; # SLURM: workload manager. # Note that these other backend examples will need tweaking and configuration.; # Please open an issue https://www.github.com/broadinstitute/cromwell if you have any questions; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions"". concurrent-job-limit = 10; # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; ## Warning: If set, Cromwell will run 'check-alive' for every job at this interval; exit-code-timeout-seconds = 360; filesystems {; local {; localization: [; # soft link does not work for docke",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:6028,configurat,configuration,6028,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['configurat'],['configuration']
Deployability,"her-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001:1232,pipeline,pipelines,1232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001,1,['pipeline'],['pipelines']
Deployability,"here has been a change in the URL format - for example, on that same [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page, ; > Call Caching can be enabled in your [Cromwell Configuration](https://cromwell.readthedocs.io/en/stable/cromwell_features/Configuring#call-caching). links to `/en/stable/cromwell_features/Configuring#call-caching`; when the current link path is `/en/stable/Configuring/#call-caching`. This is also true for the MySQL link on the page.; 3. As a user, I would expect links within documentation for a stable release to link to only ""stable"" assets. However, the Runtime Attributes link on the [Call Caching](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/) page links to the develop version of the documentation instead. Similarly, the 'Example Providers Folder' section of the [Configuration](https://cromwell.readthedocs.io/en/stable/Configuring/) page links to the `develop` branch:; > You can find a description of options and example stanzas in the [Cromwell Example Configuration](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf), along with backend provider examples in the [Example Providers Folder](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.example.backends).; 4. After seeing that a big perf bottleneck was Cromwell hashing files, I enabled all of the call caching options and also enabled `check-sibling-md5` so that it could use pre-computed hashes instead. To my surprise, this did nothing because it only works when the configured `actor-factory` is `cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory`! The documentation does **not** make that sufficiently clear:; - The `hashing-strategy` and `check-sibling-md5` options are listed under the `Local Filesystem` section (and configured under `config.filesystems.local.caching` which led me to believe they would work with any provider configured for local filesystem ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4810:1737,Configurat,Configuration,1737,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4810,1,['Configurat'],['Configuration']
Deployability,"hers.backend-dispatcher-362 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(dba9b85f)PreProcessingForVariantDiscovery_GATK4.SamToFastqAndBwaMem:6:1]: Status change from Running to Success; 2019-01-18 18:43:33,255 cromwell-system-akka.dispatchers.engine-dispatcher-5 ERROR - WorkflowManagerActor Workflow dba9b85f-e9ea-4e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.i",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:1714,pipeline,pipelines,1714,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['pipeline'],['pipelines']
Deployability,"hers.engine-dispatcher-9 INFO - WorkflowManagerActor: Workflow 075e0cf3-194b-4f53-a43d-d31f0b370f79 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Execution failed: generic::permission_denied: pulling image: docker pull: running [""docker"" ""pull"" ""gcr.io/broad-cumulus/cellranger@sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356""]: exit status 1 (standard error: ""Error response from daemon: pull access denied for gcr.io/broad-cumulus/cellranger, repository does not exist or may require 'docker login': denied: Permission denied for \""sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"" from request \""/v2/broad-cumulus/cellranger/manifests/sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"".\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:91); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:812); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:95); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1340); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(S",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:13579,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,13579,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,hey @dspeck1 or @jgainerdewar just wondering if there is a plan for a patch release to fix the current cromwell release?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885019150:70,patch,patch,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1885019150,3,"['patch', 'release']","['patch', 'release']"
Deployability,"hey @kshakir ! I'd be happy to make that change for you (the first one). For the second one, I've linked the example in the folder to the online docs, and I'd suggest that the update of the docs themselves be a separate PR. I've noticed with Cromwell (and other software, generally) that it's much cleaner / better to have smaller PRs that have scoped changes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4697#issuecomment-470279447:176,update,update,176,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4697#issuecomment-470279447,1,['update'],['update']
Deployability,"hey @kshakir ! I'm definitely glad to help. I'm not sure a remote session would be helpful to understand yaml - it's a data structure like json, and just a way to capture a workflow. I'd be happy to answer specific questions that you might have, feel free to post them on here, and no worries about the busy-ness! I hope the :fire: calms down, at least it has been sort of not so great for CA! If it helps, I'll leave you some notes here:. - adding this circle testing doesn't interfere with your current testing; - adding the build and deploy of the docker container here is a better strategy than having travis handle everything because the two can run at the same time.; - environment variables, given docker credentials, are set on the circleci project backend (once and forgotten about). This is mostly just DOCKER for pushing to docker hub.; - The yaml uses [anchors](https://discuss.circleci.com/t/using-defaults-syntax-in-config-yaml-aka-yaml-anchors/16168) in the configuration like functions, and to pipe in defaults. I name them according to what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The ma",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:537,deploy,deploy,537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,2,"['configurat', 'deploy']","['configuration', 'deploy']"
Deployability,"hey cromwell! I know the other PR is still being reviewed (and you are busy, no worries!) but I just wanted to put this example here of (one idea) for running a singularity container, as an instance or binary, just via a configuration. I don't think any special changes are needed here to the backend of cromwell, it's really just executing commands to a container with a particular set of inputs (container name, etc.) Likely we would want to add a general variable to plug in any user specific arguments (e.g., custom binds) and singularity arguments? (e.g., debug?) I don't know if this is ""how it's supposed to look"" but here is what a test run spits out for me:. ```bash; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; [2018-08-27 02:03:57,30] [info] Running with database db.url = jdbc:hsqldb:mem:3cd4a928-7743-4252-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""wri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:221,configurat,configuration,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,1,['configurat'],['configuration']
Deployability,hey friends! Just wanted to poke here again that this is still badly wanted / needed / desired / dreamed of / prayed for / sacrificial lambs... (you get the idea :P _) Any updates? Can I help?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411567369:172,update,updates,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-411567369,1,['update'],['updates']
Deployability,"hi all,; I'm trying to run cromwell 84 will the following config, with, as far as I understand, a local HTSQLDB cache :. ```; include required(classpath(""application"")); # the file below was fixed in https://github.com/broadinstitute/cromwell/issues/7007; include ""SGE.conf"". call-caching {; 	enabled = true; invalidate-bad-cache-results = false; }. database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=10000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 120000; numThreads = 1; }; }; ```. but when I execute the workflow . `java -Dconfig.file=${PWD}/app.conf -jar ${CROMWELL_JAR} run test.wdl --inputs input.json`. the configuration takes a long time with messages about `Checkpoint...` that takes about 10 minutes. . ```; (...); [2023-02-08 16:24:28,90] [info] dataFileCache commit start; [2023-02-08 16:24:28,91] [info] dataFileCache commit end; [2023-02-08 16:24:28,94] [info] checkpointClose end; [2023-02-08 16:24:28,96] [info] Checkpoint end - txts: 3051; [2023-02-08 16:24:29,05] [info] Checkpoint start; [2023-02-08 16:24:29,05] [info] checkpointClose start; [2023-02-08 16:24:29,07] [info] checkpointClose synched; [2023-02-08 16:24:29,08] [info] checkpointClose script done; [2023-02-08 16:24:29,08] [info] dataFileCache commit start; [2023-02-08 16:24:29,20] [info] dataFileCache commit end; [2023-02-08 16:24:29,53] [info] checkpointClose end; [2023-02-08 16:24:29,53] [info] Checkpoint end - txts: 3058; [2023-02-08 16:24:29,53] [info] Checkpoint start; [2023-02-08 16:24:29,53] [info] checkpointClose start; [2023-02-08 16:24:30,52] [info] checkpointClose synched; [2023-02-08 16:24:30,52] [info] checkpointClose script done; [2023-02-08 16:24:30,52] [info] dataFileCache commit start; [2023-02-08 16",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7009:906,configurat,configuration,906,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7009,1,['configurat'],['configuration']
Deployability,"hit.; [2022-10-06 15:14:54,70] [info] 12ceda02-4906-4840-80a2-514af3ccb801-EngineJobExecutionActor-test.task_A:NA:1 [ESC[38;5;2m12ceda02ESC[0m]: Could not copy a suitable cache hit for 12ceda02:test.task_A:-1:1. No copy attempts were made. Based on [StackOverflow, the issue seems to be simply that subqueries must be aliased.](https://stackoverflow.com/q/1888779/4107809) Is MariaDB not supported? . The workflow runs jobs that complete as normal. When rerunning, no call caching results are used, and all jobs simply run again. . Cromwell connects to the call caching database and successfully creates tables, for example `CALL_CACHING_AGGREGATION_ENTRY`. . <!-- Which backend are you running? -->; I am running with a SLURM backend. . <!-- Paste/Attach your workflow if possible: -->; I have a very simple example workflow. ; ```; workflow test{; call task_A {}; }. task task_A{; command{; echo 'testing'; }; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```; include required(classpath(""application"")). webservice {; }. akka {; http {; server {; }; }; }. system {; io {; }; input-read-limits {; }; job-rate-control {; jobs = 2; per = 1 second; }. abort {; scan-frequency: 30 seconds; cache {; enabled: true; concurrency: 1; ttl: 20 minutes; size: 100000; }; }. dns-cache-ttl: 3 minutes; }. workflow-options {; default {; }; }. call-caching {; enabled = true; }. google {; }. docker {; hash-lookup {; }; }. engine {; filesystems {; local {; }; }; }. languages {; WDL {; versions {; ""draft-2"" {; }; ""1.0"" {; }; }; }; CWL {; versions {; ""v1.0"" {; }; }; }; }. backend {; default = ""SLURM"". providers {. SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; runtime-attributes = """"""; Int runtime_minutes = 720; Int cpus = 1; Int requested_memory_mb_per_core = 8000; String queue = ""short""; """""". exit-code-timeout-seconds = 600. submit = """"""; sbat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6929:2246,configurat,configuration,2246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6929,1,['configurat'],['configuration']
Deployability,hotfix is in the past... 0.20+ is the future ;),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/967#issuecomment-230882303:0,hotfix,hotfix,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/967#issuecomment-230882303,1,['hotfix'],['hotfix']
Deployability,http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.copyObject(DefaultS3Client.java:466); 	at org.lerch.s3fs.S3FileSystem,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:5766,pipeline,pipeline,5766,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,1,['pipeline'],['pipeline']
Deployability,http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.listBuckets(DefaultS3Client.java:1749); 	at software.amazon.awssdk.se,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686:3268,pipeline,pipeline,3268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686,2,['pipeline'],['pipeline']
Deployability,"http://cromwell.readthedocs.io/en/develop/backends/Google/. The section on ""Application Default Credentials"" doesn't mention that google deprecated setting app default creds by default. Search google, get new commands that users must use to set the credentials, and update docs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3251:266,update,update,266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3251,1,['update'],['update']
Deployability,https://blog.travis-ci.com/2017-06-19-trusty-updates-2017-Q2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2366:45,update,updates-,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2366,1,['update'],['updates-']
Deployability,"https://broadinstitute.atlassian.net/wiki/display/DSDEEPB/Cromwell+Release. One week prior to release cut develop to a release candidate branch (e.g. release-0.18); On develop branch, update build.sbt with the next version number. Unless we want to wait until the actual release day to switch ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/405#issuecomment-175210044:67,Release,Release,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/405#issuecomment-175210044,6,"['Release', 'release', 'update']","['Release', 'release', 'release-', 'update']"
Deployability,"https://broadworkbench.atlassian.net/browse/BA-2919; All future updates to this issue will be posted in JIRA. Sorry for the inconvenience, but you will need to create a free account to access it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2919#issuecomment-506468209:64,update,updates,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2919#issuecomment-506468209,1,['update'],['updates']
Deployability,https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=desc) into [44_hotfix](https://codecov.io/gh/broadinstitute/cromwell/commit/8055dad79afe29bbfb6b4b558f997a03d38dabd4?src=pr&el=desc) will **increase** coverage by `16.84%`.; > The diff coverage is `28.57%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## 44_hotfix #5076 +/- ##; =============================================; + Coverage 62.75% 79.6% +16.84% ; =============================================; Files 1031 1031 ; Lines 26424 26433 +9 ; Branches 869 819 -50 ; =============================================; + Hits 16582 21041 +4459 ; + Misses 9842 5392 -4450; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=tree) | Coverage Î” | |; |---|---|---|; | [...lines/v2alpha1/PipelinesParameterConversions.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvUGlwZWxpbmVzUGFyYW1ldGVyQ29udmVyc2lvbnMuc2NhbGE=) | `82.85% <100%> (+54.91%)` | :arrow_up: |; | [.../main/scala/cromwell/filesystems/drs/DrsPath.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZmlsZXN5c3RlbXMvZHJzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2ZpbGVzeXN0ZW1zL2Rycy9EcnNQYXRoLnNjYWxh) | `100% <100%> (Ã¸)` | :arrow_up: |; | [...cala/cromwell/filesystems/drs/DrsPathBuilder.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZmlsZXN5c3RlbXMvZHJzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2ZpbGVzeXN0ZW1zL2Rycy9EcnNQYXRoQnVpbGRlci5zY2FsYQ==) | `92.85% <100%> (+7.14%)` | :arrow_up: |; | [...omwell/filesystems/drs/DrsPathBuilderFactory.scala](https://codecov.io/gh/broadinstitute/cromwell,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251:1101,Pipeline,PipelinesParameterConversions,1101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251,1,['Pipeline'],['PipelinesParameterConversions']
Deployability,https://cromwell.readthedocs.io/en/develop/filesystems/HTTP is linked to in both the [release notes for 35](https://github.com/broadinstitute/cromwell/releases/tag/35) and the [Filesystems overview page](https://cromwell.readthedocs.io/en/stable/filesystems/Filesystems/). It leads to a maze page.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4221:86,release,release,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4221,2,['release'],"['release', 'releases']"
Deployability,"https://cromwell.readthedocs.io/en/stable/GettingHelp/ suggests that users get support at a forum that doesn't exist anymore. Normally when I find a dead link in docs, I just replace it in a PR, but I'm really not sure if there's anything to replace it with. There is a new GATK forum, but from what I've seen it doesn't really take questions about non-GATK WDLs even in the Community/Other section. There is a Cromwell Slack, but Slack is not available in all countries, isn't indexed, and the workspace is on a free plan (some old messages are already unavailable), so it's not a good option for actual support. The same goes for the OpenWDL Slack - not available in all places, not indexed by search engines, continuously overwriting itself due to being on a free plan. Terra Support is only focused on Terra-specific usage of Cromwell, even though it's common to test WDLs locally before running them in Terra.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6801:712,continuous,continuously,712,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6801,1,['continuous'],['continuously']
Deployability,"https://github.com/broadinstitute/cromwell/blob/215cca97e6efafa7406fd89b38e21a39ce2d0d4e/cromwell.examples.conf#L440. It seems like the default configuration sends the stdout and stderr to the same filenames twice. In the config above, qsub is called with -o EG:; ```e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout```. In the script that will be executed on the node, there is some shenanigans:; ```; (; cd .../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution. bla bla bla my command invocation; ) > >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stdout') 2> >(tee '.../e1bd0801-52c1-445c-8513-72e9872a0c7d/call-thing/execution/stderr' >&2); ```. Those 'tees' are generated by cromwell somewhere, I do not know if the config has control of that. I don't know the details of what will happen, but I do not think it will be healthy. I smell race conditions. I came across this trying to debug missing log data when SGE aborts a job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3705:144,configurat,configuration,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3705,1,['configurat'],['configuration']
Deployability,"https://github.com/broadinstitute/cromwell/blob/559c191b8722c8a23e3220a8a505f171114f547d/scripts/docker-develop/Dockerfile#L28C2-L28C2; The way Java is installed in Docker needs to be updated, I suggest using openjdk instead; e.g.; apt-get install -y git wget openjdk-11-jdk. to replace lines 28-33. This also applies to the following files; [src/ci/docker-compose/cromwell-test/docker-setup.sh](https://github.com/broadinstitute/cromwell/blob/c9d4ce4287a5da9bff097244b664135171021f7e/src/ci/docker-compose/cromwell-test/docker-setup.sh#L28); [src/ci/bin/test.inc.sh](https://github.com/broadinstitute/cromwell/blob/c9d4ce4287a5da9bff097244b664135171021f7e/src/ci/bin/test.inc.sh#L834). And Building docs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7243:152,install,installed,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7243,3,"['install', 'update']","['install', 'installed', 'updated']"
Deployability,https://github.com/scala/scala/releases/tag/v2.12.3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2502:31,release,releases,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2502,1,['release'],['releases']
Deployability,"hub.com/jbwheatley/pact4s). from `0.9.0` to `0.10.1-java8`. ðŸ“œ [GitHub Release Notes](https://github.com/jbwheatley/pact4s/releases/tag/v0.10.1-java8) - [Version Diff](https://github.com/jbwheatley/pact4s/compare/v0.9.0...v0.10.1-java8). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.9.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Pair/cnv_somatic_pair_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Panel/cnv_somatic_panel_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.aws.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.inputs; centaur/src/main/resources/integrationTestCases/germline/haplotype-caller-workflow/HaplotypeCallerWF.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-production-workflow/PairedEndSingleSampleWf.options.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; centaur/src/main/resources/integrationTestCases/green/arrays/arrays.wdl; womtool/src/test/resources/validate/wdl_draft3/valid/HaplotypeCallerWF/HaplotypeCallerWF.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/cnv_somatic_pair_workflow/cnv_somatic_pair_workflow.inputs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7294:1140,integrat,integrationTestCases,1140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7294,1,['integrat'],['integrationTestCases']
Deployability,"i cannot use function as_map to convert Array[Pair[String, File]] to Map[String, File] when i using cromwell or womtoolï¼Œthe executing software edition of i used is number 62 whether cromwell or womtool ï¼ŒThe screenshot is as followsï¼š; ![image](https://user-images.githubusercontent.com/77706356/118239739-bcdf2200-b4cc-11eb-94cb-e5fdce8145da.png). <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6354:1184,configurat,configuration,1184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6354,1,['configurat'],['configuration']
Deployability,"i.e. something like:. ```; auths {; googleauth1: {; class: ""path.to.googleauth""; otherstuff: ""blah""; }; }. filesystems {; fs1 {; class: ""path.to.gcs.filesystem""; auth: googleauth1; }; }. backends {; googlepipelineservice {; class: ""path.to.pipelineservicejar""; gcsfilesystem: fs1; auth: googleauth1; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203488655:240,pipeline,pipelineservicejar,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/627#issuecomment-203488655,1,['pipeline'],['pipelineservicejar']
Deployability,"ia engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.; ```; Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so? Notice that the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/) does not say that requester pays does not work with Pipelines API version 1, it says instead `more information for Requester Pays can be found at: [Requester Pays](https://cloud.google.com/storage/docs/requester-pays)`. In any case, I have removed the Requester Pays option from the bucket, as I pretty much given up on that. I was then able to run the `hello.wdl` workflow fine using the configuration file above. I tried to run the `mutect2.wdl` workflow and then I have encountered a new issue when trying to localize a file in a bucket for which I have permissions to read without problems using my Google account. The error contained the following:; ```; command failed: AccessDeniedException: 403 xxx@xxx.gserviceaccount.com does not have storage.objects.list access to the Google Cloud Storage bucket.; ```; I have tried to fix that as follows:; ```; $",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:3968,Pipeline,Pipelines,3968,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,1,['Pipeline'],['Pipelines']
Deployability,ialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1854,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1854,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"ibration_report_filename} \; -knownSites ${dbSNP_vcf} \; -knownSites ${sep="" -knownSites "" known_indels_sites_VCFs} \; -L ${sep="" -L "" sequence_group_interval}; }; runtime {; docker: ""us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.3.2-1510681135""; memory: ""6 GB""; disks: ""local-disk "" + disk_size + "" HDD""; preemptible: preemptible_tries; }; output {; File recalibration_report = ""${recalibration_report_filename}""; }; }; ```. And here is my cromwell server config:. ```scala; include required(classpath(""application"")). webservice {; port = 8000; }. system {; workflow-restart = true; }. engine {; filesystems {. gcs {; auth = ""service-account""; }. http {}. local {; localization: [; ""hard-link"", ""soft-link"", ""copy""; ]; }; }; }. backend {; default = ""Local""; providers {. Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; max-concurrent-workflows = 1; concurrent-job-limit = 1; }; }. PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; project = ""bioinfo-XXXXXXX""; root = ""gs://XXXXXXXX""; genomics-api-queries-per-100-seconds = 1000; max-concurrent-workflows = 80; concurrent-job-limit = 200; maximum-polling-interval = 600. genomics {; # Config from google stanza; auth = ""service-account"". ; # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; localization-attempts = 3; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; }; }; }; }; }; }. # Google authentication; google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""XXXXXXXXXXXXXX@XXXXXXXXXXXX.gserviceaccount.com""; json-file = ""/var/secrets/google/key.json""; }; ]; }. # database connection; database {; profil",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4336:2151,pipeline,pipelines,2151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4336,1,['pipeline'],['pipelines']
Deployability,"ich I am able to run on AWS backend, and see the outputs in s3. However, submitting this job to my cromwell server:; `curl -X POST ""http://172.20.1.67:8001/api/workflows/v1"" -H ""accept: application/json"" -F ""workflowSource=@three_task_sequence.wdl"" -F ""workflowOptions=@workflow_options.json""; `; Where workflow_options.json content is:; ```; {; ""final_workflow_outputs_dir"": ""s3://nrglab-cromwell-genomics/cromwell-execution/out_bin_test""; }. ```. I'm getting the following error at the end of the workflow cromwell log:. ````; 2019-02-28 08:30:32,167 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686:1106,pipeline,pipeline,1106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686,1,['pipeline'],['pipeline']
Deployability,"ich backend are you running? -->; Backend: GCP Batch; <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0. task hello {. input {; String name; }; command <<<; echo 'hello ~{name}!'; >>>. output {; File response = stdout(); }. runtime {; docker: ""ubuntu:latest""; cpu: 1; memory: ""3.75 GB""; }; }; workflow test {; call hello. output {; File response = hello.response; }; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```hoco; backend {; default = ""batch""; providers {; batch {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {. # The Project To execute in; project = ""${compute_project}"". # The bucket where outputs will be written to; root = ""gs://${bucket}"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; auth = ""cromwell-service-account""; location: ""${region}""; compute-service-account = ""${compute_service_account}"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238:1780,configurat,configuration,1780,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238,1,['configurat'],['configuration']
Deployability,"ick/WorkflowStoreSlickDatabase.scala#L65) and [writing heartbeats](https://github.com/broadinstitute/cromwell/blob/develop/database/sql/src/main/scala/cromwell/database/slick/WorkflowStoreSlickDatabase.scala#L75) by removing transaction semantics from the heartbeat write query. This way, the second query no longer locks multiple rows at once. I am using Slick's [`withPinnedSession`](http://slick.lightbend.com/doc/3.2.0/dbio.html#transactions-and-pinned-sessions) to preserve the efficiency gain of having all the queries in a single session. The MySQL query log shows that `transactionally` and `withPinnedSession` both cause queries to execute in a single session, as evidenced by the setting of session variable `autocommit`:. - `database.run(action.transactionally)`:; ```; Query SET autocommit=0; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c8482924-ef9e-4b3f-930c-ab5f023eeb78'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'e79a1ee7-dd21-4a55-b52d-03f50031b75e'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4022:1172,update,update,1172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022,1,['update'],['update']
Deployability,idatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:2612,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2612,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,ient.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at cromwell.engine.backend.jes.Pipeline$.createPipeline$1(Pipeline.scala:43); at cromwell.engine.backend.jes.Pipeline$.apply(Pipeline.scala:59); at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$attemptToCreateJesRun$1(JesBackend.scala:563); at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1.apply(JesBackend.scala:573); at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1.apply(JesBackend.scala:573); at cromwell.util.TryUtil$$anonfun$5.apply(TryUtil.scala:79); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryUtil$.retryBlock(TryUtil.scala:79); at cromwell.engine.backend.jes.JesBackend$.withRetry(JesBackend.scala:123); at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$createJesRun(JesBackend.scala:573); at cromwell.engine.backend.jes.JesBackend$$anonfun$35.apply(JesBackend.scala:707); at cromwell.engine.backend.jes.JesBackend$$anonfun$35.app,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757:3736,Pipeline,Pipeline,3736,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757,1,['Pipeline'],['Pipeline']
Deployability,"if #4865 has already merged before this one, update that table",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-484928363:45,update,update,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4864#issuecomment-484928363,1,['update'],['update']
Deployability,if it's as intended you should update the swagger then :),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4139#issuecomment-424070734:31,update,update,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4139#issuecomment-424070734,1,['update'],['update']
Deployability,if you had to update the Cromwell server repo template for the `CROMWELL_BUILD_CENTAUR_256_BITS_KEY` variable could you please include those changes?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6486#issuecomment-914482739:14,update,update,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6486#issuecomment-914482739,1,['update'],['update']
Deployability,"ike a transient condition that could be overcome by retries:. 2019-05-24 12:32:07,173 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(Bat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001:1082,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1082,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"iled. The job was stopped before the command finished. PAPI error code 5. 8: Failed to pull image broadinstitut; e/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71: ""docker pull broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71"" failed: exit status 1: sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71: Pulling from broadinstitute/gatk; cromwell_1 | ae79f2514705: Pulling fs layer; cromwell_1 | 5ad56d5fc149: Pulling fs layer; cromwell_1 | 170e558760e8: Pulling fs layer; cromwell_1 | 395460e233f5: Pulling fs layer; cromwell_1 | 6f01dc62e444: Pulling fs layer; cromwell_1 | 98db058f41f6: Pulling fs layer; [...]; cromwell_1 | failed to register layer: Error processing tar file(exit status 1): write /root/.cache/pip/http/5/1/d/8/2/51d82969228464b761a16257d5eefe8e2b3dde3c1ad733721353e785: no space left on device; cromwell_1 |; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Futur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337:1815,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1815,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"ilesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L81) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L82) |; | Region not set by props | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L59) | [elerch-sdk2/AmazonS3Factory.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/AmazonS3Factory.java#L57) |; | (Attempted) props read from env | [cromwell-37/AmazonS3Factory.java](https://github.com/broadinstitute/cromwell/blob/37/filesystems/s3/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L95-L96) | [elerch-sdk2/S3FileSystemProvider.java](https://github.com/elerch/Amazon-S3-FileSystem-NIO2/blob/sdk2/src/main/java/org/lerch/s3fs/S3FileSystemProvider.java#L94-L95) |. ### Acceptance Criteria. The existing four centaur tests (two run in Travis, two nightly in Jenkins) should pass using non-default credentials:; - Switch the `aws_application.conf` to use auth scheme `custom_keys` in a rendered template with secrets embedded; - Remove the files `aws_credentials.ctmpl` and `aws_config`; - Update `testCentaurAws.sh` removing `AWS_SHARED_CREDENTIALS_FILE` and `AWS_CONFIG_FILE` exports; - **Maybe**: It isn't clear that installing the `awscli` should be required. Remove this from the `testCentaurAws.sh` and see if the tests still work using just the embedded Java S3 SDK.; - Update `.gitignore` replacing `aws_credentials` with `aws_application.conf`; - **Maybe**: Add a second backend that **does** use some form of default authorization; - If adding this test, please ensure that the non-default credentials are NOT accidentally falling-back to the default credentials. ### TL;DR. **It's unclear how cromwell calls to S3 broke in 37+, but CI testing with non-default credentials should hopefully reproduce reported errors.**",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4740:3200,Update,Update,3200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4740,3,"['Update', 'install']","['Update', 'installing']"
Deployability,"ill; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 10000. # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. # Number of workers to assign to PAPI requests; request-workers = 3. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""service-account""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # pipeline-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""service-account"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""europe-west4"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:12198,pipeline,pipeline,12198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,2,['pipeline'],"['pipeline', 'pipeline-timeout']"
Deployability,"in PAPI.; slow-job-warning-time: 24 hours. # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 10000. # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. # Number of workers to assign to PAPI requests; request-workers = 3. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""service-account""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # pipeline-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""service-account"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` location",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:11922,configurat,configuration,11922,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['configurat'],['configuration']
Deployability,"in the workbench chat room they were tracking something where rawls was spamming cromwell with abort requests. The root cause was that a workbench submission was requested to abort which causes rawls to continuously send abort requests to the submission's workflows until it is aborted, which sometimes never happens. they're not fans of this behavior. Tagging @helgridly and @jmthibault79 as people who can provide more details.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1976:203,continuous,continuously,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1976,1,['continuous'],['continuously']
Deployability,"inTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2020-08-21 11:08:59,66] [info] WorkflowManagerActor WorkflowActor-dbd5cdc0-c79a-42cd-b929-56ddb1115467 is in a terminal state: WorkflowFailedState; [2020-08-21 11:09:00,66] [info] Not triggering log of token queue status. Effective log interval = None; ```. Here is the relevant part of the wdl:. ```; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }; filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""didnt want to put this ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793:2862,Pipeline,PipelinesApiLifecycleActorFactory,2862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793,1,['Pipeline'],['PipelinesApiLifecycleActorFactory']
Deployability,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:5864,configurat,configuration,5864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['configurat'],['configuration']
Deployability,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:6039,configurat,configuration,6039,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['configurat'],['configuration']
Deployability,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:6214,configurat,configuration,6214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['configurat'],['configuration']
Deployability,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; 2019-01-31 18:43:05,122 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table EXECUTION created; 2019-01-31 18:43:05,285 INFO - chang",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:6389,configurat,configuration,6389,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['configurat'],['configuration']
Deployability,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; 2019-01-31 18:43:05,122 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table EXECUTION created; 2019-01-31 18:43:05,285 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table JES_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:6564,configurat,configuration,6564,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['configurat'],['configuration']
Deployability,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; 2019-01-31 18:43:05,122 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table EXECUTION created; 2019-01-31 18:43:05,285 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table JES_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table LOCAL_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfraz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:6739,configurat,configuration,6739,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['configurat'],['configuration']
Deployability,"increment/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; 2019-01-31 18:43:05,122 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table EXECUTION created; 2019-01-31 18:43:05,285 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table JES_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table LOCAL_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: ChangeSet changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer ran successfully in 538ms; 2019-01-31 18:43:05,650 INFO - changelog.xml: changesets/db_schema.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:6914,configurat,configuration,6914,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['configurat'],['configuration']
Deployability,"indings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/bl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3657:1862,configurat,configurations,1862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657,1,['configurat'],['configurations']
Deployability,"ing ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunct",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2057,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2057,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,ing [#4947](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=desc) into [develop](https://codecov.io/gh/broadinstitute/cromwell/commit/26085f5833cee3c9091d391102d5d0885a2b7d71?src=pr&el=desc) will **increase** coverage by `2.22%`.; > The diff coverage is `84.61%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #4947 +/- ##; ===========================================; + Coverage 62.61% 64.84% +2.22% ; ===========================================; Files 1015 1015 ; Lines 25904 25914 +10 ; Branches 811 829 +18 ; ===========================================; + Hits 16221 16804 +583 ; + Misses 9683 9110 -573; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=tree) | Coverage Î” | |; |---|---|---|; | [.../google/pipelines/v2alpha1/api/ActionBuilder.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvYXBpL0FjdGlvbkJ1aWxkZXIuc2NhbGE=) | `82.4% <84.61%> (+20.5%)` | :arrow_up: |; | [.../scala/cromiam/webservice/EngineRouteSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-Q3JvbUlBTS9zcmMvbWFpbi9zY2FsYS9jcm9taWFtL3dlYnNlcnZpY2UvRW5naW5lUm91dGVTdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...expression/renaming/BinaryOperatorEvaluators.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvbmV3LWJhc2Uvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmFzZS93ZGxvbTJ3b20vZXhwcmVzc2lvbi9yZW5hbWluZy9CaW5hcnlPcGVyYXRvckV2YWx1YXRvcnMuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...l/services/womtool/models/WomTypeJsonSupport.scala](https://code,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620:1082,pipeline,pipelines,1082,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620,1,['pipeline'],['pipelines']
Deployability,"ing a success. ```; 2023-04-18 21:59:54,599 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:108:1]: Status change from Running to Success; 2023-04-18 22:00:09,060 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:107:1]: Status change from Running to Success; 2023-04-18 22:00:18,464 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:106:1]: Status change from Running to Success; 2023-04-18 22:01:20,604 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:111:1]: Status change from Running to Success; 2023-04-18 22:14:47,728 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: Aborting workflow; 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:262:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/9178938377659283430); 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:112:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/8559201934542591362); 2023-04-18 22:14:48,295 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Successfully requested cancellation of projects/16371921765/locations/us-central1/operations/9178938377659283430; 2023-04-18 22:15:56,564 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: Status change from Running to Success; 2023-04-18 22:16:44,505 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Status change from Running to Cancelled; 2023-04-18 22:16:44,539 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: WorkflowExecutionActor [UUID(10fa31a8)] aborted: myco.pull:262:1; 2023-04-18 22:16:45,1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:5069,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,5069,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,ings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.execu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:1579,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1579,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"instance that gives PAPI Error Code 10 was able to get a virtual machine, though. Maybe there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.jav",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1174,pipeline,pipelines,1174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['pipeline'],['pipelines']
Deployability,"int-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section guides the user into creating a new service account under the current project. This would need to be selected in the configuration file with an authorization with `scheme = ""service_account""` but instead both the configuration file for [PAPIv2](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#setting-up-papiv2) and the configuration file for [PAPIv1](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#lets-get-started) are configured to use an authorization with `scheme = ""application_default""`. I find it very hard to believe that any novel user could go through the tutorial and successfully set up a Cromwell server. On a slightly different note, some of my issues would be resolved if I could run jobs using my user account rather than a service account associated with my project. In the Google [backends](https://cromwell.readthedocs.io/en/stable/backends/Google/) section of the docs there is a lonely mention of the `scheme = ""user_account""` but no further explanation. According to the [source code](https://github.com/broadinstitute/cromwell/blob/develop/cloudSupport/src/test/scala/cromwell/cloudsupport/gcp/GoogleConfigurationSpec.scala) it should be defined as:; ```; {; name = ""user-account""; scheme = ""user_account""; user = ""me""; secrets-file = ""/very/secret/file.txt""; data-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:2791,configurat,configuration,2791,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,1,['configurat'],['configuration']
Deployability,"invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-02-28 08:30:32,176 cromwell-system-akka.dispatchers.engine-dispatcher-28 ERROR - WorkflowManagerActor Workflow bd18e464-59a2-44cf-80c2-b4d93bdfe0ce failed (during FinalizingWorkflowState): software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686:7320,pipeline,pipeline,7320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686,1,['pipeline'],['pipeline']
Deployability,"io.circe:circe-parser](https://github.com/circe/circe); * [io.circe:circe-refined](https://github.com/circe/circe); * [io.circe:circe-shapes](https://github.com/circe/circe). from 0.13.0 to 0.14.1.; [GitHub Release Notes](https://github.com/circe/circe/releases/tag/v0.14.1) - [Version Diff](https://github.com/circe/circe/compare/v0.13.0...v0.14.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/92f43ce5f010fa03daeb2625a3f9c1b1be80cca5/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.13.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/rnaseq-workflow/steps/prepare_sample.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/process_alignment.cwl; centaur/src/main/resource",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6410:1343,update,update,1343,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6410,1,['update'],['update']
Deployability,"ion = ""us-west1"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default"". # Google project which will be billed for the requests; project = ""gred-cumulus-sb-01-991a49c4"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }. http { }; }. default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; # Allowed to be a String, or a li",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:3723,configurat,configuration,3723,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,2,['configurat'],['configuration']
Deployability,"ion is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I am running Cromwell on GCP, launching a workflow that shards into ~5,000 pieces. I am getting the following error: `cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out`. ```; 2019-04-29 00:02:13,419 cromwell-system-akka.dispatchers.backend-dispatcher-139 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(95b34a77)vcf2bigquery.convertVCF:2058:1]: Status chang; e from Running to Success; 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-150 ERROR - Read timed out; java.net.SocketTimeoutException: Read timed out; at java.net.SocketInputStream.socketRead0(Native Method); at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); at java.net.SocketInputStream.read(SocketInputStream.java:171); at java.net.SocketInputStream.read(SocketInputStream.java:141); at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); at sun.security.ssl.InputRecord.read(InputRecord.java:503); at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:975); at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:933); at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:1098,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1098,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"ion is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. Backend: AWS Batch. <!-- Paste/Attach your workflow if possible: -->. [Workflow](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-processing-for-variant-discovery-gatk4.wdl). [Input file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-M40job.inputs.json). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. [Configuration file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/aws.conf). Running this workflow on AWS Batch (with cromwell-36.jar) consistently fails at the same point each time. . It gets through most (looks like all but one iteration) of the scatter loop that calls the `BaseRecalibrator` task. Then cromwell just sits for a long time (~1hr) with no Batch jobs running (or runnable or starting). Then cromwell calls the `RegisterJobDefinition` API of AWS Batch, and it always fails with the following error message:. ```; 2018-12-15 23:39:03,360 cromwell-system-akka.dispatchers.backend-dispatcher-258 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; software.amazon.awssdk.services.batch.model.ClientException: a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496:1102,Configurat,Configuration,1102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496,1,['Configurat'],['Configuration']
Deployability,"ion.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:57); 	at akka.actor.Timers.aroundReceive(Timers.scala:51); 	at akka.actor.Timers.aroundReceive$(Timers.scala:40); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:57); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); 	at akka.actor.ActorCell.invoke(ActorCell.scala:583); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2024-03-12 20:24:51 cromwell-system-akka.actor.default-dispatcher-4 INFO - Message [cromwell.engine.workflow.lifecycle.EngineLifecycleActorAbortCommand$] from Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-a06a4c5e-fbf7-4c1d-ac71-b036aaf48fbc#2096097107] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-a06a4c5e-fbf7-4c1d-ac71-b036aaf48fbc/WorkflowExecutionActor-a06a4c5e-fbf7-4c1d-ac71-b036aaf48fbc#659989485] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-a06a4c5e-fbf7-4c1d-ac71-b036aaf48fbc/WorkflowExecutionActor-a06a4c5e-fbf7-4c1d-ac71-b036aaf48fbc#659989485]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; ```; {; 	""status"": ""Aborting"",; 	""id"": ""a06a4c5e-fbf7-4c1d-ac71-b036aaf48fbc""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7386:9166,configurat,configuration,9166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7386,1,['configurat'],['configuration']
Deployability,"ion.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac616",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:3511,Pipeline,PipelinesApiRequestWorker,3511,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,ion.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at cromwell.engine.backend.jes.Pipeline$.createPipeline$1(Pipeline.scala:43); at cromwell.engine.backend.jes.Pipeline$.apply(Pipeline.scala:59); at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$attemptToCreateJesRun$1(JesBackend.scala:563); at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1.apply(JesBackend.scala:573); at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1.apply(JesBackend.scala:573); at cromwell.util.TryUtil$$anonfun$5.apply(TryUtil.scala:79); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryUtil$.retryBlock(TryUtil.scala:79); at cromwell.engine.backend.jes.JesBackend$.withRetry(JesBackend.scala:123); at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$createJesRun(JesBackend.scala:573); at cromwell.engine.backend.jes.JesBackend$$anonfun$35.apply(JesBackend.scala:707); at cromwell.engine.backend.jes.JesBackend$$anonfun$35.apply(JesBackend.scala:706); at scala.util.Success.flatMap(Try.scala:231); at cro,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757:3814,Pipeline,Pipeline,3814,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757,1,['Pipeline'],['Pipeline']
Deployability,"ion.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryW",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:3454,Pipeline,PipelinesApiRequestWorker,3454,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,ion: Too Many Requests (Service: null; Status Code: 429; Request ID: cfc6e34e-d66c-11e8-be0b-dd778498cf15); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); at software.amazon.awssdk.co,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:1764,pipeline,pipeline,1764,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['pipeline'],['pipeline']
Deployability,"ip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.9.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Pair/cnv_somatic_pair_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Panel/cnv_somatic_panel_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.aws.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.inputs; centaur/src/main/resources/integrationTestCases/germline/haplotype-caller-workflow/HaplotypeCallerWF.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-production-workflow/PairedEndSingleSampleWf.options.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; centaur/src/main/resources/integrationTestCases/green/arrays/arrays.wdl; womtool/src/test/resources/validate/wdl_draft3/valid/HaplotypeCallerWF/HaplotypeCallerWF.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/cnv_somatic_pair_workflow/cnv_somatic_pair_workflow.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/joint-discovery-gatk/joint-discovery-gatk.inputs.json; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.github.jbwheatley"" } ]; ```; Or, add this to slow down future updat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7294:1524,integrat,integrationTestCases,1524,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7294,1,['integrat'],['integrationTestCases']
Deployability,"ipeline and want to share google storage buckets for genome data (bowtie2 index tar ball and other big files) with users (Google authenticated) but want users to pay for the network traffic to download genome data. My pipeline works fine if storage bucket for genome data is set as ""owner pays"". But if I set it ""requester pays"" then I get the following error `BadRequestException: 400 Bucket is requester pays bucket but no user project provided`. I googled it and found that [`gsutil` must use JSON API (not CLI)](https://cloud.google.com/storage/docs/requester-pays) for ""requester pays"" buckets. Is there any plan to support ""requester pays"" buckets for JES backend? . ```; [2017-11-18 16:01:09,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: python $(which encode_bowtie2.py) \; /cromwell_root/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/d57a5f97-8542-4fcc-89c4-b7c487957dea/call-trim_adapter/shard-0/glob-019a547c7b0dda79121d0398158a07d0/ENCFF439VSY.trim.merged.R1.fastq.gz \; \; --multimapping 4 \; \; --nth 4; [2017-11-18 16:01:09,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: python $(which encode_bowtie2.py) \; /cromwell_root/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/d57a5f97-8542-4fcc-89c4-b7c487957dea/call-trim_adapter/shard-1/glob-019a547c7b0dda79121d0398158a07d0/ENCFF463QCX.trim.merged.R1.fastq.gz \; \; --multimapping 4 \; \; --nth 4; [2017-11-18 16:01:18,97] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: job id: operations/EJSf0Yz9Kxjs8__E9aKWivQBILWN-vrbGyoPcHJvZHVjdGlvblF1ZXVl; [2017-11-18 16:01:18,97] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: job id: operations/EJWg0Yz9KxiXpeC4gsSenC4gtY36-tsbKg9wcm9kdWN0aW9uUXVldWU; [2017-11-18 16:01:30,30] [info] Jes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916:960,pipeline,pipeline-workflows,960,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916,1,['pipeline'],['pipeline-workflows']
Deployability,ipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:51); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:34); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:20); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); at akka.actor.ActorCell.invoke(ActorCell.scala:583); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$GoogleJsonException: Request conta,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:4257,Pipeline,PipelinesApiRequestWorker,4257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,2,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,"is pipeline can still work. . Related code reporting error is [here](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L65) and error is produce [here](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L77) and [this is the regex definition](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L21). I am wondering can the [code](https://github.com/broadinstitute/cromwell/blob/dbd8a2aca7253cecd852b5b44ff199e35cdb81cd/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/models/GcpLabel.scala#L77) be updated to only check the key of a label so it aligns with GCP which does not have this restriction on label values? Or use another regex `""[a-z0-9]([-a-z0-9]*[a-z0-9])?""` for label value check?. Cromwell version: I built develop container, and the tag is `87-ee2b10f-SNAP`.; Backend: GCP Batch. Please let me know if this could be something you guys would accept before I put in PR. Thanks. <!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7351:1808,update,updated,1808,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7351,2,"['configurat', 'update']","['configuration', 'updated']"
Deployability,is there a similar ticket for not-hotfix?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/999#issuecomment-225913678:34,hotfix,hotfix,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/999#issuecomment-225913678,1,['hotfix'],['hotfix']
Deployability,"ispatcher-2 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1063,pipeline,pipeline,1063,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,1,['pipeline'],['pipeline']
Deployability,issue has not recurred since update,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-454452174:29,update,update,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164#issuecomment-454452174,1,['update'],['update']
Deployability,"it end; [2023-02-08 16:32:21,82] [info] checkpointClose end; [2023-02-08 16:32:21,82] [info] Checkpoint end - txts: 5348; [2023-02-08 16:32:21,89] [error] Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.LockException: Could not acquire change log lock. Currently locked by fdb0:cafe:d0d0:ceb4:ba59:9fff:fec3:33de%p1p1 (fdb0:cafe:d0d0:ceb4:ba59:9fff:fec3:33de%p1p1) since 2/8/23, 4:23 PM; 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:270); 	at liquibase.Liquibase.lambda$update$1(Liquibase.java:214); 	at liquibase.Scope.lambda$child$0(Scope.java:180); 	at liquibase.Scope.child(Scope.java:189); 	at liquibase.Scope.child(Scope.java:179); 	at liquibase.Scope.child(Scope.java:158); 	at liquibase.Liquibase.runInScope(Liquibase.java:2405); 	at liquibase.Liquibase.update(Liquibase.java:211); 	at liquibase.Liquibase.update(Liquibase.java:197); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:74); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:46); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276); 	at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642); 	at java.base/java.lang.Thread.run(Thread.java:1589); ```. What Am I doing wrong ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7009:4229,update,updateSchema,4229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7009,1,['update'],['updateSchema']
Deployability,"it's not, in the above log):; ```; cromwell_1 | java.lang.Exception: Task germline_variant_calling.fastqc:0:1 failed. The job was stopped before the command finished. PAPI error code 10. 11: Docker run failed: command failed: usage: fastqc_docker.py [-h] -r READ -o OUTPUT_DIR [-c CONTAMINANTS]; cromwell_1 | [-a ADAPTERS] [-l LIMITS] [-f FORMAT] [-n NO_GROUP]; cromwell_1 | [-e EXTRA_OPTIONS]; cromwell_1 | fastqc_docker.py: error: argument -r/--read is required; cromwell_1 | . See logs at gs://genovic-cromwell/cromwell-execution/trio/f5454139-c51d-4d04-ae0a-9b9d4ce650aa/call-germline_variant_calling/shard-0/germline_variant_calling/5d4c4459-a91c-4d3b-8ca4-b98457134750/call-fastqc/shard-0/; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); cromwell_1 | at akka.dispatch.BatchingExec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:1946,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1946,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"itHub Release Notes](https://github.com/googleapis/google-auth-library-java/releases/tag/v1.3.0) - [Version Diff](https://github.com/googleapis/google-auth-library-java/compare/v1.1.0...v1.3.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/00809e6249b134635f71919c17c1c81603beb22d/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.1.0).; You might want to review and update them manually.; ```; CHANGELOG.md; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/variantcall_batch_region.cwl; cloud-nio/cloud-nio-impl-drs/src/main/scala/cloud/nio/impl/drs/DrsCloudNioFileSystemProvider.scala; cwl/src/test/resources/cwl/lodash.js; project/plugins.sbt; ```; </details>; <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.auth"", artifactId = ""google-auth-library-oauth2-http"" } ]; ```; </details>. labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6608:1169,integrat,integrationTestCases,1169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6608,8,"['integrat', 'update']","['integrationTestCases', 'update', 'updates']"
Deployability,"ith batch size 10000 and process rate 2 minutes.; 2018-06-13 14:29:45,255 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Parsing workflow as WDL draft-2; 2018-06-13 14:29:46,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Call-to-Backend assignments: demux_only.illumina_demux -> AWSBATCH; 2018-06-13 14:29:46,085 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - AWSBATCH [UUID(a67833cb)]: Key/s [preemptible, dx_instance_type] is/are not supported by backend. Unsupported attributes will not be part of job executions.; 2018-06-13 14:29:47,088 cromwell-system-akka.dispatchers.backend-dispatcher-91 WARN - Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:2662,update,updateCache,2662,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['update'],['updateCache']
Deployability,"ity score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.fasterxml.jackson.core:jackson-databind&package-manager=maven&previous-version=2.13.4.1&new-version=2.13.4.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). You can trigger a rebase of this PR by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/broadinstitute/cromwell/network/alerts). </details>> **Note**; > Automatic rebases have been disabled on this pull request as it has been open for over 30 days.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7110:1647,upgrade,upgrade,1647,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7110,3,['upgrade'],['upgrade']
Deployability,"java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:3741,Pipeline,PipelinesApiRequestWorker,3741,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,"json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:51); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:34); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:20); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); at akka.actor.ActorCell.invoke(ActorCell.scala:583); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$GoogleJsonException: Request contains an invalid argument.; ... 21 common frames omitted; [2021-08-13 10:45:07,42] [warn] PAPI request worker had 1 failures making 1 requests:; Unable to complete PAPI request due to a problem with the request (Request contains an invalid",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:4504,Pipeline,PipelinesApiRequestWorker,4504,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,"json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:51); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:34); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:20); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); at akka.actor.ActorCell.invoke(ActorCell.scala:583); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$GoogleJsonException: Request contains an invalid argument.; ... 21 more. [2021-08-13 10:45:10,13] [info] WorkflowManagerActor: Workflow actor for a15c46b7-5f93-46d6-94a2-28f656914866 completed with status 'Failed'. The workflow will be removed from the workflow store.; [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:7126,Pipeline,PipelinesApiRequestWorker,7126,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,"k PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContex",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:2122,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"k.basic.DatabaseConfig$$anon$1.db(DatabaseConfig.scala:102); 	at cromwell.database.slick.SlickDatabase.<init>(SlickDatabase.scala:73); 	at cromwell.database.slick.EngineSlickDatabase.<init>(EngineSlickDatabase.scala:15); 	at cromwell.database.slick.EngineSlickDatabase$.fromParentConfig(EngineSlickDatabase.scala:10); 	at cromwell.services.EngineServicesStore$.engineDatabaseInterface$lzycompute(EngineServicesStore.scala:13); 	at cromwell.services.EngineServicesStore$.engineDatabaseInterface(EngineServicesStore.scala:12); 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:27); 	at cromwell.CromwellEntryPoint$$anon$1.<init>(CromwellEntryPoint.scala:122); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:122); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:122); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:65); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1(App.scala:76); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563); 	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:926); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ```; when trying to run Cromwell 83. I see that the Java requirement has been updated to 1.11, but it's still listed as Java 1.8 in this documentation.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6830:4321,update,updated,4321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6830,1,['update'],['updated']
Deployability,"k.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-02-28 08:30:32,176 cromwell-system-akka.dispatchers.engine-dispatcher-28 ERROR - WorkflowManagerActor Workflow bd18e464-59a2-44cf-80c2-b4d93bdfe0ce failed (during FinalizingWorkflowState): software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.e",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686:7549,pipeline,pipeline,7549,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686,1,['pipeline'],['pipeline']
Deployability,"ka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$GoogleJsonException: Request contains an invalid argument.; ... 21 common frames omitted; [2021-08-13 10:45:07,42] [warn] PAPI request worker had 1 failures making 1 requests:; Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; [2021-08-13 10:45:07,54] [info] WorkflowManagerActor: Workflow a15c46b7-5f93-46d6-94a2-28f656914866 failed (during ExecutingWorkflowState): cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:5695,pipeline,pipelines,5695,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['pipeline'],['pipelines']
Deployability,"kdir/created_directory; input_dir: fastqc_execute/output_directory; out: [output_directory]; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: scala.NotImplementedError: This should not happen, please report this; at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:281); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.pollStatus(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$pollStatusAsync$1(StandardAsyncExecutionActor.scala:691); at scala.util.Try$.apply(Try.scala:209); ... 25 more. <!-- SLURM backend configuration -->; include required(classpath(""application"")). backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String queue = ""cpu""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - When a job has not been alive for longer than this timeout; # - And has still not produced an RC file; # - Then it will be marked as Failed.; # Warning: If set, Cromwell has to run 'check-alive' for every job at regular intervals (unrelated to this timeout). # exit-code-timeout-seconds = 120. submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-n "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/usr/bin/env bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4560:1808,configurat,configuration,1808,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4560,1,['configurat'],['configuration']
Deployability,"kendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Initializing to Running; ...; [2019-05-22 19:22:43,83] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:34:19,31] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:42:10,31] [error] WorkflowManagerActor Workflow 3997371c-9513-4386-a579-a72639c6e960 failed (during ExecutingWorkflowState): ; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.nio.file.NoSuchFileException: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; ...;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:10909,Pipeline,Pipeline,10909,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,1,['Pipeline'],['Pipeline']
Deployability,"ker run --rm -v ${cwd}:${docker_cwd} -i ${docker} /bin/bash < ${script}"". # Root directory where Cromwell writes job results. This directory must be; # visible and writeable by the Cromwell process as well as the jobs that Cromwell; # launches.; root: ""cromwell-executions"". filesystems {; local {; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]. caching {; duplication-strategy: [; ""soft-link""; ]. # Possible values: file, path; # ""file"" will compute an md5 hash of the file content.; # ""path"" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to ""soft-link"",; # in order to allow for the original file path to be hashed.; hashing-strategy: ""path"". # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.; # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.; check-sibling-md5: false; }; }; }; }; }; }; }. database {; db.url = ""jdbc:mysql://mysql-db/cromwell_db?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true""; db.user = ""cromwell""; db.password = ""cromwell""; db.driver = ""com.mysql.cj.jdbc.Driver""; profile = ""slick.jdbc.MySQLProfile$""; db.connectionTimeout = 15000; }; ```. and here is my cormwell dockerfile:. ```; FROM broadinstitute/cromwell:develop. RUN git clone https://github.com/vishnubob/wait-for-it.git; RUN mkdir cromwell-working-dir; WORKDIR cromwell-working-dir. COPY ./app-config /app-config. ENTRYPOINT [""/bin/sh"", ""-c""]; ```. when i submit a wdl did not use docker it was ok. but when i submit a wdl need to use docker, a error apear.; ```; /cromwell-working-dir/cromwell-executions/RNAseq/26e3c339-39d3-442f-b93e-8269dc7f9fa6/call-fastp_pe/shard-7/execution/script.submit: line 2: docker: command not found; ```. Is that means I shoud install a docker deamon in cromwell dockerfile or i cloud change some config setting to fix this. please help.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7006:3196,install,install,3196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7006,1,['install'],['install']
Deployability,"keyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/92f43ce5f010fa03daeb2625a3f9c1b1be80cca5/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.28).; You might want to review and update them manually.; ```; centaur/src/main/resources/standardTestCases/local_bourne/local_bourne.wdl; core/src/test/resources/hello_goodbye_scattered_papiv2.json; docs/developers/bitesize/workflowParsing/forkjoin_graph.svg; docs/developers/bitesize/workflowParsing/wdlToWdlom_hermes.svg; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; src/ci/bin/test.inc.sh; src/ci/docker-compose/cromwell-test/docker-setup.sh; supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/request/GetRequestHandler.scala; supportedBackends/google/pipelines/v2alpha1/src/test/scala/cromwell/backend/google/pipelines/v2alpha1/api/request/GetRequestHandlerSpec.scala; supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandler.scala; supportedBackends/goog",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6429:2714,update,update,2714,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6429,1,['update'],['update']
Deployability,keysForNode speedup [41 hotfix],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5114:24,hotfix,hotfix,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5114,1,['hotfix'],['hotfix']
Deployability,"kflow slots exist; max-workflow-launch-count = 50. // Number of seconds between workflow launches; new-workflow-poll-rate = 20. // Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers; number-of-workflow-log-copy-workers = 10; }. workflow-options {; // These workflow options will be encrypted when stored in the database; encrypted-fields: []. // AES-256 key to use to encrypt the values in `encrypted-fields`; base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". // Directory where to write per workflow logs; workflow-log-dir: ""cromwell-workflow-logs"". // When true, per workflow logs will be deleted after copying; workflow-log-temporary: true. // Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; // Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; //workflow-failure-mode: ""ContinueWhilePossible""; }. // Optional call-caching configuration.; call-caching {; enabled = true. // The Docker image specified in the 'runtime' section of a task can be used as-is; // or Cromwell can lookup this Docker image to get a complete hash. For example,; // if a task specifies docker: ""ubuntu:latest"" and if lookup-docker-hash is true,; // Then Cromwell would query DockerHub to resolve ""ubuntu:latest"" to something like; // a2c950138e95bf603d919d0f74bec16a81d5cc1e3c3d574e8d5ed59795824f47; //; // A value of 'true' means that call hashes will more accurately represent the; // Docker image that was used to run the call, but at a cost of having to make a; // request to an external service (DockerHub, GCR). If a call fails to lookup a; // Docker hash, it will fail.; lookup-docker-hash = false; }. google {. application-name = ""cromwell"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""user-via-refresh""; scheme = ""refresh_token""; client-id = ""secret_id""; client-secret = ""secret_secret"";",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480:86055,configurat,configuration,86055,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480,1,['configurat'],['configuration']
Deployability,"kflow will be removed from the workflow store.; [2021-08-13 10:45:13,98] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2021-08-13 10:45:15,05] [info] Workflow polling stopped; [2021-08-13 10:45:15,07] [info] 0 workflows released by cromid-de31b6d; [2021-08-13 10:45:15,07] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; ...; ```. Contents of hello.wdl:; ```; task hello {; String addressee; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. Contents of hello.inputs:; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```; Contents of cromwell.BROADexamples.v4.conf:; ```; # This is a ""default"" Cromwell example that is intended for you you to start with; # and edit for your needs. Specifically, you will be interested to customize; # the configuration based on your preferred backend (see the backends section; # below in the file). For backend-specific examples for you to copy paste here,; # please see the cromwell.backend.examples folder in the repository. The files; # there also include links to online documentation (if it exists). # This line is required. It pulls in default overrides from the embedded cromwell; # `reference.conf` (in core/src/main/resources) needed for proper performance of cromwell.; include required(classpath(""application"")). # Google configuration; google {. application-name = ""cromwell-demo"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""***@***.gserviceaccount.com""; json-file = ""/***/***.json""; }; ]; }. # Here is where you can define the backend providers that Cromwell understands.; # The default is a local provider.; # To add additional backend providers, you should copy paste additional backends; # ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:9072,configurat,configuration,9072,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['configurat'],['configuration']
Deployability,"kka.actor.ActorCell.invoke(ActorCell.scala:496); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2017-12-05 09:40:31,67] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2017-12-05 09:40:31,68] [info] Using noop to send events.; [2017-12-05 09:40:31,70] [info] WorkflowManagerActor WorkflowActor-6a6ee0eb-5576-43af-a64c-8ed7d288bbc5 is in a terminal state: WorkflowFailedState; [2017-12-05 09:40:35,79] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-12-05 09:40:35,81] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2017-12-05 09:40:35,81] [info] Message [cromwell.core.actor.StreamActorHelper$StreamFailed] without sender to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5 transitioned to state Failed; [2017-12-05 09:40:35,85] [info] Automatic shutdown of the async connection; [2017-12-05 09:40:35,85] [info] Gracefully shutdown sentry threads.; [2017-12-05 09:40:35,85] [info] Shutdown finished.; ```; As a work-around, I needed to replace ; ```; if ( b1 && b2 ) {; ```; with; ```; Boolean tmp = b1 && b2; if ( tmp ) {; ````",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992:5859,configurat,configuration,5859,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992,2,['configurat'],['configuration']
Deployability,"l_root\/cwl.output.json 2>\/dev\/null | jq -r '.. | .path? \/\/ .location? \/\/ empty | gsub(\\\""file:\/\/\\\""; \\\""\\\"")' > \/cromwell_root\/0c83f20c\/cwl_output_json_references.txt\"""",; ""endTime"": ""2018-08-14T16:16:57.673071Z""; },; {; ""startTime"": ""2018-08-14T16:17:05.013311Z"",; ""description"": ""Stopped running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" -m rsync -r \/google\/logs gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/pipelines-logs 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" -m rsync -r \/google\/logs gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/pipelines-logs; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"": sh: -q: unknown operand"",; ""endTime"": ""2018-08-14T16:17:06.327716Z""; },; {; ""startTime"": ""2018-08-14T16:14:28.018319Z"",; ""description"": ""Started pulling \""ubuntu@sha256:3f119dc0737f57f704ebecac8a6d8477b0f6ca1ca0332c7ee1395ed2c6a82be7\"""",; ""endTime"": ""2018-08-14T16:14:32.007439Z""; },; {; ""startTime"": ""2018-08-14T16:13:55.499871Z"",; ""description"": ""Started pulling \""google\/cloud-sdk:alpine\"""",; ""endTime"": ""2018-08-14T16:14:03.091743Z""; },; {; ""startTime"": ""2018-08-14T16:14:35.424927Z"",; ""description"": ""Started running \""\/bin\/sh -c mkdir -p $(dirname \/cromwell_root\/cgp-commons-multi-region-public\/topmed_open_access\/711e55a4-c3e9-50af-8a85-41829fb84cae\/NWD455342.recab.cram) && \/scripts\/dosUrlLocalizer.sc dos:\/\/dg.4503\/4d427aa3-5640-4f00-81ae-c33443f84acf \/cromwell_root\/cgp-commons-multi-region-p",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:5328,pipeline,pipelines-logs,5328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['pipeline'],['pipelines-logs']
Deployability,"la:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1259); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:582); 	... 16 common frames omitted; Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column '%failures[%]%:failure' in 'where clause'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(D",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:2805,UPDATE,UPDATE,2805,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['UPDATE'],['UPDATE']
Deployability,"le Backend, which prevents running wdl workflows on many machine types available on GCP, including those provisioned with modern GPUs. I believe the simplest and most general solution would be to pass the machine type directly from the wdl configuration to the Google Batch API. The idea is that this approach would be more resilient to machine types being added or deprecated on GCP, as users would only need to update their wdl workflows in such cases. An alternative approach of mapping machine specs (e.g.: cpu platform and gpu requirements) to standard machine types would potentially introduce an additional layer of maintenance with little benefit. This PR adds support for a new standardMachineType key in the runtime section, which is only parsed for the Google backend. ### Testing. I deployed this internally and verified I can successfully run the following wdl workflow:. ```; version 1.0. task nvidia_smi {; input {; String docker_version; }. command <<<; nvidia-smi. touch .done; echo ""Finished at $(date)""; >>>. runtime {; docker: <internal image>; disks: ""local-disk 50 SSD""; memory: ""32G""; preemptible: 0; gpuCount: 1; gpuType: ""nvidia-tesla-a100""; standardMachineType: ""a2-highgpu-1g""; }. output {; File done = "".done""; }; }. workflow nvidia_smi_wf {; input {; String docker_version; }; ; call nvidia_smi as nvidia_smi_call {; input:; docker_version = docker_version; }. output {; File done = "".done""; }; }; ```. ### Next steps. - [ ] Confirm this approach is in the right direction with the cromwell team.; - [ ] Work on proper unit tests and get this PR ready to be merged. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7545:1759,Release,Release,1759,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7545,5,"['Release', 'release', 'update']","['Release', 'release', 'updated']"
Deployability,"le genotype = genome_inference.vcf_file; }; }. task reads_extraction_and_merging {; input {; String in_container_pangenie; File in_forward_fastq; File in_reverse_fastq; String in_label; Int in_cores; Int in_disk; Int in_mem; }; command <<<; cat ~{in_forward_fastq} ~{in_reverse_fastq} | pigz -dcp ~{in_cores} > ~{in_label}.fastq; >>>; output {; File fastq_file = ""~{in_label}.fastq""; }; runtime {; docker: in_container_pangenie; memory: in_mem + "" GB""; cpu: in_cores; disks: ""local-disk "" + in_disk + "" SSD""; }; }. task genome_inference {; input {; String in_container_pangenie; File in_reference_genome; File in_pangenome_vcf; String in_executable; File in_fastq_file; String prefix_vcf; Int in_cores; Int in_disk; Int in_mem; }; command <<<; echo ""vcf: ~{in_pangenome_vcf}"" > /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""reference: ~{in_reference_genome}"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo $'reads:\n sample: ~{in_fastq_file}' >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""pangenie: ~{in_executable}"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""outdir: /app/pangenie"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; cd /app/pangenie/pipelines/run-from-callset; snakemake --cores ~{in_cores}; >>>; output {; File vcf_file = ""~{prefix_vcf}.vcf""; }; runtime {; docker: in_container_pangenie; memory: in_mem + "" GB""; cpu: in_cores; disks: ""local-disk "" + in_disk + "" SSD""; preemptible: 1 # can be useful for tools which execute sequential steps in a pipeline generating intermediate outputs; }; }; ```; **_Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL:_**; ![Screenshot from 2022-12-09 10-52-16](https://user-images.githubusercontent.com/98895614/206773588-2e8dbf89-03a9-4021-9495-42f2bc0b801d.png). Please help me out on how to set the resources used by Cromwell in local, what file I need to create/modify or how should I cange my code? Thanks in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6966:3302,pipeline,pipelines,3302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966,5,"['configurat', 'pipeline']","['configuration', 'pipeline', 'pipelines']"
Deployability,"leCache commit end; [2023-02-08 16:32:21,82] [info] checkpointClose end; [2023-02-08 16:32:21,82] [info] Checkpoint end - txts: 5348; [2023-02-08 16:32:21,89] [error] Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.LockException: Could not acquire change log lock. Currently locked by fdb0:cafe:d0d0:ceb4:ba59:9fff:fec3:33de%p1p1 (fdb0:cafe:d0d0:ceb4:ba59:9fff:fec3:33de%p1p1) since 2/8/23, 4:23 PM; 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:270); 	at liquibase.Liquibase.lambda$update$1(Liquibase.java:214); 	at liquibase.Scope.lambda$child$0(Scope.java:180); 	at liquibase.Scope.child(Scope.java:189); 	at liquibase.Scope.child(Scope.java:179); 	at liquibase.Scope.child(Scope.java:158); 	at liquibase.Liquibase.runInScope(Liquibase.java:2405); 	at liquibase.Liquibase.update(Liquibase.java:211); 	at liquibase.Liquibase.update(Liquibase.java:197); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:74); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:46); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276); 	at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642); 	at java.base/java.lang.Thread.run(Thread.java:1589); ```. What Am I d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7009:4132,update,updateSchema,4132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7009,1,['update'],['updateSchema']
Deployability,leLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.immutable.List.flatMap(List.scala:355); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$5(OccasionalStatusPollingActor.scala:104); 	at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245); 	at scala.collection.immutable.Set$Set1.foreach(Set.scala:97); 	at scala.collection.TraversableLike.flatMap(TraversableLike.scala:245); 	at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242); 	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.$anonfun$updateStatuses$4(OccasionalStatusPollingActor.scala:103); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.updateForStatusNames$1(OccasionalStatusPollingActor.scala:101); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor.cromwell$backend$impl$aws$OccasionalStatusPollingActor$$updateStatuses(OccasionalStatusPollingActor.scala:118); 	at cromwell.backend.impl.aws.OccasionalStatusPollingActor$$anonfun$receive$1.$anonfun$applyOrElse$1(OccasionalStatusPollingActor.scala:57); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659); 	at scala.util.Success.$anonfun$map$1(Try.scala:255); 	at scala.util.Success.map(Try.scala:213); 	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292); 	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33); 	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$an,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:5635,update,updateForStatusNames,5635,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['update'],['updateForStatusNames']
Deployability,leResponseStage.java:57); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.Execut,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:2196,pipeline,pipeline,2196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['pipeline'],['pipeline']
Deployability,"leWorkflow.ApplyBQSR:2:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Execution failed: action 11: unexpected exit status 1 was not ignored; [Delocalization] Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stderr gs://cloud-cromwell-dev/cromwell_execution/travis/PairedEndSingleSampleWorkflow/656ddc45-2d1d-4e24-a08; 6-c47fa847c658/call-ApplyBQSR/shard-2/stderr"": Your ""GCE"" credentials are invalid. Please run; $ gcloud auth login; Failure: Could not reach metadata service: [Errno 111] Connection refused. at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:536); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:543); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(Abstract",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3742:1542,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1542,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3742,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,lelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>Thatâ€™s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>Thatâ€™s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.Fo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917:3746,pipeline,pipelines,3746,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917,1,['pipeline'],['pipelines']
Deployability,ler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:51); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:34); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:20); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); at akka.actor.ActorCell.invoke(ActorCell.scala:583); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$GoogleJsonException: Request contains an invalid argument.; ... 21 common f,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:4314,Pipeline,PipelinesApiRequestWorker,4314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,ler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:51); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:34); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:20); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); at akka.actor.ActorCell.invoke(ActorCell.scala:583); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$GoogleJsonException: Request contains an invalid argument.; ... 21 more. [2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:6936,Pipeline,PipelinesApiRequestWorker,6936,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,"les.conf) file seems to mix multiple styles in terms of delimiters. Some entries are colon delimited as if they were from JSON, e.g.:. ```; workflow-options {; # These workflow options will be encrypted when stored in the database; #encrypted-fields: []. # AES-256 key to use to encrypt the values in `encrypted-fields`; #base64-encryption-key: ""AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA="". # Directory where to write per workflow logs; #workflow-log-dir: ""cromwell-workflow-logs"". # When true, per workflow logs will be deleted after copying; #workflow-log-temporary: true. # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.; # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:; #workflow-failure-mode: ""ContinueWhilePossible"". default {; # When a workflow type is not provided on workflow submission, this specifies the default type.; #workflow-type: WDL. # When a workflow type version is not provided on workflow submission, this specifies the default type version.; #workflow-type-version: ""draft-2"". # To set a default hog group rather than defaulting to workflow ID:; #hogGroup: ""static""; }; }; ```; However, most are set with the equals sign:; ```; # Google configuration; google {. #application-name = ""cromwell"". # Default: just application default; #auths = [. # Application default; #{; # name = ""application-default""; # scheme = ""application_default""; #},. # Use a refresh token; #{; # name = ""user-via-refresh""; # scheme = ""refresh_token""; # client-id = ""secret_id""; # client-secret = ""secret_secret""; #},; (etc); ```. Do both of these work? If so, is one format preferred to minimize confusion?. What prompted me to write this issue is that setting some defaults isn't yet working for me. This is probably an issue on my end, but the lack of consistency makes me wonder whether (e.g.) parameters set with colons don't actually take effect and the docs are wrong.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4913:2221,configurat,configuration,2221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4913,1,['configurat'],['configuration']
Deployability,"lets discuss this in a different channel. On Tue, Feb 7, 2017 at 7:27 AM, Geraldine Van der Auwera <; notifications@github.com> wrote:. > Uh, isn't gsa4 supposed to be reserved for GATK automated test suites and; > release machinery? Please don't use it as an experimental pod racer or; > anything like that. If you take it down it affects user-facing systems.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277984572>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACnk0gK6NNq40ngord0qDCt-hwUDqLsYks5raGNCgaJpZM4L0Um8>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278006902:215,release,release,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-278006902,1,['release'],['release']
Deployability,"lignment/shard-1/wf-alignment.cwl/96d7b606-e0fe-4305-a586-e0fc4acf76f8/call-process_alignment/shard-0/inputs/1628767813 [...]. [E::bwa_idx_load_from_disk] fail to locate the index files; ```; Is it expected to lose the original input file names when passing through the pipeline. A lot of tools are sensitive to these and this might be the underlying issue. Regarding the configuration, without `http {}` in under `engine -> filesystems` I get a complaint about it not being supported, even with `http {}` under `backend -> providers -> Local -> config -> filesystems`:; ```; java.lang.IllegalArgumentException: Either https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa exists on a filesystem not supported by this instance of Cromwell, or a failure occurred while building an actionable path from it. Supported filesystems are: LinuxFileSystem. Failures: LinuxFileSystem: Cannot build a local path from https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa (RuntimeException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:181); at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$2.applyOrElse(MaterializeWorkflowDescriptorActor.scala:176); ```; I was trying to lift off how things were done with the Google/gcp resolution so added it in there to fix this issue. Is there a different configuration approach I should be using?. Thanks again for helping with this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320:2812,configurat,configuration,2812,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320,1,['configurat'],['configuration']
Deployability,"linesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:34); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:20); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); at akka.actor.ActorCell.invoke(ActorCell.scala:583); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$GoogleJsonException: Request contains an invalid argument.; ... 21 common frames omitted; [2021-08-13 10:45:07,42] [warn] PAPI request worker had 1 failures making 1 requests:; Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; [2021-08-13 10:45:07,54] [info] WorkflowManagerActor: Workflow a15c46b7-5f93-46d6-94a2-28f656914866 failed (during ExecutingWorkflowState): cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedRespons",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:5219,Pipeline,PipelinesApiRequestManager,5219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['PipelinesApiRequestManager']
Deployability,"linesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:34); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:20); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); at akka.actor.ActorCell.invoke(ActorCell.scala:583); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$GoogleJsonException: Request contains an invalid argument.; ... 21 more. [2021-08-13 10:45:10,13] [info] WorkflowManagerActor: Workflow actor for a15c46b7-5f93-46d6-94a2-28f656914866 completed with status 'Failed'. The workflow will be removed from the workflow store.; [2021-08-13 10:45:13,98] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2021-08-13 10:45:15,05] [info] Workflow polling stopped; [2021-08-13 10:45:15,07] [info] 0 workflows released by cromid-de31b6d; [2021-08-13 10:45:15,07] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; ...; ```. Contents of hello.wdl:; ```; task hello {; String addressee; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!""; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. Contents of hello.inputs:; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```; Contents of cromwell.BROADexa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:7841,Pipeline,PipelinesApiRequestManager,7841,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['PipelinesApiRequestManager']
Deployability,"ll up to run with singularity. This is in an HPC environment with a brand new install of cromwell, where I don't have the ability to access or overwrite any global files, i.e. the application.conf file with all the defaults in it. It's a documentation issue rather than a problem with cromwell, which runs fine on the default configuration. . Documentation [here](https://cromwell.readthedocs.io/en/develop/tutorials/Containers/#singularity) suggests that I need to add code similar to that found [here](https://github.com/broadinstitute/cromwell/blob/develop/cromwell.example.backends/singularity.conf) to a config block of the backend.providers section in a configuration file similar to the file [cromwell.examples.conf](https://www.github.com/broadinstitute/cromwell/tree/develop/cromwell.examples.conf). Which, if you click that link, you'll see is broken. It might possibly be linked to [this issue](https://broadworkbench.atlassian.net/browse/BA-4810) on Jira?. As a result I have no idea what the conf file is supposed to look like, nor to be honest where it goes or how it's meant to be referenced. There's an issue [here](https://gatkforums.broadinstitute.org/wdl/discussion/12789/cromwell-configuration-on-slurm) which tells me I have to have ""include required(classpath(""application""))"" in the first line of the conf file, but apart from that I can't find anything on what the file should look like. . The documentation [here](https://cromwell.readthedocs.io/en/stable/tutorials/ConfigurationFiles/) and [here](https://cromwell.readthedocs.io/en/stable/Configuring/#overview) both suggest that the configuration files are for a server version of cromwell, whereas I have to run it from the command line, i.e. . ```; cromwell run <-o configFile?> -i inputs workflow.wdl; ```; Basically, the documentation on how to configure appears to be either non-existent (only refers to the web server version?) or broken. Updating the documentation so I can figure this out would be much appreciated.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5560:1451,configurat,configuration-on-slurm,1451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5560,3,"['Configurat', 'configurat']","['ConfigurationFiles', 'configuration', 'configuration-on-slurm']"
Deployability,ll-test_1 | 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 11:09:46 cromwell-test_1 | Caused by: liquibase.exception.LockException: java.lang.NullPointerException; 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.acquireLock(StandardLockService.java:242); 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:170); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:196); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:192); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:58); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:31); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:96); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at java.util.conc,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766:3937,update,updateSchema,3937,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766,1,['update'],['updateSchema']
Deployability,"ll?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:5514,configurat,configuration,5514,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['configurat'],['configuration']
Deployability,"log](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.30).; You might want to review and update them manually.; ```; docs/developers/bitesize/workflowParsing/wdlToWdlom_hermes.svg; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""org.yaml"", artifactId = ""snakeyaml"" }; }]; ```; </details>. labels: test-library-update, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6900:2789,update,update,2789,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6900,6,['update'],"['update', 'updates']"
Deployability,looks like the jenkins script needed to forcibly install newer versions of all dependencies. Looks to be fixed now,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-412161507:49,install,install,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3973#issuecomment-412161507,1,['install'],['install']
Deployability,"lot the multidimensional scaling plot using edgeR version 3.1.3.\nThe *.osc.gz files were loaded into the genome browser ZENBU and was used visualize the transcripts. Screen shots were captured.\nGenome_build: hg19 with Gencode V19 annotation\nSupplementary_files_format_and_content: .osc files are simple tab delimited files. They were generated by combining the isoform.results files outputed by RSEM with the gencode v19 .gtf file. It contains abundance measurements and transcript isoforms. It also contains metadata that is inputed into ZENBU.\nSupplementary_files_format_and_content: RNAseq.counts is a simple tab delimited file containing the counts for all the RNA-seq libraries for each gene (summary file of counts).""; },; ""relations"" : {; ""BioSample"" : ""https://www.ncbi.nlm.nih.gov/biosample/SAMN03610550"",; ""SRA"" : ""https://www.ncbi.nlm.nih.gov/sra?term=SRX1020495""; },; ""status"" : {; ""submitted"" : ""May 29 2015"",; ""updated"" : ""Jun 01 2015""; },; ""runs"" : [; {; ""run"" : {; ""Run"" : ""SRR2014238"",; ""ReleaseDate"" : ""2015-05-25 05:44:11"",; ""LoadDate"" : ""2015-05-25 05:38:29"",; ""AssemblyName"" : """",; ""download_path"" : ""https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238"",; ""Experiment"" : ""SRX1020495""; },; ""stats"" : {; ""spots"" : 85220810,; ""bases"" : 12953563120,; ""spots_with_mates"" : 85220810,; ""avgLength"" : 152,; ""size_MB"" : 7790.0; },; ""library"" : {; ""LibraryName"" : ""Biochain_Adult_Liver"",; ""LibraryStrategy"" : ""RNA-Seq"",; ""LibrarySelection"" : ""other"",; ""LibrarySource"" : ""TRANSCRIPTOMIC"",; ""LibraryLayout"" : ""PAIRED""; },; ""sample"" : {; ""Platform"" : ""ILLUMINA"",; ""Model"" : ""Illumina HiSeq 2000"",; ""SRAStudy"" : ""SRP058036"",; ""BioProject"" : ""PRJNA283012"",; ""Study_Pubmed_id"" : """",; ""ProjectID"" : ""283012"",; ""Sample"" : ""SRS931038"",; ""BioSample"" : ""SAMN03610550"",; ""SampleType"" : ""simple"",; ""TaxID"" : ""9606"",; ""ScientificName"" : ""Homo sapiens"",; ""SampleName"" : ""Biochain_Adult_Liver""; },; ""subject"" : {; ""Subject_ID"" : """",; ""Sex"" : ""male"",; ""Disease"" : """",; ""Tumor"" : ""no",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4519:3502,update,updated,3502,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519,2,"['Release', 'update']","['ReleaseDate', 'updated']"
Deployability,"lotypeCaller ^[[31m(BETA Tool) ^[[36mCall germline SNPs and indels via local re-assembly of haplotypes^[[0m; ^[[32m IndexFeatureFile ^[[36mCreates indices for Feature-containing files (eg VCF and BED files)^[[0m; ^[[32m LiftOverVcf ^[[36mLifts a VCF between genome builds^[[0m; ^[[32m MakeSitesOnlyVcf ^[[36mCreates a VCF bereft of genotype information from an input VCF^[[0m; ^[[32m MergeVcfs ^[[36mMerges multiple VCF files into one VCF file^[[0m; ^[[32m Mutect2 ^[[31m(BETA Tool) ^[[36mCall somatic SNVs and indels via local assembly of haplotypes^[[0m; ^[[32m RemoveNearbyIndels ^[[36m(Internal) Remove indels that are close to each other from a vcf^[[0m; ^[[32m RenameSampleInVcf ^[[36mRename a sample within a VCF^[[0m; ^[[32m SelectVariants ^[[36mSelect a subset of variants from a VCF file^[[0m; ^[[32m SortVcf ^[[36mSorts one or more VCF files^[[0m; ^[[32m SplitIntervals ^[[36mSplit intervals into sub-interval files.^[[0m; ^[[32m SplitVcfs ^[[36mSplits an input VCF file into two VCF files^[[0m; ^[[32m UpdateVCFSequenceDictionary ^[[36mUpdates the sequence dictionary in a variant file.^[[0m; ^[[32m ValidateVariants ^[[36mValidate VCF^[[0m; ^[[32m VariantFiltration ^[[36mFilter variant calls based on INFO and FORMAT annotations^[[0m; ^[[32m VariantRecalibrator ^[[36mBuild a recalibration model to score variant quality for filtering purposes^[[0m; ^[[32m VariantsToTable ^[[36mExtract specific fields from a VCF file to a tab-delimited table^[[0m; ^[[32m VcfToIntervalList ^[[36mConverts a VCF file to a Picard Interval List^[[0m. ^[[37m--------------------------------------------------------------------------------------; ^[[0m; Exception in thread ""main"" org.broadinstitute.hellbender.exceptions.UserException: '-T' is not a valid command. at org.broadinstitute.hellbender.Main.extractCommandLineProgram(Main.java:291); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:150); at org.broadinstitute.hellbender.Main.main(Main.java:233); ```. Any advise?. Thank you very much",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2673:12092,Update,UpdateVCFSequenceDictionary,12092,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2673,1,['Update'],['UpdateVCFSequenceDictionary']
Deployability,"lt""; root = ""s3://caper4-04-20-2021/out""; }; actor-factory = ""cromwell.backend.impl.aws.AwsBatchBackendLifecycleActorFactory""; }; }; }. system {; job-rate-control {; jobs = 1; per = ""2 seconds""; }; abort-jobs-on-terminate = true; graceful-server-shutdown = true; max-concurrent-workflows = 40; }; call-caching {; invalidate-bad-cache-results = true; enabled = true; }; database {; db {; connectionTimeout = 30000; numThreads = 1; url = ""jdbc:hsqldb:file:/opt/caper/default_file_db;shutdown=false;hsqldb.tx=mvcc;hsqldb.lob_compressed=true;hsqldb.default_table_type=cached;hsqldb.result_max_memory_rows=10000;hsqldb.large_data=true;hsqldb.applog=1;hsqldb.script_format=3""; }; }; aws {; application-name = ""cromwell""; auths = [; {; name = ""default""; scheme = ""default""; }; ]; region = ""us-east-1""; }; engine {; filesystems {; s3 {; auth = ""default""; }; }; }; ```. Even with the `reference` strategy, Cromwell still make a `cacheCopy` of previous outputs. Here is the `metadata.json` from a pipeline run with `reference` strategy. Cromwell still makes a `cacheCopy` directory.; ```; {; ""calls"": {; ""atac.bam2ta"": [; {; ""executionStatus"": ""Done"",; ""stdout"": ""s3://caper4-04-20-2021/out/atac/b59c0d05-5210-4341-b4f0-dcbf5b9e74c1/call-bam2ta/shard-0/bam2ta-0-stdout.log"",; ""compressedDockerSize"": 963995760,; ""shardIndex"": 0,; ""outputs"": {; ""ta"": ""s3://caper4-04-20-2021/out/atac/b59c0d05-5210-4341-b4f0-dcbf5b9e74c1/call-bam2ta/shard-0/cacheCopy/glob-199637d3015dccbe277f621a18be9eb4/ENCFF341MYG.subsampled.400.trim.merged.srt.nodup.no_chrM_MT.tn5.tagAlign.gz""; },; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": true,; ""result"": ""Cache Hit: 4f56e5c0-5c52-4f67-aa11-35be0336e2bf:atac.bam2ta:0"",; ""hashes"": {; ""output count"": ""C4CA4238A0B923820DCC509A6F75849B"",; ""runtime attribute"": {; ""docker"": ""A2DD51631B4B22941E9794A812F024BE"",; ""failOnStderr"": ""68934A3E9455FA72420237EB05902327"",; ""continueOnReturnCode"": ""CFCD208495D565EF66E7DFF9F98764DA""; },; ""output expression"": {; ""File ta"": ""9EB184A935F0EDE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6327:2730,pipeline,pipeline,2730,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6327,1,['pipeline'],['pipeline']
Deployability,"lways appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design there instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2446,configurat,configuration,2446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,1,['configurat'],['configuration']
Deployability,"ly rather than let it keep going to find out if the workflow log would eventually show an errors. So far, it seems to have considered everything a success. ```; 2023-04-18 21:59:54,599 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:108:1]: Status change from Running to Success; 2023-04-18 22:00:09,060 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:107:1]: Status change from Running to Success; 2023-04-18 22:00:18,464 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:106:1]: Status change from Running to Success; 2023-04-18 22:01:20,604 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:111:1]: Status change from Running to Success; 2023-04-18 22:14:47,728 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: Aborting workflow; 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:262:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/9178938377659283430); 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:112:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/8559201934542591362); 2023-04-18 22:14:48,295 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Successfully requested cancellation of projects/16371921765/locations/us-central1/operations/9178938377659283430; 2023-04-18 22:15:56,564 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: Status change from Running to Success; 2023-04-18 22:16:44,505 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Status change from Running to Cancelled; 2023-04-18 22:16:44,539 INFO - WorkflowExecutionActor-1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:4862,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,4862,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"ly/compare/v1.1.0...v1.1.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.1.0).; You might want to review and update them manually.; ```; CHANGELOG.md; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/variantcall_batch_region.cwl; cloud-nio/cloud-nio-impl-drs/src/main/scala/cloud/nio/impl/drs/DrsCloudNioFileSystemProvider.scala; cwl/src/test/resources/cwl/lodash.js; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" }; }]; ```; </details>. labels: sbt-plugin-update, early-semver-patch, semver-spec-patch, old-version-remains, com",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6850:1202,integrat,integrationTestCases,1202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6850,1,['integrat'],['integrationTestCases']
Deployability,mManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.copyObject(DefaultS3Client.java:466); 	at org.lerch.s3fs.S3FileSystemProvider.copy(S3FileSystemProvider.java:434); 	at java.nio.file.Files.copy(Files.java:1274); 	at better.files.File.copyTo(File.scala:663); 	at cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:5917,pipeline,pipeline,5917,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,1,['pipeline'],['pipeline']
Deployability,mManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); 	at software.amazon.awssdk.services.s3.DefaultS3Client.listBuckets(DefaultS3Client.java:1749); 	at software.amazon.awssdk.services.s3.S3Client.listBuckets(S3Client.java:2184); 	at org.lerch.s3fs.S3FileStore.getBucket(S3FileStore.java:93); 	at org.lerch.s3fs.S3FileStore.getBu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686:3419,pipeline,pipeline,3419,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686,2,['pipeline'],['pipeline']
Deployability,"main while cromwell 35 is released, that's to bad that this change is not in there yet ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425875557:26,release,released,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425875557,1,['release'],['released']
Deployability,make sure to document the configuration in readme and add a sentence that this exists and the changelog,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2056#issuecomment-284927313:26,configurat,configuration,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2056#issuecomment-284927313,1,['configurat'],['configuration']
Deployability,"mazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); cromwell_1 | 	at software.amazon.awssdk.services.sts.DefaultStsClient.getCallerIdentity(DefaultStsClient.java:673); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:86); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:76); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:91); cromwell_1 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); cromwell_1 | 	at scala.util.Try$.apply(Try.scala:213); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:91); cromwell_1 | 	... 46 more; cromwell_1 | ; cromwell_1 | 2020-03-15 16:09:58,022 cromwell-system-akka.dispatchers.engine-dispatcher-59 INFO - WorkflowManagerActor WorkflowActor-c4ee3308-f9bf-41d2-acdb-70c02b6cc4b3 is in a terminal state: WorkflowFailedState`. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5452:5725,configurat,configuration,5725,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452,1,['configurat'],['configuration']
Deployability,"mediately. ```bash; [ec2-user@ip-10-66-51-33 execution]$ pwd; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ ls -l; total 8; -rwxr--r-- 1 root root 1182 Feb 4 19:37 script; -rw-r--r-- 1 root root 296 Feb 4 19:37 stderr; -rw-r--r-- 1 root root 0 Feb 4 19:37 stdout; ```. ```bash; [ec2-user@ip-10-66-51-33 execution]$ cat stderr; /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: 3: /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution/script: Syntax error: ""("" unexpected; ```. ```; [ec2-user@ip-10-66-51-33 execution]$ cat script ; #!/bin/sh; cd /mnt/data/cromwell/PreProcessingForVariantDiscovery_GATK4/a46f0127-f6e8-4887-ae7d-c3fc08f834e4/call-GetBwaVersion/execution; spark-submit --master yarn --total-executor-cores 1 --deploy-mode cilent --class GATK4 --executor-memory 1gb InstantiatedCommand(# Not setting ""set -o pipefail"" here because /bwa has a rc=1 and we don't want to allow rc=1 to succeed ; # because the sed may also fail with that error and that is something we actually want to fail on.; /usr/local/bin/bwa 2>&1 | \; grep -e '^Version' | \; sed 's/Version: //',Map(),List(),None,None,None,List((LocalName(mem_size),WomString(1 GB)), (LocalName(bwa_path),WomString(/usr/local/bin/)), (LocalName(preemptible_tries),WomInteger(3)), (LocalName(entry_point),WomString(GATK4)), (LocalName(docker_image),WomString(227114915345.dkr.ecr.us-east-1.amazonaws.com/genomes-in-the-cloud:20190115))),List((LocalName(mem_size),WomString(1 GB)), (LocalName(bwa_path),WomString(/usr/local/bin/)), (LocalName(preemptible_tries),WomInteger(3)), (LocalName(entry_point),WomString(GATK4)), (LocalName(docker_image),WomString(227114915345.dkr.ecr.us-east-1.amazonaws.com/genomes-in-the-cloud:20190115)))); echo $? > rc; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4611:1915,deploy,deploy-mode,1915,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4611,1,['deploy'],['deploy-mode']
Deployability,mergeable now that we've (you've) released?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-373187082:34,release,released,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-373187082,1,['release'],['released']
Deployability,"metadata.xml::remove_failure_timestamp::cjllanwarne ran successfully in 5ms; 2019-01-31 20:10:51,428 ERROR - changelog.xml: changesets/failure_metadata.xml::causedByLists::cjllanwarne: Change Set changesets/failure_metadata.xml::causedByLists::cjllanwarne failed. Error: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 2019-01-31 20:10:51,492 INFO - changesets/failure_metadata.xml::causedByLists::cjllanwarne: Successfully released change log lock; 2019-01-31 20:10:51,531 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70);",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1124,UPDATE,UPDATE,1124,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['UPDATE'],['UPDATE']
Deployability,"mission denied for \""sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"" from request \""/v2/broad-cumulus/cellranger/manifests/sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"".\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:91); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:812); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:95); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1340); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1336); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingEx",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:14223,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,14223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"mitted; [2019-05-22 18:42:27,36] [info] SingleWorkflowRunnerActor: Workflow submitted 3997371c-9513-4386-a579-a72639c6e960; [2019-05-22 18:42:27,36] [info] WorkflowManagerActor Starting workflow 3997371c-9513-4386-a579-a72639c6e960; [2019-05-22 18:42:27,36] [info] WorkflowManagerActor Successfully started WorkflowActor-3997371c-9513-4386-a579-a72639c6e960; ...; [2019-05-22 19:15:20,74] [info] 755021ae-948b-47f9-94a8-66b486bda47d-SubWorkflowActor-SubWorkflow-Haplotypecaller:0:1 [755021ae]: Starting Haplotypecaller.SplitFilesByChromosome; [2019-05-22 19:15:21,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: set -e; for chr in grep -v '@' /cromwell_root/s4-pbg-hc/References/HC_Panel_v3.intervals | cut -f1 | sort | uniq; do; grep -v '@' /cromwell_root/s4-pbg-hc/References/HC_Panel_v3.intervals | grep -w $chr | awk '{ print $1"":""$2""-""$3 }' > $chr.intervals; samtools view -@ 15 -b -h /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Alignment/alignment.Alignment/6e782168-d056-4ac9-b83b-5fba843fffc1/call-baseRecalibrator/shard-0/RSM278260-6_8plex.dedup.recal.bam $chr > $chr.RSM278260-6_8plex.dedup.recal.bam; done; [2019-05-22 19:15:21,35] [info] Submitting job to AWS Batch; [2019-05-22 19:15:21,35] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignandmolvar:1.3.2; [2019-05-22 19:15:21,35] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4; [2019-05-22 19:15:21,35] [info] taskId: Haplotypecaller.SplitFilesByChromosome-None-1; [2019-05-22 19:15:21,35] [info] hostpath root: hc.Haplotypecaller/hc.SplitFilesByChromosome/755021ae-948b-47f9-94a8-66b486bda47d/None/1; [2019-05-22 19:15:21,71] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: job id: 8ec19f2b-5b49-4422-9ad1-5b51e3db9414; [2019-05-22 19:15:21,77",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:3541,Pipeline,Pipeline,3541,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,1,['Pipeline'],['Pipeline']
Deployability,ml](https://bitbucket.org/snakeyaml/snakeyaml/src) from 1.29 to 1.30.; [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/ReleaseNotes.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/ReleaseNotes.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/ReleaseNotes.rst) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/RELEASES.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/RELEASES.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/RELEASES.rst) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Releases.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Releases.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Releases.rst) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/releases.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/releases.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/releases.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyam,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6635:1038,release,releases,1038,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6635,1,['release'],['releases']
Deployability,ml](https://bitbucket.org/snakeyaml/snakeyaml/src) from 1.30 to 1.31.; [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/ReleaseNotes.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/ReleaseNotes.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/ReleaseNotes.rst) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/RELEASES.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/RELEASES.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/RELEASES.rst) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Releases.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Releases.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Releases.rst) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/releases.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/releases.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/releases.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyam,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6900:1038,release,releases,1038,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6900,1,['release'],['releases']
Deployability,mmon.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.Parti,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:2553,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"more... I restarted the workflow to see if the call caching worked properly. With the HSQLDB there was no issue in restarting. With SQLite everything worked fine until at some point early in the workflow cromwell hung. I interrupted the process, and cromwell started to shut down gracefully. `WriteMetadataActor shutting down: processing 108720 queued messages`. The processing of these messages takes more than half an hour. . The problem here is twofold: the SQLite backend is significantly slower than the HSQL in-memory database with overflow file (as expected) and the enormous amount of messages that Cromwell produces totally swamps it. (Judging from the shutdown scroll the rate is approximately 40 messages per second or 25ms per message processing time, sqlite should be able to work faster than that). EDIT: I did some research. It turns out SQLite creates a journal file and deletes it again. This means every transaction there are a few filesystem operations performed:; - Create a journal file; - Update the database (append); - Delete the journal file (rewrite the directory file). . This can be slightly improved by setting `journal_mode=truncate` which doesn't delete the journal file so the directory file doesn't have to be rewritten. `journal_mode=memory` doesn't increase the speed much and adds the ability of data corruption. The `cache_size` pragma doesn't speed up things either.; I have been testing some more and the solution for now is just to be patient. The hang is resolved after a few minutes. The long-term solution is to limit the amount of database transactions that cromwell wants to perform on the metadata database. 100K + is quite a lot. . EDIT2:; After some further impatience, I decided to drop the metadata altogether. In our specific use case where we are on a hpc cluster, with a slow NFS-based filesystem, with file-based databases limited to single projects, the metadata is not very interesting. Using the following configuration works very well initial",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906:1237,Update,Update,1237,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-735646906,1,['Update'],['Update']
Deployability,"mory: machine_mem + "" MB""; disks: ""local-disk "" + disk_size + "" HDD""; }. output {; File output_bam = ""~{name}.bam""; File output_bai = ""~{name}.bai"". }; }; ```. input:; ```; {; ""Mutect2.tumor_reads"": ""sra://SRR2619134/SRR2619134""; }; ```. wdl:; ```; include required(classpath(""application"")); google {; application-name = ""cromwell""; auths = [; { ; name = ""application-default""; scheme = ""application_default""; }; ]; }; filesystems {; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""/home/nicholas/.sra/prj_26387_D28121.ngc""; }; }; engine {; filesystems {; gcs {; auth = ""application-default""; }. }; }; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; enable-fuse = true; }; filesystems {; sra {}; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; }; }; }; }; }; system {; input-read-limits {; lines = 12800000; bool = 7; int = 19; float = 50; string = 1",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161:3079,Pipeline,PipelinesApiLifecycleActorFactory,3079,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804#issuecomment-682146161,1,['Pipeline'],['PipelinesApiLifecycleActorFactory']
Deployability,"munshi ran successfully in 661ms; 2019-01-31 19:38:58,563 ERROR - changelog.xml: changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Change Set changesets/failure_metadata.xml::failure_to_message::cjllanwarne failed. Error: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 2019-01-31 19:38:58,618 INFO - changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Successfully released change log lock; 2019-01-31 19:38:58,637 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.sc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:1186,UPDATE,UPDATE,1186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['UPDATE'],['UPDATE']
Deployability,"mwell$engine$workflow$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:103); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor$$anonfun$receive$1.applyOrElse(MaterializeWorkflowDescriptorActor.scala:66); at akka.actor.Actor$class.aroundReceive(Actor.scala:467); at cromwell.engine.workflow.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:59); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516); at akka.actor.ActorCell.invoke(ActorCell.scala:487); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238); at akka.dispatch.Mailbox.run(Mailbox.scala:220); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'shared-filesystem'; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:152); at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:170); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:184); at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:189); at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:258); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:264); at com.typesafe.config.impl.SimpleConfig.getConfig(SimpleConfig.java:37); at cromwell.engine.backend.local.SharedFileSystem$.<init>(SharedFileSystem.scala:27); at cromwell.engine.backend.local.SharedFileSystem$.<clinit>(SharedFileSystem.scala); ... 25 more; ```. I'm at a loss where to place the `shared-filesystem` key, and what should be in it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1406:6027,configurat,configuration,6027,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1406,1,['configurat'],['configuration']
Deployability,"mwell.; include required(classpath(""application"")). # Google configuration; google {. application-name = ""cromwell-demo"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""***@***.gserviceaccount.com""; json-file = ""/***/***.json""; }; ]; }. # Here is where you can define the backend providers that Cromwell understands.; # The default is a local provider.; # To add additional backend providers, you should copy paste additional backends; # of interest that you can find in the cromwell.example.backends folder; # folder at https://www.github.com/broadinstitute/cromwell; # Other backend providers include SGE, SLURM, Docker, udocker, Singularity. etc.; # Don't forget you will need to customize them for your particular use case.; backend {; # Override the default backend.; default = ""PAPIv2"". # The list of providers.; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; # Google project; project = ""***-***"". # Base bucket for workflow executions; root = ""gs://*****/cromwell-execution"". # Make the name of the backend used for call caching purposes insensitive to the PAPI version.; name-for-call-caching-purposes: PAPI. # Emit a warning if jobs last longer than this amount of time. This might indicate that something got stuck in PAPI.; slow-job-warning-time: 24 hours. # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-10",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:10535,pipeline,pipelines,10535,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['pipeline'],['pipelines']
Deployability,"mwell.backend.google.batch.api.GcpBatchApiRequestHandler.query(GcpBatchApiRequestHandler.scala:14); at cromwell.backend.google.batch.actors.GcpBatchBackendSingletonActor$$anonfun$normalReceive$1.$anonfun$applyOrElse$3(GcpBatchBackendSingletonActor.scala:80); at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:678); at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:467); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception; Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]; at io.grpc.Status.asRuntimeException(Status.java:539); ... 14 common frames omitted; Caused by: javax.net.ssl.SSLHandshakeException: General OpenSslEngine problem; at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine.handshakeException(ReferenceCountedOpenSslEngine.java:1907); at io.grpc.netty.shaded.io.netty.handler.ssl.ReferenceCountedOpenSslEngine.wrap(ReferenceCountedOpenSslEngine.java:834); at java.base/javax.net.ssl.SSLEngine.wrap(SSLEngine.java:564); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.wrap(SslHandler.java:1041); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.wrapNonAppData(SslHandler.java:927); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1409); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.unwrapNonAppData(SslHandler.java:1327); at io.grpc.netty.shaded.io.netty.handler.ssl.SslHandler.acce",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:4653,Pipeline,Pipeline,4653,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['Pipeline'],['Pipeline']
Deployability,mwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJobExecutionActor$$anonfun$receive$1.applyOrElse(AsyncBackendJobExecutionActor.scala:88); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172); 	at scala.PartialFunction$OrElse.applyOrElse(PartialFun,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:2536,pipeline,pipelines,2536,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['pipeline'],['pipelines']
Deployability,mwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncEx,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:1481,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"mwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-88f82a0-SNAP.jar server; 2019-01-31 18:42:55,062 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:5339,configurat,configuration,5339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['configurat'],['configuration']
Deployability,"mwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://docs.sentry.io/clients/java/modules/logback/; - https://docs.sentry.io/clients/java/config/; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/logback.rst; - (deprecated) https://github.com/getsentry/sentry-java/blob/raven-java-8.x/docs/modules/raven.rst; - https://docs.sentry.io/clients/java/migration/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3657:2141,Update,Update,2141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657,4,"['Update', 'configurat']","['Update', 'configuration']"
Deployability,mwell_1 | [-a ADAPTERS] [-l LIMITS] [-f FORMAT] [-n NO_GROUP]; cromwell_1 | [-e EXTRA_OPTIONS]; cromwell_1 | fastqc_docker.py: error: argument -r/--read is required; cromwell_1 | . See logs at gs://genovic-cromwell/cromwell-execution/trio/f5454139-c51d-4d04-ae0a-9b9d4ce650aa/call-germline_variant_calling/shard-0/germline_variant_calling/5d4c4459-a91c-4d3b-8ca4-b98457134750/call-fastqc/shard-0/; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); cromwell_1 | at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); cromwell_1 | at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); cromwell_1 | at scala.concurrent.BlockContext$,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:2233,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"n error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 9623 at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 9624 at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 9625 at akka.dispatch.ForkJoinExecuto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3732:1950,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1950,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"n(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-02-28 08:30:32,176 cromwell-system-akka.dispatchers.engine-dispatcher-28 ERROR - WorkflowManagerActor Workflow bd18e464-59a2-44cf-80c2-b4d93bdfe0ce failed (during FinalizingWorkflowState): software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686:7438,pipeline,pipeline,7438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686,1,['pipeline'],['pipeline']
Deployability,n.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.am,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:4457,pipeline,pipeline,4457,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,3,['pipeline'],['pipeline']
Deployability,"n/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804:1932,pipeline,pipelines,1932,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804,1,['pipeline'],['pipelines']
Deployability,"n/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2210,pipeline,pipelines,2210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['pipeline'],['pipelines']
Deployability,"n: Failed to instantiate backend filesystem:; Cannot find a filesystem with name sra in the configuration. Available filesystems: ftp, s3, gcs, oss, drs, http; 	at common.validation.Validation$ValidationChecked$.$anonfun$unsafe$2(Validation.scala:98); 	at cats.syntax.EitherOps$.valueOr$extension(either.scala:66); 	at common.validation.Validation$ValidationChecked$.unsafe$extension(Validation.scala:98); 	at cromwell.backend.BackendConfigurationDescriptor.configuredPathBuilderFactories$lzycompute(backend.scala:109); 	at cromwell.backend.BackendConfigurationDescriptor.configuredPathBuilderFactories(backend.scala:108); 	at cromwell.backend.BackendConfigurationDescriptor.pathBuilders(backend.scala:120); 	at cromwell.backend.standard.StandardInitializationActor.pathBuilders$lzycompute(StandardInitializationActor.scala:62); 	at cromwell.backend.standard.StandardInitializationActor.pathBuilders(StandardInitializationActor.scala:62); 	at cromwell.backend.google.pipelines.common.PipelinesApiInitializationActor.$anonfun$workflowPaths$2(PipelinesApiInitializationActor.scala:137); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$W",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793:1232,Pipeline,PipelinesApiInitializationActor,1232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793,1,['Pipeline'],['PipelinesApiInitializationActor']
Deployability,"nActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableB",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861:1154,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1154,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"nActor-075e0cf3-194b-4f53-a43d-d31f0b370f79 [UUID(075e0cf3)]: Starting wf_hello.hello; 2021-09-27 13:48:29,304 cromwell-system-akka.dispatchers.engine-dispatcher-28 INFO - Assigned new job execution tokens to the following groups: 075e0cf3: 1; 2021-09-27 13:48:31,233 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - BT-322 075e0cf3:wf_hello.hello:-1:1 is eligible for call caching with read = true and write = true; 2021-09-27 13:48:31,314 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - BT-322 075e0cf3:wf_hello.hello:-1:1 cache hit copying nomatch: could not find a suitable cache hit.; 2021-09-27 13:48:31,320 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - 075e0cf3-194b-4f53-a43d-d31f0b370f79-EngineJobExecutionActor-wf_hello.hello:NA:1 [UUID(075e0cf3)]: Could not copy a suitable cache hit for 075e0cf3:wf_hello.hello:-1:1. No copy attempts were made.; 2021-09-27 13:48:31,449 cromwell-system-akka.dispatchers.backend-dispatcher-33 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: `echo ""Hello Cromwell! Welcome to Cromwell . . . on AWS!""`; 2021-09-27 13:48:33,376 cromwell-system-akka.dispatchers.backend-dispatcher-33 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Adjusting boot disk size to 16 GB: 10 GB (runtime attributes) + 5 GB (user command image) + 1 GB (Cromwell support images); 2021-09-27 13:48:38,987 cromwell-system-akka.dispatchers.backend-dispatcher-33 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: job id: projects/gred-cumulus-sb-01-991a49c4/operations/15427360049616748078; 2021-09-27 13:49:07,692 cromwell-system-akka.dispatchers.backend-dispatcher-35 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from - to Running; 2021-09-27 13:50:48,340 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hell",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:11467,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,11467,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,nAttributes$.apply(PipelinesApiConfigurationAttributes.scala:307); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.defaultBuildAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:32); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.$anonfun$papiAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:34); at scala.util.Try$.apply(Try.scala:210); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.cromwell$backend$google$pipelines$common$PipelinesApiBackendLifecycleActorFactory$$build$1(PipelinesApiBackendLifecycleActorFactory.scala:109); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.robustBuildAttributes(PipelinesApiBackendLifecycleActorFactory.scala:120); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.<init>(PipelinesApiBackendLifecycleActorFactory.scala:34); at cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:10); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at cromwell.engine.backend.BackendConfigurationEntry.$anonfun$asBackendLifecycleActorFactory$1(BackendConfiguration.scala:13); at scala.util.Try$.apply(Try.scala:210); at cromwell.engine.backend.BackendConfigurationEntry.asBackendLifecycleActorFactory(BackendConfiguration.scala:14); at cromwell.engine.backend.CromwellBackends.$anonfun$backendLifecycleActorFactories$1(CromwellBackends.scala:14); at scala.collection.immutable.List.map(List.scala:246); at cromwell.engine.backend.CromwellBa,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:1847,pipeline,pipelines,1847,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,1,['pipeline'],['pipelines']
Deployability,"nStatus"": ""Failed"",; ""stdout"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669-0f81-4e19-96a0-dbe4b72cee8e/call-aggregate_data/aggregate_data-stdout.log"",; ""shardIndex"": -1,; ""outputs"": {. },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""broadgdac/aggregate_data:31"",; ""cpu"": ""1"",; ""zones"": ""us-central1-b"",; ""memory"": ""2GB""; },; ""cache"": {; ""allowResultReuse"": true; },; ""inputs"": {; ""input_array"": [""bar, baz""]; },; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ""backend"": ""JES"",; ""end"": ""2016-08-01T19:58:05.000000Z"",; ""stderr"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669-0f81-4e19-96a0-dbe4b72cee8e/call-aggregate_data/aggregate_data-stderr.log"",; ""attempt"": 1,; ""executionEvents"": [],; ""backendLogs"": {; ""log"": ""gs://fc-5539c024-3ba8-4ed1-97c3-82fed2675776/1626e6be-60ed-48b1-9bbc-a3fdef4a90f5/aggregate_data_workflow/7be16669-0f81-4e19-96a0-dbe4b72cee8e/call-aggregate_data/aggregate_data.log""; },; ""start"": ""2016-08-01T19:56:48.000000Z""; }]; },; ""outputs"": {. },; ""id"": ""7be16669-0f81-4e19-96a0-dbe4b72cee8e"",; ""submission"": ""2016-08-01T19:56:48.000000Z"",; ""status"": ""Failed"",; ""end"": ""2016-08-01T19:58:05.000000Z"",; ""start"": ""2016-08-01T19:56:48.0000",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:7302,Pipeline,Pipeline,7302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,2,['Pipeline'],['Pipeline']
Deployability,"n_pangenome_vcf=PANGENOME_VCF,; in_reference_genome=REF_GENOME,; in_executable=EXE_PATH,; in_fastq_file=reads_extraction_and_merging.fastq_file, # how to feed a task output to another one!!!; prefix_vcf=VCF_PREFIX,; in_cores=CORES,; in_disk=DISK,; in_mem=MEM; }. output {; File sample = reads_extraction_and_merging.fastq_file; File genotype = genome_inference.vcf_file; }; }. task reads_extraction_and_merging {; input {; String in_container_pangenie; File in_forward_fastq; File in_reverse_fastq; String in_label; Int in_cores; Int in_disk; Int in_mem; }; command <<<; cat ~{in_forward_fastq} ~{in_reverse_fastq} | pigz -dcp ~{in_cores} > ~{in_label}.fastq; >>>; output {; File fastq_file = ""~{in_label}.fastq""; }; runtime {; docker: in_container_pangenie; memory: in_mem + "" GB""; cpu: in_cores; disks: ""local-disk "" + in_disk + "" SSD""; }; }. task genome_inference {; input {; String in_container_pangenie; File in_reference_genome; File in_pangenome_vcf; String in_executable; File in_fastq_file; String prefix_vcf; Int in_cores; Int in_disk; Int in_mem; }; command <<<; echo ""vcf: ~{in_pangenome_vcf}"" > /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""reference: ~{in_reference_genome}"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo $'reads:\n sample: ~{in_fastq_file}' >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""pangenie: ~{in_executable}"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; echo ""outdir: /app/pangenie"" >> /app/pangenie/pipelines/run-from-callset/config.yaml; cd /app/pangenie/pipelines/run-from-callset; snakemake --cores ~{in_cores}; >>>; output {; File vcf_file = ""~{prefix_vcf}.vcf""; }; runtime {; docker: in_container_pangenie; memory: in_mem + "" GB""; cpu: in_cores; disks: ""local-disk "" + in_disk + "" SSD""; preemptible: 1 # can be useful for tools which execute sequential steps in a pipeline generating intermediate outputs; }; }; ```; **_Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6966:3014,pipeline,pipelines,3014,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966,1,['pipeline'],['pipelines']
Deployability,"nd runs that job instead. This incorrect job runs to completion, but the outputs are written to the location specified in the original job, hence that failure to read the RC file. Below is an edited workflow log that demonstrates the failure:; ```; [2019-05-22 18:42:19,86] [info] Running with database db.url = jdbc:hsqldb:mem:7e164ea8-21fd-4b3a-864c-f8a8ea97645f;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:25,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-05-22 18:42:25,86] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-05-22 18:42:25,92] [info] Running with database db.url = jdbc:hsqldb:mem:d3111f9f-5515-48da-b4c2-c9014a6eb8ab;shutdown=false;hsqldb.tx=mvcc; [2019-05-22 18:42:26,15] [warn] Unrecognized configuration key(s) for AwsBatch: auth, numCreateDefinitionAttempts, numSubmitAttempts; [2019-05-22 18:42:26,41] [info] Slf4jLogger started; [2019-05-22 18:42:26,62] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-c5da692"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-05-22 18:42:26,66] [info] Metadata summary refreshing every 2 seconds.; [2019-05-22 18:42:26,69] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,69] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-05-22 18:42:26,71] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-05-22 18:42:27,30] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-05-22 18:42:27,31] [info] SingleWorkflowRunnerActor: Version 36; [2019-05-22 18:42:27,35] [info] Unspecified type (Unspecified version) workflow 3997371c-9513-4386-a579-a72639c6e960 submitted; [2019-05-22 18:42:27,36] [info] SingleWorkflowRunnerActor: Workflow submitted 3997371c-9513-4386-a57",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:1473,configurat,configuration,1473,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,2,['configurat'],['configuration']
Deployability,"ndJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/6169035039702064455; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18:59:03,708 cromwell-system-akka.dispatchers.backend-dispatcher-96 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 19:01:18,073 cromwell-system-akka.dispatchers.backend-dispatcher-130 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from Running to Success; 2020-10-13 19:02:03,961 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - MaterializeWorkflowDescriptorActor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.read_drs_with_usa -> papi-v2-usa; 2020-10-13 19:03:01,200 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - WorkflowExecutionActor-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:5431,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,5431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"nds) and singularity arguments? (e.g., debug?) I don't know if this is ""how it's supposed to look"" but here is what a test run spits out for me:. ```bash; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; [2018-08-27 02:03:57,30] [info] Running with database db.url = jdbc:hsqldb:mem:3cd4a928-7743-4252-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-27 02:04:06,71] [info] Metadata summary refreshing every 2 seconds.; [2018-08-27 02:04:06,81] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-27 02:04:07,85] [info] JobE",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:1493,Pipeline,PipelinesApiLifecycleActorFactory,1493,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,1,['Pipeline'],['PipelinesApiLifecycleActorFactory']
Deployability,"need, but I decided to err on providing more info over less. The dashboard view has panels of grouped information, categorized by labels on the jobs. Imagine a user had an owner label and a project label on all of their jobs. The dashboard panels would be pivoted by project and owner, and show probably the first ~5-10 labels that have the most running jobs with that label. These panels would be populated with a header that is the key of the cromwell label, a list of values that match that key that the users have access to, and then a summary of their statuses. . The dashboard will be filterable by other labels, but maybe not at first. A use case example there is filtering the image above by a label `key:value` of `flag:archived`. There is a concept of flagging jobs as archived so you don't see them anymore, as a way to get your failures list down to ""inbox 0"" and say ""I've addressed those jobs, I don't want to see them anymore"". So it's possible a user could want to filter those jobs out of their dashboard view as well. v1 will not have this chart pictured and will not have the left panel of server information. ### Ticket Prioritization Suggestions; 1. I would like to start with a spike/design doc and scoping out the amount of effort it would take to support this in Cromwell before end of Q1. ; 2. This ticket can also represent the implementation if Cromwell wants, which we need by end of May to be able to do the front end work before end of Q2. . ### Current Status; Currently, I think this view would require many pings to the cromwell query endpoint with different queries to get back all of the numbers and results. . ### Risks I know of; I don't think this is blocked by the labels endpoint update in #3233, but wanted to mention it in case it is a risk. ### ACs; - The Job Manager Dashboard can be quickly filled in; - The user can choose which labels they want to get this summary information on (i.e. it's not only a fixed set of labels that are supported by Cromwell)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3348:2206,update,update,2206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3348,1,['update'],['update']
Deployability,"nerActor: received unexpected message: Done in state RunningSwraData; [2018-08-27 02:04:08,07] [info] WorkflowManagerActor Successfully started WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468; [2018-08-27 02:04:08,07] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-08-27 02:04:08,09] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-08-27 02:04:08,17] [info] MaterializeWorkflowDescriptorActor [967af8b6]: Parsing workflow as WDL draft-2; [2018-08-27 02:04:08,86] [info] MaterializeWorkflowDescriptorActor [967af8b6]: Call-to-Backend assignments: wgbs.flatten_ -> singularity; [2018-08-27 02:04:12,30] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-204674333468 [967af8b6]: Starting wgbs.flatten_; [2018-08-27 02:04:13,48] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: ; mkdir -p mapping; cat /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/write_lines_8f61fd340a04ccd930e243709dfb1bed.tmp | xargs -I % ln -s % mapping; ls mapping; [2018-08-27 02:04:13,50] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: executing: chmod u+x /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script && \; singularity \; exec \; gemBS.simg \; /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/967af8b6-0d68-44c4-b04e-204674333468/call-flatten_/execution/script &; echo $?; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: job id: 0; [2018-08-27 02:04:16,88] [info] DispatchedConfigAsyncJobExecutionActor [967af8b6wgbs.flatten_:NA:1]: Status change from - to Done; [2018-08-27 02:04:19,50] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b04e-2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:4481,pipeline,pipelines,4481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:735); at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:3174,Pipeline,PipelinesApiRequestWorker,3174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,"nfig {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Workflow 28f84555-6e06-41be-891b-84de0f35ee74 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 10. 15: Gsutil failed: failed to upload logs for ""gs://xxx/cromwell-execution/wf_hello/28f84555-6e06-41be-891b-84de0f35ee74/call-hello/"": cp failed: gsutil -h Content-type:text/plain -q -m cp /var/log/google-genomics/*.log gs://xxx/cromwell-execution/wf_hello/28f84555-6e06",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:2676,Pipeline,Pipelines,2676,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,1,['Pipeline'],['Pipelines']
Deployability,"nfiguring it at the Cromwell level, so e.g. any user of Terra (or any other hosted Cromwell with PAPIv2 backend) could get usage reports without having to configure anything. The metrics are reported in their GCP project, so a user gets automatic access to them as long as they're a viewer. We could also easily expose a link to workflow- and task-level reports in Job Manager UI, so they will be literally point-and-click away. Each timepoint is designed to be self-sufficient, as it is labeled with:; - Cromwell-specific values, such as workflow ID, task call name, index and attempt.; - GCP instance values such as instance name, zone, number of CPU cores, total memory and disk size. Here's an example graph of cpu/memory/disk utilization for one of our production workflows, as it is running right now - one can already see we could probably save ~40% of the cost:; <img width=""1869"" alt=""screen shot 2019-01-02 at 4 43 20 pm"" src=""https://user-images.githubusercontent.com/137337/50614108-c0e6e800-0ead-11e9-9ef4-02029725a44c.png"">. Reporting itself costs very little if anything at all, because Stackdriver provides a generous free tier worth ~65K instance-hours each month, and ~$0.0006 per instance-hour after that (at the current rate of 5 metric points reported each minute). @kshakir suggested using a ""vendor-neutral"" reporting library such as [Micrometer.io](http://micrometer.io/), although I have reservations around that - mostly because that may require additional setup and we want this to ""just work""; but also because the implementation is currently PAPIv2-specific anyway, so it is already non-vendor agnostic. Likewise, one could export metrics from Stackdriver monitoring if they wanted to. But we're open to the idea. Finally, I haven't added any tests yet, as it's unclear in which shape or form (if at all) you'd like to integrate this code. Thanks in advance for any feedback!; ~[@broadinstitute/wintergreen](https://github.com/orgs/broadinstitute/teams/wintergreen) team.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510:4131,integrat,integrate,4131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510,1,['integrat'],['integrate']
Deployability,"nfusion, like tabix can't tell a file wasn't already gzipped:; ```; ValueError: Unexpected tabix input: /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/cromwell_work/cromwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-prep_samples/shard-0/execution/bedprep/cleaned-8539016497173364825.gz; ```; or bwa can't find all the other associated indices:; ```; bwa mem /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/cromwell_work/cromwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-alignment/shard-1/wf-alignment.cwl/96d7b606-e0fe-4305-a586-e0fc4acf76f8/call-process_alignment/shard-0/inputs/1628767813 [...]. [E::bwa_idx_load_from_disk] fail to locate the index files; ```; Is it expected to lose the original input file names when passing through the pipeline. A lot of tools are sensitive to these and this might be the underlying issue. Regarding the configuration, without `http {}` in under `engine -> filesystems` I get a complaint about it not being supported, even with `http {}` under `backend -> providers -> Local -> config -> filesystems`:; ```; java.lang.IllegalArgumentException: Either https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa exists on a filesystem not supported by this instance of Cromwell, or a failure occurred while building an actionable path from it. Supported filesystems are: LinuxFileSystem. Failures: LinuxFileSystem: Cannot build a local path from https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa (RuntimeException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesystems; at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:211); at cromw",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320:1262,configurat,configuration,1262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320,1,['configurat'],['configuration']
Deployability,"ng as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/92f43ce5f010fa03daeb2625a3f9c1b1be80cca5/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.13.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/rnaseq-workflow/steps/prepare_sample.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/variantcall_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6410:1731,integrat,integrationTestCases,1731,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6410,1,['integrat'],['integrationTestCases']
Deployability,"ng the root of this repository to `/code`. Since we cannot have volumes in circle, we instead just test the added code to the container, also located at `/code`. Since the purpose of this container is to be a clean slate with sbt, scala to build cromwell, the primary test that is important is ensuring that `sbt assembly` runs successfully without a hitch. Any other ""docker"" tests for the actual cromwell (not building it) would not belong here, but with Docker containers meant to deploy cromwell proper. # Where does it deploy?; The container will deploy to the `CONTAINER_NAME` defined in the circle environment settings (or in the circle config at `.circleci/config.yml`. By default, it will be tagged with the commit first 10 characters, and then latest, and you can change this behavior by defining `DOCKER_TAG` either in the config or circle environment (I don't see a reason to do this). Note that deploy is ONLY set up to happen on pushes to master (and you can change this to also be develop, if you choose, or to be both and then to deploy to tags `<branch>-<commit>` or something like that. ## Background; This was first done at the repo [vsoch/cromwell](https://github.com/vsoch/cromwell/pull/1) to test since I can't set it up for the broadinstitute. The (finally) working test is at [https://circleci.com/gh/vsoch/cromwell/11](https://circleci.com/gh/vsoch/cromwell/11). I forgot that I can't have volumes, so it took me many tries to remember this, derp :P . When adding to the repository here, the following additional work will be needed for setup:. - Turn on the repository to build at circleci. The first build, since there is no `.circici/config.yml` will probably just yell at you for having ""Version 1.0"" or not finding a config.; - You will want to turn on building forked pull requests in the settings; - Under environment variables, define the following:; - `DOCKER_USER` should be the user to authenticate pushing; - `DOCKER_PASS` password for that user (**important** do ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015:1263,deploy,deploy,1263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015,2,['deploy'],['deploy']
Deployability,"ngFSM$$super$processEvent(MaterializeWorkflowDescriptorActor.scala:114) at akka.actor.LoggingFSM.processEvent(FSM.scala:799) at akka.actor.LoggingFSM.processEvent$(FSM.scala:781) at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.processEvent(MaterializeWorkflowDescriptorActor.scala:114) at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:657) at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651) at akka.actor.Actor.aroundReceive(Actor.scala:513) at akka.actor.Actor.aroundReceive$(Actor.scala:511) at cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor.aroundReceive(MaterializeWorkflowDescriptorActor.scala:114) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:527) at akka.actor.ActorCell.invoke(ActorCell.scala:496) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257) at akka.dispatch.Mailbox.run(Mailbox.scala:224) at akka.dispatch.Mailbox.exec(Mailbox.scala:234) at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). This is the command I use to start Cromwell:. gcloud alpha genomics pipelines run \ --pipeline-file wdl_runner/wdl_pipeline.yaml \ --zones us-east1-c \ --memory 5 \ --logging ""${GATK_OUTPUT_DIR}/logging"" \ --inputs-from-file WDL=""${GATK_GOOGLE_DIR}/FullSomaticPipeline.wdl"" \ --inputs-from-file WORKFLOW_INPUTS=""${GATK_GOOGLE_DIR}/FullSomaticPipeline_public_urls.json"" \ --inputs-from-file WORKFLOW_OPTIONS=""${GATK_GOOGLE_DIR}/FullSomaticPipeline.options.json"" \ --inputs WORKSPACE=""${GATK_OUTPUT_DIR}/workspace"" \ --inputs OUTPUTS=""${GATK_OUTPUT_DIR}/outputs"". Attempting to run this pipeline: https://github.com/gatk-workflows/gatk4-somatic-with-preprocessing. Any help here would be appreciated, and apologies if this is the wrong forum for this issue.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4482:2494,pipeline,pipelines,2494,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4482,3,['pipeline'],"['pipeline', 'pipeline-file', 'pipelines']"
Deployability,"ng` workflow, when running the `ApplyBQSR` task, which is run in parallel over some calculated intervals. The full error trace I get is:. ```; 2018-10-23 02:39:07,631 cromwell-system-akka.dispatchers.backend-dispatcher-53345 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(6d97fef4)GPPW.ApplyBQSR:15:1]: Error attempting to Execute; software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: null; Status Code: 429; Request ID: cfc6e34e-d66c-11e8-be0b-dd778498cf15); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:1372,pipeline,pipeline,1372,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['pipeline'],['pipeline']
Deployability,"nge https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; This is a remark on https://github.com/broadinstitute/cromwell/blob/master/docs/tutorials/HPCSlurmWithLocalScratch.md there is a feature on slum config to edit the sbatch command. You could add in a find and replace in the config to do the same as the tutorial. you can skip the first part of the tutorial by editing the slurm backend config (somewhat hotpatching the scripts on submission time). old submit ; submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". new submit for slurm auto configured job dir: ; submit = """"""; perl -i.bak -wpe 's/^tmpDir=.*/tmpdir=""\$TMPDIR""/g' ${script} && \; sbatch -J ${job_name} --tmp=${disk} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". new submit for /genomics/local/ (not tested tough): ; submit = """"""; perl -i.bak -wpe 's/^tmpDir=.*/tmpdir=""$(mkdir -p ""\/genomics_local\/\$PID_\$HOSTNAME""\/"" && echo ""\/genomics_local\/\$PID_\$HOSTNAME""\/""/g' ${script} && \; sbatch -J ${job_name} --tmp=${disk} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; <!-- This is a clear feature cant you see -->. <!-- Which backend are you running? -->; The backend I'm running on is Slurm hpc with a version 1.0 workflow. This alternative workflow has its downsides but also benefits it is up to the hpc(user) to decide what works best in their own situation. ; <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7357:2109,configurat,configuration,2109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7357,1,['configurat'],['configuration']
Deployability,"ngelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/92f43ce5f010fa03daeb2625a3f9c1b1be80cca5/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.28).; You might want to review and update them manually.; ```; centaur/src/main/resources/standardTestCases/local_bourne/local_bourne.wdl; core/src/test/resources/hello_goodbye_scattered_papiv2.json; docs/developers/bitesize/workflowParsing/forkjoin_graph.svg; docs/developers/bitesize/workflowParsing/wdlToWdlom_hermes.svg; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6429:2081,update,update,2081,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6429,1,['update'],['update']
Deployability,"ngelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/b48aba70ec793405c98788a322d160987ba51d3e/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.27).; You might want to review and update them manually.; ```; dockerHashing/src/main/scala/cromwell/docker/local/DockerCliClient.scala; docs/developers/bitesize/ci/CaaS_DEV_CD.svg; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; ```; </details>; <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; u",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6281:2081,update,update,2081,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6281,1,['update'],['update']
Deployability,"ngularity container vs a docker container - but if that's not really the case I've definitely been overcomplicating the matter. I'll admit that I've never been comfortable in my understanding of Singularity. If you are using a container, it definitely is an ""either / or"" in the sense that getting one working inside the other is pretty challenging. The reason a Dockerized cromwell doesn't work on a host (to submit jobs to other docker or singularity containers) is because of having the docker/singularity submit come from inside the container. We don't really want to do that anyway, because there is a double dependency. But on the other hand, we want to provide reproducible solutions, meaning that things are container based. In an ideal setup, I would have some (still container based) cromwell acting as more of a docker-compose setup, and issuing commands to other containers. Ideally there would be one maintained Docker container for a step in a pipeline, and then if it's run on an HPC resource (where you can't have docker) it would just be dumped into singularity (`docker://<username>/<reponame>`). But this case is a little different - I'm just talking about the cromwell ""plugin"". I don't actually understand why this is necessary, at least given that singularity containers can act like executable. If I want to run a python script, I run it in the command section, as an executable. I don't require a python plugin. Now given that Singularity changes so that we want to take advantage of more of the instance commands (e.g., we can start, stop, get a status) this might make it more like docker and warrant a plugin. But for now, it's not quite there, and making a plugin would just be a really fancy interface to run an executable. Does this make sense?. > @vsoch you're obviously well versed in all things Singularity - do you see any utility to defining the use of a Singularity container in the WDL (ie no matter what this task should always use Singularity) or is it going to",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685:1074,pipeline,pipeline,1074,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685,1,['pipeline'],['pipeline']
Deployability,"ngularity on HPC. The major issues include:; * We run N `singularity build`s, for a scatter over N items, which wastes time and CPU, and writing N large images to the filesystem simultaneously will presumably challenge the filesystem.; * We have to store N `.sif` images, which wastes space while the job is running; * We have to delete the image after each `singularity build`. My first proposed solution was #4673, which would solve the problem but require a pull request to introduce a new hook to Cromwell. And it doesn't look like the Cromwell team have been able to prioritise this. . My new thought is that we could use file locks (e.g. `flock` on linux) to deal with this issue, so that the first worker to run will create a file lock, then all subsequent workers will encounter that lock, and wait until it's removed before attempting to build or run the image. For example, we currently recommend this `submit-docker` configuration:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Build the Docker image into a singularity image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I'm instead proposing this. Note the use of a single shared image directory (`/singularity_cache` in this example), and the use of `flock` to ensure the submit scripts aren't competing with each other:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Determine the filepath to the image; DOCKER_NAME=$(sed -e 's/[^A",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063:1149,install,installed,1149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063,1,['install'],['installed']
Deployability,"nnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-13",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:3433,pipeline,pipelines,3433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['pipeline'],['pipelines']
Deployability,"nning certain highly parallel WDL workflows, I'm getting the error `cromwell.core.CromwellFatalException: software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: null; Status Code: 429; Request ID: cffe6e45-d66c-11e8-a1df-05402551b0ba)`. The specific case where this happens is in the `gatk3-data-processing` workflow, when running the `ApplyBQSR` task, which is run in parallel over some calculated intervals. The full error trace I get is:. ```; 2018-10-23 02:39:07,631 cromwell-system-akka.dispatchers.backend-dispatcher-53345 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(6d97fef4)GPPW.ApplyBQSR:15:1]: Error attempting to Execute; software.amazon.awssdk.services.batch.model.BatchException: Too Many Requests (Service: null; Status Code: 429; Request ID: cfc6e34e-d66c-11e8-be0b-dd778498cf15); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(Retry",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:1035,pipeline,pipeline,1035,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['pipeline'],['pipeline']
Deployability,"no worries, thanks for the update",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/345#issuecomment-170765478:27,update,update,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/345#issuecomment-170765478,1,['update'],['update']
Deployability,"nputs\"": [\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""Optional - location of the GRIDSS assembly BAM. This file will be created by GRIDSS.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--assembly\""\n },\n \""default\"": \"".assembly.bam\"",\n \""id\"": \""#gridss-2.9.4.cwl/assembly\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional - BED file containing regions to ignore\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--blacklist\""\n },\n \""id\"": \""#gridss-2.9.4.cwl/blacklist\""\n },\n {\n \""type\"": \""string\"",\n \""doc\"": \""portion of 6 sigma read pairs distribution considered concordantly mapped. Default: 0.995\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--concordantreadpairdistribution\""\n },\n \""default\"": \""0.995\"",\n \""id\"": \""#gridss-2.9.4.cwl/concordantreadpairdistribution\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional - configuration file use to override default GRIDSS settings.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--configuration\""\n },\n \""id\"": \""#gridss-2.9.4.cwl/configuration\""\n },\n {\n \""type\"": [\n \""null\"",\n \""boolean\""\n ],\n \""doc\"": \""Optional - use the system version of bwa instead of the in-process version packaged with GRIDSS\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--externalaligner\""\n },\n \""default\"": false,\n \""id\"": \""#gridss-2.9.4.cwl/externalaligner\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""Optional - location of GRIDSS jar\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--jar\""\n },\n \""default\"": \""/opt/gridss/gridss-2.9.4-gridss-jar-with-dependencies.jar\"",\n \""id\"": \""#gridss-2.9.4.cwl/jar\""\n },\n {\n \""type\"": \""boolean\"",\n \""doc\"": \""zero-based assembly job index (only required when performing parallel assembly across multiple computers)\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--jobindex\""\n },\n \""default\"": false,\n \""id\"": \""#gridss-2.9.4.cwl/jobindex\""\n },\n {\n \""type\"": \""boolean\"",\n \""doc\"": \""total number of assembly jobs (on",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:73552,configurat,configuration,73552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['configurat'],['configuration']
Deployability,nt.parseHTTP(HttpClient.java:678); at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1587); at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492); at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:347); at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:19,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:3259,Pipeline,PipelinesApiRequestWorker,3259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,"nt; [INFO 2020-08-04 23:40:18 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain_env. real	0m0.126s; user	0m0.014s; sys	0m0.001s; [INFO 2020-08-04 23:40:18 UTC] Downloading toolchain from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain.tar.xz. real	0m11.907s; user	0m0.428s; sys	0m1.039s; [INFO 2020-08-04 23:41:17 UTC] Configuring environment variables for cross-compilation; [INFO 2020-08-04 23:41:17 UTC] Configuring installation directories; [INFO 2020-08-04 23:41:17 UTC] Updating container's ld cache; [INFO 2020-08-04 23:41:20 UTC] Configuring kernel sources; [INFO 2020-08-04 23:41:42 UTC] Modifying kernel version magic string in source files; [INFO 2020-08-04 23:41:42 UTC] Running Nvidia installer. ERROR: The kernel module failed to load, because it was not signed by a key; that is trusted by the kernel. Please try installing the driver; again, and set the --module-signing-secret-key and; --module-signing-public-key options on the command line, or run the; installer in expert mode to enable the interactive module signing; prompts. ERROR: Unable to load the kernel module 'nvidia.ko'. This happens most; frequently when this kernel module was built against the wrong or; improperly configured kernel sources, with a version of gcc that; differs from the one used to build the target kernel, or if another; driver, such as nouveau, is present and prevents the NVIDIA kernel; module from obtaining ownership of the NVIDIA GPU(s), or no NVIDIA; GPU installed in this system is supported by this NVIDIA Linux; graphics driver release. Please see the log entries 'Kernel module load error' and 'Kernel; messages' at the end of the file; '/usr/local/nvidia/nvidia-installer.log' for more information. ERROR: Installation has failed. Please see the file; '/usr/local/nvidia/nvidia-installer.log' for details. You may find; suggestions on fixing installation problems in the README available; on the Linux driver download pa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:5155,install,installing,5155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,2,['install'],"['installer', 'installing']"
Deployability,nternal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseS,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:2753,pipeline,pipeline,2753,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,ntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.cromwell$backend$async$AsyncBackendJobExecutionActor$$robustExecuteOrRecover(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.backend.async.AsyncBackendJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:2158,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"nting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language; - `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language; - `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language. You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/broadinstitute/cromwell/network/alerts). </details>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5941:5266,upgrade,upgrade,5266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5941,33,['upgrade'],['upgrade']
Deployability,"nux-headers\n+ NVIDIA_DRIVER_VERSION=450.51.06\n+ NVIDIA_DRIVER_MD5SUM=\n+ NVIDIA_INSTALL_DIR_HOST=/var/lib/nvidia\n+ NVIDIA_INSTALL_DIR_CONTAINER=/usr/local/nvidia\n+ ROOT_MOUNT_DIR=/root\n+ CACHE_FILE=/usr/local/nvidia/.cache\n+ LOCK_FILE=/root/tmp/cos_gpu_installer_lock\n+ LOCK_FILE_FD=20\n+ set +x\n[INFO 2021-02-22 23:09:17 UTC] PRELOAD: false\n[INFO 2021-02-22 23:09:17 UTC] Running on COS build id 13310.1209.10\n[INFO 2021-02-22 23:09:17 UTC] Data dependencies (e.g. kernel source) will be fetched from https://storage.googleapis.com/cos-tools/13310.1209.10\n[INFO 2021-02-22 23:09:17 UTC] Getting the kernel source repository path.\n[INFO 2021-02-22 23:09:17 UTC] Obtaining kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n[INFO 2021-02-22 23:09:19 UTC] Downloading kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n\nreal\t0m0.072s\nuser\t0m0.013s\nsys\t0m0.006s\n[INFO 2021-02-22 23:09:19 UTC] Checking if this is the only cos-gpu-installer that is running.\n[INFO 2021-02-22 23:09:19 UTC] Checking if third party kernel modules can be installed\n[INFO 2021-02-22 23:09:19 UTC] Checking cached version\n[INFO 2021-02-22 23:09:19 UTC] Cache file /usr/local/nvidia/.cache not found.\n[INFO 2021-02-22 23:09:19 UTC] Did not find cached version, building the drivers...\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer ... \n[INFO 2021-02-22 23:09:19 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n\nreal\t0m1.891s\nuser\t0m0.181s\nsys\t0m0.449s\n[INFO 2021-02-22 23:09:21 UTC] Setting up compilation environment\n[INFO 2021-02-22 23:09:21 UTC] Obtaining tool",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:3591,install,installer,3591,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['install'],['installer']
Deployability,"ny user specific arguments (e.g., custom binds) and singularity arguments? (e.g., debug?) I don't know if this is ""how it's supposed to look"" but here is what a test run spits out for me:. ```bash; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; [2018-08-27 02:03:57,30] [info] Running with database db.url = jdbc:hsqldb:mem:3cd4a928-7743-4252-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-27 02:04:06,71] [info] Metadata summary refreshing every 2 seconds.; [2018-08-27 02:04:06,81] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 sec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:1474,pipeline,pipelines,1474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,1,['pipeline'],['pipelines']
Deployability,"o every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect this, and contribute to; > Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I; > want this to plug into Kubernetes,"" etc. The backends for HPC are going to; > be good to go with just a SLURM or SGE backend, and then commands to load; > and run/exec a Singularity container. When the time comes and Singularity; > supports services, then we can start to develop (I think) the singularity; > backend configuration for cromwell, with clean commands to get statuses,; > start and stop, and otherwise integrate into the software. You guys seem; > pretty busy, so likely your best bet would be to just wait, because the; > community is going in that direction anyway.; >; > The other representation is to rethink this. An approach that I like is to; > move away from micro managing the workflow / software, and to set; > requirements for the data. If you set standard formats (meaning everything; > from the organization of files down to the headers of a data file) on the; > data itself, then the software gets built around that. A researcher can; > have confidence that the data he is collecting will work with software; > because it's validated to the format. The developers can have confidence; > their tools will work with data because of that same format. A new graduate; > student knows how to develop a new tool because there are nicely defined; > rules. A good example is to look at the BIDS (bra",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:8398,configurat,configuration,8398,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,2,"['configurat', 'integrat']","['configuration', 'integrate']"
Deployability,"o review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/goog",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1038,update,updates,1038,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,1,['update'],['updates']
Deployability,"o use your Jira tracker, it let me log in but told me I don't have permission to see anything or do anything; 2: https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team no longer exists, and the support staff respond to questions with ""we only answer GATK isues""; 3: I am using womtool 65 and Cromwell 62. I get the same failure in both, which is that if the first line of my file is:. `version development`. As per the [WDL specifications](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#versioning) I get the error:. `ERROR: Finished parsing without consuming all tokens.`. If I do not include that line, then I get this error:. ```; Expected rbrace, got Directory.; Directory	OutputDir; ```. Does Cromwell support WDL versions pther than the default? if so, how do I specify which version to use?. Thank you,; ###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6438:1920,configurat,configuration,1920,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6438,1,['configurat'],['configuration']
Deployability,"o what they do (e.g., `dockersave`. Some quick learnings:. Let's say we create a defaults section that looks like this, to set some shared environment variables, working directory, docker container, anything we want really:. ```; defaults: &defaults; docker:; - image: docker:18.01.0-ce-git; working_directory: /tmp/src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., ch",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:2029,install,install,2029,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,2,"['deploy', 'install']","['deploy', 'install']"
Deployability,"o-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. I followed this format but got this error; ```; [2022-11-20 18:17:16,88] [warn] Failed to build PipelinesApiConfigurationAttributes on attempt 1 of 3, retrying.; cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$$anon$1: Google Pipelines API configuration is not valid: Errors:; Attempt to decode value on failed cursor: DownField(manifestFormatVersion); at cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$.apply(PipelinesApiConfigurationAttributes.scala:307); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.defaultBuildAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:32); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.$anonfun$papiAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:34); at scala.util.Try$.apply(Try.scala:210); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.cromwell$backend$google$pipelines$common$PipelinesApiBackendLifecycleActorFactory$$build$1(PipelinesApiBackendLifecycleActorFactory.scala:109); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.robustBuildAttributes(PipelinesApiBackendLifecycleActorFactory.scala:120); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.<init>(PipelinesApiBackendLifecycleActorFactory.scala:34); at cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:10); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:1329,Pipeline,PipelinesApiBackendLifecycleActorFactory,1329,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,1,['Pipeline'],['PipelinesApiBackendLifecycleActorFactory']
Deployability,"o.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockConte",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001:1314,Pipeline,PipelinesApiAsync,1314,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001,1,['Pipeline'],['PipelinesApiAsync']
Deployability,"o] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2018-08-27 02:04:27,06] [info] Gracefully shutdown sentry threads.; [2018-08-27 02:04:27,06] [info] Shutdown finished.; ```; I don't see any strings in the outputs above, but I do see files in the ""mapping"" folder of execution:. ```bash; $ tree cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/execution; cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/execution; â”œâ”€â”€ mapping; â”‚Â Â  â”œâ”€â”€ flowcell_1_1_1.fastq.gz -> /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/inputs/-1629611677/flowcell_1_1_1.fastq.gz; â”‚Â Â  â””â”€â”€ flowcell_1_1_2.fastq.gz -> /home/vanessa/Documents/Dropbox/Code/labs/cherry/pipelines/wgbs-pipeline/cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/inputs/-1629611677/flowcell_1_1_2.fastq.gz; â”œâ”€â”€ rc; â”œâ”€â”€ script; â”œâ”€â”€ script.submit; â”œâ”€â”€ stderr; â”œâ”€â”€ stderr.submit; â”œâ”€â”€ stdout; â”œâ”€â”€ stdout.submit; â””â”€â”€ write_lines_b474737d24db6afd91c22a47b2b69e4f.tmp. 1 directory, 10 files; ```; and the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files:. ```; $ cat cromwell-executions/wgbs/e56c969e-e2fe-4ba1-89d5-c12faf3139d4/call-flatten_/execution/stdout; flowcell_1_1_1.fastq.gz; flowcell_1_1_2.fastq.gz; ```; Let me know if this looks correct? What you are looking for? Completely off base? I think this will be easier with respect to getting the process ids for singularity instances with the next version, when there",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:8657,pipeline,pipelines,8657,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,2,['pipeline'],"['pipeline', 'pipelines']"
Deployability,ob was stopped before the command finished. PAPI error code 10. 11: Docker run failed: command failed: usage: fastqc_docker.py [-h] -r READ -o OUTPUT_DIR [-c CONTAMINANTS]; cromwell_1 | [-a ADAPTERS] [-l LIMITS] [-f FORMAT] [-n NO_GROUP]; cromwell_1 | [-e EXTRA_OPTIONS]; cromwell_1 | fastqc_docker.py: error: argument -r/--read is required; cromwell_1 | . See logs at gs://genovic-cromwell/cromwell-execution/trio/f5454139-c51d-4d04-ae0a-9b9d4ce650aa/call-germline_variant_calling/shard-0/germline_variant_calling/5d4c4459-a91c-4d3b-8ca4-b98457134750/call-fastqc/shard-0/; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); cromwell_1 | at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.$,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:2057,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2057,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"obExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from - to Initializing; [2019-05-22 19:19:27,42] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Initializing to Running; ...; [2019-05-22 19:21:09,63] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Initializing to Running; ...; [2019-05-22 19:22:43,83] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:34:19,31] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: Status change from Running to Succeeded; ...; [2019-05-22 19:42:10,31] [error] WorkflowManagerActor Workflow 3997371c-9513-4386-a579-a72639c6e960 failed (during ExecutingWorkflowState): ; cromwell.engine.io.IoAttempts$EnhancedCromwellIoException: [Attempted 1 time(s)] - IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; C",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:10658,Pipeline,Pipeline,10658,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,1,['Pipeline'],['Pipeline']
Deployability,"ocal/nvidia\n+ ROOT_MOUNT_DIR=/root\n+ CACHE_FILE=/usr/local/nvidia/.cache\n+ LOCK_FILE=/root/tmp/cos_gpu_installer_lock\n+ LOCK_FILE_FD=20\n+ set +x\n[INFO 2021-02-22 23:09:17 UTC] PRELOAD: false\n[INFO 2021-02-22 23:09:17 UTC] Running on COS build id 13310.1209.10\n[INFO 2021-02-22 23:09:17 UTC] Data dependencies (e.g. kernel source) will be fetched from https://storage.googleapis.com/cos-tools/13310.1209.10\n[INFO 2021-02-22 23:09:17 UTC] Getting the kernel source repository path.\n[INFO 2021-02-22 23:09:17 UTC] Obtaining kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n[INFO 2021-02-22 23:09:19 UTC] Downloading kernel_info file from https://storage.googleapis.com/cos-tools/13310.1209.10/kernel_info\n\nreal\t0m0.072s\nuser\t0m0.013s\nsys\t0m0.006s\n[INFO 2021-02-22 23:09:19 UTC] Checking if this is the only cos-gpu-installer that is running.\n[INFO 2021-02-22 23:09:19 UTC] Checking if third party kernel modules can be installed\n[INFO 2021-02-22 23:09:19 UTC] Checking cached version\n[INFO 2021-02-22 23:09:19 UTC] Cache file /usr/local/nvidia/.cache not found.\n[INFO 2021-02-22 23:09:19 UTC] Did not find cached version, building the drivers...\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer ... \n[INFO 2021-02-22 23:09:19 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n[INFO 2021-02-22 23:09:19 UTC] Downloading GPU installer from https://storage.googleapis.com/nvidia-drivers-us-public/nvidia-cos-project/85/tesla/450_00/450.51.06/NVIDIA-Linux-x86_64-450.51.06_85-13310-1209-10.cos\n\nreal\t0m1.891s\nuser\t0m0.181s\nsys\t0m0.449s\n[INFO 2021-02-22 23:09:21 UTC] Setting up compilation environment\n[INFO 2021-02-22 23:09:21 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/13310.1209.10/toolchain_env\n[INFO 2021-02-22 23:09:21 UTC] Downloading toolchain_env file fr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6195:3696,install,installed,3696,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6195,1,['install'],['installed']
Deployability,"ocumentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. When trying to configure metadata-archive in cromwell server by adding the configuration below:; ```; archive-metadata {; # A filesystem able to access the specified bucket:; filesystems {; gcs {; # A reference to the auth to use for storing and retrieving metadata:; auth = ""user-service-account""; }; }. # Which bucket to use for storing the archived metadata; bucket = ""{{ backend_bucket }}""; }; ```. when the user-service-account auth is declared up in the configuration :; ```; google {. application-name = ""cromwell"". auths = [; {; name = ""user-service-account""; scheme = ""user_service_account""; }; ]; }; ```; We got the following error in Cromwell server initialization :; cromwell_1 | [ERROR] [06/21/2023 11:55:25.094] [cromwell-system-akka.actor.default-dispatcher-30] [akka://cromwell-system/user] Failed to parse the archive-metadata config:; cromwell_1 | Failed to construct archiver path builders from factories (reason 1 of 1): Missing parameters in workflow options: user_service_account_json; cromwell_1 | akka.actor.ActorInitializationException: akka://cromwell-system/user/cromwell-service/ServiceRegistryActor/MetadataService: exception during creation; cromwell_1 | 	at akka.actor.ActorInitializationException$.apply(Actor.scala:202); cromwell_1 | 	at akka.actor.ActorCell.create(ActorCell.scala:698); cromwell_1 | 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:549); cromwell_1 | 	at akka.actor.ActorCell.systemInvoke(Act",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7171:1136,configurat,configuration,1136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7171,1,['configurat'],['configuration']
Deployability,"od -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804:1785,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1785,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"od -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFun",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:2063,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2063,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"of 3, retrying.; cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$$anon$1: Google Pipelines API configuration is not valid: Errors:; Attempt to decode value on failed cursor: DownField(manifestFormatVersion); at cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$.apply(PipelinesApiConfigurationAttributes.scala:307); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.defaultBuildAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:32); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.$anonfun$papiAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:34); at scala.util.Try$.apply(Try.scala:210); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.cromwell$backend$google$pipelines$common$PipelinesApiBackendLifecycleActorFactory$$build$1(PipelinesApiBackendLifecycleActorFactory.scala:109); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.robustBuildAttributes(PipelinesApiBackendLifecycleActorFactory.scala:120); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.<init>(PipelinesApiBackendLifecycleActorFactory.scala:34); at cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:10); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at cromwell.engine.backend.BackendConfigurationEntry.$anonfun$asBackendLifecycleActorFactory$1(BackendConfiguration.scala:13); at scala.util.Try$.apply(Try.scala:210); at cromwell.engine",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:1542,pipeline,pipelines,1542,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,1,['pipeline'],['pipelines']
Deployability,"of having multiple processes access the same embedded DB, is to research spinning up a background daemon db process, which do support multiple connections. Links to consider when defining acceptance criteria are below. . Re: our existing/proposed HSQLDB usage; - Cromwell's `database.metadata` and `database.engine` when absent both [fall back to the root `database` stanza.](https://github.com/broadinstitute/cromwell/blob/088e12d97dd18f463e6a387a6ffb002d9725cbe4/services/src/main/scala/cromwell/services/ServicesStore.scala#L12); - [""This allows each instance of a database object to use a clean, and different, in memory database.""](https://github.com/broadinstitute/cromwell/blob/a8a605ed1f2f2d2de2db9b05c395a2c87ebfc295/database/sql/src/main/scala/cromwell/database/sql/SqlDatabase.scala#L17-L39); - [""only one Java process at a time can make in-process connections to a given _file:_ database""](http://hsqldb.org/doc/guide/running-chapt.html#rgc_inprocess); - [""Several different programs can connect to the server and retrieve or update information.""](http://hsqldb.org/doc/2.0/guide/running-chapt.html#rgc_server_modes). Re: SQLite; - [""You can't add a constraint to existing table in SQLite""](https://stackoverflow.com/a/15498225); - [""use caution: this locking mechanism might not work correctly if the database file is kept on an NFS filesystem""](https://www.sqlite.org/faq.html#q5); - We often change our table uniqueness using [Liquibase](https://github.com/broadinstitute/cromwell/pull/3553/files#diff-76feec217bb5aaed111d4c3c11ead546). HSQLDB stopped supporting [unique indexes years ago](http://www.hsqldb.org/doc/1.8/guide/ch09.html#create_index-section), so Cromwell only uses their cousin the [unique constraint](https://community.oracle.com/thread/1033157). Meanwhile SQLite does not allow adding/dropping a unique constraint without [copying the whole table](https://stackoverflow.com/a/42013422) but does support [adding/dropping a unique index](https://www.sqlite.org/lang_crea",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194:2743,update,update,2743,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3786#issuecomment-398204194,1,['update'],['update']
Deployability,"of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 25000. # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. #docker-image-cache-manifest-file = ""gs://xxxxx-xxxxx/xxxxx.json"". # Number of workers to assign to PAPI requests; request-workers = 3. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # pipeline-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:2269,configurat,configuration,2269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['configurat'],['configuration']
Deployability,ogle.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExe,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1544,pipeline,pipelines,1544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['pipeline'],['pipelines']
Deployability,oh i see that's great -- i never liked positional args. thanks! we'll plan to update docs accordingly -- when do you expect to release this/29?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-313215779:78,update,update,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2369#issuecomment-313215779,2,"['release', 'update']","['release', 'update']"
Deployability,oinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:4085,pipeline,pipeline,4085,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,1,['pipeline'],['pipeline']
Deployability,"oinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$GoogleJsonException: Request contains an invalid argument.; ... 21 common frames omitted; [2021-08-13 10:45:07,42] [warn] PAPI request worker had 1 failures making 1 requests:; Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; [2021-08-13 10:45:07,54] [info] WorkflowManagerActor: Workflow a15c46b7-5f93-46d6-94a2-28f656914866 failed (during ExecutingWorkflowState): cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:51); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:5900,pipeline,pipelines,5900,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['pipeline'],['pipelines']
Deployability,oinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.ama,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:4212,pipeline,pipeline,4212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,1,['pipeline'],['pipeline']
Deployability,okay I've just updated the circle badge to be cromwell on broadinstitute (which doesn't technically exist yet) but the purpose was to add all the commits where I failed like a struggling panda to get the testing setup :) All is working now!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-413851686:15,update,updated,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-413851686,1,['update'],['updated']
Deployability,"okay sorry I was confused then - @geoffjentry suggested that the backend.conf was part of the cromwell base:. > The idea is that this would be in the Cromwell configuration and not per-workflow (but see below). In general that makes sense because a lot of the HPC-style use cases we see people never want to use actual Docker. As opposed to a workflow or pipeline that uses it. For example, here is the pipeline that I was working on that has a `backend.conf` that runs Singularity:. https://github.com/vsoch/wgbs-pipeline/pull/1/files#diff-f6baca157827c4888c394eab694e000c. But this is not a part of cromwell, or relevant to this repo - it's just a configuration file provided with the workflow. I was under the impression that we wanted to write something that would be integrated into cromwell to interact with Singularity, and not a configuration file provided with a particular pipeline (such as the wgbs in the example above). Do you mean that there is a template folder (or some other docs) where the ""suggested singularity backend"" would be provided? Something different? What am I missing?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958:159,configurat,configuration,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-413205958,8,"['configurat', 'integrat', 'pipeline']","['configuration', 'integrated', 'pipeline']"
Deployability,"okay, well I just updated the linked repo to have this ""hello"" example. I've concluded that singularity makes absolutely no sense as a backend. it's more logical as just a binary that is run (Local) or with other backend (e.g., slurm). See here --> https://github.com/vsoch/wgbs-pipeline/blob/add/singularity/docs/pages/docs/tutorial/getting-started/index.md#step-4-run-test-case for the test case, and then the example wdl I'll just show you here, because it's stupid. It's just running a command, lol. ```; task dinosaur {; String singularity_container = ""shub://vsoch/hello-world""; output {; String roar = read_string(stdout()); }; command {; singularity --silent \; run \; ${singularity_container}; }; }. workflow wf_hello {; call dinosaur; output {; dinosaur.roar; }; }; ```; I think trying to use cromwell just makes it a lot **more** complicated. But I guess it's useful / helping users in other ways, so it's worth it to show how to run a singularity container as a command? /shrug. I don't totally follow what you are saying, to be honest. cromwell running singularity is just the same as cromwell running anything else, and I'm convinced (after this testing) there is no special use case. Docker has a backend and workflow runtime (because you run a docker container, the thing in a workflow) Singularity you are trying to shove into a backend or a workflow, and it has to be one or the other. It's not really making sense as a backend because it's more like an executable. I think before coming up with tools around something, it would be nice to have a clear definition of how singularity even fits in here to be special or different from any other binary.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416389694:18,update,updated,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416389694,2,"['pipeline', 'update']","['pipeline', 'updated']"
Deployability,"ome content' > {name}""); items.append({""name"": f""item-{item}"", ""path"": name}). with open(""results.json"", ""w"") as fh:; json.dump({'samples': items}, fh); CODE; >>>. runtime {; docker: ""python:3.8""; memory: ""1 GB""; cpu: 1; preemptible: 3; disks: ""local-disk "" + 10 + "" HDD""; }. output {; Collection results = read_json(""results.json""); }; }. workflow TestStruct {; input {; Int items; }. call GenerateComplexObject {; input:; items=items; }. output {; Collection out = GenerateComplexObject.results; }; }; ```. When using local backend I have no problem, but when using PAPIv2 (`cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory`) the files from `Test` struct (path) do not delocalize. ```bash; gsutil ls gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/GenerateComplexObject.log; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/gcs_delocalization.sh; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/gcs_localization.sh; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/gcs_transfer.sh; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/rc; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/results.json; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/script; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/stderr; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/stdout; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/pipelines-logs/; ```. Is it possible to use structs as intended on the example? I'm using Cromwell 52, do not know if it works in previous versions.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5592:2383,pipeline,pipelines-logs,2383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592,1,['pipeline'],['pipelines-logs']
Deployability,"ompare/v0.9.0...v0.10.1-java8). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.9.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Pair/cnv_somatic_pair_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Panel/cnv_somatic_panel_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.aws.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.inputs; centaur/src/main/resources/integrationTestCases/germline/haplotype-caller-workflow/HaplotypeCallerWF.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-production-workflow/PairedEndSingleSampleWf.options.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; centaur/src/main/resources/integrationTestCases/green/arrays/arrays.wdl; womtool/src/test/resources/validate/wdl_draft3/valid/HaplotypeCallerWF/HaplotypeCallerWF.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/cnv_somatic_pair_workflow/cnv_somatic_pair_workflow.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/joint-discovery-gatk/joint-discovery-gatk.inputs.json; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7294:1360,integrat,integrationTestCases,1360,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7294,1,['integrat'],['integrationTestCases']
Deployability,"ompleting successfully; [2023-03-29 12:35:42,13] [warn] b303ae23-e1e5-4cde-832b-70114e9efdad-BackendCacheHitCopyingActor-b303ae23:expanse_figures.CBL_assoc:-1:1-20000000025 [b303ae23expanse_figures.CBL_assoc:NA:1]: Unrecognized runtime attribute keys: shortTask, dx_timeout; [2023-03-29 13:07:47,67] [info] BT-322 58e64982:expanse_figures.CBL_assoc:-1:1 cache hit copying success with aggregated hashes: initial = B4BFDDD19BC42B30ED73AB035F6BF1DE, file = C3078AB9F63DD3A59655953B1975D6CF.; [2023-03-29 13:07:47,67] [info] 58e64982-cf3d-4e77-ad72-acfda8299d1b-EngineJobExecutionActor-expanse_figures.CBL_assoc:NA:1 [58e64982]: Call cache hit process had 0 total hit failures before completing successfully; ```. Can someone help me diagnose why call caching isn't near instantaneous, and what I can do to make it much faster? Happy to provide more information as necessary. Thanks!. Config:; ```; # See https://cromwell.readthedocs.io/en/stable/Configuring/; # this configuration only accepts double quotes! not singule quotes; include required(classpath(""application"")). system {; abort-jobs-on-terminate = true; io {; number-of-requests = 30; per = 1 second; }; file-hash-cache = true; }. # necessary for call result caching; # will need to stand up the MySQL server each time before running cromwell; # stand it up on the same node that's running cromwell; database {; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.cj.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true""; user = ""root""; password = ""pass""; connectionTimeout = 5000; }; }. call-caching {; enabled = true; invalidate-bad-cache-results = true; }. docker {; hash-lookup {; enabled = true; method = ""remote""; }; }. backend {; # which backend do you want to use?; # Right now I don't know how to choose this via command line, only here; default = ""Local"" # For running jobs on an interactive node; #default = ""SLURM"" # For running jobs by submitting them from an interactive node to the clu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:2988,configurat,configuration,2988,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['configurat'],['configuration']
Deployability,"omwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; ```. Of the last 6 commands, the 1st, 2nd, 3rd, and 5th command pass. The 4th and 6th does not",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5085:1265,release,releases,1265,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085,1,['release'],['releases']
Deployability,"on.AggregatedMessageException: Failed to instantiate backend filesystem:; Cannot find a filesystem with name sra in the configuration. Available filesystems: ftp, s3, gcs, oss, drs, http; 	at common.validation.Validation$ValidationChecked$.$anonfun$unsafe$2(Validation.scala:98); 	at cats.syntax.EitherOps$.valueOr$extension(either.scala:66); 	at common.validation.Validation$ValidationChecked$.unsafe$extension(Validation.scala:98); 	at cromwell.backend.BackendConfigurationDescriptor.configuredPathBuilderFactories$lzycompute(backend.scala:109); 	at cromwell.backend.BackendConfigurationDescriptor.configuredPathBuilderFactories(backend.scala:108); 	at cromwell.backend.BackendConfigurationDescriptor.pathBuilders(backend.scala:120); 	at cromwell.backend.standard.StandardInitializationActor.pathBuilders$lzycompute(StandardInitializationActor.scala:62); 	at cromwell.backend.standard.StandardInitializationActor.pathBuilders(StandardInitializationActor.scala:62); 	at cromwell.backend.google.pipelines.common.PipelinesApiInitializationActor.$anonfun$workflowPaths$2(PipelinesApiInitializationActor.scala:137); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.disp",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793:1215,pipeline,pipelines,1215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793,1,['pipeline'],['pipelines']
Deployability,on.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at cromwell.engine.backend.jes.Pipeline$.createPipeline$1(Pipeline.scala:43); at cromwell.engine.backend.jes.Pipeline$.apply(Pipeline.scala:59); at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$attemptToCreateJesRun$1(JesBackend.scala:563); at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1.apply(JesBackend.scala:573); at cromwell.engine.backend.jes.JesBackend$$anonfun$cromwell$engine$backend$jes$JesBackend$$createJesRun$1.apply(JesBackend.scala:573); at cromwell.util.TryUtil$$anonfun$5.apply(TryUtil.scala:79); at scala.util.Try$.apply(Try.scala:192); at cromwell.util.TryUtil$.retryBlock(TryUtil.scala:79); at cromwell.engine.backend.jes.JesBackend$.withRetry(JesBackend.scala:123); at cromwell.engine.backend.jes.JesBackend.cromwell$engine$backend$jes$JesBackend$$createJesRun(JesBackend.scala:573); at cromwell.engine.backend.jes.JesBackend$$anonfun$35.apply(JesBackend.scala:707); at cromwell.engine.backend.jes.JesBackend$$anonfun$35.apply(JesBackend.scal,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757:3763,Pipeline,Pipeline,3763,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757,1,['Pipeline'],['Pipeline']
Deployability,"on.WorkflowExecutionActor.akka$actor$Timers$$super$aroundReceive(WorkflowExecutionActor.scala:57); 	at akka.actor.Timers.aroundReceive(Timers.scala:51); 	at akka.actor.Timers.aroundReceive$(Timers.scala:40); 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.aroundReceive(WorkflowExecutionActor.scala:57); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); 	at akka.actor.ActorCell.invoke(ActorCell.scala:583); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2024-03-12 18:49:15 cromwell-system-akka.actor.default-dispatcher-3 INFO - Message [cromwell.engine.workflow.lifecycle.EngineLifecycleActorAbortCommand$] from Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-262d278a-cc62-4458-9150-f31976c2c554#401797350] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-262d278a-cc62-4458-9150-f31976c2c554/WorkflowExecutionActor-262d278a-cc62-4458-9150-f31976c2c554#-742739735] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-262d278a-cc62-4458-9150-f31976c2c554/WorkflowExecutionActor-262d278a-cc62-4458-9150-f31976c2c554#-742739735]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; ```; {; 	""status"": ""Aborting"",; 	""id"": ""262d278a-cc62-4458-9150-f31976c2c554""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7385:7404,configurat,configuration,7404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7385,1,['configurat'],['configuration']
Deployability,"onComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4$$Lambda$1952/113291290.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); [2018-03-09 15:56:36,88] [warn] Localhost hostname lookup failed, keeping the value 'unavailable'; java.util.concurrent.TimeoutException: null; 	at java.util.concurrent.FutureTask.get(FutureTask.java:205); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.updateCache(EventBuilder.java:491); 	at com.getsentry.raven.event.EventBuilder$HostnameCache.getHostname(EventBuilder.java:477); 	at com.getsentry.raven.event.EventBuilder.autoSetMissingValues(EventBuilder.java:97); 	at com.getsentry.raven.event.EventBuilder.build(EventBuilder.java:410); 	at com.getsentry.raven.logback.SentryAppender.buildEvent(SentryAppender.java:324); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:230); 	at com.getsentry.raven.logback.SentryAppender.append(SentryAppender.java:37); 	at ch.qos.logback.core.AppenderBase.doAppend(AppenderBase.java:82); 	at ch.qos.logback.core.spi.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:51); 	at ch.qos.logback.classic.Logger.appendLoopOnAppenders(Logger.java:270); 	at ch.qos.logback.classic.Logger.callAppenders(Logger.java:257); 	at ch.qos.logback.classic.Logger.buildLoggingEventAndAppend(Logger.java:421); 	at ch.qos.logback.classic.Logger.filterAndLog_0_Or3Plus(Logger.java:383); 	at ch.qos.l",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:18041,update,updateCache,18041,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['update'],['updateCache']
Deployability,"ons in Google Cloud. The admin that set up my account also had no idea what `compute.zones.list` mean. I don't see this in the [tutorial](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A refere",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1223,configurat,configuration,1223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,1,['configurat'],['configuration']
Deployability,"oogle-cloud-resourcemanager](https://github.com/googleapis/java-resourcemanager) from 1.0.4 to 1.1.2.; [GitHub Release Notes](https://github.com/googleapis/java-resourcemanager/releases/tag/v1.1.2) - [Changelog](https://github.com/googleapis/java-resourcemanager/blob/master/CHANGELOG.md) - [Version Diff](https://github.com/googleapis/java-resourcemanager/compare/v1.0.4...v1.1.2). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/6472b97b3365f2800f4202d1bf6b1d647bd2b0cc/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.0.4).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/variantcall_batch_region.cwl; project/Dependencies.scala; ```; </details>; <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-resourcemanager"" } ]; ```; </details>. labels: library-update, semver-minor, old-version-remains",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6519:1063,update,update,1063,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6519,10,"['integrat', 'update']","['integrationTestCases', 'update', 'updates']"
Deployability,oogle.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:92); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaFor,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1702,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1702,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"oot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/6169035039702064455; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18:59:03,708 cromwell-system-akka.dispatchers.backend-dispatcher-96 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 19:01:18,073 cromwell-system-akka.dispatchers.backend-dispatcher-130 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from Running to Success; 2020-10-13 19:02:03,961 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - MaterializeWorkflowDescriptorActor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:5210,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,5210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,or Workflow bd18e464-59a2-44cf-80c2-b4d93bdfe0ce failed (during FinalizingWorkflowState): software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.ama,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686:7928,pipeline,pipeline,7928,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686,1,['pipeline'],['pipeline']
Deployability,"or later</h3>; <p>A local information disclosure vulnerability in <code>TemporaryFolder</code> has been fixed. See the published <a href=""https://github.com/junit-team/junit4/security/advisories/GHSA-269g-pwp5-87pp"">security advisory</a> for details.</p>; <h1>Test Runners</h1>; <h3>[Pull request <a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1669"">#1669</a>:](<a href=""https://github-redirect.dependabot.com/junit-team/junit/pull/1669"">junit-team/junit#1669</a>) Make <code>FrameworkField</code> constructor public</h3>; <p>Prior to this change, custom runners could make <code>FrameworkMethod</code> instances, but not <code>FrameworkField</code> instances. This small change allows for both now, because <code>FrameworkField</code>'s constructor has been promoted from package-private to public.</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/junit-team/junit4/commit/1b683f4ec07bcfa40149f086d32240f805487e66""><code>1b683f4</code></a> [maven-release-plugin] prepare release r4.13.1</li>; <li><a href=""https://github.com/junit-team/junit4/commit/ce6ce3aadc070db2902698fe0d3dc6729cd631f2""><code>ce6ce3a</code></a> Draft 4.13.1 release notes</li>; <li><a href=""https://github.com/junit-team/junit4/commit/c29dd8239d6b353e699397eb090a1fd27411fa24""><code>c29dd82</code></a> Change version to 4.13.1-SNAPSHOT</li>; <li><a href=""https://github.com/junit-team/junit4/commit/1d174861f0b64f97ab0722bb324a760bfb02f567""><code>1d17486</code></a> Add a link to assertThrows in exception testing</li>; <li><a href=""https://github.com/junit-team/junit4/commit/543905df72ff10364b94dda27552efebf3dd04e9""><code>543905d</code></a> Use separate line for annotation in Javadoc</li>; <li><a href=""https://github.com/junit-team/junit4/commit/510e906b391e7e46a346e1c852416dc7be934944""><code>510e906</code></a> Add sub headlines to class Javadoc</li>; <li><a href=""https://github.com/junit-team/junit4/commit/610155b8c22138329f0723eec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5941:1818,release,release-plugin,1818,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5941,2,['release'],"['release', 'release-plugin']"
Deployability,or$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableSta,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:3480,pipeline,pipeline,3480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,1,['pipeline'],['pipeline']
Deployability,"or.scala:54); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); 	at akka.actor.ActorCell.invoke(ActorCell.scala:583); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); 	at akka.dispatch.Mailbox.run(Mailbox.scala:229); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2023-11-07 14:51:17,39] [info] Message [cromwell.engine.workflow.lifecycle.EngineLifecycleActorAbortCommand$] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-4e522458-e360-45e8-be15-2fc99652d692#-686070856] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-4e522458-e360-45e8-be15-2fc99652d692/WorkflowExecutionActor-4e522458-e360-45e8-be15-2fc99652d692#-1420206102] was not delivered. [1] dead letters encountered, no more dead letters will be logged. If this is not an expected behavior, then [Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-4e522458-e360-45e8-be15-2fc99652d692/WorkflowExecutionActor-4e522458-e360-45e8-be15-2fc99652d692#-1420206102]] may have terminated unexpectedly, This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; In fact, the program becomes unresponsive to even a Ctrl+C kill command and I have to close the terminal entirely to stop it. . The WDL passes `womtool validate` (version 84) and was run using Cromwell version 84. . When run in Terra, the workflow just immediate goes into an aborting state without any helpful error message. It would be great to incorporate this type of support for `None` inside struct fields.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7249:6747,configurat,configuration,6747,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7249,1,['configurat'],['configuration']
Deployability,"or/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:9733,configurat,configuration,9733,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['configurat'],['configuration']
Deployability,"ore attempting to build or run the image. For example, we currently recommend this `submit-docker` configuration:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Build the Docker image into a singularity image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I'm instead proposing this. Note the use of a single shared image directory (`/singularity_cache` in this example), and the use of `flock` to ensure the submit scripts aren't competing with each other:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Determine the filepath to the image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=/singularity_cache/$DOCKER_NAME.sif. # Wait for an exclusive lock on the image ; (; flock --exclusive 200; # Build the image; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi; ) 200>/var/lock/$IMAGE. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I haven't tested this on our HPC cluster (it's down for maintenance sadly!), but I'm interested if this makes sense as something we could get into the containers tutorial in order to recommend to users. @illusional, @vsoch @geoffjentry",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063:2019,install,installed,2019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063,1,['install'],['installed']
Deployability,"org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.33).; You might want to review and update them manually.; ```; core/src/test/resources/hello_goodbye_scattered_papiv2.json; docs/developers/bitesize/ci/Cromwell_Deployment_Strategies.svg; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; src/ci/resources/papi_v2_reference_image_manifest.conf; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; O",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7081:2552,configurat,configuration,2552,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7081,1,['configurat'],['configuration']
Deployability,"orker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/xxx/o?projection=full&userProject=xxx&uploadType=multipart; {; ""code"" : 403,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project."",; ""reason"" : ""forbidden""; } ],; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.""; }; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:555); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:475); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:592); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.create(HttpStorageRpc.java:305); 	... 22 more; ```. I have no idea what `serviceusage.services.use` is and how to activate it. The tutorial is also very weirdly written. It seems like there used to be a tutorial about JES/PAPIv1 and then it got updated with a notice for PAPIv2. It is completely unclear whether the user is supposed to use JES/PAPIv1 or PAPIv2. I do not understand why it has to be so complicated. Can Cromwell provide some useful message about what to do to set the required permissions?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594:8668,update,updated,8668,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594,1,['update'],['updated']
Deployability,"orkerThread.java:107); 2019-02-28 08:30:32,176 cromwell-system-akka.dispatchers.engine-dispatcher-28 ERROR - WorkflowManagerActor Workflow bd18e464-59a2-44cf-80c2-b4d93bdfe0ce failed (during FinalizingWorkflowState): software.amazon.awssdk.services.s3.model.S3Exception: Access Denied (Service: S3Client; Status Code: 403; Request ID: FA1C7E97A7A33EDC); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4686:7801,pipeline,pipeline,7801,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4686,1,['pipeline'],['pipeline']
Deployability,"orkflow failure, for what seems like a transient condition that could be overcome by retries:. 2019-05-24 12:32:07,173 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001:1065,pipeline,pipelines,1065,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001,1,['pipeline'],['pipelines']
Deployability,"ort feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. Backend: AWS Batch. <!-- Paste/Attach your workflow if possible: -->. [Workflow](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-processing-for-variant-discovery-gatk4.wdl). [Input file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-M40job.inputs.json). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. [Configuration file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/aws.conf). Running this workflow on AWS Batch (with cromwell-36.jar) consistently fails at the same point each time. . It gets through most (looks like all but one iteration) of the scatter loop that calls the `BaseRecalibrator` task. Then cromwell just sits for a long time (~1hr) with no Batch jobs running (or runnable or starting). Then cromwell calls the `RegisterJobDefinition` API of AWS Batch, and it always fails with the following error message:. ```; 2018-12-15 23:39:03,360 cromwell-system-akka.dispatchers.backend-dispatcher-258 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attemptin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496:1003,configurat,configuration,1003,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496,1,['configurat'],['configuration']
Deployability,"ot for the missing input (gender_mask_bed); call CallSampleGender as CallSampleGender {; input:; analysis_directory = analysis_directory,; ref_fasta = ref_fasta,; ref_fasta_index = ref_fasta_index,; ref_dict = ref_dict,; genome_mask = genome_mask,; genome_mask_index = genome_mask_index,; ploidy_map = ploidy_map,; header_bam = ExtractBamSubset.header_bam,; header_bam_index = IndexHeaders.header_index,; gender_mask_bed = gender_mask_bed,; read_count_index = Index.read_count_index; }; }; ```. And here's the task, which does have the missing variable **File gender_mask_bed**. ```; task CallSampleGender {; String analysis_directory; File ref_fasta; File ref_fasta_index; File ref_dict; File genome_mask; File genome_mask_index; File ploidy_map; File header_bam; File header_bam_index; File gender_mask_bed; File read_count_index. command {; mkdir ${analysis_directory}; java -Xmx4000m \; -classpath /usr/gitc/svtoolkit2.00/lib/SVToolkit.jar:/usr/gitc/svtoolkit2.00/lib/gatk/GenomeAnalysisTK.jar \; org.broadinstitute.sv.apps.CallSampleGender \; -R ${ref_fasta} \; -genomeMaskFile ${genome_mask} \; -ploidyMapFile ${ploidy_map} \; -md ${analysis_directory} \; -I ${header_bam} \; -bedFile ${gender_mask_bed} \; -O sample_gender.report.txt; }; runtime {; docker: ""kbergin/kbergin_test""; memory: ""4 GB""; }; output{; File sample_gender_report = ""sample_gender.report.txt""; }; }; ```. ---. @knoblett commented on [Mon Apr 25 2016](https://github.com/broadinstitute/wdltool/issues/8#issuecomment-214441149). In running a highlight command today, I noticed that the wdltool caught an undeclared input error. Has this bug been fixed, or is wdltool still not picking up undeclared inputs within tasks only? (my example is missing a declared input in the workflow's call to the task, due to a typo.). ![screen shot 2016-04-25 at 12 55 02 pm](https://cloud.githubusercontent.com/assets/13629186/14791714/0ca069f8-0ae5-11e6-8b80-37e2a7fecae9.png). I am using the Cromwell 0.18 release and wdltool 0.1 release.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2874:5756,release,release,5756,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2874,2,['release'],['release']
Deployability,"ou don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/92f43ce5f010fa03daeb2625a3f9c1b1be80cca5/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.28).; You might want to review and update them manually.; ```; centaur/src/main/resources/standardTestCases/local_bourne/local_bourne.wdl; core/src/test/resources/hello_goodbye_scattered_papiv2.json; docs/developers/bitesize/workflowParsing/forkjoin_graph.svg; docs/developers/bitesize/workflowParsing/wdlToWdlom_hermes.svg; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; src/ci/bin/test.inc.sh; src/ci/docker-compose/cromwell-test/docker-setup.sh; supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/request/GetRequestHandler.scala; supportedBackends/google/pipelines/v2alpha1/src/test/scala/cromwell/backend/google/pipelines/v2alpha1/api/request/GetRequestHandlerSpec.scala; supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandler.scala; supportedBackends/google/pipelines/v2beta/src/test/scala/cromwell/backend/google/pipelines/v2beta/api/request/GetRequestHandlerSpec.scala; ```; </details>; <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; </details>. labels: test-library-update, old-version-remains",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6429:3295,pipeline,pipelines,3295,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6429,12,"['pipeline', 'update']","['pipelines', 'update', 'updates']"
Deployability,"ould not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:2618,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"ount of time. This might indicate that something got stuck in PAPI.; slow-job-warning-time: ""24 hours"". # Set this to the lower of the two values ""Queries per 100 seconds"" and ""Queries per 100 seconds per user"" for; # your project.; #; # Used to help determine maximum throughput to the Google Genomics API. Setting this value too low will; # cause a drop in performance. Setting this value too high will cause QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 1000. # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Number of workers to assign to PAPI requests; request-workers = 3. genomics {; # A reference to an auth defined in the `google` stanza at the top.; # This auth is used to create pipelines and manipulate auth JSONs.; auth = ""application-default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""us-west1"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; # There is no logic to determine if the error was transient or not, everything is retried upon failure; # Defaults to 3; localization-attempts = 3. # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:2472,pipeline,pipelines,2472,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['pipeline'],['pipelines']
Deployability,"oups: 075e0cf3: 1; 2021-09-27 13:48:31,233 cromwell-system-akka.dispatchers.engine-dispatcher-12 INFO - BT-322 075e0cf3:wf_hello.hello:-1:1 is eligible for call caching with read = true and write = true; 2021-09-27 13:48:31,314 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - BT-322 075e0cf3:wf_hello.hello:-1:1 cache hit copying nomatch: could not find a suitable cache hit.; 2021-09-27 13:48:31,320 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - 075e0cf3-194b-4f53-a43d-d31f0b370f79-EngineJobExecutionActor-wf_hello.hello:NA:1 [UUID(075e0cf3)]: Could not copy a suitable cache hit for 075e0cf3:wf_hello.hello:-1:1. No copy attempts were made.; 2021-09-27 13:48:31,449 cromwell-system-akka.dispatchers.backend-dispatcher-33 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: `echo ""Hello Cromwell! Welcome to Cromwell . . . on AWS!""`; 2021-09-27 13:48:33,376 cromwell-system-akka.dispatchers.backend-dispatcher-33 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Adjusting boot disk size to 16 GB: 10 GB (runtime attributes) + 5 GB (user command image) + 1 GB (Cromwell support images); 2021-09-27 13:48:38,987 cromwell-system-akka.dispatchers.backend-dispatcher-33 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: job id: projects/gred-cumulus-sb-01-991a49c4/operations/15427360049616748078; 2021-09-27 13:49:07,692 cromwell-system-akka.dispatchers.backend-dispatcher-35 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from - to Running; 2021-09-27 13:50:48,340 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from Running to Failed; 2021-09-27 13:50:49,875 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Workflow 075e0cf3-194b-4f53-a43d-d31f0b370f79 failed (during Execu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:11692,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,11692,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"ow, I'm trying to get that WDL uploaded to Terra and the WOMtool validation step continues to pass me a fatal error that I can't seem to figure out. I've reduced the WDL to a single step that can reproduce this error and pasted below. I can't imagine I'm the first person to have this issue, but couldn't find evidence of it on the interwebs! In sum, I have a WDL that appears to be working fine (via miniwdl), but WOMtool (and Dockstore for that matter) finds a fatal error that prevents me from using it on Terra. Please help, thanks!!!. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; `ERROR: Unexpected symbol (line 6, col 5) when parsing 'setter'. Expected equal, got ""String"". String bam_to_reads_mem_size ^ $setter = :equal $e -> $1 `. <!-- Which backend are you running? -->; `womtool v61`; `miniwdl v1.5.2`. <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0 . #WORKFLOW DEFINITION; workflow StripReadsFromBam {; String bam_to_reads_disk_size; String bam_to_reads_mem_size. #converts BAM to FASTQ (R1 + R2); call BamToReads {; 	input:; 	disk_size = bam_to_reads_disk_size,; 	mem_size = bam_to_reads_mem_size; }. #Outputs single reads file; output {; File outputReads = BamToReads.outputReads; }; }. #Task Definitions; task BamToReads {; File InputBam; String SampleName; String disk_size; String mem_size. #Calls samtools view to do the conversion; command {; #Set -e and -o says if any command I run fails in this script, make sure to return a failure; set -e; set -o pipefail. samtools fastq -c9 -@4 -n -o ${SampleName}.fq.gz ${InputBam} . }. #Run time attributes:; runtime {; docker: ""drpintothe2nd/ac3_xysupp""; memory: mem_size; cpu: ""4""; disks: ""local-disk "" + disk_size + "" HDD""; 	}; ; output {; 	File outputReads = ""${SampleName}.fq.gz""; 	}; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Running locally on windows 10 WSL, ubuntu 20.04",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6767:2298,configurat,configuration,2298,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767,1,['configurat'],['configuration']
Deployability,"ow_store_state_widening.xml::workflow-store-state-widening::tjeandet: WORKFLOW_STORE_ENTRY.WORKFLOW_STATE datatype was changed to varchar(20); 2018-06-07 12:16:10,983 INFO - changelog.xml: changesets/workflow_store_state_widening.xml::workflow-store-state-widening::tjeandet: ChangeSet changesets/workflow_store_state_widening.xml::workflow-store-state-widening::tjeandet ran successfully in 0ms; 2018-06-07 12:16:10,985 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::workflow-store-restarted-column::tjeandet: Columns RESTARTED(BOOLEAN) added to WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,985 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::workflow-store-restarted-column::tjeandet: ChangeSet changesets/workflow_store_restarted_column.xml::workflow-store-restarted-column::tjeandet ran successfully in 1ms; 2018-06-07 12:16:10,987 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet: Data updated in WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,987 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet: Data updated in WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,987 INFO - changelog.xml: changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet: ChangeSet changesets/workflow_store_restarted_column.xml::update-restartable::tjeandet ran successfully in 1ms; 2018-06-07 12:16:10,989 INFO - changelog.xml: changesets/workflow_store_workflow_root_column.xml::workflow-store-workflow-root-column::tjeandet: Columns WORKFLOW_ROOT(VARCHAR(100)) added to WORKFLOW_STORE_ENTRY; 2018-06-07 12:16:10,989 INFO - changelog.xml: changesets/workflow_store_workflow_root_column.xml::workflow-store-workflow-root-column::tjeandet: ChangeSet changesets/workflow_store_workflow_root_column.xml::workflow-store-workflow-root-column::tjeandet ran successfully in 0ms; 2018-06-07 12:16:10,992 INFO - changelog.xml: changesets/workflow_store_horizontal_db.xml::workf",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:89002,update,update-restartable,89002,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,2,['update'],"['update-restartable', 'updated']"
Deployability,"pResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:3701,Pipeline,PipelinesApiRequestWorker,3701,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,patch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:38); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: java.nio.file.NoSuchFileException: /users/leepc12/code/atac-seq-pipeline/test/cromwell-executions/test/132d7527-a0af-4f08-8291-d935e7cd5632/call-t1/execution/glob-c91e47329777637e2370464651ba47aa.list; at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); at sun.nio.fs.UnixPath.toRealPath(UnixPath.java:837); at cromwell.core.path.NioPathMethods.toRealPath(NioPathMethods.scala:70); at cromwell.core.path.NioPathMethods.toRealPath$(NioPathMethods.scala:70); at cromwell.core.path.DefaultPath.toRealPath(DefaultPathBuilder.scala:53); at cromwell.backend.io.GlobFunctions.glob(GlobFunctions.scala:45); at cromwell.backend.io.GlobFunctions.glob$(GlobFunctions.scala:43); at cromwell.backend.standard.StandardExpressionFunctions.glob(StandardExpressionFunctions.scala:23); at cromwell.backend.wdl.ReadLikeFunctions.$anonfun$glob$1(ReadLikeFunctions.scala:168); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2972:6890,pipeline,pipeline,6890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2972,1,['pipeline'],['pipeline']
Deployability,"patcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18:59:03,708 cromwell-system-akka.dispatchers.backend-dispatcher-96 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 19:01:18,073 cromwell-system-akka.dispatchers.backend-dispatcher-130 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from Running to Success; 2020-10-13 19:02:03,961 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - MaterializeWorkflowDescriptorActor [UUID(efe9c9a5)]: Call-to-Backend assignments: drs_usa_jdr.localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.skip_localize_jdr_drs_with_usa -> papi-v2-usa, drs_usa_jdr.read_drs_with_usa -> papi-v2-usa; 2020-10-13 19:03:01,200 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Restarting drs_usa_jdr.skip_localize_jdr_drs_with_usa, drs_usa_jdr.localize_jdr_drs_with_usa, drs_usa_jdr.read_drs_with_usa; 2020-10-13 19:03:02,934 cromwell-system-akka.d",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:5657,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,5657,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,patchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001:1415,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1415,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"pdates [com.eed3si9n:sbt-assembly](https://github.com/sbt/sbt-assembly) from 1.1.0 to 1.1.1.; [GitHub Release Notes](https://github.com/sbt/sbt-assembly/releases/tag/v1.1.1) - [Version Diff](https://github.com/sbt/sbt-assembly/compare/v1.1.0...v1.1.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.1.0).; You might want to review and update them manually.; ```; CHANGELOG.md; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/variantcall_batch_region.cwl; cloud-nio/cloud-nio-impl-drs/src/main/scala/cloud/nio/impl/drs/DrsCloudNioFileSystemProvider.scala; cwl/src/test/resources/cwl/lodash.js; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pul",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6850:976,integrat,integrationTestCases,976,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6850,1,['integrat'],['integrationTestCases']
Deployability,"pected exit status 1 was not ignored; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3732:1596,pipeline,pipelines,1596,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732,1,['pipeline'],['pipelines']
Deployability,"ped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecuto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804:1644,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1644,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"ped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecuto",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1922,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1922,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,pelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:37); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:26); 	at software.amazon.awssdk.core.internal.http.AmazonSyncHttpClient$RequestExecutionBuilderImpl.execute(AmazonSyncHttpClient.java:240); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.invoke(BaseSyncClientHandler.java:96); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:120); 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:73); 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); 	at software.amazon.awssdk.services.batch.DefaultBat,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:3189,pipeline,pipeline,3189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,pelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:51); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:34); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:20); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); at akka.actor.ActorCell.invoke(ActorCell.scala:583); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:19,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:4062,Pipeline,PipelinesApiRequestWorker,4062,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,2,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,"pelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(S",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804:1305,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"pelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(S",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1583,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1583,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,pes to constructor.; 	at com.google.auth.oauth2.ServiceAccountCredentials.refreshAccessToken(ServiceAccountCredentials.java:402); 	at com.google.auth.oauth2.OAuth2Credentials.refresh(OAuth2Credentials.java:157); 	at com.google.auth.oauth2.OAuth2Credentials.getRequestMetadata(OAuth2Credentials.java:145); 	at com.google.auth.oauth2.ServiceAccountCredentials.getRequestMetadata(ServiceAccountCredentials.java:603); 	at com.google.auth.http.HttpCredentialsAdapter.initialize(HttpCredentialsAdapter.java:91); 	at com.google.api.client.http.HttpRequestFactory.buildRequest(HttpRequestFactory.java:88); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:423); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.buildHttpRequest(AbstractGoogleClientRequest.java:399); 	at cromwell.backend.google.pipelines.v1alpha2.GenomicsFactory$$anon$1.runRequest(GenomicsFactory.scala:85); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline(PipelinesApiRunCreationClient.scala:53); 	at cromwell.backend.google.pipelines.common.api.clients.PipelinesApiRunCreationClient.runPipeline$(PipelinesApiRunCreationClient.scala:48); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.runPipeline(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$createNewJob$19(PipelinesApiAsyncBackendJobExecutionActor.scala:572); 	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629:1404,pipeline,pipelines,1404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-664685629,1,['pipeline'],['pipelines']
Deployability,"pi.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:143); at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:84); at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1040); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch requ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:3680,pipeline,pipelines,3680,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['pipeline'],['pipelines']
Deployability,piConfigurationAttributes.scala:307); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.defaultBuildAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:32); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.$anonfun$papiAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:34); at scala.util.Try$.apply(Try.scala:210); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.cromwell$backend$google$pipelines$common$PipelinesApiBackendLifecycleActorFactory$$build$1(PipelinesApiBackendLifecycleActorFactory.scala:109); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.robustBuildAttributes(PipelinesApiBackendLifecycleActorFactory.scala:120); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.<init>(PipelinesApiBackendLifecycleActorFactory.scala:34); at cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:10); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at cromwell.engine.backend.BackendConfigurationEntry.$anonfun$asBackendLifecycleActorFactory$1(BackendConfiguration.scala:13); at scala.util.Try$.apply(Try.scala:210); at cromwell.engine.backend.BackendConfigurationEntry.asBackendLifecycleActorFactory(BackendConfiguration.scala:14); at cromwell.engine.backend.CromwellBackends.$anonfun$backendLifecycleActorFactories$1(CromwellBackends.scala:14); at scala.collection.immutable.List.map(List.scala:246); at cromwell.engine.backend.CromwellBackends.<init>(CromwellBackend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:1864,Pipeline,PipelinesApiLifecycleActorFactory,1864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,1,['Pipeline'],['PipelinesApiLifecycleActorFactory']
Deployability,"pl.Parseable.parseValue(Parseable.java:190); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:152); at com.typesafe.config.impl.SimpleIncluder.fromBasename(SimpleIncluder.java:185); ... 48 more; Caused by: java.io.IOException: resource not found on classpath: application.conf; at com.typesafe.config.impl.Parseable$ParseableResources.rawParseValue(Parseable.java:726); at com.typesafe.config.impl.Parseable$ParseableResources.rawParseValue(Parseable.java:701); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:180); ... 51 more; ```. <!-- Which backend are you running? -->. The backend I'm running on is slurm and local . <!-- Paste/Attach your workflow if possible: -->. Workflow link. <!-- Def not an joke about best practices. Also thanks for publishing the gatk best practices and the warp pipelines -->. https://github.com/mmterpstra/Bestie. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Below are the first few lines of the config shown. ```; #see also https://cromwell.readthedocs.io/en/stable/backends/SLURM/; # include the application.conf at the top; include required(classpath(""application"")); workflow-options {; workflow-failure-mode = ContinueWhilePossible; delete_intermediate_output_files = true; final_workflow_outputs_dir = ""cromwell-results"",; use_relative_output_paths = true,; final_workflow_log_dir = ""cromwell-logs"",; final_call_logs_dir = ""cromwell-call_logs""; }; database {; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = """"""; jdbc:hsqldb:file:cromwell-executions/cromwell-db/cromwell-db;; shutdown=false;; hsqldb.default_table_type=cached;hsqldb.tx=mvcc;; hsqldb.result_max_memory_rows=50000;; hsqldb.large_data=true;; hsqldb.applog=1;; hsqldb.lob_compressed=true;; hsqldb.script_format=3; """"""; connectionTimeout = 120000; numThreads = 1; }; }; backend {; default = ""Loca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:5714,configurat,configuration,5714,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['configurat'],['configuration']
Deployability,ps://bitbucket.org/snakeyaml/snakeyaml/src/master/RELEASES.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/RELEASES.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/RELEASES.rst) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/ReleaseNotes.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/ReleaseNotes.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/ReleaseNotes.rst) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Releases.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Releases.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Releases.rst) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/releases.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/releases.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/releases.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/m,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7081:1127,release,releases,1127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7081,1,['release'],['releases']
Deployability,ps://bitbucket.org/snakeyaml/snakeyaml/src/master/ReleaseNotes.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/ReleaseNotes.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/ReleaseNotes.rst) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/RELEASES.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/RELEASES.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/RELEASES.rst) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Releases.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Releases.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Releases.rst) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/releases.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/releases.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/releases.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6635:1128,release,releases,1128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6635,2,['release'],['releases']
Deployability,"ps://broadworkbench.atlassian.net/browse/WX-1307), [WX-1308](https://broadworkbench.atlassian.net/browse/WX-1308), [WX-983](https://broadworkbench.atlassian.net/browse/WX-983). PR creates a GHA (currently runs on dispatch, can be updated to run on schedule of choice) that creates a billing project and BEE, attaches the BEE to a static landing zone, creates a workspace and provisions an app within the BEE, submits a workflow (basic hello world) to Cromwell, and performs app/workspace/billing project cleanup afterwards. BEE template is flagged by Janitor for post workflow cleanup to ensure no lingering resources. Workspace deletion and billing project deletion are finicky due to invariable timing of the deletion itself (can be either extremely short or longer than 12 minutes), so those two steps are handled by either an exception block (workspace deletion) or `continue-on-error` (billing project) to ensure that failures there do not reflect a failure on the test against Cromwell. Workspace provisioning and app creation are necessary for running tests against Cromwell, so a failure there will be reported as a failure on the Cromwell test. (As an aside, this could be rectified by having a static testing app that's always running in a dedicated testing environment. Test could be updated to run submissions against it so as long as that app is kept up to date. [WX-1306]: https://broadworkbench.atlassian.net/browse/WX-1306?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-1307]: https://broadworkbench.atlassian.net/browse/WX-1307?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-1308]: https://broadworkbench.atlassian.net/browse/WX-1308?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-983]: https://broadworkbench.atlassian.net/browse/WX-983?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7236:1382,update,updated,1382,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7236,1,['update'],['updated']
Deployability,"ps://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl; (https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl Ln 3 Col 1) Failed to import https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl, exceeded import_max_depth; circular imports?; None; ```. ## Backends; * Terra (kind of); * Mac OS on womtool 76; * Ubuntu on womtool 56. I say ""kind of"" for Terra since I can't see the error message -- if you try to upload this workflow to the Broad Methods Repo, a 500 error will result, and I've a hunch that's the result of the stack overflow. ## Example workflow; ```; version 1.0. import ""https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl""; # note: that workflow also has the line import ""https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segment_scatter.wdl""; # I meant to actually import ""https://raw.githubusercontent.com/aofarrel/Stuart-WDL/segment_scatter/segfault.wdl"". workflow Segment_Scatter {; 	input {; 		# if you input 10 files and n_segments = 5, each segment gets 2 files; 		Array[File] input_files; 		Int n_segments; 	}. 	call segfault.segfault {; 		input:; 			inputs = input_files,; 			n_segments = n_segments; 	}. 	scatter(segment in segfault.segments) {; 		call echo_files {; 			input:; 				files_to_echo = segment; 		}; 	}; }. task echo_files {; 	input {; 		Array[File] files_to_echo; 	}. 	command <<<; 	python3 << CODE; 	files = [""~{sep='"",""' files_to_echo}""]; 	for file in files:; 		print(file); 	CODE; 	>>>. 	runtime {; 		docker: ""ashedpotatoes/sranwrp:1.1.0""; 		memory: ""4 GB""; 	}; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6964:4019,configurat,configuration,4019,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6964,1,['configurat'],['configuration']
Deployability,"pt-epilogue as unrecognized. Do you have any suggestions?. > Yes, the script epilogue is exactly where the change should be. The script is generated by AwsBatchJob.scala; > [â€¦](#); > On Sun, Oct 25, 2020 at 8:37 PM Luyu ***@***.***> wrote: Hi Luyu, Thanks for the feedback. This is an interesting case. Normally if there is a few minutes gap between workflows the instances will be terminated by batch and the disks will be reclaimed so each workflow starts from scratch. However in your case there isnâ€™t a pause in work long enough for Batch to shut down the instances. Also because these files are written to a mounted disk they are not deleted when the container terminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark â€¦ <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu *@*.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323> <#4323 <#4323>>. But none of them works for me. Thank you in advance for any suggestions. â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974 <#5974>>, or unsubscribe https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA . Hi Mark, Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383:1064,pipeline,pipeline,1064,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718305383,1,['pipeline'],['pipeline']
Deployability,ptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImp,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:5106,pipeline,pipeline,5106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,3,['pipeline'],['pipeline']
Deployability,"ptional: true}; input_vcf_idx: {localization_optional: true}; bam: {localization_optional: true}; bai: {localization_optional: true}; }. command {; set -e. export GATK_LOCAL_JAR=~{default=""/root/gatk.jar"" runtime_params.gatk_override}. gatk --java-options ""-Xmx~{command_mem}m"" FilterAlignmentArtifacts \; -V ~{input_vcf} \; -I ~{bam} \; --bwa-mem-index-image ~{realignment_index_bundle} \; ~{realignment_extra_args} \; -O ~{output_vcf}; }. runtime {; docker: runtime_params.gatk_docker; bootDiskSizeGb: runtime_params.boot_disk_size; memory: machine_mem + "" MB""; disks: ""local-disk "" + runtime_params.disk + "" HDD""; preemptible: runtime_params.preemptible; maxRetries: runtime_params.max_retries; cpu: runtime_params.cpu; }. output {; File filtered_vcf = ""~{output_vcf}""; File filtered_vcf_idx = ""~{output_vcf_idx}""; }; }. task Funcotate {; input {; File ref_fasta; File ref_fai; File ref_dict; File input_vcf; File input_vcf_idx; String reference_version; String output_file_base_name; String output_format; Boolean compress; Boolean use_gnomad; # This should be updated when a new version of the data sources is released; # TODO: Make this dynamically chosen in the command.; File? data_sources_tar_gz = ""gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.6.20190124s.tar.gz""; String? control_id; String? case_id; String? sequencing_center; String? sequence_source; String? transcript_selection_mode; File? transcript_selection_list; Array[String]? annotation_defaults; Array[String]? annotation_overrides; Array[String]? funcotator_excluded_fields; Boolean? filter_funcotations; File? interval_list. String? extra_args. # ==============; Runtime runtime_params; Int? disk_space #override to request more disk than default small task params. # You may have to change the following two parameter values depending on the task requirements; Int default_ram_mb = 3000; # WARNING: In the workflow, you should calculate the disk space as an input to this task (disk_space_gb). Please see [T",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:33908,update,updated,33908,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,2,"['release', 'update']","['released', 'updated']"
Deployability,"pu: 1; memory: ""3.75 GB""; }; }; workflow test {; call hello. output {; File response = hello.response; }; }; ```. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ```hoco; backend {; default = ""batch""; providers {; batch {; actor-factory = ""cromwell.backend.google.batch.GcpBatchBackendLifecycleActorFactory""; config {. # The Project To execute in; project = ""${compute_project}"". # The bucket where outputs will be written to; root = ""gs://${bucket}"". # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # batch-timeout = 7 days. genomics {; auth = ""cromwell-service-account""; location: ""${region}""; compute-service-account = ""${compute_service_account}"". # Specifies the minimum file size for `gsutil cp` to use parallel composite uploads during delocalization.; # Parallel composite uploads can result in a significant improvement in delocalization speed for large files; # but may introduce complexities in downloading such files from GCS, please see; # https://cloud.google.com/storage/docs/gsutil/commands/cp#parallel-composite-uploads for more information.; #; # If set to 0 parallel composite uploads are turned off. The default Cromwell configuration turns off; # parallel composite uploads, this sample configuration turns it on for files of 150M or larger.; parallel-composite-upload-threshold=""150M""; }. filesystems {; gcs {; auth = ""cromwell-service-account"". # For billing; project = ""${billing_project}"". caching {; # When a cache hit is found, the following duplication ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238:2060,pipeline,pipeline,2060,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238,1,['pipeline'],['pipeline']
Deployability,"qc:0:1]: `/app/fastqc_docker.py --output-dir . --read ""/cromwell_root/genovic-test-data/cardiom/NA12878_CARDIACM_MUTATED_L001_R1.fastq.gz"" --format fastq`; ```. Cromwell failing with an error because the `--read` argument is missing (even though you can see it's not, in the above log):; ```; cromwell_1 | java.lang.Exception: Task germline_variant_calling.fastqc:0:1 failed. The job was stopped before the command finished. PAPI error code 10. 11: Docker run failed: command failed: usage: fastqc_docker.py [-h] -r READ -o OUTPUT_DIR [-c CONTAMINANTS]; cromwell_1 | [-a ADAPTERS] [-l LIMITS] [-f FORMAT] [-n NO_GROUP]; cromwell_1 | [-e EXTRA_OPTIONS]; cromwell_1 | fastqc_docker.py: error: argument -r/--read is required; cromwell_1 | . See logs at gs://genovic-cromwell/cromwell-execution/trio/f5454139-c51d-4d04-ae0a-9b9d4ce650aa/call-germline_variant_calling/shard-0/germline_variant_calling/5d4c4459-a91c-4d3b-8ca4-b98457134750/call-fastqc/shard-0/; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.F",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:1692,pipeline,pipelines,1692,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['pipeline'],['pipelines']
Deployability,question : is there docker image for building cromwell ? . with all dependencies installed.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5868:81,install,installed,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5868,1,['install'],['installed']
Deployability,"quire 'docker login': denied: Permission denied for \""sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"" from request \""/v2/broad-cumulus/cellranger/manifests/sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"".\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:91); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:812); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:95); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1340); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1336); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:92); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at akka.dispatch.BatchingExecu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:14206,pipeline,pipelines,14206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['pipeline'],['pipelines']
Deployability,"r and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.conf` to a noop -OR- ensure that when an error is generated that the latest version of `sentry` does not output a ""suitable DSN"" warning~; - Update docs for Cromwell+Sentry in both `docs/Logging.md` and `docs/Configuring.md`; - ~Update `CHANGELOG.md` with configuration changes for Cromwell+Sentry~ Edit: Not necessary if still using sentry style configuration. **Links:**; - http://cromwell.readthedocs.io/en/develop/Configuring/#workflow-log-directory; - http://cromwell.readthedocs.io/en/develop/Logging/#workflow-logs; - (video) [Episode #108 - Tracking Errors with Sentry](https://www.youtube.com/watch?v=n5hWUD2CXd8); - https://sentry.io/for/java/; - https://docs.sentry.io/clients/java/; - https://d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3657:1719,configurat,configuration,1719,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657,1,['configurat'],['configuration']
Deployability,r code 10. 11: Docker run failed: command failed: usage: fastqc_docker.py [-h] -r READ -o OUTPUT_DIR [-c CONTAMINANTS]; cromwell_1 | [-a ADAPTERS] [-l LIMITS] [-f FORMAT] [-n NO_GROUP]; cromwell_1 | [-e EXTRA_OPTIONS]; cromwell_1 | fastqc_docker.py: error: argument -r/--read is required; cromwell_1 | . See logs at gs://genovic-cromwell/cromwell-execution/trio/f5454139-c51d-4d04-ae0a-9b9d4ce650aa/call-germline_variant_calling/shard-0/germline_variant_calling/5d4c4459-a91c-4d3b-8ca4-b98457134750/call-fastqc/shard-0/; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); cromwell_1 | at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); cromwell_1 | at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); cromwell_1 |,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:2122,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"r details.</p>; <h1>Test Runners</h1>; <h3>[Pull request <a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1669"">#1669</a>:](<a href=""https://github-redirect.dependabot.com/junit-team/junit/pull/1669"">junit-team/junit#1669</a>) Make <code>FrameworkField</code> constructor public</h3>; <p>Prior to this change, custom runners could make <code>FrameworkMethod</code> instances, but not <code>FrameworkField</code> instances. This small change allows for both now, because <code>FrameworkField</code>'s constructor has been promoted from package-private to public.</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/junit-team/junit4/commit/1b683f4ec07bcfa40149f086d32240f805487e66""><code>1b683f4</code></a> [maven-release-plugin] prepare release r4.13.1</li>; <li><a href=""https://github.com/junit-team/junit4/commit/ce6ce3aadc070db2902698fe0d3dc6729cd631f2""><code>ce6ce3a</code></a> Draft 4.13.1 release notes</li>; <li><a href=""https://github.com/junit-team/junit4/commit/c29dd8239d6b353e699397eb090a1fd27411fa24""><code>c29dd82</code></a> Change version to 4.13.1-SNAPSHOT</li>; <li><a href=""https://github.com/junit-team/junit4/commit/1d174861f0b64f97ab0722bb324a760bfb02f567""><code>1d17486</code></a> Add a link to assertThrows in exception testing</li>; <li><a href=""https://github.com/junit-team/junit4/commit/543905df72ff10364b94dda27552efebf3dd04e9""><code>543905d</code></a> Use separate line for annotation in Javadoc</li>; <li><a href=""https://github.com/junit-team/junit4/commit/510e906b391e7e46a346e1c852416dc7be934944""><code>510e906</code></a> Add sub headlines to class Javadoc</li>; <li><a href=""https://github.com/junit-team/junit4/commit/610155b8c22138329f0723eec22521627dbc52ae""><code>610155b</code></a> Merge pull request from GHSA-269g-pwp5-87pp</li>; <li><a href=""https://github.com/junit-team/junit4/commit/b6cfd1e3d736cc2106242a8be799615b472c7fec""><code>b6cfd1e</code></a> Explicitly wrap float pa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5941:2001,release,release,2001,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5941,1,['release'],['release']
Deployability,r$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.executeOrRecover(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.withRetry(AsyncBackendJobExecutionActor.scala:61); 	at cromwell.backend.async.AsyncBackendJo,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:1983,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1983,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"r-wf_hello.hello:NA:1 [UUID(075e0cf3)]: Could not copy a suitable cache hit for 075e0cf3:wf_hello.hello:-1:1. No copy attempts were made.; 2021-09-27 13:48:31,449 cromwell-system-akka.dispatchers.backend-dispatcher-33 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: `echo ""Hello Cromwell! Welcome to Cromwell . . . on AWS!""`; 2021-09-27 13:48:33,376 cromwell-system-akka.dispatchers.backend-dispatcher-33 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Adjusting boot disk size to 16 GB: 10 GB (runtime attributes) + 5 GB (user command image) + 1 GB (Cromwell support images); 2021-09-27 13:48:38,987 cromwell-system-akka.dispatchers.backend-dispatcher-33 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: job id: projects/gred-cumulus-sb-01-991a49c4/operations/15427360049616748078; 2021-09-27 13:49:07,692 cromwell-system-akka.dispatchers.backend-dispatcher-35 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from - to Running; 2021-09-27 13:50:48,340 cromwell-system-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(075e0cf3)wf_hello.hello:NA:1]: Status change from Running to Failed; 2021-09-27 13:50:49,875 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Workflow 075e0cf3-194b-4f53-a43d-d31f0b370f79 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Execution failed: generic::permission_denied: pulling image: docker pull: running [""docker"" ""pull"" ""gcr.io/broad-cumulus/cellranger@sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356""]: exit status 1 (standard error: ""Error response from daemon: pull access denied for gcr.io/broad-cumulus/cellranger, repository does not exist or may require 'docker login': denied: Pe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:12224,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,12224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"r/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.io.IOException: Could not read from s3://s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; Caused by: java.nio.file.NoSuchFileException: s3://s3.amazonaws.com/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-HC_GVCF/shard-6/HC_GVCF-6-rc.txt; ...; [2019-05-22 19:42:10,31] [info] WorkflowManagerActor WorkflowActor-3997371c-9513-4386-a579-a72639c6e960 is in a terminal state: WorkflowFailedState; [2019-05-22 19:42:59,50] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; ...; Workflow 3997371c-9513-4386-a579-a72639c6e960 transitioned to state Failed; ```. Pulling the actual AWS Batch Job parameters for the ""failed"" job (7c2d29c2-f04e-4b3f-8579-915a6fbc9033) I see the following:; ```; {""jobs"": [{; ""status"": ""SUCCEEDED"", ; ""container"": {; ""mountPoints"": [{""sourceVolume"": ""local-disk"", ""containerPath"": ""/cromwell_root""}], ; ""taskArn"": ""arn:aws:ecs:us-east-1:260062248592:task/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""logStreamName"": ""hc_Haplotypecaller-hc_HC_GVCF/default/78221618-403c-4b10-b9e1-6c1534a44723"", ; ""image"": ""260062248592.dkr.ecr.us-east",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:11743,Pipeline,Pipeline,11743,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,1,['Pipeline'],['Pipeline']
Deployability,"r/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:6:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr15.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr15.RSM278260-6_8plex.dedup.recal.bam \; -O RSM278260-6_8plex.hc.gvcf.gz \; -ERC GVCF \; \; [2019-05-22 19:19:19,34] [info] Submitting job to AWS Batch; [2019-05-22 19:19:19,34] [info] dockerImage: 260062248592.dkr.ecr.us-east-1.amazonaws.com/s4-alignandmolvar:1.3.2; [2019-05-22 19:19:19,34] [info] jobQueueArn: arn:aws:batch:us-east-1:260062248592:job-queue/GenomicsDefaultQueue-80d8b8f0-15ed-11e9-b8b7-12ddf705bbc4; [2019-05-22 19:19:19,34] [info] taskId: Haplotypecaller.HC_GVCF-Some(1)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:7342,Pipeline,Pipeline,7342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,1,['Pipeline'],['Pipeline']
Deployability,"r: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 2019-01-31 19:38:58,618 INFO - changesets/failure_metadata.xml::failure_to_message::cjllanwarne: Successfully released change log lock; 2019-01-31 19:38:58,637 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.sc",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:1458,Update,UpdateVisitor,1458,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['Update'],['UpdateVisitor']
Deployability,rActor: Workflow a15c46b7-5f93-46d6-94a2-28f656914866 failed (during ExecutingWorkflowState): cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:51); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:34); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:20); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); at akka.actor.ActorCell.invoke(ActorCell.scala:583); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:6564,Pipeline,PipelinesApiRequestWorker,6564,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,r] No configuration setting found for key 'services' ; akka.actor.ActorInitializationException: akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor: exception during creation ; at akka.actor.ActorInitializationException$.apply(Actor.scala:193) ; at akka.actor.ActorCell.create(ActorCell.scala:669) ; at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:523) ; at akka.actor.ActorCell.systemInvoke(ActorCell.scala:545) ; at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:283) ; at akka.dispatch.Mailbox.run(Mailbox.scala:224) ; at akka.dispatch.Mailbox.exec(Mailbox.scala:235) ; at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ; at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) ; at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) ; at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ; Caused by: com.typesafe.config.ConfigException$Missing: No configuration setting found for key 'services' ; at com.typesafe.config.impl.SimpleConfig.findKeyOrNull(SimpleConfig.java:156) ; at com.typesafe.config.impl.SimpleConfig.findOrNull(SimpleConfig.java:174) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:188) ; at com.typesafe.config.impl.SimpleConfig.find(SimpleConfig.java:193) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:268) ; at com.typesafe.config.impl.SimpleConfig.getObject(SimpleConfig.java:41) ; at cromwell.services.ServiceRegistryActor$.serviceNameToPropsMap(ServiceRegistryActor.scala:35) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCe,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577:1426,configurat,configuration,1426,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577,1,['configurat'],['configuration']
Deployability,"ral scatter-gather blocks. We switched to a separate metadata database to address the Java heap problem that occurs with the in-memory database. We enabled debug logging to try and troubleshoot an unrelated problem. Most of the log output at the increased level is appears to be from HSQL. After running for about 8 hrs, the following error appears in the output and Cromwell hangs:; ```; Exception in thread ""Exec Stream Pumper"" java.lang.OutOfMemoryError: Required array length 2147483639 + 39 is too large; 	at java.base/jdk.internal.util.ArraysSupport.hugeLength(ArraysSupport.java:649); 	at java.base/jdk.internal.util.ArraysSupport.newLength(ArraysSupport.java:642); 	at java.base/java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:100); 	at java.base/java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:132); 	at org.apache.commons.io.output.ProxyOutputStream.write(ProxyOutputStream.java:92); 	at org.apache.commons.io.output.TeeOutputStream.write(TeeOutputStream.java:68); 	at org.apache.commons.exec.StreamPumper.run(StreamPumper.java:108); 	at java.base/java.lang.Thread.run(Thread.java:1623); ```. We are running Cromwell using Dockstore as a wrapper using the following command:; ```; dockstore workflow launch --local-entry BiobankScrubWorkflow.wdl --json inputs.json > dockstore.log 2>&1 &; ```. At the time the OOME occurs, the size of the dockstore.log file is approx 2147485425 bytes. Based on the ""Saving copy of Cromwell stdout to..."" messages at the end of a successful Cromwell run, it would appear that Cromwell is internally buffering the stdout and stderr streams to save at the end of the run. So when the size of the stdout or stderr exceeds the Java buffer max size, the OOME occurs. The Cromwell configuration we are using is the default with the exception of uncommenting the `database -> metadata` block and updating the docker run command to mount the local GCloud SDK configuration into the container to enable access to GCP resources.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7217:2027,configurat,configuration,2027,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7217,2,['configurat'],['configuration']
Deployability,rc) from 1.33 to 2.0.; [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/RELEASES.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/RELEASES.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/RELEASES.rst) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/ReleaseNotes.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/ReleaseNotes.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/ReleaseNotes.rst) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Releases.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Releases.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Releases.rst) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/releases.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/releases.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/releases.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7081:1059,Release,Release,1059,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7081,1,['Release'],['Release']
Deployability,"rce file the same bucket as the workflow bucket? If not, are they in the same region?; > [â€¦](#); > On Wed, Nov 11, 2020 at 4:28 AM Luyu ***@***.***> wrote: Hi, The improved multipart copying (api: CreateMultipartUpload) doesn't work for me. The cromwell server always checks the existence of the cached file before the copying finishes. In Cromwell v51 and before, some small files <100GB were able to be successfully cached. However, with Cromwell v53, even a 6GB result file got a problem of caching and has to rerun. Is there any way to prevent the timeout of the actor? Hi, In Cromwell 52 we updated the S3 module to perform multithreaded, multipart copies to improve the size of results that may be cached. There are also additional improvements that have recently been merged into dev and should appear in the next release version (or you could build from source) v52+ requires a new AWS configuration. Instructions are in https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf â€¦ <#m_3227077625045957240_> On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout exception during cache copying on AWS S3. The cache file size is 133GB. Given the file size, more time should be allowed for cache copying. Is there any config option that can tune this? Thank you in advance for any suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure copying cache results for job BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out waiting for a response to copy s3://xxxxx/cromwell-execution/Germ line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVariantDiscovery_GATK4/95aed08f-3045-45e4-94c9-ba0230851136 /call-SamToFastqAndBwaMem/shard-0/39T_R.unmerged.bam to s3://xxxxx/cromwell-execution/Germline_Somatic_Calling/c25a8561-808f-4b46-9bd2-ef0488 8c0031/call-Tumor_Bam/PreProcess",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046:1424,Install,Installing,1424,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726476046,1,['Install'],['Installing']
Deployability,"rce); * [io.circe:circe-generic](https://github.com/circe/circe); * [io.circe:circe-generic-extras](https://github.com/circe/circe-generic-extras); * [io.circe:circe-literal](https://github.com/circe/circe); * [io.circe:circe-optics](https://github.com/circe/circe-optics); * [io.circe:circe-parser](https://github.com/circe/circe); * [io.circe:circe-refined](https://github.com/circe/circe); * [io.circe:circe-shapes](https://github.com/circe/circe). from 0.13.0 to 0.14.1.; [GitHub Release Notes](https://github.com/circe/circe/releases/tag/v0.14.1) - [Version Diff](https://github.com/circe/circe/compare/v0.13.0...v0.14.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/92f43ce5f010fa03daeb2625a3f9c1b1be80cca5/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.13.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/rnaseq-workflow/steps/prepare_sample.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6410:1105,configurat,configuration,1105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6410,1,['configurat'],['configuration']
Deployability,"rcion defined from '1' of type 'eu.timepit.refined.api.Refined' to 'Int'.; at wom.types.WomType.coerceRawValue(WomType.scala:36); at wom.types.WomType.coerceRawValue$(WomType.scala:27); at wom.types.WomIntegerType$.coerceRawValue(WomIntegerType.scala:9); at cromwell.backend.impl.sfs.config.DeclarationValidation.$anonfun$extractWdlValueOption$1(DeclarationValidation.scala:113); at scala.Option.map(Option.scala:146); at cromwell.backend.impl.sfs.config.DeclarationValidation.extractWdlValueOption(DeclarationValidation.scala:113); at cromwell.backend.impl.sfs.config.ConfigAsyncJobExecutionActor.$anonfun$runtimeAttributeInputs$1(ConfigAsyncJobExecutionActor.scala:163). <!-- Which backend are you running? -->; I use the SGE backend ; <!-- Paste/Attach your workflow if possible: -->; this is my WDL workflow; """"""; workflow testsge{; String Outdir; String JobName=""filter""; call filter{input:outdir=Outdir,jobname=JobName}; }. task filter{; String outdir; String jobname; command<<<; echo ""test successful"" >>${outdir}/log.stdout; echo 1; perl -we '{print STDERR 2;}'; Script=""${jobname}""; Sleep=$SGE_TASK_ID; QsubRcControl=3; QsubType=1; >>>; runtime{; backend:""SGE""; memory:""1 GB""; sge_queue:""test.q -P test -t 1-3""; sge_project:""test""; jobs_name:""${jobname}""; }; output{; String presuccess=""done""; Int rc=read_lines(stdout())[0]; }; }; """"""; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; this my runtime-attributes setting in the reference.conf file. runtime-attributes = """"""; Int cpu = 1; Float? memory_gb; String? jobs_name; String? sge_queue; String? sge_project; """""". submit = """"""; qsub \; -clear \; -terse \; -N ${job_name} \; -wd ${cwd}/execution \; -o ${out}.qsub \; -e ${err}.qsub \; ${""-l vf="" + memory_gb +""g,num_proc="" + cpu} \; ${""-q "" + sge_queue} \; -binding ${""linear:"" + cpu} \; ${script} | perl -ne 's/\..*//;print;'; """""". kill = ""qdel ${job_id}""; check-alive = ""qstat -j ${job_id}""; job-id-regex = ""(\\d+)""; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3805:2252,configurat,configuration,2252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3805,1,['configurat'],['configuration']
Deployability,"rds spending 10 minutes in ""Waiting for quota"" when this problem happens. The instance that gives PAPI Error Code 10 was able to get a virtual machine, though. Maybe there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$B",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1092,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1092,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"re the build breakage, I need to have the wdl4s snapshot published by the other PR's merge to develop and then update the build.sbt to reference that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/510#issuecomment-193330854:111,update,update,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/510#issuecomment-193330854,1,['update'],['update']
Deployability,re.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); at software.amazon.awss,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:1881,pipeline,pipeline,1881,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['pipeline'],['pipeline']
Deployability,re.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.j,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:4573,pipeline,pipeline,4573,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,3,['pipeline'],['pipeline']
Deployability,"reads; File db; File query; String result_name; String results_folder; String mode = ""blastx"". call diamond_blast {; input:; threads = threads,; database = db,; name = result_name,; query = query,; mode = mode. }. call copy as copy_results {; input:; files = [diamond_blast.out],; destination = results_folder; }. output {; File out = copy_results.out[0]; }. }. task diamond_blast {. Int threads; File database; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pipelines/results/graywhale/transcriptome/diamond/blastp""; ```; when I run the workflow many times with input changes I get the following:; ```. name | status | stdout | stderr | cache | shard; -- | -- | -- | -- | -- | --; Diamond_Blast.copy_results | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stderr | Cache Hit: fdda40c0-a501-456c-a903-954aa52af83d:Diamond_Blast.copy_results:-1 | -1; Diamond_Blast.diamond_blast | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3044:1747,pipeline,pipelines,1747,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044,1,['pipeline'],['pipelines']
Deployability,"red; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.run",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3732:1613,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1613,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"related to #2399 and #2830 . As a **Cromwell dev**, I want to **automatically release Cromwell once Travis is green**, so that **Travis doesn't release when it's failing**.; - effort: small; - risk: small; - business value: small (to medium); - have there been any regressions that Travis would have caught but it was red when we released?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964:78,release,release,78,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964,3,['release'],"['release', 'released']"
Deployability,releaseHold endpoint returns inaccurate status code,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3911:0,release,releaseHold,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3911,1,['release'],['releaseHold']
Deployability,repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/92f43ce5f010fa03daeb2625a3f9c1b1be80cca5/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.13.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/rnaseq-workflow/steps/prepare_sample.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/variantcall_batch_region.cwl; ```; </details>; <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6410:1927,integrat,integrationTestCases,1927,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6410,1,['integrat'],['integrationTestCases']
Deployability,"ridss/gridss.sh\"",\n \""arguments\"": [\n {\n \""prefix\"": \""--threads\"",\n \""valueFrom\"": \""$get_threads_val(inputs)\""\n }\n ],\n \""inputs\"": [\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""Optional - location of the GRIDSS assembly BAM. This file will be created by GRIDSS.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--assembly\""\n },\n \""default\"": \"".assembly.bam\"",\n \""id\"": \""#gridss-2.9.4.cwl/assembly\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional - BED file containing regions to ignore\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--blacklist\""\n },\n \""id\"": \""#gridss-2.9.4.cwl/blacklist\""\n },\n {\n \""type\"": \""string\"",\n \""doc\"": \""portion of 6 sigma read pairs distribution considered concordantly mapped. Default: 0.995\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--concordantreadpairdistribution\""\n },\n \""default\"": \""0.995\"",\n \""id\"": \""#gridss-2.9.4.cwl/concordantreadpairdistribution\""\n },\n {\n \""type\"": [\n \""null\"",\n \""File\""\n ],\n \""doc\"": \""Optional - configuration file use to override default GRIDSS settings.\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--configuration\""\n },\n \""id\"": \""#gridss-2.9.4.cwl/configuration\""\n },\n {\n \""type\"": [\n \""null\"",\n \""boolean\""\n ],\n \""doc\"": \""Optional - use the system version of bwa instead of the in-process version packaged with GRIDSS\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--externalaligner\""\n },\n \""default\"": false,\n \""id\"": \""#gridss-2.9.4.cwl/externalaligner\""\n },\n {\n \""type\"": [\n \""null\"",\n \""string\""\n ],\n \""doc\"": \""Optional - location of GRIDSS jar\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--jar\""\n },\n \""default\"": \""/opt/gridss/gridss-2.9.4-gridss-jar-with-dependencies.jar\"",\n \""id\"": \""#gridss-2.9.4.cwl/jar\""\n },\n {\n \""type\"": \""boolean\"",\n \""doc\"": \""zero-based assembly job index (only required when performing parallel assembly across multiple computers)\\n\"",\n \""inputBinding\"": {\n \""prefix\"": \""--jobindex\""\n },\n \""default\""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:73446,configurat,configuration,73446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['configurat'],['configuration']
Deployability,"ries:. 2019-05-24 12:32:07,173 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID\; (a309b1f1)assemble_denovo.filter_to_taxon:NA:1]: Status change from Running to Failed; 2019-05-24 12:32:08,258 cromwell-system-akka.dispatchers.engine-dispatcher-74 ERROR - WorkflowManagerActor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecut",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001:1148,Pipeline,PipelinesApiAsyn,1148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001,1,['Pipeline'],['PipelinesApiAsyn']
Deployability,"ring] phenolist = paaaa.phenotype_out; }; }; task paaaa {; input{; String phenotype; File annotation; }; command <<<; set -eux; >>>; output {; File plots = phenotype + ""_plot.pdf""; String phenotype_out = phenotype; }; }; ```; The difference between the two being different task names, ""aaaa"" and ""paaaa"", respectively. Note that both of the workflows have an input named ""phenolist"", and an output named ""phenolist"". ; When running `womtool-85.jar inputs working.wdl`, the results are; ```json; {; ""gwas_validation.phenolist"": ""File"",; ""gwas_validation.aaaa.annotation"": ""File""; }; ```; As they should. For the failing workflow, the results are; ```; {; ""gwas_validation.phenolist"": ""File""; }; ```; As you can see, the input ""gwas_validation.paaaa.annotation"": ""File"" has been dropped.; womtool and cromwell-84 (not tested on cromwell-85) also drop all outputs from the outputs, returning only ""{}"".; The task also fails on cromwell, since cromwell does not recognize that there should be any other inputs than ""gwas_validation.phenolist"", and raises an error on that. ; Please note that this is a minified example. renaming variables, erasing variables, adding variables etc can change the failing example to working and vice versa. Based on the behaviour of the examples, I suspect this is related to building the workflow graph somehow. I have included the workflows as well as their womgraphs as files in this issue. [fail.graph.txt](https://github.com/broadinstitute/cromwell/files/11033034/fail.graph.txt); [fail.wdl.txt](https://github.com/broadinstitute/cromwell/files/11033035/fail.wdl.txt); [work.graph.txt](https://github.com/broadinstitute/cromwell/files/11033036/work.graph.txt); [work.wdl.txt](https://github.com/broadinstitute/cromwell/files/11033037/work.wdl.txt); <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; ## Configuration:; Latest womtool-85.jar, downloaded from releases OR; cromwell-84.jar OR; womtool-84.jar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7096:3290,configurat,configuration,3290,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7096,3,"['Configurat', 'configurat', 'release']","['Configuration', 'configuration', 'releases']"
Deployability,"rkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); cromwell_1 | 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); cromwell_1 | 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); cromwell_1 | ; cromwell_1 | 2024-01-11 11:09:38 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - BT-322 0845428a:myworkflow.mytask:-1:1 is not eligible for call caching; ```; <!-- Which backend are you running? -->; Used backend: ; GCPBATCH. Callcaching works with PAPIv2, not on GCPBATCH.; <!-- Paste/Attach your workflow if possible: -->; workflow used for testing:; ```; workflow myworkflow {; call mytask; }. task mytask {; String str = ""!""; command <<<; echo ""hello world ${str}""; >>>; output {; String out = read_string(stdout()); }. runtime{; docker: ""eu.gcr.io/project/image_name:tag""; cpu: ""1""; memory: ""500 MB""; disks: ""local-disk 5 HDD""; zones: ""europe-west1-b europe-west1-c europe-west1-d""; preemptible: 2; noAddress: true; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; We are using cromwell through broadinstitute/cromwell:87-ecd44b6 image.; cromwell configuration:; ```; include required(classpath(""application"")). system.new-workflow-poll-rate=1. // increase timeout for http requests..... getting meta-data can timeout for large workflows.; akka.http.server.request-timeout=600s. # Maximum number of input file bytes allowed in order to read each type.; # If exceeded a FileSizeTooBig exception will be thrown.; system {; 	job-rate-control {; 	 jobs = 100; 	 per = 1 second; 	}; input-read-limits {; lines = 128000000; bool = 7; int = 19; float = 50; string = 1280000; json = 12800000; tsv = 1280000000; map = 128000000; object = 128000000; }. # If 'true', a SIGTERM or SIGINT will trigger Cromwell to attempt to gracefully shutdown in server mode,; # in particular clearing up all queued database writes before letting the JVM shut down.; # The shutdown",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:7083,configurat,configuration,7083,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['configurat'],['configuration']
Deployability,"rkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2020-08-21 11:08:59,66] [info] WorkflowManagerActor WorkflowActor-dbd5cdc0-c79a-42cd-b929-56ddb1115467 is in a terminal state: WorkflowFailedState; [2020-08-21 11:09:00,66] [info] Not triggering log of token queue status. Effective log interval = None; ```. Here is the relevant part of the wdl:. ```; backend {; default = PAPIv2; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; concurrent-job-limit = 10000; max-concurrent-workflows = 10000; genomics-api-queries-per-100-seconds = 10000; maximum-polling-interval = 300; max-workflow-launch-count = 2000; // Google project; project = ""calico-uk-biobank""; compute-service-account = ""default""; // Base bucket for workflow executions; root = ""nicholas-b-test""; // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }; genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; }; filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; }; sra {; class = ""cromwell.filesystems.sra.SraPathBuilderFactory""; docker-image = ""fusera/fusera:alpine""; ngc = ""didnt want to put this up online""; }; }; }; }; }; }; ```. I ran this with cromwell 52. Any suggestions would be appreciated",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793:3606,Pipeline,Pipelines,3606,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793,1,['Pipeline'],['Pipelines']
Deployability,"rkflow written up and running locally in WDL, and I'm getting started with the JES backend. I've done a fair amount of work in google genomics before this, but this is my first use of cromwell/WDL. Some details first. I first noticed this error on 0.19.2, but went back to check 0.19 and HEAD of the develop branch. where it occurs as well. For completeness, here's my WDL file:. ```; cat ~/workflows/hello-jes.wdl ; task jes_task {; command {; echo ""Hello JES!""; }; runtime {; docker: ""ubuntu:latest""; memory: ""4G""; cpu: ""3""; zones: ""us-central1-c us-central1-a""; disks: ""/mnt/mnt1 3 SSD, /mnt/mnt2 500 HDD""; }; }; workflow jes_workflow {; call jes_task; }; ```. and the console output:. ```; [2016-04-28 15:35:51,218] [info] JesBackend [1cb9c1d2:jes_task]: echo ""Hello JES!""; Apr 28, 2016 3:35:51 PM com.google.api.client.googleapis.services.AbstractGoogleClient <init>; WARNING: Application name is not set. Call Builder#setApplicationName.; [2016-04-28 15:35:51,646] [info] JES Pipeline [1cb9c1d2:jes_task]: Inputs:; exec -> disk:local-disk relpath:exec.sh; [2016-04-28 15:35:51,647] [info] JES Pipeline [1cb9c1d2:jes_task]: Outputs:; jes_task-rc.txt -> disk:local-disk relpath:jes_task-rc.txt; [2016-04-28 15:35:51,648] [info] JES Pipeline [1cb9c1d2:jes_task]: Mounts:; c98942d68bf4c33728f1adef1bfd9ccc -> /mnt/mnt1 (3GB PERSISTENT_SSD); 4fd1d1e01455dfdd4eabcf02c1abaf55 -> /mnt/mnt2 (500GB PERSISTENT_HDD); local-disk -> /cromwell_root (10GB PERSISTENT_SSD); [2016-04-28 15:35:51,728] [warn] JesBackend [1cb9c1d2:jes_task]: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/refe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757:1042,Pipeline,Pipeline,1042,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757,1,['Pipeline'],['Pipeline']
Deployability,"rkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Prom",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804:1551,pipeline,pipelines,1551,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804,1,['pipeline'],['pipelines']
Deployability,"rkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1254); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Prom",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1829,pipeline,pipelines,1829,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['pipeline'],['pipelines']
Deployability,"rminates. I think this fix is simple if I add a cleanup step. I will do this ASAP. Thanks, Mark; > [â€¦](#); > On Sat, Oct 24, 2020 at 5:27 AM Luyu ***@***.***> wrote: Hi, I have set up a Cromwell platform on AWS batch according to https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/ If I run GATK Best Practice pipeline for one sample, it works perfectly. However, when I ran this pipeline for 10+ samples concurrently, many AWS EC2 instances were re-used by AWS batch. Cromwell didn't clean up the localized S3 files and output files produced by previous tasks. This quickly inflated EBS cost when EBS autoscaling is enabled. One of my instances went up to 9.1TB and hit the upper bound for autoscaling, then the running task failed due to no space. I have checked Cromwell documents and some materials from AWS, as well as issue #4323 <#4323>. But none of them works for me. Thank you in advance for any suggestions. â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <#5974>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EPKRNY6TFQPVAG2Q4DSMKMZZANCNFSM4S5OX5IA> . Hi Mark,. Thanks for your reply. I think I find a workaround (probably close to a real solution). I find the script for a container to run is generated at https://github.com/broadinstitute/cromwell/blob/491082aa3e5b3bd5657f339c959260951333e638/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L435 . The `SCRIPT_EPILOGUE` has a default value `sync`. This could be set by adding `script-epilogue = ""sync; ls | grep -v 'rc.txt' | xargs rm -rf` in the cromwell.conf, referring to https://github.com/broadinstitute/cromwell/blob/8e5ca8791d2ba685965a655f2404972f2c80299d/cromwell.example.backends/LocalExample.conf#L43 I'm not sure if this works for AWS batch backend. **UPDATE: `SCRIPT_EPILOGUE` is not valid for AWS backend. This workaround doesn't work ... :(**. Thanks,; Luyu",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694:2322,UPDATE,UPDATE,2322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-716242694,1,['UPDATE'],['UPDATE']
Deployability,"roadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; I'm having issues running CWL workflows with Cromwell 44 whereas previously with 36.1, it passes. I have workarounds but I'm wondering which ones are issues and which ones are design changes. Here's my test script:; ```; #!/bin/bash; set -o pipefail; set -o nounset; set -o xtrace. wget https://github.com/broadinstitute/cromwell/releases/download/44/cromwell-44.jar; wget https://github.com/broadinstitute/cromwell/releases/download/36.1/cromwell-36.1.jar; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/1st-tool.cwl; wget https://raw.githubusercontent.com/common-workflow-language/common-workflow-language/master/v1.0/examples/echo-job.yml; zip imports.zip 1st-tool.cwl echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-36.1.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl; java -jar cromwell-44.jar run 1st-tool.cwl --inputs echo-job.yml --type cwl --imports imports.zip; ```. Of the last 6 commands, the 1st, 2nd, 3rd, and 5th command pass. The 4th and 6th does not. So my two issues are:; 1. 44 seems to have troubles figuring out what the lang",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5085:1351,release,releases,1351,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5085,1,['release'],['releases']
Deployability,"rom Running to Success; 2023-04-18 22:00:09,060 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:107:1]: Status change from Running to Success; 2023-04-18 22:00:18,464 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:106:1]: Status change from Running to Success; 2023-04-18 22:01:20,604 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:111:1]: Status change from Running to Success; 2023-04-18 22:14:47,728 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: Aborting workflow; 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:262:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/9178938377659283430); 2023-04-18 22:14:47,729 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8):myco.pull:112:1] Aborted StandardAsyncJob(projects/16371921765/locations/us-central1/operations/8559201934542591362); 2023-04-18 22:14:48,295 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Successfully requested cancellation of projects/16371921765/locations/us-central1/operations/9178938377659283430; 2023-04-18 22:15:56,564 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:112:1]: Status change from Running to Success; 2023-04-18 22:16:44,505 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(10fa31a8)myco.pull:262:1]: Status change from Running to Cancelled; 2023-04-18 22:16:44,539 INFO - WorkflowExecutionActor-10fa31a8-acbe-4ab7-a96a-6550ec08df12 [UUID(10fa31a8)]: WorkflowExecutionActor [UUID(10fa31a8)] aborted: myco.pull:262:1; 2023-04-18 22:16:45,159 INFO - $f [UUID(10fa31a8)]: Copying workflow logs from /cromwell-workflow-logs/workflow.10fa31a8-acbe-4ab7-a96a-6550ec08df12.log to gs://f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7121:5144,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,5144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7121,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"ror code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9606 Execution failed: action 14: unexpected exit status 1 was not ignored; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$Abstrac",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3732:1450,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1450,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"rs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:06,298 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:40,200 cromwell-system-akka.dispatchers.backend-dispatcher-109 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:42,570 cromwell-system-akka.dispatchers.backend-dispatcher-40 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:42,897 cromwell-system-akka.dispatchers.backend-dispatcher-140 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 19:03:58,493 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Workflow drs_usa_jdr complete. Final Outputs:; ""drs_usa_jdr.path1"": ""/cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json"",; ""drs_usa_jdr.map1"": {; ""drs_usa_jdr.size1"": ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:8375,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,8375,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"rt images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,896 cromwell-system-akka.dispatchers.backend-dispatcher-91 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/6169035039702064455; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 18:58:34,415 cromwell-system-akka.dispatchers.backend-dispatcher-88 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 18:59:03,708 cromwell-system-akka.dispatchers.backend-dispatcher-96 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Status change from - to Running; 2020-10-13 18:59:03,760 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)d",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:4695,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,4695,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:834); Caused by: java.lang.ArrayIndexOutOfBoundsException: null; 2021-12-06 17:03:51,401 cromwell-system-akka.dispatchers.service-dispatcher-9 ERROR - Error summarizing metadata; java.sql.SQLException: null; at com.mysql.cj.jdbc.exceptions.SQLError.createSQLException(SQLError.java:129); at com.mysql.cj.jdbc.exceptions.SQLExceptionsMapping.translateException(SQLExceptionsMapping.java:122); at com.mysql.cj.jdbc.ConnectionImpl.setAutoCommit(ConnectionImpl.java:2045); at com.zaxxer.hikari.pool.ProxyConnection.setAutoCommit(ProxyConnection.java:388); at com.zaxxer.hikari.pool.HikariProxyConnection.setAutoCommit(HikariProxyConnection.java); at slick.jdbc.JdbcBackend$BaseSession.startInTransaction(JdbcBackend.scala:511); at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:37); at slick.jdbc.JdbcActionComponent$StartTransaction$.run(JdbcActionComponent.scala:34); at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276); at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276); at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128); at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628); at java.base/java.lang.Thread.run(Thread.java:834); Caused by: java.lang.ArrayIndexOutOfBoundsException: null; > . These two days I also saw some people here have a similar error, it seems that this is not the process of the problem, error of this step process can run normally, but he cannot update the data in the database, cause I will continue to run again don't think this step is to run, and start to run this step, then the metadata can't collect the mistakes, then the process stopped again, I don't know how to solve this anymore, can anyone give me some help?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6583:3736,update,update,3736,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6583,1,['update'],['update']
Deployability,"ry twice and upon secondary success return a `TestCanceled` event to each `Reporter` _without_ the original exception. The original error `Outcome` does not seem to be forwarded to the `Reporter`. Instead we may need to implement our own fork of `withRetry` that captures and forwards the original exception before retrying the test, wiring the original error to our custom `Reporter` in some way or via some singleton cache. For an external system to aggregate the errors something like https://logit.io/ could be used but https://sentry.io/ is specifically built for error triage. As the above features will only be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.con",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:2533,Update,Update,2533,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,1,['Update'],['Update']
Deployability,"ry_GATK4.MergeBamAlignment:0:1 failed. The job was stopped before the command finished. PAPI error code 5. 8: Failed to pull image broadinstitut; e/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71: ""docker pull broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71"" failed: exit status 1: sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71: Pulling from broadinstitute/gatk; cromwell_1 | ae79f2514705: Pulling fs layer; cromwell_1 | 5ad56d5fc149: Pulling fs layer; cromwell_1 | 170e558760e8: Pulling fs layer; cromwell_1 | 395460e233f5: Pulling fs layer; cromwell_1 | 6f01dc62e444: Pulling fs layer; cromwell_1 | 98db058f41f6: Pulling fs layer; [...]; cromwell_1 | failed to register layer: Error processing tar file(exit status 1): write /root/.cache/pip/http/5/1/d/8/2/51d82969228464b761a16257d5eefe8e2b3dde3c1ad733721353e785: no space left on device; cromwell_1 |; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.F",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337:1798,pipeline,pipelines,1798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337,1,['pipeline'],['pipelines']
Deployability,"s change from Running to Failed; 2021-09-27 13:50:49,875 cromwell-system-akka.dispatchers.engine-dispatcher-9 INFO - WorkflowManagerActor: Workflow 075e0cf3-194b-4f53-a43d-d31f0b370f79 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_hello.hello:NA:1 failed. The job was stopped before the command finished. PAPI error code 7. Execution failed: generic::permission_denied: pulling image: docker pull: running [""docker"" ""pull"" ""gcr.io/broad-cumulus/cellranger@sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356""]: exit status 1 (standard error: ""Error response from daemon: pull access denied for gcr.io/broad-cumulus/cellranger, repository does not exist or may require 'docker login': denied: Permission denied for \""sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"" from request \""/v2/broad-cumulus/cellranger/manifests/sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"".\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:91); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:812); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:95); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1340); 	at cromwell.backend",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:13501,pipeline,pipelines,13501,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['pipeline'],['pipelines']
Deployability,"s commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=/home/jeremiah/cromwell.mysql.conf -jar server/target/scala-2.12/cromwell-37-88f82a0-SNAP.jar server; 2019-01-31 18:42:55,062 INFO - Running with database db.url = jdbc:mysql://localhost/cromwell?rewriteBatchedStatements=true; 2019-01-31 18:43:01,372 INFO - Successfully acquired change log lock; 2019-01-31 18:43:04,396 INFO - Creating database history table with name: cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,492 INFO - Reading from cromwell.DATABASECHANGELOG; 2019-01-31 18:43:04,918 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,926 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,927 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,928 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configur",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:5164,configurat,configuration,5164,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['configurat'],['configuration']
Deployability,"s problem. The VM is not a preemptible and I'm using Cromwell v32. There's a lot of shards spending 10 minutes in ""Waiting for quota"" when this problem happens. The instance that gives PAPI Error Code 10 was able to get a virtual machine, though. Maybe there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$Abst",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1009,pipeline,pipelines,1009,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['pipeline'],['pipelines']
Deployability,"s used to work on Cromwell 48, we updated to the latest Cromwell 52, still had the same errors, see belowL. 2020-08-04 23:44:00,228 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - WorkflowManagerActor Workflow f1dca11c-ea29-48b1-9691-9f30c9e59154 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_quip_lymphocyte_segmentation_v03232020.quip_lymphocyte_segmentation:NA:2 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: generic::unknown: installing drivers: container exited with unexpected exit code 1: + COS_DOWNLOAD_GCS=https://storage.googleapis.com/cos-tools; + COS_KERNEL_SRC_GIT=https://chromium.googlesource.com/chromiumos/third_party/kernel; + COS_KERNEL_SRC_ARCHIVE=kernel-src.tar.gz; + TOOLCHAIN_URL_FILENAME=toolchain_url; + TOOLCHAIN_ARCHIVE=toolchain.tar.xz; + TOOLCHAIN_ENV_FILENAME=toolchain_env; + CHROMIUMOS_SDK_GCS=https://storage.googleapis.com/chromiumos-sdk; + ROOT_OS_RELEASE=/root/etc/os-release; + KERNEL_SRC_DIR=/build/usr/src/linux; + NVIDIA_DRIVER_VERSION=418.40.04; + NVIDIA_DRIVER_MD5SUM=; + NVIDIA_INSTALL_DIR_HOST=/var/lib/nvidia; + NVIDIA_INSTALL_DIR_CONTAINER=/usr/local/nvidia; + ROOT_MOUNT_DIR=/root; + CACHE_FILE=/usr/local/nvidia/.cache; + LOCK_FILE=/root/tmp/cos_gpu_installer_lock; + LOCK_FILE_FD=20; + set +x; [INFO 2020-08-04 23:40:07 UTC] Checking if this is the only cos-gpu-installer that is running.; [INFO 2020-08-04 23:40:07 UTC] Running on COS build id 12871.1174.0; [INFO 2020-08-04 23:40:07 UTC] Checking if third party kernel modules can be installed; [INFO 2020-08-04 23:40:07 UTC] Checking cached version; [INFO 2020-08-04 23:40:07 UTC] Cache file /usr/local/nvidia/.cache not found.; [INFO 2020-08-04 23:40:07 UTC] Did not find cached version, building the drivers...; [INFO 2020-08-04 23:40:07 UTC] Downloading GPU installer ...; [INFO 2020-08-04 23:40:09 UTC] Downloading from https://storage.googleapis.com/nvidia-drivers-us-public/tesla/418.40.04/NVIDIA-Linux-x86_64",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:2835,release,release,2835,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,1,['release'],['release']
Deployability,"s"" then I get the following error `BadRequestException: 400 Bucket is requester pays bucket but no user project provided`. I googled it and found that [`gsutil` must use JSON API (not CLI)](https://cloud.google.com/storage/docs/requester-pays) for ""requester pays"" buckets. Is there any plan to support ""requester pays"" buckets for JES backend? . ```; [2017-11-18 16:01:09,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: python $(which encode_bowtie2.py) \; /cromwell_root/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/d57a5f97-8542-4fcc-89c4-b7c487957dea/call-trim_adapter/shard-0/glob-019a547c7b0dda79121d0398158a07d0/ENCFF439VSY.trim.merged.R1.fastq.gz \; \; --multimapping 4 \; \; --nth 4; [2017-11-18 16:01:09,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: python $(which encode_bowtie2.py) \; /cromwell_root/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/d57a5f97-8542-4fcc-89c4-b7c487957dea/call-trim_adapter/shard-1/glob-019a547c7b0dda79121d0398158a07d0/ENCFF463QCX.trim.merged.R1.fastq.gz \; \; --multimapping 4 \; \; --nth 4; [2017-11-18 16:01:18,97] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: job id: operations/EJSf0Yz9Kxjs8__E9aKWivQBILWN-vrbGyoPcHJvZHVjdGlvblF1ZXVl; [2017-11-18 16:01:18,97] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: job id: operations/EJWg0Yz9KxiXpeC4gsSenC4gtY36-tsbKg9wcm9kdWN0aW9uUXVldWU; [2017-11-18 16:01:30,30] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: Status change from - to Initializing; [2017-11-18 16:01:30,30] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: Status change from - to Initializing; [2017-11-18 17:44:21,09] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:1:1]: Status ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916:1325,pipeline,pipeline-genome-data,1325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916,1,['pipeline'],['pipeline-genome-data']
Deployability,"s/GraphPrint.scala:114:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:157:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:328); 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	 at cromwell.core.path.NioPathMethods.subpath(NioPathMethods.scala:18); 	 at cromwell.core.path.NioPathMethods.subpath$(NioPathMethods.scala:18); 	 at cromwell.core.path.DefaultPath.subpath(DefaultPathBuilder.scala:55); 	 at cromwell.backend.io.JobPathsWithDocker.toDockerPath(JobPathsWithDocker.scala:56); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$mapCommandLineWomFile$1(SharedF",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:1577,install,install,1577,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['install'],['install']
Deployability,s/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>Thatâ€™s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>Thatâ€™s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917:3817,pipeline,pipelines,3817,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917,3,"['Pipeline', 'pipeline']","['PipelinesApiRequestWorker', 'pipelines']"
Deployability,"s/failure_metadata.xml::failure_to_message::cjllanwarne: Successfully released change log lock; 2019-01-31 19:38:58,637 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::failure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseExceptio",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:1718,update,updateSchema,1718,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['update'],['updateSchema']
Deployability,"s/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Change Set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.ru",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606:1555,Update,UpdateVisitor,1555,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606,1,['Update'],['UpdateVisitor']
Deployability,"s/v1-docker-biocontainers-samtools-1-3-1"",; ""gcr.io/gcp-runtimes/ubuntu_16_0_4:latest"": ""projects/broad-dsde-cromwell-dev/global/images/v1-docker-gcr-io-gcp-runtimes-ubuntu-16-0-4-latest"",; ...; }; ```. I followed this format but got this error; ```; [2022-11-20 18:17:16,88] [warn] Failed to build PipelinesApiConfigurationAttributes on attempt 1 of 3, retrying.; cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$$anon$1: Google Pipelines API configuration is not valid: Errors:; Attempt to decode value on failed cursor: DownField(manifestFormatVersion); at cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$.apply(PipelinesApiConfigurationAttributes.scala:307); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.defaultBuildAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:32); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.$anonfun$papiAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:34); at scala.util.Try$.apply(Try.scala:210); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.cromwell$backend$google$pipelines$common$PipelinesApiBackendLifecycleActorFactory$$build$1(PipelinesApiBackendLifecycleActorFactory.scala:109); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.robustBuildAttributes(PipelinesApiBackendLifecycleActorFactory.scala:120); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.<init>(PipelinesApiBackendLifecycleActorFactory.scala:34); at cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:10); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.Delegat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:1192,Pipeline,PipelinesApiBackendLifecycleActorFactory,1192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,1,['Pipeline'],['PipelinesApiBackendLifecycleActorFactory']
Deployability,"sApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 2020-10-13 18:58:03,926 cromwell-system-akka.dispatchers.backend-dispatcher-63 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); 2020-10-13 18:58:05,110 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:2833,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2833,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,sApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:51); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:34); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:20); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); at akka.actor.ActorCell.invoke(ActorCell.scala:583); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:3942,Pipeline,PipelinesApiRequestWorker,3942,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,"s_again.xml::standardize_column_names_again::kshakir: Unique constraint added to WORKFLOW_STORE_ENTRY(WORKFLOW_EXECUTION_UUID); 2018-06-07 12:16:10,858 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: Index IX_JOB_STORE_ENTRY_WEU created; 2018-06-07 12:16:10,858 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: Index IX_WORKFLOW_METADATA_SUMMARY_ENTRY_WN created; 2018-06-07 12:16:10,859 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: Index IX_WORKFLOW_METADATA_SUMMARY_ENTRY_WS created; 2018-06-07 12:16:10,859 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: Index IX_WORKFLOW_STORE_ENTRY_WS created; 2018-06-07 12:16:10,859 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: Data updated in SUMMARY_STATUS_ENTRY; 2018-06-07 12:16:10,859 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: Data updated in SUMMARY_STATUS_ENTRY; 2018-06-07 12:16:10,860 INFO - changelog.xml: changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir: ChangeSet changesets/standardize_column_names_again.xml::standardize_column_names_again::kshakir ran successfully in 54ms; 2018-06-07 12:16:10,880 INFO - Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; 2018-06-07 12:16:10,896 INFO - [RenameWorkflowOptionsInMetadata] 100%; 2018-06-07 12:16:10,896 INFO - changelog.xml: changesets/rename_workflow_options_in_metadata.xml::rename_workflow_options_in_metadata::tjeandet: RenameWorkflowOptionsInMetadata complete.; 2018-06-07 12:16:10,896 INFO - changelog.xml: changesets/rename_workflow_options_in_metadata.xml::rename_workflow_options_in_metadata::tjeandet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:68611,update,updated,68611,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['update'],['updated']
Deployability,"sample_list.data,; sex_mismatch_sample_list = sex_mismatch_sample_list.data,; low_genotyping_quality_sample_list = low_genotyping_quality_sample_list.data; }; }. call tasks.load_shared_covars { input:; script_dir = script_dir,; fam_file = fam_file,; sc_pcs = sc_pcs,; sc_assessment_ages = sc_assessment_ages; }. if (!is_binary) {; call tasks.load_continuous_phenotype { input :; script_dir = script_dir,; sc = sc_phenotype,; qced_sample_list = all_qced_sample_lists.data[0],; assessment_ages_npy = load_shared_covars.assessment_ages,; categorical_covariate_names = categorical_covariate_names,; categorical_covariate_scs = categorical_covariate_scs; }; }; if (is_binary) {; call tasks.load_binary_phenotype { input:; script_dir = script_dir,; sc = sc_phenotype,; qced_sample_list = all_qced_sample_lists.data[0],; sc_year_of_birth = sc_year_of_birth,; sc_month_of_birth = sc_month_of_birth,; sc_date_of_death = sc_date_of_death,; date_of_most_recent_first_occurrence_update = date_of_most_recent_first_occurrence_update,; is_zero_one_neg_nan = is_zero_one_neg_nan; }; }; # regardless of continuous or binary, get the outputs and move on; File pheno_data = select_first([load_continuous_phenotype.data, load_binary_phenotype.data]); File covar_names = select_first([load_continuous_phenotype.covar_names, load_binary_phenotype.covar_names]); File pheno_readme = select_first([load_continuous_phenotype.README, load_binary_phenotype.covar_names]). output {; Array[File] out_sample_lists = all_qced_sample_lists.data; File assessment_ages = load_shared_covars.assessment_ages; File shared_covars = load_shared_covars.shared_covars; File shared_covar_names = load_shared_covars.covar_names; File pheno_data_out = pheno_data; File covar_names_out = covar_names; File pheno_readme_out = pheno_readme; }; }; ```. tasks.wdl; ```; version 1.0. # any input file with a default relative to the script_dir; # needs to be supplied by the user, it won't be the product of another task; # if input files to tasks ca",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269:7128,continuous,continuous,7128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6971#issuecomment-1355222269,1,['continuous'],['continuous']
Deployability,sbt test patch + exposed jes bug fix,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/902:9,patch,patch,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/902,1,['patch'],['patch']
Deployability,"scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$GoogleJsonException: Request contains an invalid argument.; ... 21 common frames omitted; [2021-08-13 10:45:07,42] [warn] PAPI request worker had 1 failures making 1 requests:; Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; [2021-08-13 10:45:07,54] [info] WorkflowManagerActor: Workflow a15c46b7-5f93-46d6-94a2-28f656914866 failed (during ExecutingWorkflowState): cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:5716,Pipeline,PipelinesApiRequestManager,5716,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['PipelinesApiRequestManager']
Deployability,"scovery_GATK4/b75cd521-c92d-4264-98f4-7a6571860bb3/call-SamToFastqAndBwaMem/shard-0/execution/script; 5035, /bin/bash/n/no_backup2/dbmi/park/gem_wgs/.PreProcessing/.NFRI_S013_M1d.bam/.sh/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/b75cd521-c92d-4264-98f4-7a6571860bb3/call-SamToFastqAndBwaMem/shard-0/execution/script; 24804, /bin/bash/n/no_backup2/dbmi/park/gem_wgs/.PreProcessing/.NFRI_S013_N1.bam/.sh/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/99b4c5f7-50c2-4888-a448-934329112f83/call-MergeBamAlignment/shard-0/execution/script; 3387, /bin/bash/n/no_backup2/dbmi/park/gem_wgs/.PreProcessing/.NFRI_S013_N1.bam/.sh/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/99b4c5f7-50c2-4888-a448-934329112f83/call-MergeBamAlignment/shard-0/execution/script; ; ; I am not sure if this was happening because you started multiple identical jobs or if it is a bug in the cromwell pipeline but it is the reason why those child jobs are hanging now.; ``` . Just to clarify, I did not submit multiple identical jobs. Everything on my end was kosher. . In any case, theres the issue - its not the first I've had with Cromwell (you can see previous discussions related to this at e.g. https://gatkforums.broadinstitute.org/wdl/discussion/comment/50624#Comment_50624) . I had to make lots of modifications to my configuration file before I got it to work as well as it does now. So if you have any idea what is causing this or any changes to my configuration file you'd recommend id greatly appreciate your help. Thanks a lot,. Alon. [rc.tmp.txt](https://github.com/broadinstitute/cromwell/files/2540522/rc.tmp.txt); [script.submit.txt](https://github.com/broadinstitute/cromwell/files/2540523/script.submit.txt); [script.txt](https://github.com/broadinstitute/cromwell/files/2540524/script.txt). [stderr.txt](https://github.com/broadinstitute/cromwell/files/2540525/stderr.txt); [stdout.submit.txt](https://github.com/broadinstitute/cromwell/files/2540526/stdout.submit.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4347:8310,configurat,configuration,8310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4347,2,['configurat'],['configuration']
Deployability,"seems like this was already implemented as a backend configuration option `glob-link-command`, so I simply updated the configuration example files to include an example of how to set `glob-link-command` to use soft-links instead. Was able to run on beeGFS with the updated configuration.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892:53,configurat,configuration,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5250#issuecomment-549502892,5,"['configurat', 'update']","['configuration', 'updated']"
Deployability,set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query SET autocommit=1; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-92c0-cc03aaeb689d'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '5e423c7e-7857-4884-814f-787b98c54491'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'db449c5a-5138-42ec-992a-d88f29a78693'; Query SET autocommit=0; ```; - `database.run(action)`:; ```; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6a68865c-13ea-410e-a774-9b5e58f45523'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '669f97ea-1256-4732-a4ac-a565ff749e41'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c3994549-e8e1-4478-9bf3-ef46cc16f505'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '8d535b7f-4832-429c-9144-c3c39eeb7006'; Query update ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4022:2948,update,update,2948,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022,1,['update'],['update']
Deployability,set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query SET autocommit=1; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-92c0-cc03aaeb689d'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '5e423c7e-7857-4884-814f-787b98c54491'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'db449c5a-5138-42ec-992a-d88f29a78693'; Query SET autocommit=0; ```; - `database.run(action)`:; ```; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6a68865c-13ea-410e-a774-9b5e58f45523'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '669f97ea-1256-4732-a4ac-a565ff749e41'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'c3994549-e8e1-4478-9bf3-ef46cc16f505'; Query update ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4022:2761,update,update,2761,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022,1,['update'],['update']
Deployability,set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query SET autocommit=1; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-92c0-cc03aaeb689d'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '5e423c7e-7857-4884-814f-787b98c54491'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'db449c5a-5138-42ec-992a-d88f29a78693'; Query SET autocommit=0; ```; - `database.run(action)`:; ```; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6a68865c-13ea-410e-a774-9b5e58f45523'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '669f97ea-1256-4732-a4ac-a565ff749e41'; Query update ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4022:2574,update,update,2574,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022,1,['update'],['update']
Deployability,set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'f0bae536-32c2-4f15-93af-f03515668faf'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9892d137-40b5-420c-94b4-88481c8ad249'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '4447f78f-85d2-4c27-8d2f-ea230ca130c1'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:43:00.194' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '3a43b3bf-2cd5-4470-8131-05ff8016ccbb'; Query commit; ```; - `database.run(action.withPinnedSession)`:; ```; Query SET autocommit=1; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '9fa0610c-6345-4abc-9240-883d1bb10f34'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6df0ea00-027e-4fb7-9bbe-67bbed69f966'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'd5748deb-5a28-4678-92c0-cc03aaeb689d'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '5e423c7e-7857-4884-814f-787b98c54491'; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:55:47.844' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = 'db449c5a-5138-42ec-992a-d88f29a78693'; Query SET autocommit=0; ```; - `database.run(action)`:; ```; Query update `WORKFLOW_STORE_ENTRY` set `HEARTBEAT_TIMESTAMP` = '2018-08-20 15:32:10.097' where `WORKFLOW_STORE_ENTRY`.`WORKFLOW_EXECUTION_UUID` = '6a68865c-13ea-410e-a774-9b5e58f45523'; Query update ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4022:2387,update,update,2387,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4022,1,['update'],['update']
Deployability,set aws default region in integration tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4326:26,integrat,integration,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4326,1,['integrat'],['integration']
Deployability,"shared file system or a network file system when running a spark job in the spark standalone cluster mode. This implementation takes a wdl with the backend configuration specified as ""Spark"" and then generates the appropriate spark commands and monitoring process to ensure the job runs to completion. Meaning, details of the spark internals are completely abstracted from the user provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.â€‚â€‚. Current deploy modes supported for any spark job:; â€‚â€‚a - Client deploy mode using the spark standalone cluster manager; â€‚â€‚b - Cluster deploy mode using the spark standalone cluster manager; â€‚â€‚c - Client deploy mode using Yarn resource manager; â€‚â€‚d - Cluster deploy mode using Yarn resource manager; â€‚â€‚; Future PR Plans:; â€‚â€‚In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).; â€‚â€‚This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation. â€‚â€‚In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily because hdfs is spark's natural file system.; â€‚â€‚Note that this doesn't actually prevent spark from writing to the hdfs, in order words, the spark application can write or read from the hdfs if given hdfs locations as arguments. Reason for r",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1339:973,deploy,deploy,973,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339,5,['deploy'],['deploy']
Deployability,should really go against hotfix though,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/846#issuecomment-219845066:25,hotfix,hotfix,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/846#issuecomment-219845066,1,['hotfix'],['hotfix']
Deployability,should there be a hotfix version too?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/856#issuecomment-220454280:18,hotfix,hotfix,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/856#issuecomment-220454280,1,['hotfix'],['hotfix']
Deployability,"size is the problem. Does the issue persist after restarting the server? I committed a change to; the develop branch a few weeks ago that does a better job of cleaning up; the copying resources. If the restart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > â€¦ <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutExcepti",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1078,update,updated,1078,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,1,['update'],['updated']
Deployability,slick/SlickDatabase.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-ZGF0YWJhc2Uvc3FsL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2RhdGFiYXNlL3NsaWNrL1NsaWNrRGF0YWJhc2Uuc2NhbGE=) | `84.78% <0%> (-0.64%)` | :arrow_down: |; | [...a1/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvUGlwZWxpbmVzQXBpQXN5bmNCYWNrZW5kSm9iRXhlY3V0aW9uQWN0b3Iuc2NhbGE=) | `6.84% <0%> (-0.2%)` | :arrow_down: |; | [...d/google/pipelines/v2alpha1/api/Localization.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy92MmFscGhhMS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9iYWNrZW5kL2dvb2dsZS9waXBlbGluZXMvdjJhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (Ã¸)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (Ã¸)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96% <0%> (+0.39%)` | :arrow_up: |; | [...n/scala/cromwell/core/path/BetterFileMethods.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9jb3JlL3BhdGgvQmV0dGVyRmlsZU1ldGhvZHMuc2NhbGE=) | `30.1% <0%> (+1.02%)` | :arrow_up: |; | [...wl/src/main/scala/cwl/ExpressionInter,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:2635,pipeline,pipelines,2635,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842,2,"['Pipeline', 'pipeline']","['PipelinesApiJobPaths', 'pipelines']"
Deployability,"snakeyaml/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/00809e6249b134635f71919c17c1c81603beb22d/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.29).; You might want to review and update them manually.; ```; centaur/src/test/resources/centaur/test/metadata/failingInSeveralWaysMetadata.json; core/src/test/resources/hello_goodbye_scattered_papiv2.json; docs/developers/bitesize/ci/Cromwell_Deployment_Strategies.svg; docs/developers/bitesize/workflowParsing/forkjoin_graph.svg; docs/developers/bitesize/workflowParsing/wdlToWdlom_hermes.svg; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; ```; </details>; <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6635:2553,configurat,configuration,2553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6635,1,['configurat'],['configuration']
Deployability,"snakeyaml/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.30).; You might want to review and update them manually.; ```; docs/developers/bitesize/workflowParsing/wdlToWdlom_hermes.svg; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependenc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6900:2553,configurat,configuration,2553,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6900,1,['configurat'],['configuration']
Deployability,sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: software.amazon.awssdk.services.s3.model.NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 289B06CE5822B3C0); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:114); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); 	at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); 	at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:3604,pipeline,pipeline,3604,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,1,['pipeline'],['pipeline']
Deployability,"spatcher-28 ERROR - Failure fetching statuses for AWS jobs in Initializing. No updates will occur.; software.amazon.awssdk.services.batch.model.BatchException: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:1104,pipeline,pipeline,1104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033,1,['pipeline'],['pipeline']
Deployability,"spatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861:1088,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1088,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"src; ```. This syntax says ""find the section defined as defaults (above) and insert it here. ```; <<: *defaults; ```; so you don't write it twice!. This is similar, but it's like a named anchor and pointer. I might have this under a jobs step. ```; - run: *dothething; ```; which might be in reference to this. ```; dothething: &dothething; name: Do the thing; command: |; echo ""Do the thing!""; echo ""Do it again!""; ```. - The main runtime in the file is the workflow jobs section, which just does a build and deploy.; - the base container that is run is one of circle's ready to docker docker images `docker:18.01.0-ce-git`; - The main steps are to load cache, install dependencies, build the container, run to test, and then save the cache and deploy. That's really it :); - you interact with the environment by writing it to `BASH_ENV` and sourcing that, which needs to be done in each step separately (e.g., a ""run"" section); - most of the weird if statement logic is just to test if the user (you) has defined an environment variable (somewhere) and if not, go to default or just skip a step.; - the easiest way to ""read"" the file is to go to the bottom and start at ""workflows"" that describe the highest level of things, e.g. ""run all these steps under build, and trigger based on these filters and branches."" TLDR **workflows** define a dependency graph sort of deal.; - The details for the steps for each workflow are under ""jobs"" that are the seconds above that, so the names of jobs are going to correspond to workflows. In ""jobs"" you can then trace back up to find the corresponding step. TLDR **jobs** are all the steps of stuff to do.; - Steps that aren't found (e.g., checkout) are circle provided steps that you can read about in their docs --> https://circleci.com/docs/2.0/configuration-reference/. And importantly, in the beginning it all just looks weird. You will come back to the same file and be a little confused. This yaml business is weird, but I promise it clicks over time!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635:3157,configurat,configuration-reference,3157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-415952635,1,['configurat'],['configuration-reference']
Deployability,"ss; 2019-01-18 18:43:33,255 cromwell-system-akka.dispatchers.engine-dispatcher-5 ERROR - WorkflowManagerActor Workflow dba9b85f-e9ea-4e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(Batching",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:1890,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"sted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; ```; The output itself from the process seems fine, and the workflow even proceeds to go on to some next steps using the outputs from this before freezing indefinitely (presumably due to this exception). This is a larger test run to test scaling, as smaller tests have been working cleanly for me, so although the reproducer is public it's rather large to download and setup. Does this error give any clues that might make it easier to produce a smaller reproducible case? Is there any other information I can provide that would be helpful. Thanks so much for the CWL support and helping with all these issues.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:11020,configurat,configuration,11020,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['configurat'],['configuration']
Deployability,"stem-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowExecutionActor-efe9c9a5-cd24-4c78-b39d-d9f10cc754de [UUID(efe9c9a5)]: Job results retrieved (FetchedFromJobStore): 'drs_usa_jdr.skip_localize_jdr_drs_with_usa' (scatter index: None, attempt 1); 2020-10-13 19:03:03,392 cromwell-system-akka.dispatchers.engine-dispatcher-34 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:03,667 cromwell-system-akka.dispatchers.engine-dispatcher-30 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 19:03:06,298 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/3237339977528305328; 2020-10-13 19:03:06,299 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/15450562168035605133; 2020-10-13 19:03:40,191 cromwell-system-akka.dispatchers.backend-dispatcher-41 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:40,200 cromwell-system-akka.dispatchers.backend-dispatcher-109 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Status change from - to Success; 2020-10-13 19:03:42,570 cromwell-system-akka.dispatchers.backend-dispatcher-40 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.r",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:7646,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,7646,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"stions?; >; > Yes, the script epilogue is exactly where the change should be. The script; > is generated by AwsBatchJob.scala; > â€¦ <#m_-2379693136183385899_>; > On Sun, Oct 25, 2020 at 8:37 PM Luyu *@*.*> wrote: Hi Luyu, Thanks for; > the feedback. This is an interesting case. Normally if there is a few; > minutes gap between workflows the instances will be terminated by batch and; > the disks will be reclaimed so each workflow starts from scratch. However; > in your case there isnâ€™t a pause in work long enough for Batch to shut down; > the instances. Also because these files are written to a mounted disk they; > are not deleted when the container terminates. I think this fix is simple; > if I add a cleanup step. I will do this ASAP. Thanks, Mark â€¦; > <#m_-3989886626109986556_> On Sat, Oct 24, 2020 at 5:27 AM Luyu @.*>; > wrote: Hi, I have set up a Cromwell platform on AWS batch according to; > https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-overview/; > If I run GATK Best Practice pipeline for one sample, it works perfectly.; > However, when I ran this pipeline for 10+ samples concurrently, many AWS; > EC2 instances were re-used by AWS batch. Cromwell didn't clean up the; > localized S3 files and output files produced by previous tasks. This; > quickly inflated EBS cost when EBS autoscaling is enabled. One of my; > instances went up to 9.1TB and hit the upper bound for autoscaling, then; > the running task failed due to no space. I have checked Cromwell documents; > and some materials from AWS, as well as issue #4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323> <#4323; > <https://github.com/broadinstitute/cromwell/issues/4323>>>. But none of; > them works for me. Thank you in advance for any suggestions. â€” You are; > receiving this because you are subscribed to this thread. Reply to this; ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965:1467,pipeline,pipeline,1467,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5974#issuecomment-718815965,1,['pipeline'],['pipeline']
Deployability,"t akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375); at java.lang.Thread.run(Thread.java:745); [2015-12-18 08:43:19,174] [info] Message [cromwell.engine.workflow.WorkflowManagerActor$RestartWorkflows] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,180] [info] Message [akka.actor.Status$Failure] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,182] [error] WorkflowManagerActor: Workflow failed submission: cannot create children while terminating or terminated; java.lang.IllegalStateException: cannot create children while terminating or terminated; at akka.actor.dungeon.Children$class.makeChild(Children.scala:199); at akka.actor.dungeon.Children$class.actorOf(Children.scala:37); at akka.actor.ActorCell.actorOf(ActorCell.scala:369); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$11.apply(WorkflowManagerActor.scala:246); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$11.apply(WorkflowManagerActor.scala:245); at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); at scala.util.Try$.apply(Try.scala:192); at scala.util.Success.map(Try.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at sc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/334:2776,configurat,configuration,2776,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/334,1,['configurat'],['configuration']
Deployability,t cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$.apply(PipelinesApiConfigurationAttributes.scala:307); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.defaultBuildAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:32); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.$anonfun$papiAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:34); at scala.util.Try$.apply(Try.scala:210); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.cromwell$backend$google$pipelines$common$PipelinesApiBackendLifecycleActorFactory$$build$1(PipelinesApiBackendLifecycleActorFactory.scala:109); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.robustBuildAttributes(PipelinesApiBackendLifecycleActorFactory.scala:120); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.<init>(PipelinesApiBackendLifecycleActorFactory.scala:34); at cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:10); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at cromwell.engine.backend.BackendConfigurationEntry.$anonfun$asBackendLifecycleActorFactory$1(BackendConfiguration.scala:13); at scala.util.Try$.apply(Try.scala:210); at cromwell.engine.backend.BackendConfigurationEntry.asBackendLifecycleActorFactory(BackendConfiguration.scala:14); at cromwell.engine.backend.CromwellBackends.$anonfun$backendLifecycleActorFactories$1(CromwellBackends.scala:14); at scala.collection.immuta,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:1768,Pipeline,PipelinesApiBackendLifecycleActorFactory,1768,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,1,['Pipeline'],['PipelinesApiBackendLifecycleActorFactory']
Deployability,t software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:72); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:57); at software.amazon.awssdk.core.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:40); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:40); at software.amazon.awssdk.core.http.pipeline.stages.TimerExceptionHandlingStage.execute(TimerExceptionHandlingStage.java:30); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:139); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:1996,pipeline,pipeline,1996,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['pipeline'],['pipeline']
Deployability,"t this database has not had the ANALYZE sqlite3 command run on it. Doing so can dramatically speed up queries, and is done by default for databases created with gffutils >0.8.7.1 (this database was created with version 0.8.2) Consider calling the analyze() method of this object.; ""method of this object."" % self.version); Traceback (most recent call last):; File ""/usr/local/bin/bcbio_nextgen.py"", line 223, in <module>; runfn.process(kwargs[""args""]); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/runfn.py"", line 58, in process; out = fn(fnargs); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/utils.py"", line 52, in wrapper; return apply(f, *args, **kwargs); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/distributed/multitasks.py"", line 208, in pipeline_summary; return qcsummary.pipeline_summary(*args); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/pipeline/qcsummary.py"", line 70, in pipeline_summary; data[""summary""] = _run_qc_tools(work_bam, work_data); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/pipeline/qcsummary.py"", line 162, in _run_qc_tools; out = qc_fn(bam_file, data, cur_qc_dir); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/qc/qualimap.py"", line 347, in run_rnaseq; metrics = _parse_metrics(metrics); File ""/usr/local/share/bcbio-nextgen/anaconda/lib/python2.7/site-packages/bcbio/qc/qualimap.py"", line 210, in _parse_metrics; out.update({name: float(metrics[name])}); TypeError: float() argument must be a string or a number; ```. This is what the command Cromwell generated looks like:. ```; 'bcbio_nextgen.py' 'runfn' 'pipeline_summary' 'cwl' 'sentinel_runtime=cores,2,ram,4096' 'sentinel_parallel=multi-parallel' 'sentinel_outputs=qcout_rec:summary__qc;summary__metrics;resources;description;reference__fasta__base;config__algorithm__coverage_interval;genome",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277:5325,pipeline,pipeline,5325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-436054277,1,['pipeline'],['pipeline']
Deployability,"t-epilogue` variable in the configuration (this is not explained in the Cromwell documentation but it is explained [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/LocalExample.conf)). Maybe the problem could have been solved by also replacing `$stdoutRedirection` and `$stderrRedirection` with something like `$stdoutRedirectionTmp` and `$stderrRedirectionTmp` and then replace:; ```; mv $rcTmpPath $rcPath; ```; with:; ```; mv $stdoutRedirectionTmp $stdoutRedirection; mv $stderrRedirectionTmp $stderrRedirection; mv $rcTmpPath $rcPath; ```; This way `stdout` and `stderr` would have been created in the NFS filesystem at the same time as the `rc` file and would increase the likelihood that they would all have been synced at the same time. However, this would not give the intended behavior when running in Google Cloud. Another problem that I have noticed is that there are multiple places in the Cromwell documentation that advise, when running Cromwell with SLURM, to use configurations such as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/Containers/#configuration)):; ```; sbatch \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; ```; or as (see [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#e-now-add-the-following-text-after-line-479-ie-after-the-line-reading-default-slurm-ensure-that-the-lines-that-show-line-breaks-in-this-document-are-in-fact-single-lines-in-referenceconf), or [here](https://cromwell.readthedocs.io/en/stable/backends/SLURM/), or [here](https://cromwell.readthedocs.io/en/stable/tutorials/HPCSlurmWithLocalScratch/#configure-the-execution-environment-for-cromwell), or [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/cromwell.example.backends/slurm.conf)); ```; sbatch \; -o ${out} \; -e ${err} \; ```; which overwrites `stdout` and `stderr` written by the `script` file, whic",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:2912,configurat,configurations,2912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['configurat'],['configurations']
Deployability,tInObject(UTF8StreamJsonParser.java:826); 		at com.fasterxml.jackson.core.json.UTF8StreamJsonParser.nextToken(UTF8StreamJsonParser.java:723); 		at com.google.api.client.json.jackson2.JacksonParser.nextToken(JacksonParser.java:55); 		at com.google.api.client.json.JsonParser.startParsing(JsonParser.java:221); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:380); 		at com.google.api.client.json.JsonParser.parse(JsonParser.java:355); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:87); 		at com.google.api.client.json.JsonObjectParser.parseAndClose(JsonObjectParser.java:81); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:250); 		at com.google.api.client.googleapis.auth.oauth2.GoogleCredential.fromStream(GoogleCredential.java:226); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.HasApiClientGoogleCredentialStream.apiClientGoogleCredential$(GoogleAuthMode.scala:210); 		at cromwell.cloudsupport.gcp.auth.UserServiceAccountMode.apiClientGoogleCredential(GoogleAuthMode.scala:237); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.$anonfun$apply$1(PipelinesApiVMAuthentication.scala:41); 		at scala.Option.flatMap(Option.scala:171); 		at cromwell.backend.google.pipelines.common.authentication.PipelinesApiDockerCredentials$.apply(PipelinesApiVMAuthentication.scala:37); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.$anonfun$dockerCredentials$1(PipelinesApiConfiguration.scala:24); 		at scala.Option.map(Option.scala:146); 		at cromwell.backend.google.pipelines.common.PipelinesApiConfiguration.<init>(PipelinesApiConfiguration.scala:23); 		at cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:12); 		... 29 common frames omitted; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4553:4828,pipeline,pipelines,4828,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553,15,"['Pipeline', 'pipeline']","['PipelinesApiConfiguration', 'PipelinesApiDockerCredentials', 'PipelinesApiLifecycleActorFactory', 'PipelinesApiVMAuthentication', 'pipelines']"
Deployability,"t` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files â€¦ Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A similar script that installs the singularity binaries plus a small cluster(?) and then uses a working config file to run our Centaur test suite would be amazing for users. After it's all working, users are being pointed to docs under https://cromwell.readthedocs.io/, such as https://cromwell.readthedocs.io/en/stable/backends/TES/. A similar entry should be added for a working/tested singularity setup. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/4039/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:1837,install,installs,1837,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519,2,['install'],['installs']
Deployability,"ta/cardiom/NA12878_CARDIACM_MUTATED_L001_R1.fastq.gz"" --format fastq`; ```. Cromwell failing with an error because the `--read` argument is missing (even though you can see it's not, in the above log):; ```; cromwell_1 | java.lang.Exception: Task germline_variant_calling.fastqc:0:1 failed. The job was stopped before the command finished. PAPI error code 10. 11: Docker run failed: command failed: usage: fastqc_docker.py [-h] -r READ -o OUTPUT_DIR [-c CONTAMINANTS]; cromwell_1 | [-a ADAPTERS] [-l LIMITS] [-f FORMAT] [-n NO_GROUP]; cromwell_1 | [-e EXTRA_OPTIONS]; cromwell_1 | fastqc_docker.py: error: argument -r/--read is required; cromwell_1 | . See logs at gs://genovic-cromwell/cromwell-execution/trio/f5454139-c51d-4d04-ae0a-9b9d4ce650aa/call-germline_variant_calling/shard-0/germline_variant_calling/5d4c4459-a91c-4d3b-8ca4-b98457134750/call-fastqc/shard-0/; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); cromwell_1 | at scala.concurrent.impl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:1770,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,tage$RetryExecutor.doExecute(RetryableStage.java:139); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:105); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:5361,pipeline,pipeline,5361,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,3,['pipeline'],['pipeline']
Deployability,"taken in the last few weeks, and resulting thoughts from; > them. I started with this issue board actually, and a general goal to ""Add; > Singularity to Cromwell."" Ok.; > Question 1: How do I develop Cromwell?; >; > It first was hard for me to know where to start to develop Cromwell,; > because the docs just went into how to compile it on a host. So it made; > sense to make it easy for the developer to develop Cromwell so I made a; > Dockerfile to do that:; >; > - #4002 <https://github.com/broadinstitute/cromwell/pull/4002>; >; > Woohoo merged! We needed to have tests too, so I followed up on that:; >; > - #4015 <https://github.com/broadinstitute/cromwell/pull/4015>; >; > But unfortunately it was decided that CircleCI was too new / needed to; > learn stuff (this is ok!) so it's going to be closed.; > Question 2: How do we add a Singularity backend?; >; > But this is actually ok, because we realize that we don't need to add; > Singularity to Cromwell proper, it can just be a backend! But I didn't; > understand wdl, or any of the formats, so my crew in Cherry lab gave me a; > solid repo to startwith, and then it started to click!; >; > - vsoch/wgbs-pipeline#1 <https://github.com/vsoch/wgbs-pipeline/pull/1>; >; > I was waiting for the Dockerfile test PR to pass, but realized it probably; > wouldn't, so I jumped on adding the example backend workflows (still; > without totally understanding what/why/how, but figuring out as I went):; >; > - #4039 <https://github.com/broadinstitute/cromwell/pull/4039>; >; > Question 3: But what about Cromwell+Singularity on Travis?; >; > I got confused again when there were requests for additional tests; > <https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519>; > (and something entirely different) that it made me step back. I had this; > growing feeling that started to solidify that there are too many layers. I; > am developing things and I *still* don't understand (or think Singularity; > is ready yet) to be any k",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:10872,pipeline,pipeline,10872,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,1,['pipeline'],['pipeline']
Deployability,"tart solves the problem then you may want; to build from the develop branch until the next release is sent out. Also, is the bucket containing the source file the same bucket as the; workflow bucket? If not, are they in the same region?. On Wed, Nov 11, 2020 at 4:28 AM Luyu <notifications@github.com> wrote:. > Hi,; >; > The improved multipart copying (api: CreateMultipartUpload) doesn't work; > for me. The cromwell server always checks the existence of the cached file; > before the copying finishes. In Cromwell v51 and before, some small files; > <100GB were able to be successfully cached. However, with Cromwell v53,; > even a 6GB result file got a problem of caching and has to rerun. Is there; > any way to prevent the timeout of the actor?; >; > Hi, In Cromwell 52 we updated the S3 module to perform multithreaded,; > multipart copies to improve the size of results that may be cached. There; > are also additional improvements that have recently been merged into dev; > and should appear in the next release version (or you could build from; > source) v52+ requires a new AWS configuration. Instructions are in; > https://aws-genomics-workflows.s3.amazonaws.com/Installing+the+Genomics+Workflow+Core+and+Cromwell.pdf; > â€¦ <#m_3227077625045957240_>; > On Sat, Oct 24, 2020 at 8:27 PM Luyu *@*.***> wrote: Hi, I got a timeout; > exception during cache copying on AWS S3. The cache file size is 133GB.; > Given the file size, more time should be allowed for cache copying. Is; > there any config option that can tune this? Thank you in advance for any; > suggestions. Backend: AWS Batch Cromwell version: 51 Error log: Failure; > copying cache results for job; > BackendJobDescriptorKey_CommandCallNode_PreProcessingForVariantDiscovery_GATK4.SamTo; > FastqAndBwaMem:0:1 (TimeoutException: The Cache hit copying actor timed out; > waiting for a response to copy s3://xxxxx/cromwell-execution/Germ; > line_Somatic_Calling/441619a4-7ca8-490b-bd04-2f9981d3db0f/call-Tumor_Bam/PreProcessingForVa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055:1312,release,release,1312,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5977#issuecomment-726473055,2,"['configurat', 'release']","['configuration', 'release']"
Deployability,"task (vs passing through an optional value from its caller), you could use a default value:. Although this would prevent me from using select_first() as often, in the actual workflow (which I didn't post, as it's monstrous compared to the toy example), I also have to use defined() to build the path of an optional TSV file which is either going to be in the zipped directory, or passed in directory, or not used at all. Setting an default value means I now have to check for equality with an empty string instead of using defined. In the end it'd be just as verbose and probably a little harder to debug then select_first(). > Regarding the difference in behavior of cromwell vs miniwdl, I don't think miniwdl's support for this form is backed by the WDL spec. As I see it, the spec is explicit with regard to optional parameters to Standard Library functions. For example, the signature for `size()` is `Float size(File?|Array[File?], [String])`. Since the signature for `basename()` is `String basename(String|File, [String])`, it doesn't look like it should support `String?` as an input. I'm a little confused, where are you seeing this? [sub()](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#string-substring-string-string) is titled `String sub(String, String, String)` and [size()](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#float-sizefile-string) is titled `Float size(File, [String])` in the 1.0 spec. I get that sub() [has examples with compound types](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#acceptable-compound-input-types) but I took that to mean ""here's some examples with arrays plus an optional for comparison"" since File? isn't a compound type (if I am understanding [this](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#compound-types) correctly). If size() accepts optionals in spite of the spec continuously saying `File` instead of `File?` except in one example, I don't see why basename() and sub() cannot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233394358:1964,continuous,continuously,1964,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6840#issuecomment-1233394358,1,['continuous'],['continuously']
Deployability,"tch interval of 3333 milliseconds; ...; [2021-08-13 10:44:56,67] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Parsing workflow as WDL draft-2; [2021-08-13 10:44:58,79] [info] MaterializeWorkflowDescriptorActor [a15c46b7]: Call-to-Backend assignments: wf_hello.hello -> PAPIv2; [2021-08-13 10:45:00,31] [info] Not triggering log of token queue status. Effective log interval = None; [2021-08-13 10:45:01,35] [info] WorkflowExecutionActor-a15c46b7-5f93-46d6-94a2-28f656914866 [a15c46b7]: Starting wf_hello.hello; [2021-08-13 10:45:02,34] [info] Assigned new job execution tokens to the following groups: a15c46b7: 1; [2021-08-13 10:45:04,75] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: echo ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; [2021-08-13 10:45:05,68] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); [2021-08-13 10:45:07,36] [error] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:2947,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2947,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,tch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 11:09:46 cromwell-test_1 | Caused by: liquibase.exception.LockException: java.lang.NullPointerException; 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.acquireLock(StandardLockService.java:242); 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:170); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:196); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:192); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:58); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:31); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:96); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.Threa,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766:4061,update,updateSchema,4061,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766,1,['update'],['updateSchema']
Deployability,"tchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: job id: 8ec19f2b-5b49-4422-9ad1-5b51e3db9414; [2019-05-22 19:15:21,77] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: Status change from - to Initializing; [2019-05-22 19:15:26,67] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: Status change from Initializing to Running; [2019-05-22 19:19:12,28] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.SplitFilesByChromosome:NA:1]: Status change from Running to Succeeded; [2019-05-22 19:19:18,44] [info] 755021ae-948b-47f9-94a8-66b486bda47d-SubWorkflowActor-SubWorkflow-Haplotypecaller:0:1 [755021ae]: Starting Haplotypecaller.HC_GVCF (23 shards); [2019-05-22 19:19:19,34] [info] AwsBatchAsyncBackendJobExecutionActor [755021aeHaplotypecaller.HC_GVCF:1:1]: set -e; sambamba index -t 4 /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-313957810a5e411f50b17b2a7d630ef7/chr10.RSM278260-6_8plex.dedup.recal.bam; gatk HaplotypeCaller \; --java-options -Djava.io.tmpdir='' \; -R /cromwell_root/s4-ngs-resources-sandbox/Genomic/Broad/hg19/ucsc.hg19.fasta \; --dbsnp /cromwell_root/s4-ngs-resources-sandbox/Variant/Broad/hg19/dbsnp_138.hg19.vcf.gz \; --native-pair-hmm-threads 16 \; -L /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948b-47f9-94a8-66b486bda47d/call-SplitFilesByChromosome/glob-6f4bc12a708659d4f5f3eecd1cdffff7/chr10.intervals \; -I /cromwell_root/s4-pbg-hc/HC_Dev_Run_5/Pipeline/RSM278260-6_8plex/pipeline_workflow/3997371c-9513-4386-a579-a72639c6e960/call-Haplotypecaller/shard-0/hc.Haplotypecaller/755021ae-948",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5004:5383,Pipeline,Pipeline,5383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5004,1,['Pipeline'],['Pipeline']
Deployability,"tchers.engine-dispatcher-5 ERROR - WorkflowManagerActor Workflow dba9b85f-e9ea-4e78-9a04-ed1babbb9ebc failed (during ExecutingWorkflowState): java.lang.Exception: Task PreProcessingForVariantDiscovery_GATK4.MergeBamAlignment:23:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""broadinstitute/gatk@sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71""]: exit status 1 (standard error: ""failed to register layer: Error processing tar file(exit status 1): write /opt/miniconda/envs/gatk/lib/python3.6/site-packages/sklearn/datasets/__pycache__/olivetti_faces.cpython-36.pyc: no space left on device\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); 	at ; cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495:1956,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1956,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861#issuecomment-455657495,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"te keys: backend; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.engine-dispatcher-26 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.localize_jdr_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.localize_jdr_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,678 cromwell-system-akka.dispatchers.backend-dispatcher-63 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:57:58,747 cromwell-system-akka.dispatchers.engine-dispatcher-20 INFO - efe9c9a5-cd24-4c78-b39d-d9f10cc754de-EngineJobExecutionActor-drs_usa_jdr.read_drs_with_usa:NA:1 [UUID(efe9c9a5)]: Could not copy a suitable cache hit for efe9c9a5:drs_usa_jdr.read_drs_with_usa:-1:1. No copy attempts were made.; 2020-10-13 18:57:58,881 cromwell-system-akka.dispatchers.backend-dispatcher-83 WARN - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: Unrecognized runtime attribute keys: backend; 2020-10-13 18:58:01,299 cromwell-system-akka.dispatchers.backend-dispatcher-83 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.read_drs_with_usa:NA:1]: `echo file is read by the engine`; 2020-10-13 18:58:01,433 cromwell-system-akka.dispatchers.backend-dispatcher-81 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.skip_localize_jdr_drs_with_usa:NA:1]: `echo gs://broad-jade-dev-data-bucket/ca8edd48-e954-4c20-b911-b017fedffb67/585f3f19-985f-43b0-ab6a-79fa4c8310fc > path1`; 2020-10-13 18:58:01,809 cromwell-system-akka.dispatchers.backend-dispatcher-38 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(efe9c9a5)drs_usa_jdr.localize_jdr_drs_with_usa:NA:1]: `echo /cromwell_root/jade.datarepo-dev.broadinstitute.org/v1_f90f5d7f-c507-4e56-abfc-b965a66023fb_585f3f19-985f-43b0-ab6a-79fa4c8310fc/hello_jade.json > path1; 20",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335:2392,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,2392,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5938#issuecomment-707961335,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"te/cromwell/blob/5c8f932b6e1a5706286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5804:1227,pipeline,pipelines,1227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5804,1,['pipeline'],['pipelines']
Deployability,"te/cromwell/blob/5c8f932b6e1a5706286913e21c78dc296dd5c79c/supportedBackends/google/pipelines/v2alpha1/src/main/scala/cromwell/backend/google/pipelines/v2alpha1/api/ContainerSetup.scala; error:; ```; [2020-08-25 10:40:46,26] [info] WorkflowManagerActor Workflow 282f5595-171e-4296-a7fa-9bd9f7a2f33b failed (during ExecutingWorkflowState): java.lang.Exception: Task Mutect2.renameBamIndex:NA:1 failed. The job was stopped before the command finished. PAPI error code 9. Execution failed: generic::failed_precondition: while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": unexpected exit status 1 was not ignored; [ContainerSetup] Unexpected exit status 1 while running ""/bin/bash -c mkdir -p /cromwell_root && chmod -R a+rwx /cromwell_root"": chmod: changing permissions of '/cromwell_root/sra-SRR2806786': Function not implemented; chmod: changing permissions of '/cromwell_root/sra-SRR2806786/.initialized': Function not implemented. 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:88); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:695); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:707); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:704); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:92); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1258); 	at cromwell.backend",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929:1505,pipeline,pipelines,1505,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5793#issuecomment-680258929,1,['pipeline'],['pipelines']
Deployability,"ted right now. You could change the; CloudFormation template so that your customized mount is mounted to the EC2; nodes. It is also possible to mount EFS directly into an AWS Batch; container through the job definition but that would require changes in; Cromwell's AWS Batch backend. Using EFS with containers for Cromwell; workflows is something we are investigating but there are some stress tests; that we need to do at scale to see if sufficient IOPs are available. On Wed, May 5, 2021 at 11:29 AM microbioticajon ***@***.***>; wrote:. > Hi Guys,; >; > This is more of a question/request than a bug report. Apologies if this is; > not the place to ask.; >; > Im trying to run Cromwell with an AWS backend. A number of our workflows; > make extensive use of very large reference files. To avoid localising the; > same huge file over and over (wasting time and space) I want to copy these; > reference files to an additional volume during batch node initialisation; > and mount to each container (rather than using File arguments I would use a; > simple String argument to prevent localisation - I appreciate this is a; > hack). I am already doing this with a different pipeline framework with; > some success, however it requires the JobDefinition to specify the mount; > locations between the node(host) and job container; >; > Is it possible to provide additional mount/volume instructions to the aws; > batch backend in the cromwell.conf?; >; > If this is possible, I cannot see any specific examples in the Cromwell; > docs. If this is not currently possible, could I request adding the ability; > to define additional mount points as a feature request??; >; > Kind Regards,; > Jon; >; > â€”; > You are receiving this because you are subscribed to this thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/6334>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EI7XBOPHMWSYW3EDFTTMFP7HANCNFSM44FGDSRQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484:1207,pipeline,pipeline,1207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6334#issuecomment-919302484,1,['pipeline'],['pipeline']
Deployability,"terializeWorkflowDescriptorActor [a15c46b7]: Call-to-Backend assignments: wf_hello.hello -> PAPIv2; [2021-08-13 10:45:00,31] [info] Not triggering log of token queue status. Effective log interval = None; [2021-08-13 10:45:01,35] [info] WorkflowExecutionActor-a15c46b7-5f93-46d6-94a2-28f656914866 [a15c46b7]: Starting wf_hello.hello; [2021-08-13 10:45:02,34] [info] Assigned new job execution tokens to the following groups: a15c46b7: 1; [2021-08-13 10:45:04,75] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: echo ""Hello World! Welcome to Cromwell . . . on Google Cloud!""; [2021-08-13 10:45:05,68] [info] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Adjusting boot disk size to 12 GB: 10 GB (runtime attributes) + 1 GB (user command image) + 1 GB (Cromwell support images); [2021-08-13 10:45:07,36] [error] PipelinesApiAsyncBackendJobExecutionActor [a15c46b7wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:3073,pipeline,pipelines,3073,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['pipeline'],['pipelines']
Deployability,"testing/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The rerun uses this:; ```; samtools index /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-samtoolsIndex/inputs/exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam /exports/sasc/biowdl-testing/src/04_RNA-seq/cromwell-executions/pipeline/14df3e76-ea79-4423-bfa2-1198de69cd1b/call-sample/shard-0/sample/0380d883-7bc4-4ef5-ac60-b3bbee90e53d/call-library/shard-0/library/c6c844c8-6e18-44ac-9cff-01a9da900159/call-starAlignment/AlignStar/bb11fd8a-33f9-4343-bf4b-bf397e063ecc/call-star/analysis/04_RNA-seq/samples/S1/lib_lib1/star/S1-lib1.Aligned.sortedByCoord.out.bam.bai; ```; The second argument changes from the intended path to a path inside of the execution folder. It looks like the output from the preceding mapping job gets linked to in the execution folder after restarting the workflow. Which is used as output for that job(?). This output `File` is used to determine what the name for the output of the indexing call is. Which is now different because it now points at the link in the execution folder, rather than the actual output the mapping job produced. As such the expected output doesn't exist and the job gets rerun. ; Am I correct in these statements? If so, is there a way this can be avoided? (ie. Is there a way the original output path can be remembered between restarts?)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3717:1936,pipeline,pipeline,1936,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3717,1,['pipeline'],['pipeline']
Deployability,"th_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=$TAG -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }; ```. ### Input for the workflow is this:; ```; #input WDL. {; ""scMeth.sampleName"": ""sub"",; ""scMeth.input_fastq1"": ""sub_1.fastq.gz"",; ""scMeth.input_fastq2"": ""sub_2.fastq.gz"",; ""scMeth.file_format"": ""fastq"",; ""scMeth.command"": ""moveBarcodeToID.pl"",; ""scMeth.low_quality_cutoff"": 21,; ""scMeth.read_length_cutoff"": 62,; ""scMeth.TAG"": ""'length='"",; ""scMeth.bases"": 6,; ""scMeth.trim_start_R1"": 11,; ""scMeth.trim_end_R1"": -16,; ""scMeth.trim_start_R2"": 25,; ""scMeth.trim_end_R2"": -2,; ""scMeth.trimAdapters.sampleName"": ""sub"",; ""scMeth.adapters_1"": ""AGATCGGAAGAGCACACGTCTGAAC"",; ""scMeth.adapters_2"": ""AGATCGGAAGAGCGTCGTGTAGGGA""; }. ```. ### configuration named as `your_2.conf` file is:; ```; include required(classpath(""application"")); ```. ### Run as:; `java -jar -Dconfig.file=your_2.conf cromwell-42.jar run -i scMeth_input_3.json scMeth_v2.wdl.sh`. ### Error is:. ```; [2019-07-10 14:32:46,75] [info] Running with database db.url = jdbc:hsqldb:mem:fad09ca5-b589-4874-b5de-bbd1dc0064fe;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,36] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-07-10 14:32:53,38] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-07-10 14:32:53,46] [info] Running with database db.url = jdbc:hsqldb:mem:39174976-89f7-4769-a52c-7d5a4afc6cf4;shutdown=false;hsqldb.tx=mvcc; [2019-07-10 14:32:53,81] [info] Slf4jLogger started; [2019-07-10 14:32:54,07] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1cf43fa"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""failureShutdownDuration"" : ""5 minutes"",; ""writeB",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:3983,configurat,configuration,3983,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['configurat'],['configuration']
Deployability,"thanks for the docker image! just wondering, when should we expect the next cromwell release?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1955561946:85,release,release,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7238#issuecomment-1955561946,1,['release'],['release']
Deployability,"that was generated; EMAIL=$(gcloud beta iam service-accounts create MyServiceAccount --description ""to run cromwell"" --display-name ""cromwell service account"" --format json | jq '.email' | sed -e 's/\""//g'); ```; does not work. It errors out with:; ```; ERROR: (gcloud.beta.iam.service-accounts.create) argument NAME: Bad value [MyServiceAccount]: Service account name must be between 6 and 30 characters (inclusive), must begin with a lowercase letter, and consist of lowercase alphanumeric characters that can be separated by hyphens.; ```; I believe `MyServiceAccount` needs to change to `my-service-account` (similarly to how it is used [here](https://cromwell.readthedocs.io/en/stable/backends/Google/) for `scheme = ""service_account""`). 2) The following code in the [permissions](https://cromwell.readthedocs.io/en/stable/tutorials/PipelinesApi101/#permissions) section:; ```; # add all the roles to the service account; for i in storage.objectCreator storage.objectViewer genomics.pipelinesRunner genomics.admin iam.serviceAccountUser storage.objects.create; do; gcloud projects add-iam-policy-binding MY-GOOGLE-PROJECT --member serviceAccount:""$EMAIL"" --role roles/$i; done; ```; does not work. When trying to add role `storage.objects.create` it errors out with:; ```; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/storage.objects.create is not supported for this resource.; ```; and there is clearly an extra role missing as roles `storage.objectCreator`, `storage.objectViewer`, `genomics.pipelinesRunner`, `genomics.admin`, `iam.serviceAccountUser` (corresponding to roles Storage Object Creator, Storage Object Viewer, Genomics Pipelines Runner, Genomics Admin, Service Account User) are not sufficient to create files inside Google buckets. 3) The [permissions](https://cromwell.readthedocs.io/en/stable/tutori",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349:1395,pipeline,pipelinesRunner,1395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-666071349,1,['pipeline'],['pipelinesRunner']
Deployability,"the JSON file as an array of 4 strings; call testtask{input: str=strings[idx]}; }; ```; Error:; ```; [2018-10-08 13:27:31,22] [error] WorkflowManagerActor Workflow c2ac7273-c209-4e74-b1f0-a208e89922d8 failed (during ExecutingWorkflowState): Can't index Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))) with index Success(WdlInteger(0)); wdl4s.wdl.WdlExpressionException: Can't index Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))) with index Success(WdlInteger(0)); ```; ### Zip(); WDL code:; ```; Array[String]? strings1; Array[String]? strings2. Array[Pair[String,String]] string_pair = zip(strings1,strings2); ```; Error:; ```; [2018-10-08 13:31:20,27] [error] WorkflowManagerActor Workflow 832af5bf-2c7e-4e4a-80c5-bc0787910477 failed (during MaterializingWorkflowDescriptorState): Workflow input processing failed:; Workflow has invalid declarations: Could not evaluate workflow declarations:; Test_optional.string_pair:; Invalid parameters for engine function zip: Vector(Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))), Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""a"", ""b"", ""c"", ""d""])))). Requires exactly two evaluated array values of equal length.; cromwell.engine.workflow.lifecycle.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Workflow has invalid declarations: Could not evaluate workflow declarations:; Test_optional.string_pair:; Invalid parameters for engine function zip: Vector(Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""1"", ""2"", ""3"", ""4""]))), Success(WdlOptionalValue(WdlMaybeEmptyArrayType(WdlStringType),Some([""a"", ""b"", ""c"", ""d""])))). Requires exactly two evaluated array values of equal length.; ```. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4218:3315,configurat,configuration,3315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4218,1,['configurat'],['configuration']
Deployability,"the other errors I get are all from the workflow itself due to not preserving the original file names. The numerical hashes for files get passed directly into the downstream tools, stripping off any extensions or other identifying information. This results in tool confusion, like tabix can't tell a file wasn't already gzipped:; ```; ValueError: Unexpected tabix input: /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/cromwell_work/cromwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-prep_samples/shard-0/execution/bedprep/cleaned-8539016497173364825.gz; ```; or bwa can't find all the other associated indices:; ```; bwa mem /home/chapmanb/drive/work/cwl/test_bcbio_cwl/gcp/cromwell_work/cromwell-executions/main-somatic.cwl/93ef2d1c-88ee-4dc2-af0a-e0ea86bc785e/call-alignment/shard-1/wf-alignment.cwl/96d7b606-e0fe-4305-a586-e0fc4acf76f8/call-process_alignment/shard-0/inputs/1628767813 [...]. [E::bwa_idx_load_from_disk] fail to locate the index files; ```; Is it expected to lose the original input file names when passing through the pipeline. A lot of tools are sensitive to these and this might be the underlying issue. Regarding the configuration, without `http {}` in under `engine -> filesystems` I get a complaint about it not being supported, even with `http {}` under `backend -> providers -> Local -> config -> filesystems`:; ```; java.lang.IllegalArgumentException: Either https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa exists on a filesystem not supported by this instance of Cromwell, or a failure occurred while building an actionable path from it. Supported filesystems are: LinuxFileSystem. Failures: LinuxFileSystem: Cannot build a local path from https://storage.googleapis.com/bcbiodata/test_bcbio_cwl/testdata/genomes/hg19/seq/hg19.fa (RuntimeException) Please refer to the documentation for more information on how to configure filesystems: http://cromwell.readthedocs.io/en/develop/backends/HPC/#filesys",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320:1160,pipeline,pipeline,1160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4184#issuecomment-425997320,1,['pipeline'],['pipeline']
Deployability,they're `grep` and `wc` commands. I'll update the documentation accordingly.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-232419836:39,update,update,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-232419836,1,['update'],['update']
Deployability,"thinking about a more elegant way to solve the awkwardness of running a scatter while using Singularity on HPC. The major issues include:; * We run N `singularity build`s, for a scatter over N items, which wastes time and CPU, and writing N large images to the filesystem simultaneously will presumably challenge the filesystem.; * We have to store N `.sif` images, which wastes space while the job is running; * We have to delete the image after each `singularity build`. My first proposed solution was #4673, which would solve the problem but require a pull request to introduce a new hook to Cromwell. And it doesn't look like the Cromwell team have been able to prioritise this. . My new thought is that we could use file locks (e.g. `flock` on linux) to deal with this issue, so that the first worker to run will create a file lock, then all subsequent workers will encounter that lock, and wait until it's removed before attempting to build or run the image. For example, we currently recommend this `submit-docker` configuration:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; module load Singularity/3.0.1; ; # Build the Docker image into a singularity image; DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker}); IMAGE=${cwd}/$DOCKER_NAME.sif; if [ ! -f $IMAGE ]; then; singularity pull $IMAGE docker://${docker}; fi. # Submit the script to SLURM; sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execution/stderr \; -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${script}""; """"""; ```. I'm instead proposing this. Note the use of a single shared image directory (`/singularity_cache` in this example), and the use of `flock` to ensure the submit scripts aren't competing with each other:. ```; submit-docker = """"""; # Ensure singularity is loaded if it's installed as a module; modul",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063:1068,configurat,configuration,1068,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063,1,['configurat'],['configuration']
Deployability,this has been fixed on the `30_hotfix` branch but not released. The next release will have the fix.; In the meantime you could either build the jar yourself either from `develop` or from `30_hotffix`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3325#issuecomment-368998293:54,release,released,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3325#issuecomment-368998293,2,['release'],"['release', 'released']"
Deployability,"this is a critical issue of cromwell/wdltool, how did Broad avoid this issue in its internal pipeline using cromwell/wdl? this feature of WDL seems to be so commonly used in pipeline development.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2756#issuecomment-337931672:93,pipeline,pipeline,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2756#issuecomment-337931672,2,['pipeline'],['pipeline']
Deployability,this is fixed and the README appears to have been be updated too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-268377951:53,update,updated,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1753#issuecomment-268377951,1,['update'],['updated']
Deployability,this will spit out more informative errors when a user encounters a parsing error:. ![image](https://user-images.githubusercontent.com/165320/33690130-25f3b10c-dab0-11e7-8221-275ea9abec20.png). Unfortunately this has to be done by hand as the `Coproduct` parser in circe only returns the `CNil` by virtue of its automaticness. As such we will continue to hit `CNil` in deeper parts of the code(as seen in this example!) unless we write `Decoder`s by hand for all of our coproducts or figure out a way to implement error accumulation in Coproduct decoder derivation and submit a patch to Circe. For reference the offending code is [here](https://github.com/circe/circe/blob/master/modules/shapes/src/main/scala/io/circe/shapes/CoproductInstances.scala#L18),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3022:578,patch,patch,578,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3022,1,['patch'],['patch']
Deployability,"tible and I'm using Cromwell v32. There's a lot of shards spending 10 minutes in ""Waiting for quota"" when this problem happens. The instance that gives PAPI Error Code 10 was able to get a virtual machine, though. Maybe there is a timeout for ""Waiting for quota"" which causes all other shards to fail with Error Code 10 even though there was nothing wrong with this particular shard?. ```; Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; java.lang.Exception: Task to_bam_workflow.BaseRecalibrator:3:1 failed. The job was stopped before the command finished. PAPI error code 10. Message: 14: VM ggp-8822042418103915125 stopped unexpectedly.; 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:73); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:520); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:527); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:77); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1019); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1015); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingEx",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985:1026,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,1026,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3855#issuecomment-414289985,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"tils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatements(AbstractJdbcDatabase.java:1259); 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:582); 	... 16 common frames omitted; Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column '%failures%causedBy:%' in 'where clause'; 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Delegating",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:2728,UPDATE,UPDATE,2728,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['UPDATE'],['UPDATE']
Deployability,"tion/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: job id: 26767; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: job id: 26770; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: job id: 26763; [2016-11-24 15:22:46,77] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: Collector-printPairStringString; [2016-11-24 15:22:46,80] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-2119125994] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-d6475258-0f55-449c-be0b-e08e1e0c5049#337013427] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2016-11-24 15:22:46,81] [error] WorkflowManagerActor Workflow d6475258-0f55-449c-be0b-e08e1e0c5049 failed (during ExecutingWorkflowState): WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); scala.MatchError: WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$.write(WdlValueJsonFormatter.scala:10); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$$anonfun$write$2.apply(WdlValueJsonFormatter.scala:17); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$$anonfun$write$2.apply(WdlValueJsonFormatter.scala:17); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); 	at scala.collection.Iterator$class.foreach(I",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1703:4517,configurat,configuration,4517,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703,1,['configurat'],['configuration']
Deployability,"titute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL3BhY2thZ2Uuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...ool/src/main/scala/womtool/validate/Validate.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d29tdG9vbC9zcmMvbWFpbi9zY2FsYS93b210b29sL3ZhbGlkYXRlL1ZhbGlkYXRlLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...king/expression/files/BiscayneFileEvaluators.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvbGlua2luZy9leHByZXNzaW9uL2ZpbGVzL0Jpc2NheW5lRmlsZUV2YWx1YXRvcnMuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...in/scala/cromwell/backend/impl/bcs/BcsDocker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvYmNzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2JhY2tlbmQvaW1wbC9iY3MvQmNzRG9ja2VyLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...in/scala/cromwell/services/metadata/metadata.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c2VydmljZXMvc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvc2VydmljZXMvbWV0YWRhdGEvbWV0YWRhdGEuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | ... and [645 more](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=footer). Last update [26085f5...01c37f1](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620:4534,update,update,4534,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620,1,['update'],['update']
Deployability,"to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis/java-storage-nio/pull/774) (in [`0.123.18`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.18)) that raises my eyebrows a little. It's _probably_ fine, but there is new usage of `StorageOptionsUtil.getDefaultInstance()` for which I don't know the lifecycle or how else it's used. This is the type of thing that I'd watch out for in terms of thread safety, which is the root of the problem that caused us to rollback before. In summary, it's probably safe to go all the way to the most recent version. In fact, my gut feeling is that the risk is low enough to be outweighed b",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:1557,release,releases,1557,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,2,"['release', 'update']","['releases', 'updates']"
Deployability,tor Workflow a309b1f1-2b35-4396\; -9f42-bcb3c2d01724 failed (during ExecutingWorkflowState): java.lang.Exception: Task assemble_denovo.filter_to_taxon:NA:1 failed. The \; job was stopped before the command finished. PAPI error code 2. The zone 'projects/viral-comp-dev/zones/us-central1-b' does not have e\; nough resources available to fulfill the request. '(resource type:compute)'.; at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBack\; endJobExecutionActor.scala:84); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyn\; cBackendJobExecutionActor.scala:629); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:636); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsync\; BackendJobExecutionActor.scala:88); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1114); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionAc\; tor.scala:1110); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(Abst,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5001:1480,Pipeline,PipelinesApiAsync,1480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5001,1,['Pipeline'],['PipelinesApiAsync']
Deployability,tor [a15c46b7wf_hello.hello:NA:1]: Error attempting to Execute; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$UserPAPIApiException: Unable to complete PAPI request due to a problem with the request (Request contains an invalid argument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:51); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:34); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:20); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); at akka.actor.ActorCell.invoke(ActorCell.scala:583); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:3977,Pipeline,PipelinesApiRequestWorker,3977,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['Pipeline'],['PipelinesApiRequestWorker']
Deployability,"tor [d57a5f97atac.filter:1:1]: python $(which encode_filter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/d57a5f97-8542-4fcc-89c4-b7c487957dea/call-bowtie2/shard-1/glob-3bcbe4e7489c90f75e0523ac6f3a9385/ENCFF463QCX.trim.merged.R1.bam \; \; --multimapping 4 \; \; \; \; --nth 2; [2017-11-18 19:30:15,43] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.filter:1:1]: job id: operations/EMi1zpL9KxjduPXRuKr-7gYgtY36-tsbKg9wcm9kdWN0aW9uUXVldWU; [2017-11-18 19:30:26,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.filter:1:1]: Status change from - to Initializing; [2017-11-18 21:25:57,96] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: Status change from Initializing to Failed; [2017-11-18 21:25:58,22] [error] WorkflowManagerActor Workflow d57a5f97-8542-4fcc-89c4-b7c487957dea failed (during ExecutingWorkflowState): Task atac.bowtie2:0:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar -> /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar (cp failed: gsutil -q -m cp gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.\nCommandException: 1 file/object could not be transferred.\n)""; java.lang.Exception: Task atac.bowtie2:0:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar -> /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar (cp failed: gsutil -q -m cp gs://atac-seq-pipeline",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916:3857,pipeline,pipeline-genome-data,3857,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916,1,['pipeline'],['pipeline-genome-data']
Deployability,"torage.objectViewer"" | column -t; roles/iam.serviceAccountUser iam.serviceAccounts.actAs; roles/iam.serviceAccountUser iam.serviceAccounts.get; roles/iam.serviceAccountUser iam.serviceAccounts.list; roles/iam.serviceAccountUser resourcemanager.projects.get; roles/iam.serviceAccountUser resourcemanager.projects.list; roles/lifesciences.workflowsRunner lifesciences.operations.cancel; roles/lifesciences.workflowsRunner lifesciences.operations.get; roles/lifesciences.workflowsRunner lifesciences.operations.list; roles/lifesciences.workflowsRunner lifesciences.workflows.run; roles/storage.objectAdmin resourcemanager.projects.get; roles/storage.objectAdmin resourcemanager.projects.list; roles/storage.objectAdmin storage.objects.create; roles/storage.objectAdmin storage.objects.delete; roles/storage.objectAdmin storage.objects.get; roles/storage.objectAdmin storage.objects.getIamPolicy; roles/storage.objectAdmin storage.objects.list; roles/storage.objectAdmin storage.objects.setIamPolicy; roles/storage.objectAdmin storage.objects.update; roles/storage.objectCreator resourcemanager.projects.get; roles/storage.objectCreator resourcemanager.projects.list; roles/storage.objectCreator storage.objects.create; roles/storage.objectViewer resourcemanager.projects.get; roles/storage.objectViewer resourcemanager.projects.list; roles/storage.objectViewer storage.objects.get; roles/storage.objectViewer storage.objects.list; ```; Somehow the [tutorial](https://cromwell.readthedocs.io/en/develop/tutorials/PipelinesApi101/) suggests to add roles `storage.objectCreator` and `storage.objectViewer` but these do not include one of the four permissions `storage.objects.delete`, `storage.objects.getIamPolicy`, `storage.objects.setIamPolicy`, or `storage.objects.update` that are further added when adding also role `storage.objectAdmin` and at least one of these must be further needed by Cromwell. Either than by trial and error, I still do not understand how users are supposed to understand this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955:3531,update,update,3531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-685188955,2,['update'],['update']
Deployability,tp.StreamManagingStage.execute(StreamManagingStage.java:56); at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientHandler.java:45); at software.amazon.awssdk.services.batch.DefaultBatchClient.registerJobDefinition(DefaultBatchClient.java:644); at cromwell.backend.impl.aws.AwsBatchJob.$anonfun$createDefinition$2(AwsBatchJob.scala:198); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IO,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:3329,pipeline,pipeline,3329,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['pipeline'],['pipeline']
Deployability,"travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); 9620 at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 9621 at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 9622 at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3732:1765,pipeline,pipelines,1765,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732,1,['pipeline'],['pipelines']
Deployability,tributes$$anon$1: Google Pipelines API configuration is not valid: Errors:; Attempt to decode value on failed cursor: DownField(manifestFormatVersion); at cromwell.backend.google.pipelines.common.PipelinesApiConfigurationAttributes$.apply(PipelinesApiConfigurationAttributes.scala:307); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.defaultBuildAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:32); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.$anonfun$papiAttributes$1(PipelinesApiBackendLifecycleActorFactory.scala:34); at scala.util.Try$.apply(Try.scala:210); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.cromwell$backend$google$pipelines$common$PipelinesApiBackendLifecycleActorFactory$$build$1(PipelinesApiBackendLifecycleActorFactory.scala:109); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory$.robustBuildAttributes(PipelinesApiBackendLifecycleActorFactory.scala:120); at cromwell.backend.google.pipelines.common.PipelinesApiBackendLifecycleActorFactory.<init>(PipelinesApiBackendLifecycleActorFactory.scala:34); at cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory.<init>(PipelinesApiLifecycleActorFactory.scala:10); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490); at cromwell.engine.backend.BackendConfigurationEntry.$anonfun$asBackendLifecycleActorFactory$1(BackendConfiguration.scala:13); at scala.util.Try$.apply(Try.scala:210); at cromwell.engine.backend.BackendConfigurationEntry.asBackendLifecycleActorFactory(BackendConfiguration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:1623,Pipeline,PipelinesApiBackendLifecycleActorFactory,1623,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,1,['Pipeline'],['PipelinesApiBackendLifecycleActorFactory']
Deployability,"tries: 1; gpuCount: 1; zones: ""us-east1-d us-east1-c us-central1-a us-central1-c us-west1-a us-west1-b""; ##gpuType: ""nvidia-tesla-k80""; gpuType: ""nvidia-tesla-t4""; nvidiaDriverVersion: ""418.40.04""; ##nvidiaDriverVersion: ""418.87.00""; ; }. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; #### Recently, Our All workflows with GPU failed under the same configurations which most of workflows used to work on Cromwell 48, we updated to the latest Cromwell 52, still had the same errors, see belowL. 2020-08-04 23:44:00,228 cromwell-system-akka.dispatchers.engine-dispatcher-38 INFO - WorkflowManagerActor Workflow f1dca11c-ea29-48b1-9691-9f30c9e59154 failed (during ExecutingWorkflowState): java.lang.Exception: Task wf_quip_lymphocyte_segmentation_v03232020.quip_lymphocyte_segmentation:NA:2 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: generic::unknown: installing drivers: container exited with unexpected exit code 1: + COS_DOWNLOAD_GCS=https://storage.googleapis.com/cos-tools; + COS_KERNEL_SRC_GIT=https://chromium.googlesource.com/chromiumos/third_party/kernel; + COS_KERNEL_SRC_ARCHIVE=kernel-src.tar.gz; + TOOLCHAIN_URL_FILENAME=toolchain_url; + TOOLCHAIN_ARCHIVE=toolchain.tar.xz; + TOOLCHAIN_ENV_FILENAME=toolchain_env; + CHROMIUMOS_SDK_GCS=https://storage.googleapis.com/chromiumos-sdk; + ROOT_OS_RELEASE=/root/etc/os-release; + KERNEL_SRC_DIR=/build/usr/src/linux; + NVIDIA_DRIVER_VERSION=418.40.04; + NVIDIA_DRIVER_MD5SUM=; + NVIDIA_INSTALL_DIR_HOST=/var/lib/nvidia; + NVIDIA_INSTALL_DIR_CONTAINER=/usr/local/nvidia; + ROOT_MOUNT_DIR=/root; + CACHE_FILE=/usr/local/nvidia/.cache; + LOCK_FILE=/root/tmp/cos_gpu_installer_lock; + LOCK_FILE_FD=20; + set +x; [INFO 2020-08-04 23:40:07 UTC] Checking if this is the only cos-gpu-installer that is running.; [INFO 2020-08-04 23:40:07 UTC] Running on COS build id 12871.1174.0; [INFO 2020-08-04 23:40:07 UTC] Checking if third party",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:2361,install,installing,2361,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,1,['install'],['installing']
Deployability,"try/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/stderr"",; ""callRoot"": ""gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom"",; ""attempt"": 1,; ""executionEvents"": [; {; ""description"": ""CallCacheReading"",; ""startTime"": ""2020-08-29T00:00:44.174Z"",; ""endTime"": ""2020-08-29T00:00:44.237Z""; },; {; ""startTime"": ""2020-08-29T00:00:42.044Z"",; ""description"": ""Pending"",; ""endTime"": ""2020-08-29T00:00:42.064Z""; },; {; ""description"": ""RunningJob"",; ""startTime"": ""2020-08-29T00:00:44.237Z"",; ""endTime"": ""2020-08-29T00:04:05.347Z""; },; {; ""startTime"": ""2020-08-29T00:00:42.531Z"",; ""endTime"": ""2020-08-29T00:00:44.174Z"",; ""description"": ""PreparingJob""; },; {; ""startTime"": ""2020-08-29T00:00:42.064Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2020-08-29T00:00:42.516Z""; },; {; ""endTime"": ""2020-08-29T00:00:42.531Z"",; ""description"": ""WaitingForValueStore"",; ""startTime"": ""2020-08-29T00:00:42.516Z""; }; ],; ""backendLogs"": {; ""log"": ""gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/fail_oom.log""; },; ""start"": ""2020-08-29T00:00:42.022Z""; }; ]; },; ""outputs"": {},; ""workflowRoot"": ""gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/"",; ""actualWorkflowLanguage"": ""WDL"",; ""id"": ""87492280-9828-4afa-b53e-bec675103c42"",; ""inputs"": {},; ""labels"": {; ""cromwell-workflow-id"": ""cromwell-87492280-9828-4afa-b53e-bec675103c42"",; ""caper-backend"": ""gcp"",; ""caper-user"": ""leepc12""; },; ""submission"": ""2020-08-29T00:00:38.568Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""The compute backend terminated the job. If this termination is unexpected, examine likely causes such as preemption, running out of disk or memory on the compute instance, or exceeding the backend's maximum job duration.""; }; ],; ""message"": ""Workflow failed""; }; ],; ""end"": ""2020-08-29T00:04:06.071Z"",; ""start"": ""2020-08-29T00:00:38.789Z""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815:6577,pipeline,pipeline-test-runs,6577,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815,2,['pipeline'],['pipeline-test-runs']
Deployability,"ts out for me:. ```bash; $ java -jar -Dconfig.file=backends/backend.conf -Dbackend.default=singularity cromwell-34.jar run runners/test.wdl -i data/TEST-YEAST/inputs.json -o workflow_opts/singularity.json; [2018-08-27 02:03:57,30] [info] Running with database db.url = jdbc:hsqldb:mem:3cd4a928-7743-4252-8bd9-e905ebe70980;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:05,58] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-27 02:04:05,60] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-27 02:04:05,75] [info] Running with database db.url = jdbc:hsqldb:mem:c850e4aa-3449-4d7e-bf04-4593fe287777;shutdown=false;hsqldb.tx=mvcc; [2018-08-27 02:04:06,15] [warn] This actor factory is deprecated. Please use cromwell.backend.google.pipelines.v1alpha2.PipelinesApiLifecycleActorFactory for PAPI v1 or cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory for PAPI v2; [2018-08-27 02:04:06,16] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-27 02:04:06,16] [info] Using noop to send events.; [2018-08-27 02:04:06,43] [info] Slf4jLogger started; [2018-08-27 02:04:06,64] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-be06fbc"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2018-08-27 02:04:06,71] [info] Metadata summary refreshing every 2 seconds.; [2018-08-27 02:04:06,81] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-27 02:04:07,85] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-27 02:04:07,88] [info] SingleWorkflowRunnerActor: Version ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:1585,Pipeline,PipelinesApiLifecycleActorFactory,1585,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,1,['Pipeline'],['PipelinesApiLifecycleActorFactory']
Deployability,"ts_folder; }. output {; File out = copy_results.out[0]; }. }. task diamond_blast {. Int threads; File database; File query; String name; String mode. command {; diamond ${mode} -d ${database} -q ${query} \; --more-sensitive -o ${name}.m8 \; -f 6 sseqid qseq score pident stitle qcovhsp qtitle \; }. runtime {; docker: ""quay.io/comp-bio-aging/diamond:latest""; }. output {; File out = name + "".m8""; }. }. task copy {; Array[File] files; String destination. command {; mkdir -p ${destination}; cp -L -R -u ${sep=' ' files} ${destination}; }. output {; Array[File] out = files; }; }; ```; and here is an example of the input:; ```json. Diamond_Blast.mode = ""blastp""; Diamond_Blast.query = ""/pipelines/indexes/GRAY_WHALE/NTJE01P.1.fasta""; Diamond_Blast.threads = 8; Diamond_Blast.result_name = ""graywhale_in_minkywhale_blastp""; Diamond_Blast.db = ""/pipelines/indexes/diamond/MINKY_WHALE_GCF_000493695.1.dmnd""; Diamond_Blast.results_folder = ""/pipelines/results/graywhale/transcriptome/diamond/blastp""; ```; when I run the workflow many times with input changes I get the following:; ```. name | status | stdout | stderr | cache | shard; -- | -- | -- | -- | -- | --; Diamond_Blast.copy_results | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-copy_results/execution/stderr | Cache Hit: fdda40c0-a501-456c-a903-954aa52af83d:Diamond_Blast.copy_results:-1 | -1; Diamond_Blast.diamond_blast | Done | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stdout | /pipelines/cromwell_1/cromwell-executions/Diamond_Blast/da1376f4-6f31-403f-941d-e8c36483897e/call-diamond_blast/execution/stderr | Cache Miss | -1; ```; that shows that the task diamond_blast recomputes while the copy task copies the cached result, in other words it sends the same file as pr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3044:1998,pipeline,pipelines,1998,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3044,1,['pipeline'],['pipelines']
Deployability,ttps://bitbucket.org/snakeyaml/snakeyaml/src) from 1.33 to 2.0.; [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/RELEASES.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/RELEASES.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/RELEASES.rst) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/ReleaseNotes.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/ReleaseNotes.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/ReleaseNotes.rst) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Releases.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Releases.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Releases.rst) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/releases.markdown) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/releases.md) - [Release Notes](https://bitbucket.org/snakeyaml/snakeyaml/src/master/releases.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/CHANGES.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/snakeyaml/snakeyaml/sr,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7081:1043,release,releases,1043,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7081,1,['release'],['releases']
Deployability,tus 1: sha256:1532dec11e05c471f827f59efdd9ff978e63ebe8f7292adf56c406374c131f71: Pulling from broadinstitute/gatk; cromwell_1 | ae79f2514705: Pulling fs layer; cromwell_1 | 5ad56d5fc149: Pulling fs layer; cromwell_1 | 170e558760e8: Pulling fs layer; cromwell_1 | 395460e233f5: Pulling fs layer; cromwell_1 | 6f01dc62e444: Pulling fs layer; cromwell_1 | 98db058f41f6: Pulling fs layer; [...]; cromwell_1 | failed to register layer: Error processing tar file(exit status 1): write /root/.cache/pip/http/5/1/d/8/2/51d82969228464b761a16257d5eefe8e2b3dde3c1ad733721353e785: no space left on device; cromwell_1 |; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1099); cromwell_1 | at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1095); cromwell_1 | at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); cromwell_1 | at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); cromwell_1 | at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); cromwell_1 | at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); cromwell_1 | at akka.dispatch.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4337:2146,pipeline,pipelines,2146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4337,1,['pipeline'],['pipelines']
Deployability,"tutorials/PipelinesApi101/). The following command does not work:; ```; $ gcloud projects add-iam-policy-binding xxx --member serviceAccount:xxx@xxx.gserviceaccount.com --role roles/compute.zones.list; ERROR: Policy modification failed. For a binding with condition, run ""gcloud alpha iam policies lint-condition"" to identify issues in condition.; ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/compute.zones.list is not supported for this resource.; ```; I have no idea what I should do. Why can't Cromwell simply provide the command line needed to change the permission?. As for Requester Pays, following the [documentation](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) I have set up the `project` field in the `gcs` filesystem configuration (completely unclear which one in the documentation, as according to the tutorial there are two, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, n",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:1383,configurat,configuration,1383,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,1,['configurat'],['configuration']
Deployability,tware.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTracki,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1503,pipeline,pipeline,1503,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,"two in the JIRA issue tracker back in August.; > Reposting here since it didn't get a response over there:; > https://broadworkbench.atlassian.net/browse/BA-6548; >; > Hello everyone,; >; > I am attempting to use the AWS Batch backend for Cromwell to run a wdl; > script which runs several subjobs in parallel. I believe the correct; > parlance is a scatter. I noticed that in some of the jobs of the scatter,; > some reference files failed to download from S3 even though they existed; > (Connection Reset by Peer). This failure caused the overall job to fail; > after one hour of running.; >; > I believe this issue was reported and fixed before, around May 2019, but; > recently, in June 2020, it appears the AWS Batch backend was majorly; > overhauled (by @markjschreiber <https://github.com/markjschreiber>,; > thanks! Also, tagging you because I suspect you might be the resident; > expert here :) ), and the previous fix (using the ecs proxy image) was; > supposedly obsoleted.; >; > I also see that the s3fs library appears to be vendored into cromwell, and; > after digging around, it appears that one might be able to set retries via; > an environment variable(?). But even then, I feel like if that were to; > work, it would be much nicer if it was configurable through cromwell's; > config file somehow.; >; > So that brings me to my final question. Is there some configuration that; > allows me to retry failed downloads some number of times before failing the; > whole job? Or, perhap there is some alternative configuraiton which I've; > overlooked and someone could point me to it? Thanks!; >; > In addition, just wondering if perhaps there is a service limit I might be; > running into?; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/5946>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6ENB62FDV4UVUUQGAE3SKXWAPANCNFSM4SQ7HRGQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780:2464,configurat,configuration,2464,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5946#issuecomment-710428780,1,['configurat'],['configuration']
Deployability,"t}""; """""". submit-docker = """"""; # SINGULARITY_CACHEDIR needs to point to a directory accessible by; # the jobs (i.e. not lscratch). Might want to use a workflow local; # cache dir like in run.sh; source /work/share/ac7m4df1o5/bin/cromwell/set_singularity_cachedir.sh; SINGULARITY_CACHEDIR=/work/share/ac7m4df1o5/bin/cromwell/singularity-cache; source /work/share/ac7m4df1o5/bin/cromwell/test.sh ${docker}; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; if [ -z $SINGULARITY_CACHEDIR ]; then; CACHE_DIR=$HOME/.singularity; else; CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; echo ""SINGULARITY_CACHEDIR $SINGULARITY_CACHEDIR""; LOCK_FILE=$CACHE_DIR/singularity_pull_flock. # we want to avoid all the cromwell tasks hammering each other trying; # to pull the container into the cache for the first time. flock works; # on GPFS, netapp, and vast (of course only for processes on the same; # machine which is the case here since we're pulling it in the master; # process before submitting).; #flock --exclusive --timeout 1200 $LOCK_FILE \; # singularity exec --containall docker://${docker} \; # echo ""successfully pulled ${docker}!"" &> /dev/null. # Ensure singularity is loaded if it's installed as a module; module load apps/singularity/3.7.3. # Build the Docker image into a singularity image; #IMAGE=$(echo $SINGULARITY_CACHEDIR/pull/${docker}.sif|sed ""s#:#_#g""); #singularity build $IMAGE docker://${docker}. # Submit the script to SLURM; sbatch \; --wait \; --job-name=${job_name} \; --chdir=${cwd} \; --output=${cwd}/execution/stdout \; --error=${cwd}/execution/stderr \; --time=${runtime_minutes} \; ${""--cpus-per-task="" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --partition=wzhcexclu06 \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} $SINGULARITY_CACHEDIR/pull/$docker_image.sif ${job_shell} ${docker_script}""; """""". kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }. }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:8887,install,installed,8887,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['install'],['installed']
Deployability,"ub.com/jbwheatley/pact4s); * [io.github.jbwheatley:pact4s-scalatest](https://github.com/jbwheatley/pact4s). from `0.9.0` to `0.10.1-java8`. ðŸ“œ [GitHub Release Notes](https://github.com/jbwheatley/pact4s/releases/tag/v0.10.1-java8) - [Version Diff](https://github.com/jbwheatley/pact4s/compare/v0.9.0...v0.10.1-java8). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.9.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Pair/cnv_somatic_pair_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Panel/cnv_somatic_panel_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.aws.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.inputs; centaur/src/main/resources/integrationTestCases/germline/haplotype-caller-workflow/HaplotypeCallerWF.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-production-workflow/PairedEndSingleSampleWf.options.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; centaur/src/main/resources/integrationTestCases/green/arrays/arrays.wdl; womtool/src/test/resources/validate/wdl_draft3/valid/HaplotypeCallerWF/HaplotypeCallerWF.inputs.json; womtool/src/test/resources/vali",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7294:1085,update,update,1085,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7294,1,['update'],['update']
Deployability,uld be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; cromwell.backend.validation.ValidatedRuntimeAttributesBuilder$$anon$1: Runtime attribute validation failed:; :; Disk strings should be of the format 'local-disk SIZE TYPE' or '/mount/point SIZE TYPE' but got: '10 HDD'; 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build(ValidatedRuntimeAttributesBuilder.scala:62); 	at cromwell.backend.validation.ValidatedRuntimeAttributesBuilder.build$(ValidatedRuntimeAttributesBuilder.scala:56); 	at cromwell.backend.standard.StandardValidatedRuntimeAttributesBuilder$StandardValidatedRuntimeAttributesBuilderImpl.build(StandardValidatedRuntimeAttributesBuilder.scala:20); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes(StandardCachingActorHelper.scala:56); 	at cromwell.backend.standard.StandardCachingActorHelper.validatedRuntimeAttributes$(StandardCachingActorHelper.scala:54); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.validatedRuntimeAttributes(PipelinesApiAsyncBackendJobExecutionActor.scala:87); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues(StandardCachingActorHelper.scala:75); 	at cromwell.backend.standard.StandardCachingActorHelper.startMetadataKeyValues$(StandardCachingActorHelper.scala:74); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues$lzycompute(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.startMetadataKeyValues(PipelinesApiAsyncBackendJobExecutionActor.scala:534); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:951,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4918:1384,pipeline,pipelines,1384,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4918,1,['pipeline'],['pipelines']
Deployability,"ull: running [""docker"" ""pull"" ""gcr.io/broad-cumulus/cellranger@sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356""]: exit status 1 (standard error: ""Error response from daemon: pull access denied for gcr.io/broad-cumulus/cellranger, repository does not exist or may require 'docker login': denied: Permission denied for \""sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"" from request \""/v2/broad-cumulus/cellranger/manifests/sha256:a3e918f232f7ae125cf46bd38bff928bb92dafc8a8f9213c5e52be1de7924356\"".\n""); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:91); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:803); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.$anonfun$handleExecutionFailure$1(PipelinesApiAsyncBackendJobExecutionActor.scala:815); 	at scala.util.Try$.apply(Try.scala:213); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:812); 	at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:95); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1340); 	at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$11.applyOrElse(StandardAsyncExecutionActor.scala:1336); 	at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:417); 	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41); 	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64); 	at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecuto",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6506:13918,Pipeline,PipelinesApiAsyncBackendJobExecutionActor,13918,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6506,1,['Pipeline'],['PipelinesApiAsyncBackendJobExecutionActor']
Deployability,"ults: Default values for annotations, when values are unspecified. Specified as <ANNOTATION>:<VALUE>. For example: ""Center:Broad""; ## funco_annotation_overrides: Values for annotations, even when values are unspecified. Specified as <ANNOTATION>:<VALUE>. For example: ""Center:Broad""; ## funcotator_excluded_fields: Annotations that should not appear in the output (VCF or MAF). Specified as <ANNOTATION>. For example: ""ClinVar_ALLELEID""; ## funco_filter_funcotations: If true, will only annotate variants that have passed filtering (. or PASS value in the FILTER column). If false, will annotate all variants in the input file. Default: true; ## funcotator_extra_args: Any additional arguments to pass to Funcotator. Default: """"; ##; ## Outputs :; ## - One VCF file and its index with primary filtering applied; secondary filtering and functional annotation if requested; a bamout.bam; ## file of reassembled reads if requested; ##; ## Cromwell version support; ## - Successfully tested on v34; ##; ## LICENSING :; ## This script is released under the WDL source code license (BSD-3) (see LICENSE in; ## https://github.com/broadinstitute/wdl). Note however that the programs it calls may; ## be subject to different licenses. Users are responsible for checking that they are; ## authorized to run all programs before running this script. Please see the docker; ## pages at https://hub.docker.com/r/broadinstitute/* for detailed licensing information; ## pertaining to the included programs. struct Runtime {; String gatk_docker; File? gatk_override; Int max_retries; Int preemptible; Int cpu; Int machine_mem; Int command_mem; Int disk; Int boot_disk_size; }. workflow Mutect2 {; input {; # Mutect2 inputs; File? intervals; File ref_fasta; File ref_fai; File ref_dict; File file_tumor_reads; Array[File] all_tumor_reads = read_lines(file_tumor_reads); File file_tumor_reads_indexes; Array[File] all_tumor_reads_indexes = read_lines(file_tumor_reads_indexes); Array[Pair[File,File]] tumor_reads_and_in",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:6020,release,released,6020,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['release'],['released']
Deployability,ument.).; at cromwell.backend.google.pipelines.v2beta.api.request.RunRequestHandler$$anon$1.onFailure(RunRequestHandler.scala:33); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:51); at com.google.api.client.googleapis.batch.json.JsonBatchCallback.onFailure(JsonBatchCallback.java:47); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseAndCallback(BatchUnparsedResponse.java:209); at com.google.api.client.googleapis.batch.BatchUnparsedResponse.parseNextResponse(BatchUnparsedResponse.java:149); at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:267); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:51); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:34); at akka.actor.Actor.aroundReceive(Actor.scala:539); at akka.actor.Actor.aroundReceive$(Actor.scala:537); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:20); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:614); at akka.actor.ActorCell.invoke(ActorCell.scala:583); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:268); at akka.dispatch.Mailbox.run(Mailbox.scala:229); at akka.dispatch.Mailbox.exec(Mailbox.scala:241); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestMan,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:4236,pipeline,pipelines,4236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,2,['pipeline'],['pipelines']
Deployability,"unts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ delocalization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Delocalization; timeout: 300s; - alwaysRun: true; commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/1xxxxxx.sh && chmod u+x /tmp/1xxxxxx.sh; && sh /tmp/1xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; - alwaysRun: true; commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; environment:; MEM_SIZE: '2.0'; MEM_UNIT: GB; resources:; virtualMachine:; bootDiskSizeGb: 12; bootImage: projects/cos-cloud/global/images/family/cos-stable; disks:; - name: local-disk; sizeGb: 10; type: pd-ssd; labels:; cromwell-workflow-id: xxxxxx; goog-pipelines-worker: 'true'; wdl-task-name: hello; machineType: custom-1-2048; network: {}; nvidiaDriverVersion: 450.51.06; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/cloudkms; - https://www.googleapis.com/auth/userinfo.email; - https://www.googleapis.com/auth/userinfo.profile; - https://www.googleapis.com/auth/monitoring.write; - https://www.googleapis.com/auth/bigquery; - https://www.googleapis.com/auth/cloud-platform; volumes:; - persistentDisk:; sizeGb: 10; type: pd-ssd; volume: local-disk; zones:; - us-central1-a; - us-central1-b; timeout: 604800s; startTime: '2021-08-03T15:22:07.789742627Z'; name: projects/xxxxxx/locations/us-central1/operations/xxxxxx; response:; '@type': type.googleapis.com/cloud.lifesciences.pipelines.RunPipelineResponse. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:21195,pipeline,pipelines-worker,21195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,2,['pipeline'],"['pipelines', 'pipelines-worker']"
Deployability,update,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6645:0,update,update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6645,1,['update'],['update']
Deployability,update DB docs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3418:0,update,update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3418,1,['update'],['update']
Deployability,update akka version,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1792:0,update,update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1792,1,['update'],['update']
Deployability,update changelog again,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2272:0,update,update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2272,1,['update'],['update']
Deployability,update changelog to reflect reality,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2263:0,update,update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2263,1,['update'],['update']
Deployability,update docker documentation,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7193:0,update,update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7193,1,['update'],['update']
Deployability,update documentation about LOCAL ssd,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5224:0,update,update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5224,1,['update'],['update']
Deployability,update logback,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1967:0,update,update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1967,1,['update'],['update']
Deployability,update metadata summary defaults,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4830:0,update,update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4830,1,['update'],['update']
Deployability,update readme on spark backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2033:0,update,update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2033,1,['update'],['update']
Deployability,update readme to reflect website,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/584:0,update,update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/584,1,['update'],['update']
Deployability,update title to include triage. Just because someone put in a PBE TODO doesn't mean it absolutely has to be done. Use good judgement ;),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-236272656:0,update,update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-236272656,1,['update'],['update']
Deployability,update to latest akka,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1636:0,update,update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1636,1,['update'],['update']
Deployability,update wdl4s,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1788:0,update,update,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1788,1,['update'],['update']
Deployability,updated,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4756#issuecomment-474512847:0,update,updated,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4756#issuecomment-474512847,1,['update'],['updated']
Deployability,updated HPCIntro.md file with instructions to install Cromwell to use local scratch device [BT-592],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6698:0,update,updated,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6698,2,"['install', 'update']","['install', 'updated']"
Deployability,"updated Single sample wdl; added haplotypcaller, jointdiscovery, and data preprocessing wdls. All wdls using gatk4; increased travis heartbeat to 180, ; increased max time for travis to 3 hours",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3212:0,update,updated,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3212,1,['update'],['updated']
Deployability,updated rest api docs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3570:0,update,updated,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3570,1,['update'],['updated']
Deployability,updated wdl4s ref,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2678:0,update,updated,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2678,1,['update'],['updated']
Deployability,updates for subprojected wdl4s,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2482:0,update,updates,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2482,1,['update'],['updates']
Deployability,upgrade wdl4s,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1957:0,upgrade,upgrade,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1957,1,['upgrade'],['upgrade']
Deployability,"uration if this is the case; 2019-01-31 18:43:04,929 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,934 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,936 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:04,937 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case; 2019-01-31 18:43:05,044 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table WORKFLOW_EXECUTION created; 2019-01-31 18:43:05,122 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table EXECUTION created; 2019-01-31 18:43:05,285 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table JES_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: Table LOCAL_JOB created; 2019-01-31 18:43:05,477 INFO - changelog.xml: changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer: ChangeSet changesets/db_schema.xml::db_schema_other_table_alldb::scottfrazer ran successfully in 538ms; 2019-01-31 18:43:05,650 INFO - changelog.xml: changesets/db_schema.xml::db_schema_symbol_table_mysql::scottfrazer: Table SYMBOL created; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:7089,configurat,configuration,7089,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['configurat'],['configuration']
Deployability,"uration outside of standard `docker run` commands, it's trivial to distribute Cromwell jobs across Swarm nodes. However, Swarm provides a series of [filters](https://github.com/docker/swarm/tree/master/scheduler/filter) and constrains that control how the scheduler distributes containers to nodes. For example, I might be interested in limiting the execution of a Cromwell job to a specific region / datacenter. This requires you to specify filters in the `docker run` command with the environment flag, `-e`. For example, to run a container on Swarm nodes that run in the `us-east` region:. ```; â€º docker run -d --name my_image -e constraint:region!=us-east* my_container; ```. Obviously, this configuration should _not_ be managed in the WDL document. Instead, it would be great for the Cromwell command-line tool and REST API to support additional runtime options for specifying Docker environment variables. For example:. ```; â€º cromwell run --docker-env ""constraint:region!=us-east*"" my_workflow.wdl -; ```. > Hint: Docker supports daemon labels. In the above case, the workflow would; > execute on a Swarm node whose Docker daemon that was started with:; > ; > ```; > docker daemon --label region=us-east; > ```. As for the API, the POST action to `/api/workflows/:version` would allow for multiple Docker env strings. The other feature I would like to request is translating `memory` and `cpu` configuration options (at the task level) to Docker via `--memory` and `--cpuset-cpus` `docker run` flags, respectively. These options are currently only used for the JES backend, but it seems as though they can also be used for the Local backend if Docker is specified. So, to summarize:; 1. Allow Docker `-e` flags to be specified for all tasks in a given workflow.; 2. Allow task `memory` and `cpu` options in a WDL document to be translated to `--memory` and `--cpuset-cpus` in the `docker run` command. Please let me know if there's anything I can do to help this move forward. Cheers! :beers:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/375:1670,configurat,configuration,1670,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/375,1,['configurat'],['configuration']
Deployability,"ure_metadata.xml::causedByLists::cjllanwarne failed. Error: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 2019-01-31 20:10:51,492 INFO - changesets/failure_metadata.xml::causedByLists::cjllanwarne: Successfully released change log lock; 2019-01-31 20:10:51,531 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.ru",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1362,Update,UpdateVisitor,1362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['Update'],['UpdateVisitor']
Deployability,"urn code of OOM is just 137.; Why don't we have something like `memoryRetryReturnCode`. I think it's too dangerous too set `continueOnReturnCode` as `true`.; Cromwell will pass any failure in all tasks.; So I set it as `[0, 137]` to catch `SIGKILL` due to OOM.; I also tried with `true` though. Here is my simple OOM tester WDL. I tested it with PAPIv2 beta based on Life Sciences API. ```wdl; version 1.0. workflow mem_retry {; call fail_oom; }. task fail_oom {; command {; set -e; # This one-liner triggers OOM and hence 137 (SIGKILL); # https://askubuntu.com/a/823798; tail /dev/zero # <====== This WDL works fine without this line; }; runtime {; cpu: 1; memory: ""2 GB""; docker: ""ubuntu:latest""; continueOnReturnCode: [0, 137]; }; }; ```. Google backend (PAPI2 beta) in `backend.conf`, ; ```; config {; memory-retry {; error-keys = [""OutOfMemoryError"", ""Killed""]; multiplier = 1.5; }; }; ```. STDERR of task:; ```; $ gsutil cat gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/stderr; /cromwell_root/script: line 28: 17 Killed tail /dev/zero; ```. RC of task. It's weird that this is not caught in `metadata.json`.; ```; $ gsutil cat gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/rc; 137; ```. `memory_retry_rc`: So Cromwell found that it's failed due to OOM.; ```; $ gsutil cat gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/memory_retry_rc; 0; ```. `metadata.json`; ```; {; ""workflowName"": ""mem_retry"",; ""workflowProcessingEvents"": [; {; ""timestamp"": ""2020-08-29T00:00:38.724Z"",; ""cromwellVersion"": ""53"",; ""cromwellId"": ""cromid-0a29b92"",; ""description"": ""PickedUp""; },; {; ""description"": ""Finished"",; ""cromwellId"": ""cromid-0a29b92"",; ""timestamp"": ""2020-08-29T00:04:06.072Z"",; ""cromwellVersion"": ""53""; }; ],; ""metadataSource"": ""Unarchived"",; ""actualWorkflowLanguageVersion"": ""1.0"",; ""submittedFiles"": {; ""workflow"": ""version",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815:1230,pipeline,pipeline-test-runs,1230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815,1,['pipeline'],['pipeline-test-runs']
Deployability,"use QPS based locks from Google.; # 1000 is the default ""Queries per 100 seconds per user"", 50000 is the default ""Queries per 100 seconds""; # See https://cloud.google.com/genomics/quotas for more information; genomics-api-queries-per-100-seconds = 25000. # Polling for completion backs-off gradually for slower-running jobs.; # This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. # Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; # account = """"; # token = """"; }. #docker-image-cache-manifest-file = ""gs://xxxxx-xxxxx/xxxxx.json"". # Number of workers to assign to PAPI requests; request-workers = 3. # Optional configuration to use high security network (Virtual Private Cloud) for running jobs.; # See https://cromwell.readthedocs.io/en/stable/backends/Google/ for more details.; # virtual-private-cloud {; # network-label-key = ""network-key""; # auth = ""application-default""; # }. # Global pipeline timeout; # Defaults to 7 days; max 30 days; # pipeline-timeout = 7 days. genomics {; # A reference to an auth defined in the `google` stanza at the top. This auth is used to create; # Pipelines and manipulate auth JSONs.; auth = ""application-default"". // alternative service account to use on the launched compute instance; // NOTE: If combined with service account authorization, both that serivce account and this service account; // must be able to read and write to the 'root' GCS path; compute-service-account = ""default"". # Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://lifesciences.googleapis.com/"". # Currently Cloud Life Sciences API is available only in `us-central1` and `europe-west2` locations.; location = ""us-central1"". # Restrict access to VM metadata. Useful in cases when untrusted containers are running under a service; # account not owned by the submitting user; restrict-metadata-access = false. # Pipelines v2 only: specify the number of times localizati",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:2549,pipeline,pipeline,2549,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,2,['pipeline'],"['pipeline', 'pipeline-timeout']"
Deployability,use upgraded version of akka,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/108:4,upgrade,upgraded,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/108,1,['upgrade'],['upgraded']
Deployability,"used as outputs... so I decide to report here. This is an example I prepared:. ```wdl; version 1.0. struct Test {; String name; File path; }. struct Collection {; Array[Test] samples; }. task GenerateComplexObject {; input {; Int items; }. command <<<; python <<CODE; import sys; import os; import json; items = []; for item in range(0, ~{items}):; name = f""test{item}.txt""; os.system(f""echo 'some content' > {name}""); items.append({""name"": f""item-{item}"", ""path"": name}). with open(""results.json"", ""w"") as fh:; json.dump({'samples': items}, fh); CODE; >>>. runtime {; docker: ""python:3.8""; memory: ""1 GB""; cpu: 1; preemptible: 3; disks: ""local-disk "" + 10 + "" HDD""; }. output {; Collection results = read_json(""results.json""); }; }. workflow TestStruct {; input {; Int items; }. call GenerateComplexObject {; input:; items=items; }. output {; Collection out = GenerateComplexObject.results; }; }; ```. When using local backend I have no problem, but when using PAPIv2 (`cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory`) the files from `Test` struct (path) do not delocalize. ```bash; gsutil ls gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/GenerateComplexObject.log; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/gcs_delocalization.sh; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/gcs_localization.sh; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/gcs_transfer.sh; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/rc; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/results.json; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/script; gs://********/TestaStruct/47bc869c-041f-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5592:1132,pipeline,pipelines,1132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592,1,['pipeline'],['pipelines']
Deployability,"ustom_labels.xml::replace_empty_custom_labels::rmunshi: Change Set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.sc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606:1575,Update,UpdateVisitor,1575,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606,1,['Update'],['UpdateVisitor']
Deployability,"ustom_labels::rmunshi failed. Error: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 2019-01-31 19:14:34,471 INFO - changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi: Successfully released change log lock; 2019-01-31 19:14:34,501 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadP",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606:1697,update,update,1697,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606,1,['update'],['update']
Deployability,"ut an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9606 Execution failed: action 14: unexpected exit status 1 was not ignored; 9607 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9608 java.lang.Exception: Task JointGenotyping.ApplyRecalibration:16:1 failed. Job exited without an error, exit code 0. PAPI error code 9. Message: Execution failed: action 14: unexpected exit status 1 was not ignored; 9609 Execution failed: action 14: unexpected exit status 1 was not ignored; 9610 Unexpected exit status 1 while running ""/bin/sh -c gsutil cp /cromwell_root/stdout gs://cloud-cromwell-dev/cromwell_execution/travis/JointGenotyping/c9dfd3ed-8be8-413f-af46-4692142b3248/call-ApplyRecalibration/shard-16/stdout""; 9611 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:76); 9612 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:532); 9613 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:539); 9614 at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:80); 9615 at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionResult$3(StandardAsyncExecutionActor.scala:1037); 9616 at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); 9617 at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); 9618 at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); 9619 at akka",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3732:1433,pipeline,pipelines,1433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3732,1,['pipeline'],['pipelines']
Deployability,"ut files (generated by previous calls) correctly when I restart a workflow with callcaching enabled. Inputs which originate from the same directory are getting split up in the inputs directory inside cromwell-executions. For example, given a workflow like this:; ```wdl; version 1.0; workflow test {; call touch as t1 {; input:; name = ""[...]/cromwell_test/testFile""; }; call touch as t2 {; input:; name = ""[...]/cromwell_test/companionCube""; }; call blah {; input:; file = t1.out,; filesBuddy = t2.out; } ; }; task touch {; input {; String name; }; command {; touch ~{name}; }; output {; File out = name; }; }; task blah {; input {; File file; File filesBuddy; }; command {; exit 1; }; }; ```; We get the following (correct) localization for the initial run:; ```; [...]/cromwell_test/cromwell-executions/test/6d5571c1-6612-4107-bcf5-0b40791d17fb/call-blah/inputsâŸ« ls -R; .:; 1981809718. ./1981809718:; companionCube testFile. ```; And the following (incorrect) localization for the rerun:; ```; [...]/cromwell_test/cromwell-executions/test/8aa01c35-d095-483d-84e5-3d72ead73344/call-blah/inputsâŸ« ls -R; .:; -1523004024 -1523004025. ./-1523004024:; companionCube. ./-1523004025:; testFile; ```. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4965:1948,configurat,configuration,1948,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4965,1,['configurat'],['configuration']
Deployability,ute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPip,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:2114,pipeline,pipeline,2114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,"util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:157:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:328); 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	 at cromwell.core.path.NioPathMethods.subpath(NioPathMethods.scala:18); 	 at cromwell.core.path.NioPathMethods.subpath$(NioPathMethods.scala:18); 	 at cromwell.core.path.DefaultPath.subpath(DefaultPathBuilder.scala:55); 	 at cromwell.backend.io.JobPathsWithDocker.toDockerPath(JobPathsWithDocker.scala:56); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$mapCommandLineWomFile$1(SharedFileSystemAsyncJobExecutionActor.scala:147); 	 at wom.values.WomS",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:1640,install,install,1640,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['install'],['install']
Deployability,"utionActor [d57a5f97atac.filter:1:1]: job id: operations/EMi1zpL9KxjduPXRuKr-7gYgtY36-tsbKg9wcm9kdWN0aW9uUXVldWU; [2017-11-18 19:30:26,72] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.filter:1:1]: Status change from - to Initializing; [2017-11-18 21:25:57,96] [info] JesAsyncBackendJobExecutionActor [d57a5f97atac.bowtie2:0:1]: Status change from Initializing to Failed; [2017-11-18 21:25:58,22] [error] WorkflowManagerActor Workflow d57a5f97-8542-4fcc-89c4-b7c487957dea failed (during ExecutingWorkflowState): Task atac.bowtie2:0:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar -> /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar (cp failed: gsutil -q -m cp gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.\nCommandException: 1 file/object could not be transferred.\n)""; java.lang.Exception: Task atac.bowtie2:0:1 failed. JES error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar -> /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar (cp failed: gsutil -q -m cp gs://atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar /mnt/local-disk/atac-seq-pipeline-genome-data/mm10/bowtie2_index/mm10_no_alt_analysis_set_ENCODE.fasta.tar, command failed: BadRequestException: 400 Bucket is requester pays bucket but no user project provided.\nCommandException: 1 file/object could not be transferred.\n)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916:4198,pipeline,pipeline-genome-data,4198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916,5,['pipeline'],['pipeline-genome-data']
Deployability,"utput files together.\n"",; ""inputBinding"": {; ""prefix"": ""--jobnodes""; },; ""default"": false,; ""id"": ""#gridss-2.9.4.cwl/jobnodes""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""size of JVM heap for assembly and variant calling.\n"",; ""inputBinding"": {; ""prefix"": ""--jvmheap""; },; ""default"": ""$(get_max_memory_from_runtime_memory(runtime.ram))m"",; ""id"": ""#gridss-2.9.4.cwl/jvmheap""; },; {; ""type"": ""boolean"",; ""doc"": ""keep intermediate files. Not recommended except for debugging due to the high disk usage.\n"",; ""inputBinding"": {; ""prefix"": ""--keepTempFiles""; },; ""default"": false,; ""id"": ""#gridss-2.9.4.cwl/keepTempFiles""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""comma separated labels to use in the output VCF for the input files.\nSupporting read counts for input files with the same label are aggregated\n(useful for multiple sequencing runs of the same sample).\nLabels default to input filenames, unless a single read group with a non-empty sample name\nexists in which case the read group sample name is used\n(which can be disabled by \""useReadGroupSampleNameCategoryLabel=false\"" in the configuration file).\nIf labels are specified, they must be specified for all input files.\n"",; ""inputBinding"": {; ""prefix"": ""--labels""; },; ""id"": ""#gridss-2.9.4.cwl/labels""; },; {; ""type"": [; ""null"",; ""int""; ],; ""doc"": ""Optional - maximum coverage. Regions with coverage in excess of this are ignored.\n"",; ""inputBinding"": {; ""prefix"": ""--maxcoverage""; },; ""id"": ""#gridss-2.9.4.cwl/maxcoverage""; },; {; ""type"": ""boolean"",; ""doc"": ""do not use JNI native code acceleration libraries (snappy, GKL, ssw, bwa).\n"",; ""inputBinding"": {; ""prefix"": ""--nojni""; },; ""default"": false,; ""id"": ""#gridss-2.9.4.cwl/nojni""; },; {; ""type"": ""string"",; ""doc"": ""output gzipped VCF file\n"",; ""inputBinding"": {; ""prefix"": ""--output""; },; ""id"": ""#gridss-2.9.4.cwl/output""; },; {; ""type"": [; ""null"",; ""string""; ],; ""doc"": ""Optional - additional standard Picard command line options.\nUseful options include VALIDATION_S",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:13573,configurat,configuration,13573,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,1,['configurat'],['configuration']
Deployability,va:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:51); 	at software.amazon.awssdk.core.internal.http.StreamManagingStage.execute(StreamManagingStage.java:33); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.executeWithTimer(ApiCallTimeoutTrackingStage.java:79); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:60); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallTimeoutTrackingStage.execute(ApiCallTimeoutTrackingStage.java:42); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.ama,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:2004,pipeline,pipeline,2004,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,2,['pipeline'],['pipeline']
Deployability,va:66); 	at software.amazon.awssdk.core.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:47); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:56); 	at software.amazon.awssdk.core.http.StreamManagingStage.execute(StreamManagingStage.java:42); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.executeWithTimer(ClientExecutionTimedStage.java:71); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:55); 	at software.amazon.awssdk.core.http.pipeline.stages.ClientExecutionTimedStage.execute(ClientExecutionTimedStage.java:39); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:239); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:35); 	at software.amazon.awssdk.core.http.pipeline.stages.ExecutionFailureExceptionReportingStage.execute(ExecutionFailureExceptionReportingStage.java:24); 	at software.amazon.awssdk.core.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:281); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.doInvoke(SyncClientHandlerImpl.java:149); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.invoke(SyncClientHandlerImpl.java:131); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:100); 	at software.amazon.awssdk.core.client.SyncClientHandlerImpl.execute(SyncClientHandlerImpl.java:76); 	at software.amazon.awssdk.core.client.SdkClientHandler.execute(SdkClientH,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3760:5625,pipeline,pipeline,5625,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3760,3,['pipeline'],['pipeline']
Deployability,"version numbers - please make sure you're using a compatible set of libraries. ; Uncaught error from thread [default-akka.actor.default-dispatcher-5]: akka.actor.ActorCell.addFunctionRef(Lscala/Function2;)Lakka/actor/FunctionRef;, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for for ActorSystem[default]; java.lang.NoSuchMethodError: akka.actor.ActorCell.addFunctionRef(Lscala/Function2;)Lakka/actor/FunctionRef;; ...; ```; I'm essentially seeing exactly the behaviour described in reference [1] below, which is eviction warnings at compile time and then the runtime blow-up. The root cause seems to be that akka-http depends on an older version of akka-actor (2.4.19) than that specified for the project (2.5.3). Running `dependencyTree` task confirms:; ```; [info] +-com.typesafe.akka:akka-http-spray-json_2.12:10.0.9 [S]; [info] | +-com.typesafe.akka:akka-http_2.12:10.0.9 [S]; [info] | | +-com.typesafe.akka:akka-http-core_2.12:10.0.9 [S]; [info] | | +-com.typesafe.akka:akka-parsing_2.12:10.0.9 [S]; [info] | | | +-com.typesafe.akka:akka-actor_2.12:2.4.19 (evicted by: 2.5.3); ```; If I explicitly add dependency on the latest akka-stream as suggested in [2] and [3], the problem goes away:; ```; diff --git a/project/Dependencies.scala b/project/Dependencies.scala; index 0d77e2d3..7254fc61 100644; --- a/project/Dependencies.scala; +++ b/project/Dependencies.scala; @@ -141,6 +141,7 @@ object Dependencies {; ; val cromwellApiClientDependencies = List(; ""com.typesafe.akka"" %% ""akka-actor"" % akkaV,; + ""com.typesafe.akka"" %% ""akka-stream"" % akkaV,; ""com.typesafe.akka"" %% ""akka-http-spray-json"" % akkaHttpV,; ""com.github.pathikrit"" %% ""better-files"" % betterFilesV,; ""org.scalatest"" %% ""scalatest"" % scalatestV % Test,; ```. References:; [1] https://github.com/akka/akka-http/issues/1101#issuecomment-299864185; [2] https://github.com/akka/akka-http/issues/1101#issuecomment-299923102; [3] http://akka.io/blog/news/2017/05/03/akka-http-10.0.6-released#compatibility-notes",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2579:2599,release,released,2599,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2579,1,['release'],['released']
Deployability,"w you to submit; * Terra will accept the escaped version `^chrEBV$|^NC|_random$|Un_|^HLA\\-|_alt$|hap\\d$` as an input if entered manually or hardcoded, and will interpret it as `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$`. Only tested via Terra-Cromwell, as I was previously told local-Cromwell is a lower development priority. ## expected behavior; 1. A user inputting a string as a variable vs that exact same string being a hardcoded default should be handled the same way.; 2. If Cromwell is supposed to handle `/` by requiring they be escaped as `//`, that should be documented if it isn't already.; 3. womtool should throw a warning when it sees a hardcoded variable/default with a `/` inside of it, and that warning should guide the user as to how it will be interpreted at runtime.; 4. The same workflow running in Cromwell and miniwdl should not result in different interpretations of a given string. The WDL spec should be updated to stop inconsistencies like this from happening, [since it currently does not seem to give guidance on this particular issue](https://github.com/openwdl/wdl/blob/main/versions/1.0/SPEC.md#whitespace-strings-identifiers-constants). (Linking WDL 1.0 spec since that's what Cromwell currently supports.). Personally I'd prefer if strings were interpreted literally, ie `/` does not need to be escaped, but that might conflict with the typical JSON standard. Consistency is more important than my mild distaste for escaping. ## related issues; https://github.com/broadinstitute/cromwell/issues/3990#issuecomment-415665749. ## miniwdl comparison (not necessarily the ideal, just to illustrate point 4 on expected behavior); * miniwdl will accept `^chrEBV$|^NC|_random$|Un_|^HLA\-|_alt$|hap\d$` as a variable default or as hardcoded variable, and will handle it literally as written, as long as the workflow author followed shellcheck's recommendation and used quotes in the command section (ie `--excludePatt ""~{excludePattern}""`); * miniwdl will also accept ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7167:2217,update,updated,2217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7167,1,['update'],['updated']
Deployability,"w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/t.log; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry 2> \/dev\/null || true; sleep 30; done\"""",; ""endTime"": ""2018-08-14T16:14:33.113135Z""; },; {; ""startTime"": ""2018-08-14T16:17:00.937007Z"",; ""description"": ""Started running \""\/bin\/sh -c retry() { for i in `seq 3`; do gsutil -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" -m rsync -r \/google\/logs gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/pipelines-logs 2> gsutil_output.txt; RC_GSUTIL=$?; if [[ \\\""$RC_GSUTIL\\\"" -eq 1 && grep -q \\\""Bucket is requester pays bucket but no user project provided.\\\"" gsutil_output.txt ]]; then\\n echo \\\""Retrying with user project dos-testing\\\"" && gsutil -u dos-testing -h \\\""Content-Type: text\/plain; charset=UTF-8\\\"" -m rsync -r \/google\/logs gs:\/\/fc-f5576422-7954-4da1-8005-30c2df8d37d5\/984b5570-abe7-470f-b5cc-9243bf98518c\/w\/f8a1e7ee-3286-4071-a1d6-e68667b732de\/call-t\/pipelines-logs; fi ; RC=$?; if [[ \\\""$RC\\\"" -eq 0 ]]; then break; fi; sleep 5; done; return \\\""$RC\\\""; }; retry\"""",; ""endTime"": ""2018-08-14T16:17:05.013311Z""; },; {; ""startTime"": ""2018-08-14T16:16:45.309551Z"",; ""description"": ""Started pulling \""stedolan\/jq@sha256:a61ed0bca213081b64be94c5e1b402ea58bc549f457c2682a86704dd55231e09\"""",; ""endTime"": ""2018-08-14T16:16:56.518719Z""; },; {; ""startTime"": ""2018-08-14T16:13:25.620872Z"",; ""description"": ""Worker \""google-pipelines-worker-4247201c1820698c5c935fb230c4a278\"" assigned in \""us-central1-f\"""",; ""endTime"": ""2018-08-14T16:13:55.499871Z""; },; {; ""startTime"": ""2018-08-14T16:14:27.981711Z"",; ""description"": ""Stopped pulling \""broadinstitute\/cromwell-dos:34-d8acfe3\"""",; ""endTime"": ""2018-08-14T16:14:28.018319Z""; },; {; ""startTime"": ""2018-08-14T16:14:34.534911Z"",; ""description"": ""Started running \""\/bin\/bash -c mkdir -p \/cromwell_root && chmod -R a+rwx \/cromwell_roo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4162:14451,pipeline,pipelines-logs,14451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4162,1,['pipeline'],['pipelines-logs']
Deployability,"ward with the cache hit.; duplication-strategy = ""copy""; }; }; }. default-runtime-attributes {; cpu: 4; failOnStderr: false; continueOnReturnCode: 0; memory: ""2 GB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""us-central1-a"", ""us-central1-b""]; }. include ""papi_v2_reference_image_manifest.conf""; }; }; }; }; ```. WDL:. ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. input. ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. Gcloud log (edited):. ```; done: true; metadata:; '@type': type.googleapis.com/google.cloud.lifesciences.v2beta.Metadata; createTime: '2021-08-03T15:21:55.984657Z'; endTime: '2021-08-03T15:24:03.533702405Z'; events:; - description: Worker released; timestamp: '2021-08-03T15:24:03.533702405Z'; workerReleased:; instance: google-pipelines-worker-xxxxxx; zone: us-central1-b; - containerStopped:; actionId: 19; description: Stopped running ""-c python -c 'import base64; print(base64.b64decode(\""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh""; timestamp: '2021-08-03T15:24:02.823519462Z'; - containerStarted:; actionId: 19; description: Started running ""-c python -c 'import base64; print(base64.b64decode(\""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh""; timestamp: '2021-08-03T15:23:57.785552960Z'; - containerStopped:; actionId: 18; description: Stopped running ""-c python -c 'import base64; print(base64.b64decode(\""xxxxxx""));'; > /tmp/1xxxxxx.sh && chmod u+x /tmp/1xxxxxx.sh; && sh /tmp/1xxxxxx.sh""; timestamp: '2021-08-03T15:23:57.673915859Z'; - containerStarted:; actionId: 18; description: Started running ""-c python -c 'import base64; print(base64.b64decode(\""xxxx",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:6681,release,released,6681,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['release'],['released']
Deployability,wdl4s and lenthall updated. Fixed test failures and re-singletoned the factory hashing pools.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245914423:19,update,updated,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1379#issuecomment-245914423,1,['update'],['updated']
Deployability,"well-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunct",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861:1235,pipeline,pipelines,1235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861,1,['pipeline'],['pipelines']
Deployability,"well-system-akka.dispatchers.backend-dispatcher-101 INFO - JesAsyncBackendJobExecutionActor [UUID(3d01da76)test.hello:NA:1]: JesAsyncBackendJobExecutionActor [UUID(3d01da76):test.hello:NA:1] Status change from Running to Success; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:38,977 cromwell-system-akka.dispatchers.engine-dispatcher-57 INFO - WorkflowExecutionActor-3d01da76-98f9-4751-a3c0-efc61ef67030 [UUID(3d01da76)]: Workflow test complete. Final Outputs:; Feb 13 11:51:38 gce-cromwell-alpha102 docker/cromwell-app[837]: ""test.hello.response"": ""gs://fc-cd1f5468-d0f9-4416-8cdc-9464482022dd/8ee1f938-a92c-48df-a4cc-7a0683413547/test/3d01da76-98f9-4751-a3c0-efc61ef67030/call-hello/hello-stdout.log""; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.engine-dispatcher-67 INFO - WorkflowManagerActor WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030 is in a terminal state: WorkflowSucceededState; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,178 cromwell-system-akka.dispatchers.io-dispatcher-10 INFO - $f [UUID(3d01da76)]: Copying workflow logs from /cromwell-workflow-logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log to /8ee1f938-a92c-48df-a4cc-7a0683413547/workflow.logs/workflow.3d01da76-98f9-4751-a3c0-efc61ef67030.log; Feb 13 11:51:39 gce-cromwell-alpha102 docker/cromwell-app[837]: 2017-02-13 16:51:39,184 cromwell-system-akka.actor.default-dispatcher-154 INFO - Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/cromwell-service/SubWorkflowStoreActor#1592013866] to Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-3d01da76-98f9-4751-a3c0-efc61ef67030#1939109793] was not delivered. [6] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'ak; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986:3890,configurat,configuration,3890,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279542986,1,['configurat'],['configuration']
Deployability,what about the stats endpoint? . https://github.com/broadinstitute/cromwell#get-apiworkflowsversionstats. Note: If you're not on the release you probably don't have this endpoint,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249220630:133,release,release,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1480#issuecomment-249220630,1,['release'],['release']
Deployability,when Cromwell updated to version 53 Womtool relaxed validation schemes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6212:14,update,updated,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6212,1,['update'],['updated']
Deployability,"which seems inappropriate. The following should have been used instead:; ```; sbatch \; -o ${out}.sbatch \; -e ${err}.sbatch \; ```; Similarly to how it is advised for [SGE](https://cromwell.readthedocs.io/en/stable/backends/SGE/) where `${out}.qsub` and `${err}.qsub` are used in place of `${out}` and `${err}`. The current workaround suggested by @honestAnt is instead to use in the Cromwell configuration file something like this:; ```; submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; mv ${cwd}/execution/rc ${cwd}/execution/rc.tmp; sleep 60; mv ${cwd}/execution/rc.tmp ${cwd}/execution/rc; """"""; ```; A better alternative would be to use in the Cromwell configuration file something like this (as suggested [here](https://github.com/broadinstitute/cromwell/blob/8a1297fb0e44a11421eed98c5885188972337ce9/src/ci/resources/local_provider_config.inc.conf)):; ```; script-epilogue = ""sleep 60 && sync"". submit-docker = """"""; ...; sbatch \; --wait \; -J=${job_name} \; -D ${cwd} \; -o ${out}.sbatch \; -e ${err}.sbatch \; -t ${runtime_minutes} \; -c ${cpu} \; --mem=${memory_mb} \; --wrap ""singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${docker_script}""; """"""; ```; But both options are way less than ideal and the choice of `60` might not be sufficient for all NFS configurations. If anybody wants to try to trigger this issue on an NFS shared filesystem setup, the following WDL should do the trick:; ```; version 1.0. workflow main {; scatter (idx in range(256)) {; call main {; input:; i = idx; }; }; output { Array[Int] n = main.n }; }. task main {; input {; Int i; }. command <<<; set -euo pipefail; echo ~{i*i}; >>>. output {; Int n = read_int(stdout()); }. runtime {; docker: ""debian:stable-slim""; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956:5408,configurat,configurations,5408,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1622071956,1,['configurat'],['configurations']
Deployability,why not fix this more narrowly so the update of input expressions isn't broken for non-shards?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/363#issuecomment-170081954:38,update,update,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/363#issuecomment-170081954,1,['update'],['update']
Deployability,"wo, but I have included `project` in both ...) in the configuration file as follows:; ```; include required(classpath(""application"")). google {; application-name = ""cromwell""; auths = [; {; name = ""application-default""; scheme = ""application_default""; }; ]; }. engine {; filesystems {; gcs {; auth = ""application-default""; project = ""xxx""; }; }; }. backend {; default = ""JES""; providers {; JES {; actor-factory = ""cromwell.backend.impl.jes.JesBackendLifecycleActorFactory""; config {; // Google project; project = ""xxx"". // Base bucket for workflow executions; root = ""gs://xxx/cromwell-execution"". // Polling for completion backs-off gradually for slower-running jobs.; // This is the maximum polling interval (in seconds):; maximum-polling-interval = 600. // Optional Dockerhub Credentials. Can be used to access private docker images.; dockerhub {; // account = """"; // token = """"; }. genomics {; // A reference to an auth defined in the `google` stanza at the top. This auth is used to create; // Pipelines and manipulate auth JSONs.; auth = ""application-default""; // Endpoint for APIs, no reason to change this unless directed by Google.; endpoint-url = ""https://genomics.googleapis.com/""; // This allows you to use an alternative service account to launch jobs, by default uses default service account; compute-service-account = ""default"". // Pipelines v2 only: specify the number of times localization and delocalization operations should be attempted; // There is no logic to determine if the error was transient or not, everything is retried upon failure; // Defaults to 3; localization-attempts = 3; }. filesystems {; gcs {; // A reference to a potentially different auth for manipulating files via engine functions.; auth = ""application-default""; project = ""xxx""; }; }; }; }; }; }; ```. I then run with the command:; ```; java -Dconfig.file=google.conf -jar cromwell-52.jar run hello.wdl -i hello.inputs; ```; And I get the error:; ```; [2020-07-28 16:01:35,86] [info] WorkflowManagerActor Wo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471:2328,Pipeline,Pipelines,2328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665256471,1,['Pipeline'],['Pipelines']
Deployability,"wom/src/main/scala/wom/views/GraphPrint.scala:114:57: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${digraph.nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:157:49: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^; [error] /home/cromwell/cromwell/cromwell/wom/src/main/scala/wom/views/GraphPrint.scala:166:48: type mismatch;; [error] found : java.util.stream.Stream[String]; [error] required: scala.collection.GenTraversableOnce[?]; [error] | ${nodes.toList.flatMap(_.dotString.lines).mkString(System.lineSeparator() + "" "")}; [error] ^. I've tried it with the following javas, but no difference:. sdk install java 11.0.15-tem ; sdk install java 11.0.15-tem ; sdk install java 11.0.14.1-tem; sdk install java 11.0.14-tem. I've switched to cromwell version 78 and managed to 'sbt assembly' w/o errors. While executing jointGenotyping.wdl I've run into the following error that I'm unable to debug:. 2022-05-09 13:21:41,743 ERROR - DispatchedConfigAsyncJobExecutionActor [UUID(d5a90666)JointGenotyping.CheckSamplesUnique:NA:1]: Error attempting to Execute; java.lang.IllegalArgumentException: null; 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:328); 	 at java.base/sun.nio.fs.UnixPath.subpath(UnixPath.java:43); 	 at cromwell.core.path.NioPathMethods.subpath(NioPathMethods.scala:18); 	 at cromwell.core.path.NioPathMethods.subpath$(NioPathMethods.scala:18); 	 at cromwell.core.path.DefaultPath.subpath(DefaultPathBuilder.scala:55); 	 at cromwell.backend.io.JobPathsWithDocker.toDockerPath(JobPathsWithDocker.scala:56); 	 at cromwell.backend.sfs.SharedFileSystemAsyncJobExecutionActor.$anonfun$mapC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6757:1546,install,install,1546,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6757,1,['install'],['install']
Deployability,"would you guys update ""cromwell/cromwell.example.backends/AWS.conf"" . it seams this file for old version .",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5857:15,update,update,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5857,1,['update'],['update']
Deployability,x.run(Mailbox.scala:225); 11:09:46 cromwell-test_1 | 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 11:09:46 cromwell-test_1 | 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 11:09:46 cromwell-test_1 | Caused by: liquibase.exception.LockException: java.lang.NullPointerException; 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.acquireLock(StandardLockService.java:242); 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:170); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:196); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:192); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:58); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:31); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:96); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at s,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766:3824,update,update,3824,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766,1,['update'],['update']
Deployability,"xample I prepared:. ```wdl; version 1.0. struct Test {; String name; File path; }. struct Collection {; Array[Test] samples; }. task GenerateComplexObject {; input {; Int items; }. command <<<; python <<CODE; import sys; import os; import json; items = []; for item in range(0, ~{items}):; name = f""test{item}.txt""; os.system(f""echo 'some content' > {name}""); items.append({""name"": f""item-{item}"", ""path"": name}). with open(""results.json"", ""w"") as fh:; json.dump({'samples': items}, fh); CODE; >>>. runtime {; docker: ""python:3.8""; memory: ""1 GB""; cpu: 1; preemptible: 3; disks: ""local-disk "" + 10 + "" HDD""; }. output {; Collection results = read_json(""results.json""); }; }. workflow TestStruct {; input {; Int items; }. call GenerateComplexObject {; input:; items=items; }. output {; Collection out = GenerateComplexObject.results; }; }; ```. When using local backend I have no problem, but when using PAPIv2 (`cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory`) the files from `Test` struct (path) do not delocalize. ```bash; gsutil ls gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/GenerateComplexObject.log; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/gcs_delocalization.sh; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/gcs_localization.sh; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/gcs_transfer.sh; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/rc; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/results.json; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/script; gs://********/TestaStruct/47bc869c-041f-443d-b0bd-d45a1dd203ff/call-GenerateComplexObject/stderr; g",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5592:1151,Pipeline,PipelinesApiLifecycleActorFactory,1151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5592,1,['Pipeline'],['PipelinesApiLifecycleActorFactory']
Deployability,"xception: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 6312adeb-b603-48ff-8a3b-fd099e6805ef); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033:1254,pipeline,pipeline,1254,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-572126033,1,['pipeline'],['pipeline']
Deployability,"xception: The security token included in the request is expired (Service: Batch, Status Code: 403, Request ID: 842776aa-1862-43dc-a286-95d0b902319e); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleErrorResponse(HandleResponseStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.handleResponse(HandleResponseStage.java:73); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:58); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.HandleResponseStage.execute(HandleResponseStage.java:41); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:63); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.ApiCallAttemptTimeoutTrackingStage.execute(ApiCallAttemptTimeoutTrackingStage.java:36); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:77); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.TimeoutExceptionHandlingStage.execute(TimeoutExceptionHandlingStage.java:39); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.doExecute(RetryableStage.java:115); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage$RetryExecutor.execute(RetryableStage.java:88); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:64); 	at software.amazon.awssdk.core.internal.http.pipeline.stages.RetryableStage.execute(RetryableStage.java:44); 	at software.amazon.awssdk.core.internal.http.pipeline.RequestPipelineBuilder$ComposingRequestPipelineStage.execute(RequestPipelineBuilder.java:205); ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119:1213,pipeline,pipeline,1213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5162#issuecomment-540113119,1,['pipeline'],['pipeline']
Deployability,"xecuted `sbt assembly` to create the `womtool.jar` following [the document](https://cromwell.readthedocs.io/en/develop/WOMtool/). Below is the log. The full log is [here](https://gist.github.com/junaruga/2264c715606deee88b40de0de4e7a1b0) on the latest develop branch <54fed3e172e2138cd956c0b9663c05a8a5d34dbc>. ```; $ sbt assembly; ...; [error] /home/jaruga/git/broadinstitute/cromwell/cloud-nio/cloud-nio-spi/src/main/scala/cloud/nio/spi/UnixPath.scala:72:7: `override` modifier required to override concrete member:; [error] <defaultmethod> def isEmpty(): Boolean (defined in trait CharSequence; [error] def isEmpty: Boolean = path.isEmpty; [error] ^; [error] one error found; ...; [error] /home/jaruga/git/broadinstitute/cromwell/centaur/src/main/scala/centaur/api/DaemonizedDefaultThreadFactory.scala:17:26: method getSecurityManager in class System is deprecated; [error] private val s = System.getSecurityManager; [error] ^; [error] one error found; ...; ```. ## My environment. <!-- Which backend are you running? -->. * Fedora Linux 36. ```; $ java --version ; openjdk 17.0.4 2022-07-19; OpenJDK Runtime Environment (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8); OpenJDK 64-Bit Server VM (Red_Hat-17.0.4.0.8-1.fc36) (build 17.0.4+8, mixed mode, sharing). $ scala --version; Scala code runner version 2.13.8 -- Copyright 2002-2021, LAMP/EPFL and Lightbend, Inc. $ sbt --version; WARNING: A terminally deprecated method in java.lang.System has been called; WARNING: System::setSecurityManager has been called by sbt.TrapExit$ (file:/home/jaruga/.sbt/boot/scala-2.12.14/org.scala-sbt/sbt/1.5.5/run_2.12-1.5.5.jar); WARNING: Please consider reporting this to the maintainers of sbt.TrapExit$; WARNING: System::setSecurityManager will be removed in a future release. sbt version in this project: 	1.5.5; sbt script version: 1.7.1; ```. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6902:2234,release,release,2234,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6902,2,"['configurat', 'release']","['configuration', 'release']"
Deployability,"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\""]: exit status 1 (standard error: \""error pulling image configuration: error parsing HTTP 400 response body: invalid character '<' looking for beginning of value: \\\""<?xml version='1.0' encoding='UTF-8'?><Error><Code>UserProjectMissing</Code><Message>Bucket is a requester pays bucket but no user project provided.</Message><Details>Bucket is Requester Pays bucket but no billing project id provided for non-owner.</Details></Error>\\\""\\n\"")"",`. I understand that the issue is that the Google bucket where the docker is located is requester pays and Cromwell does not know what to do in this case, but it is not immediately clear what I should do to fix it. It would be a great improvement if Cromwell could interpret this response and provide a more informative error message so that the user could immediately know what needs to be addressed. In particular, I am not fully sure what I should be doing. These are excerpts from my configuration file:; ```; ...; engine {; filesystems {; gcs {; auth = ""service-account""; project = ""xxx""; }; }; }; ...; services {; MetadataService {; ...; config {; carbonite-metadata-service {; filesystems {; gcs {; auth = ""service-account""; }; }; ...; }; }; }; }; ...; backend {; default = PAPIv2. providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; project = ""xxx""; ...; filesystems {; gcs {; auth = ""service-account""; project = ""xxx""; ...; }; }; ...; }; }; }; }; ...; ```; Where should the configuration for telling Cromwell which project to use when pulling dockers be?. I also do not understand why this issue arises at all as the Google bucket with the dockers is a us multi-region bucket and the computation is in us-central1, so there should be no egress costs when pulling the docker and therefore no need for a billing project. Clearly I am not understanding this problem entirely. I would be grateful for a clarification. Thank you!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235:1666,pipeline,pipelines,1666,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235,3,"['Pipeline', 'configurat', 'pipeline']","['PipelinesApiLifecycleActorFactory', 'configuration', 'pipelines']"
Deployability,x}@media only screen and (min-resolution:192dpi){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat 0% 0%/100% 100%;-moz-border-image:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) 0}}@media only screen and (-webkit-min-device-pixel-ratio:2){#logo{background:url(//www.google.com/images/branding/googlelogo/2x/googlelogo_color_150x54dp.png) no-repeat;-webkit-background-size:100% 100%}}#logo{display:inline-block;height:54px;width:150px}; </style>; <a href=//www.google.com/><span id=logo aria-label=Google></span></a>; <p><b>502.</b> <ins>Thatâ€™s an error.</ins>; <p>The server encountered a temporary error and could not complete your request.<p>Please try again in 30 seconds. <ins>Thatâ€™s all we know.</ins>. 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1137); 	at com.google.api.client.googleapis.batch.BatchRequest.execute(BatchRequest.java:233); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.runBatch(PipelinesApiRequestWorker.scala:59); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.cromwell$backend$google$pipelines$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); 	at akka.actor.Actor.aroundReceive(Actor.scala:517); 	at akka.actor.Actor.aroundReceive$(Actor.scala:515); 	at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); 	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); 	at akka.actor.ActorCell.invoke(ActorCell.scala:557); 	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); 	at akka.dispatch.Mailbox.run(Mailbox.scala:225); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:235); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.ja,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4917:3625,pipeline,pipelines,3625,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4917,1,['pipeline'],['pipelines']
Deployability,"y be implemented for ScalaTest, any tests using ScalaCheck directly should be refactored to use ScalaTest's ""ScalaCheck-style"" property based testing. That way any failing property based tests will be tracked as well using our reporting. Because this feature is likely to be used across all cromwell artifacts/subprojects we should decide if we either want to either:; 1. Update every project in `build.sbt` with a `.dependsOn(common, ""test->test"")`; 2. Add scalatest and sentry as `Provided` dependencies to `common` such that they won't be transitively included by default; 3. Create a new `cromwell.test` artifact and use either of the above outside of `cromwell.common`. **A/C:**; - Switch tests directly using scalacheck over to scalatest's scalacheck-style specs; - Create a custom scalatest helper/reporter that retries a failed test a configurable number of times; - Add custom reporter to scalatest settings in `Testing.scala`; - Assuming using sentry for error reporting from Travis:; - Add sentry DSN configuration values to Vault; - Update `build_application.inc.conf` to use a noop sentry DSN by default; - Create a `sentry_application.inc.conf.ctmpl` file that uses sentry configuration values from Vault; - `build_application.inc.conf` attempts to import a `sentry_application.inc.conf` file that overrides the sentry configuration; - NOTE: When `build_application.inc.conf` is missing it will be skipped by the HOCON library. **Links:**; - https://github.com/broadinstitute/cromwell/issues/3657; - http://www.scalatest.org/user_guide/using_the_runner#specifyingReporters; - http://www.scalatest.org/user_guide/writing_scalacheck_style_properties; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/concurrent/Eventually.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Retries.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/Reporter.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalatest/events/TestFailed.html; - http://doc.scalatest.org/3.0.1-2.12/org/scalat",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3658:3173,configurat,configuration,3173,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3658,2,"['Update', 'configurat']","['Update', 'configuration']"
Deployability,"yYy9tYWluL3NjYWxhL2N3bC9Xb3JrZmxvdy5zY2FsYQ==) | `95.52% <100%> (+7.46%)` | :arrow_up: |; | [...ain/scala/wdl/transforms/base/wdlom2wom/Util.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvbmV3LWJhc2Uvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmFzZS93ZGxvbTJ3b20vVXRpbC5zY2FsYQ==) | `100% <100%> (Ã¸)` | :arrow_up: |; | [...dlom2wom/WdlDraft2WomWorkflowDefinitionMaker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvZHJhZnQyL3NyYy9tYWluL3NjYWxhL3dkbC90cmFuc2Zvcm1zL2RyYWZ0Mi93ZGxvbTJ3b20vV2RsRHJhZnQyV29tV29ya2Zsb3dEZWZpbml0aW9uTWFrZXIuc2NhbGE=) | `87.5% <75%> (-12.5%)` | :arrow_down: |; | [...m2wom/WdlDraft2WomCommandTaskDefinitionMaker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvZHJhZnQyL3NyYy9tYWluL3NjYWxhL3dkbC90cmFuc2Zvcm1zL2RyYWZ0Mi93ZGxvbTJ3b20vV2RsRHJhZnQyV29tQ29tbWFuZFRhc2tEZWZpbml0aW9uTWFrZXIuc2NhbGE=) | `95.23% <75%> (-4.77%)` | :arrow_down: |; | [.../scala/cromiam/webservice/EngineRouteSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-Q3JvbUlBTS9zcmMvbWFpbi9zY2FsYS9jcm9taWFtL3dlYnNlcnZpY2UvRW5naW5lUm91dGVTdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | ... and [626 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=footer). Last update [da601c8...ae566b9](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509805758:4346,update,update,4346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509805758,1,['update'],['update']
Deployability,"ye sorry about that, did a rollback and saved the other changes to this PR: https://github.com/broadinstitute/cromwell/pull/4205",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-427017126:27,rollback,rollback,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-427017126,1,['rollback'],['rollback']
Deployability,yeah I hesitated to argue because I know these are very stable and basically will never be used differently. I just kinda like the configuration all in one place personally. The one place also makes the scanning effort easier.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2499#issuecomment-318475624:131,configurat,configuration,131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2499#issuecomment-318475624,1,['configurat'],['configuration']
Deployability,"yep, it's required to run the fastq->bam pipeline. ---. Kristian Cibulskis; Chief Architect, Data Sciences & Data Engineering; Broad Institute of MIT and Harvard; kcibul@broadinstitute.org; 617-714-7637. On Mon, Aug 24, 2015 at 9:31 AM, Jeff Gentry notifications@github.com; wrote:. > @kcibul https://github.com/kcibul we'll need this on staging for demo?; > ; > â€”; > Reply to this email directly or view it on GitHub; > https://github.com/broadinstitute/cromwell/pull/153#issuecomment-134201403; > .",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/153#issuecomment-134204617:41,pipeline,pipeline,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/153#issuecomment-134204617,1,['pipeline'],['pipeline']
Deployability,yes this is in the 0.22 release,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1271#issuecomment-253887769:24,release,release,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1271#issuecomment-253887769,1,['release'],['release']
Deployability,"you recently released version ""28"", then found a bug, and changed the JAR download, but still called it ""28"". This has caused problems for brew, conda etc who package releases and use SHA256 to verify. It would have been better if you made it a ""29"" release, and delete the ""28"" release. Thank you for considering this opinion.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2480:13,release,released,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2480,4,['release'],"['release', 'released', 'releases']"
Deployability,"ystem-akka.dispatchers.backend-dispatcher-34 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(b2e34f33)Arrays.AutoCall:NA:1]: job id: projects/broad-dsde-cromwell-dev/operations/4612525402041750773; ...; 2018-07-04 07:20:37,086 cromwell-system-akka.dispatchers.engine-dispatcher-29 ERROR - WorkflowManagerActor Workflow b2e34f33-e643-437f-aa38-b62f6d44f2dc failed (during ExecutingWorkflowState): java.lang.Exception: Task Arrays.AutoCall:NA:1 failed. The job was stopped before the command finished. PAPI error code 2. Execution failed: pulling image: docker pull: running [""docker"" ""pull"" ""us.gcr.io/broad-gotc-dev/autocall:dev-3.0.0-1527695536""]: exit status 1 (standard error: ""Error response from daemon: repository us.gcr.io/broad-gotc-dev/autocall not found: does not exist or no pull access\n""); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:551); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:558); at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:83); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1072); at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$handleExecutionResult$5.applyOrElse(StandardAsyncExecutionActor.scala:1068); at scala.concurrent.Future.$anonfun$recoverWith$1(Future.scala:413); at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatc",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3861:1071,pipeline,pipelines,1071,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3861,1,['pipeline'],['pipelines']
Deployability,"zing a Blob file from Terra Prod, on a patched Cromwell in Terra Dev. This PR with foreign Blob URL:. ```; 2023-12-20 18:12:17.197 Tes.Runner.Transfer.BlobOperationPipeline[0] Completed download. Total bytes: 7,811,366,912 Filename: /mnt/batch/tasks/workitems/TES-ybjxkg-D5_v2-4yab26tn3af2kf6dfa755sbg5oeqevqw-6cylhedz/job-1/a7123170_f41bbba17a6f4409940127a60234695d-1/wd/wd/cromwell-executions/localizer_workflow/a7123170-1652-45b8-a8ba-c7bef84acac4/call-localizer_task/inputs/lz304a1e79fd7359e5327eda.blob.core.windows.net/sc-705b830a-d699-478e-9da6-49661b326e77/inputs/Rocky-9.2-aarch64-dvd.iso; 2023-12-20 18:12:17.200 Tes.Runner.Transfer.ProcessedPartsProcessor[0] All parts were successfully processed.; 2023-12-20 18:12:17.200 Tes.Runner.Transfer.PartsReader[0] All part read operations completed successfully.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.PartsWriter[0] All part write operations completed successfully.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Pipeline processing completed.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Waiting for processed part processor to complete.; 2023-12-20 18:12:17.201 Tes.Runner.Transfer.BlobOperationPipeline[0] Processed parts completed.; 2023-12-20 18:12:17.204 Tes.Runner.Executor[0] Executed Download. Time elapsed: 00:00:13.0435715 Bandwidth: 571.12 MiB/s; 2023-12-20 18:12:17.208 Tes.RunnerCLI.Commands.CommandHandlers[0] Total bytes transferred: 7,811,369,114; /cromwell-executions/localizer_workflow/a7123170-1652-45b8-a8ba-c7bef84acac4/call-localizer_task/execution; ```. This PR with a regular HTTPS URL from the 'net:; ```; 2023-12-20 18:42:08.430 Tes.Runner.Transfer.BlobOperationPipeline[0] Completed download. Total bytes: 1,553,924,096 Filename: /mnt/batch/tasks/workitems/TES-ybjxkg-D5_v2-4yab26tn3af2kf6dfa755sbg5oeqevqw-6cylhedz/job-1/f9b357bc_8d135cf26c4345599dbd046d5892d274-1/wd/wd/cromwell-executions/localizer_workflow/f9b357bc-4a13-4923-9b90-0f707ae9f435/call-localizer_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7347:1014,Pipeline,Pipeline,1014,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7347,1,['Pipeline'],['Pipeline']
Deployability,"} ${singularity_image} /bin/bash ${script}"". filesystems {; local {; localization: [; ""soft-link"", ""hard-link"", ""copy""; ]; } ## end local; } ## end file systems; } ## end config; } ## End Local`. Oddly, when running the workflow I get a submit docker error. ie. as per below. I have no idea why it's looking for docker as I'm not knowingly using it. I'm not using docker in my run time parameters. I have been able to get standalone working on another workflow by passing a singularity container to each task command output but I was wondering if there was a more elegant solution I could use such as just changing to a pre-made provider. I have searched Google and through here but not found anything. I did find one issue here but they seemed to want to use docker where as I don't. . Thanks for the help!. `task submit {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {; singularity run -B ${head_directory}:${head_directory} ${singularity_image} /bin/bash ${script}; }; }. task submit_docker {. String job_id; String job_name; String cwd; String out; String err; String script; String job_shell. String docker_cwd; String docker_cid; String docker_script; String docker_out; String docker_err. String head_directory = ""/data/MGP""; String singularity_image = ""/data/MGP/sing/metaGenPipe.simg"". command {. # make sure there is no preexisting Docker CID file; rm -f ${docker_cid}; # run as in the original configuration without --rm flag (will remove later); docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd}:delegated \; ${docker} ${docker_script}. # get the return code (working even if the container was detached); rc=$(docker wait cat ${docker_cid}). # remove the container after waiting; docker rm cat ${docker_cid}. # return exit code; exit $rc. }; }`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5862:2252,configurat,configuration,2252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5862,1,['configurat'],['configuration']
Deployability,"} \; 		${""-L "" + panel} \; 		-O ${outputdir}/${sample_id}_raw.vcf.gz; }; output {; 	String unfiltered_vcf = ""${outputdir}/${sample_id}_raw.vcf.gz""; }; }; task GetPileupSummaries{. 	String gatk; String bam_file; 	String ref_fasta; 	String variants_for_contamination ; 	String sample_id ; 	String outputdir ; 	String variants_for_intervals. 	command {; 	 set -e . 	 ${gatk} GetPileupSummaries -R ${ref_fasta} \; 	 -I ${bam_file} -V ${variants_for_contamination} \; 	 -L ${variants_for_intervals} \; 	 -O ${outputdir}/${sample_id}.pileups.table. 	}; 	output {; 		String pileups_table = ""${outputdir}/${sample_id}.pileups.table""; 	}; }; task CalculateContamination {. 		String tumor_pileups; 		String? normal_pileups; 		String gatk; 		String outputdir; 		String sample_id. 	command {; 		set -e ; 		${gatk} CalculateContamination -I ${tumor_pileups} \; 		-O ${outputdir}/${sample_id}.contamination.table \; 		--tumor-segmentation ${outputdir}/${sample_id}.segments.table \; 		${""-matched "" + normal_pileups}; 	}; 	output {; 		String contamination_table = ""${outputdir}/${sample_id}.contamination.table""; 		String maf_segments = ""${outputdir}/${sample_id}.segments.table""; 	}; }; task Filter {. 		String ref_fasta; 		String unfiltered_vcf; 		String sample_id; 		String outputdir; 		String gatk ; 		String variants_for_intervals; 		String contamination_table; 		String maf_segments. 	command {; 		set -e ; 		${gatk} FilterMutectCalls -V ${unfiltered_vcf} \; 		-R ${ref_fasta} -O ${outputdir}/${sample_id}.clean.vcf.gz \; 		--contamination-table ${contamination_table} \; 		-L ${variants_for_intervals} \; 		--tumor-segmentation ${maf_segments} \; 		--filtering-stats ${outputdir}/${sample_id}.filtering.stats; 	}; 	output {; 		String filtered_vcf = ""${outputdir}/${sample_id}.clean.vcf.gz""; 		String filtering_stats = ""${outputdir}/${sample_id}.filtering.stats""; 	}; }; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: --; >; The LocalExample.conf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5859:5007,configurat,configuration,5007,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5859,1,['configurat'],['configuration']
Deployability,"},; ""inputs"": {},; ""backendLabels"": {; ""wdl-task-name"": ""fail-oom"",; ""cromwell-workflow-id"": ""cromwell-87492280-9828-4afa-b53e-bec675103c42""; },; ""labels"": {; ""wdl-task-name"": ""fail_oom"",; ""cromwell-workflow-id"": ""cromwell-87492280-9828-4afa-b53e-bec675103c42""; },; ""failures"": [; {; ""causedBy"": [],; ""message"": ""The compute backend terminated the job. If this termination is unexpected, examine likely causes such as preemption, running out of disk or memory on the compute instance, or exceeding the backend's maximum job duration.""; }; ],; ""jobId"": ""projects/99884963860/locations/us-central1/operations/1374639517116411519"",; ""monitoringLog"": ""gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/monitoring.log"",; ""backend"": ""gcp"",; ""end"": ""2020-08-29T00:04:05.346Z"",; ""stderr"": ""gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/stderr"",; ""callRoot"": ""gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom"",; ""attempt"": 1,; ""executionEvents"": [; {; ""description"": ""CallCacheReading"",; ""startTime"": ""2020-08-29T00:00:44.174Z"",; ""endTime"": ""2020-08-29T00:00:44.237Z""; },; {; ""startTime"": ""2020-08-29T00:00:42.044Z"",; ""description"": ""Pending"",; ""endTime"": ""2020-08-29T00:00:42.064Z""; },; {; ""description"": ""RunningJob"",; ""startTime"": ""2020-08-29T00:00:44.237Z"",; ""endTime"": ""2020-08-29T00:04:05.347Z""; },; {; ""startTime"": ""2020-08-29T00:00:42.531Z"",; ""endTime"": ""2020-08-29T00:00:44.174Z"",; ""description"": ""PreparingJob""; },; {; ""startTime"": ""2020-08-29T00:00:42.064Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2020-08-29T00:00:42.516Z""; },; {; ""endTime"": ""2020-08-29T00:00:42.531Z"",; ""description"": ""WaitingForValueStore"",; ""startTime"": ""2020-08-29T00:00:42.516Z""; }; ],; ""backendLogs"": {; ""log"": ""gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/fail_oom.log""; },; """,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815:5538,pipeline,pipeline-test-runs,5538,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815,2,['pipeline'],['pipeline-test-runs']
Deployability,"}; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. Contents of hello.inputs:; ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```; Contents of cromwell.BROADexamples.v4.conf:; ```; # This is a ""default"" Cromwell example that is intended for you you to start with; # and edit for your needs. Specifically, you will be interested to customize; # the configuration based on your preferred backend (see the backends section; # below in the file). For backend-specific examples for you to copy paste here,; # please see the cromwell.backend.examples folder in the repository. The files; # there also include links to online documentation (if it exists). # This line is required. It pulls in default overrides from the embedded cromwell; # `reference.conf` (in core/src/main/resources) needed for proper performance of cromwell.; include required(classpath(""application"")). # Google configuration; google {. application-name = ""cromwell-demo"". auths = [; {; name = ""application-default""; scheme = ""application_default""; },; {; name = ""service-account""; scheme = ""service_account""; service-account-id = ""***@***.gserviceaccount.com""; json-file = ""/***/***.json""; }; ]; }. # Here is where you can define the backend providers that Cromwell understands.; # The default is a local provider.; # To add additional backend providers, you should copy paste additional backends; # of interest that you can find in the cromwell.example.backends folder; # folder at https://www.github.com/broadinstitute/cromwell; # Other backend providers include SGE, SLURM, Docker, udocker, Singularity. etc.; # Don't forget you will need to customize them for your particular use case.; backend {; # Override the default backend.; default = ""PAPIv2"". # The list of providers.; providers {; PAPIv2 {; actor-factory = ""cromwell.backend.google.pipelines.v2beta.PipelinesApiLifecycleActorFactory""; config {; # Google p",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:9601,configurat,configuration,9601,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['configurat'],['configuration']
Deployability,"}; }; output {; File out = ""${outfilename}""; }; }. workflow test1 {; String name. call hello {; input: outfilename=""${name}.txt"", name = ""${name}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Using default configuration. Output:; ```; [2020-02-11 10:13:03,33] [info] Running with database db.url = jdbc:hsqldb:mem:89c116e0-5bca-4467-aaff-ae492c2ebbaf;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:14,71] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2019-02-11 10:13:14,75] [info] [RenameWorkflowOptionsInMetadata] 100%; [2019-02-11 10:13:15,05] [info] Running with database db.url = jdbc:hsqldb:mem:6b5d8035-4932-4680-b912-34885765f705;shutdown=false;hsqldb.tx=mvcc; [2019-02-11 10:13:15,63] [info] Slf4jLogger started; [2019-02-11 10:13:16,02] [info] Workflow heartbeat configuration:; {; ""cromwellId"" : ""cromid-1ddecb5"",; ""heartbeatInterval"" : ""2 minutes"",; ""ttl"" : ""10 minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; [2019-02-11 10:13:16,08] [info] Metadata summary refreshing every 2 seconds.; [2019-02-11 10:13:16,20] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,23] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2019-02-11 10:13:16,25] [warn] 'docker.hash-lookup.gcr-api-queries-per-100-seconds' is being deprecated, use 'docker.hash-lookup.gcr.throttle' instead (see reference.conf); [2019-02-11 10:13:16,26] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2019-02-11 10:13:16,33] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2019-02-11 10:13:17,45] [info] SingleWorkflowRunnerActor: Version 37; [2019-02-11 10:13:17,46] [info] SingleWorkflowRunnerActor: Submitting workflow; [2019-02-11 10:13:17,59] [info] Unspecifi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626:1829,configurat,configuration,1829,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626,1,['configurat'],['configuration']
Deployability,"~Same code change as the hotfixes.~. The code change from the hotfixes, plus a shutdown hook to cleanly close the HTTP backend as suggested by their docs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3938:25,hotfix,hotfixes,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3938,2,['hotfix'],['hotfixes']
Deployability,"~This PR is set to merge to `rsa_ss_jdr_integration_with_test` as it contains the JDR integration test. This way the localizer changes are tested against 2 drs centaur tests.~; ~Once [PR-5719](https://github.com/broadinstitute/cromwell/pull/5719) is merged to develop, the base of this PR will change to develop.~. ~Do not merge this PR before PR-5719 is merged to develop.~. Update: [PR-5719](https://github.com/broadinstitute/cromwell/pull/5719) has been merged. The base of this PR has now been updated to `develop`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5740:86,integrat,integration,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5740,3,"['Update', 'integrat', 'update']","['Update', 'integration', 'updated']"
Deployability,"~{dollar}{DATA_SOURCES_FOLDER}; tar -zvxf ~{dollar}{potential_gnomad_gz}; cd -; else; echo ""ERROR: Cannot find gnomAD folder: ~{dollar}{potential_gnomad_gz}"" 1>&2; false; fi; done; fi. # Run Funcotator:; gatk --java-options ""-Xmx~{runtime_params.command_mem}m"" Funcotator \; --data-sources-path $DATA_SOURCES_FOLDER \; --ref-version ~{reference_version} \; --output-file-format ~{output_format} \; -R ~{ref_fasta} \; -V ~{input_vcf} \; -O ~{output_file} \; ~{interval_list_arg} ~{default="""" interval_list} \; --annotation-default normal_barcode:~{default=""Unknown"" control_id} \; --annotation-default tumor_barcode:~{default=""Unknown"" case_id} \; --annotation-default Center:~{default=""Unknown"" sequencing_center} \; --annotation-default source:~{default=""Unknown"" sequence_source} \; ~{""--transcript-selection-mode "" + transcript_selection_mode} \; ~{transcript_selection_arg}~{default="""" sep="" --transcript-list "" transcript_selection_list} \; ~{annotation_def_arg}~{default="""" sep="" --annotation-default "" annotation_defaults} \; ~{annotation_over_arg}~{default="""" sep="" --annotation-override "" annotation_overrides} \; ~{excluded_fields_args}~{default="""" sep="" --exclude-field "" funcotator_excluded_fields} \; ~{filter_funcotations_args} \; ~{extra_args_arg}; # Make sure we have a placeholder index for MAF files so this workflow doesn't fail:; if [[ ""~{output_format}"" == ""MAF"" ]] ; then; touch ~{output_maf_index}; fi; >>>. runtime {; docker: runtime_params.gatk_docker; bootDiskSizeGb: runtime_params.boot_disk_size; memory: runtime_params.machine_mem + "" MB""; disks: ""local-disk "" + select_first([disk_space, runtime_params.disk]) + "" HDD""; preemptible: runtime_params.preemptible; maxRetries: runtime_params.max_retries; cpu: runtime_params.cpu; }. output {; File funcotated_output_file = ""~{output_file}""; File funcotated_output_file_index = ""~{output_file_index}""; }; }; ```; <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:38766,configurat,configuration,38766,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['configurat'],['configuration']
Deployability,"~~Also, this is going to be 0.17 so the original version was correct.. maybe we should update the doc to update the SBT version on the first Develop commit AFTER branching?~~. I didn't realise this was for us to continue with after the Develop branch was separated. :+1:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/405#issuecomment-175233017:87,update,update,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/405#issuecomment-175233017,2,['update'],['update']
Deployability,~~TODO~~ DONE:; - [x] Confirm existing spec TODOs are ok; - [x] Confirm reviewers are ok with naming; - [x] Add same patch to develop,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/769:117,patch,patch,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/769,1,['patch'],['patch']
Deployability,"~~possibly yes~~; probably not, however I will update the doc to point to what will (help).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3462#issuecomment-377018271:47,update,update,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3462#issuecomment-377018271,1,['update'],['update']
Deployability,"â€¦ all moving in the right direction. Test Infrastructure updated to handle the test re-enabled here, including ThreeStep",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/868:57,update,updated,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/868,1,['update'],['updated']
Deployability,"â€¦ all moving in the right direction. Test Infrastructure updated to handle the test re-enabled here, including ThreeStep. There are a few intermittent test failures, bugged as #850 #876 #877",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/878:57,update,updated,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/878,1,['update'],['updated']
Deployability,"â€¦ed to mount that into the docker image by using JES disk input parameters, which it turns out you have to specify both at pipeline time and runtime (question out to Garrick). This is important because currently if you try to localize or produce more than 10gb of input your pipeline will die. This bumps that up to 100G of local ssd. Eventually, after getting some road time, we might want to think about how to parameterize this so users could specify/override",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/153:123,pipeline,pipeline,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/153,2,['pipeline'],['pipeline']
Deployability,"â€¦lines. ^^ that title-split was made by github. Apparently github line-splitting has a sense of humour now?. ---. This test has burned me a few times recently, but nothing obvious in the code has changed for many months. Did we update the akka http library in the last few days?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5471:228,update,update,228,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5471,1,['update'],['update']
Deployability,"â€¦n. Closes #1022 . Known sharp corners I'm inclined to fix later in the interest of getting this rolling:; - The docker image shouldn't be pointing at my own organization; - 32 CPU seems like overkill. OTOH I've seen futures time out at 16 cpu. One could argue (and I would) that this is BS, however for this testing I want to make sure futures never time out. We can/should address the ""why"" separately; - It's painful to dig into why a test failed on the centaur side. I don't know yet a way to make that easier, but this at least makes it possible",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1050:97,rolling,rolling,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1050,1,['rolling'],['rolling']
Deployability,"â€¦old performance increase under load prior to DB gumming up. . Things to note:; - Effectively removes Metadata acks & failure notices (see #1811) via no longer emitting the messages but does not fully remove them. They still technically exist, I'll remove them as part of a separate PR; - Completely reworks `CromwellApiServiceSpec` to actually be testing `CromwellApiService` and not a general integration test of our REST endpoints. Two specific tests didn't make the cut (#1828 and #1829) I'll address in separate PRs. There were other tests which did not make the cut but were already effectively being tested in their appropriate units.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1836:395,integrat,integration,395,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1836,1,['integrat'],['integration']
Deployability,"â€¦t specified. Instead of failing if there is no ""auth_bucket"" in the workflow options, fallback to the workflow execution directory.; This also fix a bug where the auth file would not be uploaded if we only have docker configuration but no refresh token / auth_bucket option.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/320:219,configurat,configuration,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/320,1,['configurat'],['configuration']
Deployability,ðŸ‘ . Does this need to be on hotfix as well? @jsotobroad @kcibul . [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1213/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1213#issuecomment-235719480:28,hotfix,hotfix,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1213#issuecomment-235719480,1,['hotfix'],['hotfix']
Deployability,"ðŸ‘ ; I feel that we should hotfix this in to the next Firecloud release, given the improvement we saw in Alpha. The problem was probably exacerbated recently with the increase to the metadata batch size (bigger batches == more slowdown in List.append).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317799694:26,hotfix,hotfix,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2485#issuecomment-317799694,2,"['hotfix', 'release']","['hotfix', 'release']"
Deployability,ðŸ‘ I thought Kristian mentioned this is important for Firecloud... does this also need to go into hotfix?. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/823/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/823#issuecomment-218793047:97,hotfix,hotfix,97,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/823#issuecomment-218793047,1,['hotfix'],['hotfix']
Deployability,ðŸ‘ swagger yaml to be updated. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1293/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1293#issuecomment-240136679:21,update,updated,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1293#issuecomment-240136679,1,['update'],['updated']
Deployability,ðŸ‘ when above issues are addressed w/ comments/updates. [![Approved with PullApprove](https://img.shields.io/badge/one_reviewer-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/3682/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell) [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/3682/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3682#issuecomment-391699492:46,update,updates,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3682#issuecomment-391699492,1,['update'],['updates']
Deployability,ðŸ‘ ðŸ‘ ðŸ‘ ; Thanks so much! I've just updated my system with the latest Cromwell and this is perfect,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4825#issuecomment-495405207:34,update,updated,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4825#issuecomment-495405207,1,['update'],['updated']
Energy Efficiency, 	at scala.collection.AbstractTraversable.to(Traversable.scala:104); 	at scala.collection.TraversableOnce.toList(TraversableOnce.scala:294); 	at scala.collection.TraversableOnce.toList$(TraversableOnce.scala:294); 	at scala.collection.AbstractTraversable.toList(Traversable.scala:104); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4(EcmaScriptUtil.scala:111); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4$adapted(EcmaScriptUtil.scala:107); 	at scala.collection.MapLike$MappedValues.$anonfun$foreach$3(MapLike.scala:253); 	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:789); 	at scala.collection.immutable.Map$Map2.foreach(Map.scala:146); 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 	at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:253); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1(EcmaScriptUtil.scala:107); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1$adapted(EcmaScriptUtil.scala:97); 	at cwl.internal.EnhancedRhinoSandbox.eval(EnhancedRhinoSandbox.scala:61); 	at cwl.internal.EcmaScriptUtil$.evalRaw(EcmaScriptUtil.scala:69); 	at cwl.internal.EcmaScriptUtil$.evalStructish(EcmaScriptUtil.scala:97); 	at cwl.ExpressionEvaluator$.eval(ExpressionEvaluator.scala:76); 	at cwl.ExpressionEvaluator$.evaluator$1(ExpressionEvaluator.scala:40); 	at cwl.ExpressionEvaluator$.$anonfun$evalExpression$1(ExpressionEvaluator.scala:43); 	at cwl.ExpressionInterpolator$.interpolate(ExpressionInterpolator.scala:140); 	at cwl.ExpressionEvaluator$.evalExpression(ExpressionEvaluator.scala:43); 	at cwl.EvaluateExpression$.$anonfun$script$2(EvaluateExpression.scala:11); 	at cwl.ExpressionEvaluator$.eval(ExpressionEvaluator.scala:35); 	at cwl.CommandLineBindingCommandPart.$anonfun$instantiate$5(CwlExpressionCommandPart.scala:79); 	at scala.Option.flatMap(Option.scala:171); 	at cwl.CommandLineBindingCommandPart.instantiate(CwlExpressionCommandPart.scala:78); 	at w,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787:2649,adapt,adapted,2649,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787,1,['adapt'],['adapted']
Energy Efficiency," ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi; cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.${barcode}.R1.trimmed.gz -p ${sampleName}.${barcode}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; cpu : 2; memory : '${memory_task2} MB'; time : 24; }. output {; File fastq_trimmed_R1 = ""${sampleName}.${barcode}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.${barcode}.R2.trimmed.gz""; }; }. ### Below my json file:; {; ""scMethTask3.monitoring_script"": ""monitoring.sh"",; ""scMethTask3.command"": ""moveBarcodeToID.pl"",; ""scMethTask3.meta_data"": ""test_no_barcode.txt"",; ""scMethTask3.bases"": 6,. ""scMethTask3.memory_task1"":45,. ""scMethTask3.TAG"": ""'length='"",; ""scMethTask3.low_quality_cutoff"": 21,; ""scMethTas",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:5776,monitor,monitoring,5776,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,2,['monitor'],['monitoring']
Energy Efficiency," - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:1918,schedul,scheduleOnce,1918,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,1,['schedul'],['scheduleOnce']
Energy Efficiency," I see what you mean but it's the only way. When in SGE your job is killed he never get to the point of $? > rc; > so cromwell can not detect is a job is killed, meaning it will end in a endless loop polling for rc what never will come anymore; > in this case a drmaa connection would be better; > but not so sure if that still works on a start of a server; > I think there it's bound to a session; > ; > Peter van 't Hof @ffinfo Aug 26 19:11; > but only have seen the dmraa implementation inside Gatk Queue; > ; > Peter van 't Hof @ffinfo Aug 26 19:28; > when using qstat i would use it only once for the complete pool instead executing it for each job; > so then you get an output like this:; > `; > job-ID prior name user state submit/start at queue slots ja-task-ID; > 9923549 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > 9923550 0.00000 cromwell_1 pjvan_thof qw 08/26/2016 17:23:16 1; > `; > this is only 2 jobs but having a lot of jobs this will reduce the load a lot; > ; > kshakir @kshakir Aug 26 21:21; > True, Cromwell will end up in an endless loop if someone terminates the SGE job, or if the rc file doesnâ€™t appear in general. One could use isAlive intermittently, but it was introduced mainly for recovering jobs at re-startup, & I would not have isAlive poll as often as we check for the rc file. Btw, GATK Queue actually only checks drmaa every 30 seconds, so that it doesnâ€™t overload dispatchers. Something like isAlive could be checked with similar frequency. All this is a bigger discussion that could be tracked in a git issue.; > Separately, I am hearing from multiple people that the rc poll logs are spam. ; > ; > Peter van 't Hof @ffinfo Aug 26 21:44; > As already suggested in the PR, a actor pool would be better I think but that's not a small change indeed; > mostly jobs are running way longer that 10 or 30 sec does not matter a lot ; > ; > Peter van 't Hof @ffinfo Aug 26 21:50; > On our cluster we need something like retries but if it goes to an endless lo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348:1197,reduce,reduce,1197,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-242961348,1,['reduce'],['reduce']
Energy Efficiency," at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread11"" #16 dae",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5405,Schedul,ScheduledThreadPoolExecutor,5405,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Schedul'],['ScheduledThreadPoolExecutor']
Energy Efficiency," https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Im using FireCloud (workspace: broad-firecloud-dsde/dsde-methods-sv-dev); <!-- Paste/Attach your workflow if possible: -->; The WDL can be found in GATK's repo: [cnv_germline_cohort_workflow.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/cnv_germline_cohort_workflow.wdl) that imports [cnv_common_tasks.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/cnv_common_tasks.wdl). This is the graph that ```wdltools``` output for that WDL; [graph.pdf](https://github.com/broadinstitute/cromwell/files/2406647/graph.pdf); ![graph](https://user-images.githubusercontent.com/791104/45901323-88187c80-bdb0-11e8-91df-c9a61a12a96a.png). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. As you can see in the monitor's ""Failure"" [report ](https://portal.firecloud.org/#workspaces/broad-firecloud-dsde/dsde-methods-sv-dev/monitor/88f444ae-0898-4b5e-af0c-ede98216641d/6d980272-4aa7-4d32-ab90-84880a0723b2)```GermlineCNVCallerCohortMode``` scatter task never get calls before the dependent ```PostprocessGermineCNVCalls```.; <img width=""788"" alt=""screen shot 2018-09-21 at 3 21 43 pm"" src=""https://user-images.githubusercontent.com/791104/45901815-47b9fe00-bdb2-11e8-9043-9f771ee8bd9e.png"">. The log confirms this if one searches for ""Starting"":; ```; 2018-09-20 22:45:12,561 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.PreprocessIntervals; 2018-09-20 23:03:42,454 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.CollectCounts (95 shards), CNVGermlineCohortWorkflow.ScatterIntervals; 2018-09-21 02:12:52,275 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a072",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4136:1461,monitor,monitor,1461,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136,1,['monitor'],['monitor']
Energy Efficiency," quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memory_task1; Int memory_task2. #Start the call. Array[Array[String]] files_and_metadata = read_tsv(meta_data). scatter(files_and_metadata_row in files_and_metadata) {; String sampleName = files_and_metadata_row[0]; File f1 = files_and_metadata_row[1]; File f2 = files_and_metadata_row[2]; String? barcode = files_and_metadata_row[3]; #if the barcode is passed, proceed with it.; if (defined(barcode)) {; call trimCellBarcode {; input:; f1=f1,; f2=f2,; sampleName=sampleName,; barcode=barcode,; monitoring_script=monitoring_script,; command=command,; memory_task1=memory_task1,; bases=bases; }; }; #if the barcode is not passed, proceed with the trimming of the adapters only; if (!defined(barcode)) {; call trimAdaptersWithoutBarcodes{; input:; input_r1=f1,; input_r2=f2,; sampleName=sampleName,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; }; call trimAdapters {; input:; input_r1=trimCellBarcode.fastqDebarcodedR1,; input_r2=trimCellBarcode.fastqDebarcodedR2,; sampleName=sampleName,; barcode=barcode,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; } ; }. task trimCellBarcode {; File f1; File f2; String sampleName; String? barcode; File command; Int bases; File? monitoring_script; Int memory_task1. command <<<; set -euo pip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:2658,adapt,adapters,2658,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,1,['adapt'],['adapters']
Energy Efficiency," tasks in a workflow, or for a single task call across many workflow runs, etc. This can be very powerful to quickly determine outlier tasks that could use optimization, without the need for any configuration or code (or any changes to the workflow). It's also much easier than the current state-of-the-art, i.e. parsing task-level monitoring logs. 2) Scripts can easily get aggregate statistics on resource utilization and could produce suggestions based on those. This could provide a path towards automatic runtime configuration based on the models trained with historical data. One could also detect situations like out-of-memory calls and automatically adjust resources according to those. It would also be pretty easy to add logic for estimation of task call-level cost based on the pricing of associated resources. This could provide a long-sought feature of real-time cost monitoring/control (thanks to @TimothyTickle for the suggestion). Monitoring is done using the new ""monitoring action"" for PAPIv2, which currently uses the hard-coded [quay.io/broadinstitute/cromwell-monitor](https://quay.io/repository/broadinstitute/cromwell-monitor) image, built from https://github.com/broadinstitute/cromwell-monitor (I wasn't sure if that code belonged here or in a separate repo). This is advantageous to just using it as a _monitoring_script_, because it removes all assumptions on the ""user"" Docker image (for the task itself). For example, we don't have to assume a particular distribution or presence of Python and its libraries. So it should work exactly the same for any task. Per @geoffjentry's suggestion, we've [consulted](https://groups.google.com/forum/#!topic/google-genomics-discuss/caYM7oHbfx0) with the Google Genomics team, and they don't see any apparent issues with the concept. We could expose this as a workflow option like `monitoring_image`, and allow configuring it at the Cromwell level, so e.g. any user of Terra (or any other hosted Cromwell with PAPIv2 backend) could g",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510:1350,Monitor,Monitoring,1350,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510,2,"['Monitor', 'monitor']","['Monitoring', 'monitoring']"
Energy Efficiency," trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; }; call trimAdapters {; input:; input_r1=trimCellBarcode.fastqDebarcodedR1,; input_r2=trimCellBarcode.fastqDebarcodedR2,; sampleName=sampleName,; barcode=barcode,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; } ; }. task trimCellBarcode {; File f1; File f2; String sampleName; String? barcode; File command; Int bases; File? monitoring_script; Int memory_task1. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:3965,monitor,monitoring,3965,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,2,['monitor'],['monitoring']
Energy Efficiency,"""real return code"" stored; submit-docker = """"""; docker run \; --cidfile ${docker_cid} \; -i \; ${""--user "" + docker_user} \; --entrypoint /bin/bash \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; rc=$(docker wait `cat ${docker_cid}`); docker rm `cat ${docker_cid}`; exit $rc; """"""; }; ```. The log shows the following stack-trace:. ```; [2018-03-09 15:31:16,47] [error] Failed to properly flush metadata to database; java.sql.SQLException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:2624,adapt,adapted,2624,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['adapt'],['adapted']
Energy Efficiency,"# Summary. Currently Cromwell will retry tasks that fail in Pipelines API due to preemption, with the number of retries configurable on a task by task basis. It would be very helpful if this could be generalized, so that I could tell Cromwell to retry all tasks that fail -- for any reason, not just preemption. I imagine this being configured via a workflow option like ""failed_task_retries: 3"", which would tell Cromwell to run each task in the workflow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161:606,Green,Green,606,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161,2,"['Green', 'reduce']","['Green', 'reduce']"
Energy Efficiency,"# What happened:. On 8/25/18, Dockerhub performed some scheduled maintenance. Cromwell subsequently failed to start new jobs as PAPI reported 500 errors from Dockerhub. # What should have happened:. Cromwell should be resilient to outages in its dependencies, in this case docker hosts. It should *not* report as down, but instead should be in a ""degraded"" state where jobs may be submitted/finished/etc. but new jobs will not be started until the docker host is back to full health. This should be a nuanced status check. GCR images may still be pulled when Dockerhub is down, so those jobs should proceed as planned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4056:55,schedul,scheduled,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4056,1,['schedul'],['scheduled']
Energy Efficiency,"# What we have today. All runnable jobs are ""submitted"" (started but do not automatically run). We then apply backpressure using a token system on runnable jobs. Jobs which are runnable but not given a token reside in memory. # What we want. We do not pull any new jobs to be run unless all of the following conditions are met:. * The number of running jobs is below a threshold; * Dockerhub is healthy; * PAPI is healthy; * Database is healthy; * GCR is healthy!; * [insert other dependent systems here please]. # What this will give us. * A more powerful pull-based architecture (no need for backpressure gymnastics); * More resiliency to failures of dependent systems; * Less memory pressure. # What the author of this issue does not yet know / needs investigation. * Whether new runnable jobs from a new workflow are started(""submitted"" might be better word) automatically or persisted to the DB before they are run. This issue generally assumes this algorithm applies at the time of retrieving runnable jobs from the DB. # Technical miscellanies. * [fs2 Signal](https://github.com/functional-streams-for-scala/fs2/blob/072776fc8ba5ec41c9e8cdd0c28b6e719375112a/core/shared/src/main/scala/fs2/concurrent/Signal.scala) Is useful to share mutable state between threads, in this case the health status of our dependent services.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4138:548,power,powerful,548,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4138,1,['power'],['powerful']
Energy Efficiency,"# [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=h1) Report; > :exclamation: No coverage uploaded for pull request base (`develop@9ec815d`). [Click here to learn what that means](https://docs.codecov.io/docs/error-reference#section-missing-base-commit).; > The diff coverage is `95.12%`. [![Impacted file tree graph](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/graphs/tree.svg?width=650&token=DJALPpnS9I&height=150&src=pr)](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree). ```diff; @@ Coverage Diff @@; ## develop #5086 +/- ##; ==========================================; Coverage ? 78.36% ; ==========================================; Files ? 1038 ; Lines ? 26695 ; Branches ? 887 ; ==========================================; Hits ? 20920 ; Misses ? 5775 ; Partials ? 0; ```. | [Impacted Files](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=tree) | Coverage Î” | |; |---|---|---|; | [core/src/main/scala/cromwell/util/JsonEditor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5086/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC91dGlsL0pzb25FZGl0b3Iuc2NhbGE=) | `95.12% <95.12%> (Ã¸)` | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=footer). Last update [9ec815d...d2705e2](https://codecov.io/gh/broadinstitute/cromwell/pull/5086?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119:1499,Power,Powered,1499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5086#issuecomment-515048119,1,['Power'],['Powered']
Energy Efficiency,"## Call-caching problems with path+modtime; I have been doing some call-caching benchmarking on the [BioWDL RNA-seq](https://github.com/biowdl/RNA-seq) pipeline and it turns out any `path` or `path+modtime` strategies do not work with containers. As is reported in these issues: #5405, #5370, #5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450:837,power,power,837,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450,1,['power'],['power']
Energy Efficiency,"## Motivation; * Test longevity of Cromwell, to prove that it can continuously run at scale over multiple days.; * This is Green team ideally running 20k jobs/day okr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4794:123,Green,Green,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4794,1,['Green'],['Green']
Energy Efficiency,"## Why; Green team submits a lot of workflows at once, and then immediately uses the /query endpoint but gets stale data; ## Author's Proposed approach ; 1. Submit many jobs, let them run to completion; 1. Turn on summarizer; ## Measure; * How long it took to summarize the data; ## Challenges to consider; ### How to tell when summarization is finished?; Can poll DB table to see where summary pointer is vs. size of metadata table",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4792:8,Green,Green,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4792,1,['Green'],['Green']
Energy Efficiency,### Description. * Don't run E2E test automatically on a schedule; * Don't automatically update terra-helmfile when Cromwell PRs merge. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7524:57,schedul,schedule,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7524,1,['schedul'],['schedule']
Energy Efficiency,### Description. - Necessary but not sufficient to finish the story; - Breaking out into separate PR for clarity and to verify tests are still green. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7479:143,green,green,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7479,1,['green'],['green']
Energy Efficiency,"### Description. Jira: https://broadworkbench.atlassian.net/browse/WX-1835. Note: the scheduled logging will happen even if `quota-exhaustion-job-start-control` is disabled. This is because even if JobTokenDispenser doesn't account for quota exhausted groups, Cromwell is always recording which groups are in quota exhaustion. And scheduled logging will help easily see that list even if the feature `quota-exhaustion-job-start-control` is disabled. Example logs at different times:; ```; 2024-09-12 18:32:11 cromwell-system-akka.dispatchers.engine-dispatcher-24 INFO - GroupMetricsActor configured to log groups experiencing quota exhaustion at interval of 5 minutes.; 2024-09-12 18:37:11 cromwell-system-akka.dispatchers.engine-dispatcher-58 INFO - Hog groups currently experiencing quota exhaustion: 3. Group IDs: [cromwell-dev, sshah-test-1, sshah-test-2].; ....; 2024-09-12 18:42:11 cromwell-system-akka.dispatchers.engine-dispatcher-71 INFO - Hog groups currently experiencing quota exhaustion: 1. Group IDs: [cromwell-dev].; ....; 2024-09-12 19:02:12 cromwell-system-akka.dispatchers.engine-dispatcher-60 INFO - Hog groups currently experiencing quota exhaustion: 0. Group IDs: [].; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7539:86,schedul,scheduled,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7539,2,['schedul'],['scheduled']
Energy Efficiency,"### Description. Minor fixes to publish correct monitoring log metadata as well as a GCP Batch ""alt"" Centaur test for a slightly changed metadata path. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7549:48,monitor,monitoring,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7549,1,['monitor'],['monitoring']
Energy Efficiency,"### Description. The `google_legacy_machine_selection` workflow option that exists on PAPI v2 to pick JES-shaped machines was not completely wired in to the GCP Batch backend. However when I completed the wiring job and ran the `hello_google_legacy_machine_selection` Centaur test I got this:. ```; Task wf_hello.hello:NA:1 failed: Job failed when Batch tries to schedule it:; Batch Error: code - CODE_MACHINE_TYPE_NOT_FOUND, description - ; machine type predefined-1-2048 for job job-xyz, project 8675309, region us-central1, zones (if any) us-central1-b is not available.; ```. So basically `google_legacy_machine_selection` does not seem to work on GCP Batch. If anyone is using this undocumented feature to emulate the behavior of JES from many years ago, we should let them know that this support is going to be dropped when GCP Batch is rolled out. . ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7547:363,schedul,schedule,363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7547,1,['schedul'],['schedule']
Energy Efficiency,"### What happened. On 10/10/2018, around 11:15 AM, there was a spike in backpressure and 403 copy failures. It was discovered that a user had submitted workflows attempting to access buckets it did not have access to. . ![image](https://user-images.githubusercontent.com/16748522/46764755-59087300-ccab-11e8-9163-afd953710adf.png); Purple line- backpressure; Light green line- 403 copy failures. ### What was done to fix it. The situation was discussed with the user, and once he aborted all his workflows, Cromwell slowly returned to its normal state. The issue was resolved around 1:50 PM. ### Potential causes. The user had reused a WDL from another user, but he didn't have access to their Google Cloud buckets. This workflow contained job that ran 5000 split intervals against dataset of approx 1300 samples. Each of the 5000 outputs would be copied, per workflow, per sample. Depending on the number of samples the other user had previously run, each interval-output-for-each-sample tried call caching to other user's workspace. This resulted in a lot of attempts to copy files and then failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4229:365,green,green,365,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4229,1,['green'],['green']
Energy Efficiency,"$anonfun$run$1.applyOrElse(CromwellServer.scala); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:433); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala); at scala.concurrent.impl.CallbackRunnable.run(Redefined); at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(Redefined); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java). ""cromwell-system-scheduler-1"" #14 prio=5 os_prio=31 tid=0x00007fb76aa14800 nid=0x6103 runnable [0x00000001295b3000]; java.lang.Thread.State: RUNNABLE; at com.jprofiler.agent.InstrumentationCallee.exitFilteredMethod(Native Method); at com.jprofiler.agent.InstrumentationCallee.__ejt_filter_exitMethod(ejt:86); at akka.actor.LightArrayRevolverScheduler.clock(Scheduler.scala:213); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Redefined); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Redefined); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.actor.default-dispatcher-4"" #13 prio=5 os_prio=31 tid=0x00007fb76b38c000 nid=0x5f03 waiting on condition [0x000000012ac3d000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c002f9e0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(Redefined); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(Redefined); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java). ""cromwell-system-scheduler-1"" #10 prio=5 os_prio=31 tid=0x00007fb76b20f000 nid=0x5a07 runnable [0x000000012a0e1000]; java.lang.Thread.State: RUNNABLE; at com.jprofiler.agent.InstrumentationCallee.exitFilteredMethod(Native Method); at com.jprofiler.agent.Instrume",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:46950,Schedul,Scheduler,46950,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Schedul'],['Scheduler']
Energy Efficiency,"()); 	}; }; ```; error log; ```; $ java -jar /users/leepc12/code/cromwell/./target/scala-2.12/cromwell-31-d716fd2-SNAP.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 20:11:15,13] [info] Running with database db.url = jdbc:hsqldb:mem:7e58cfd2-b9b6-47f9-bda1-6fe045e7a665;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:21,83] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 20:11:21,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 20:11:22,02] [info] Running with database db.url = jdbc:hsqldb:mem:e02f9206-cb15-468a-929a-82676a83a9b8;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:22,47] [info] Slf4jLogger started; [2017-12-05 20:11:22,67] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 20:11:22,68] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82] [info] Workflow 159210e6-fa6a-4a99-b386-5931ae245324 submitted.; [2017-12-05 20:11:23,82] [info] SingleWorkflowRunnerActor: Workflow submitted 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,82] [info] 1 new workflows fetched; [2017-12-05 20:11:23,82] [info] WorkflowManagerActor Starting workflow 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-05 20:11:24,82] [error] WorkflowManagerActor Workflow 159210e6-fa6a-4a9",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3007:1566,monitor,monitor,1566,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3007,1,['monitor'],['monitor']
Energy Efficiency,"()); 	}; }; ```; error log; ```; $ java -jar /users/leepc12/code/cromwell/./target/scala-2.12/cromwell-31-d716fd2-SNAP.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 20:11:15,13] [info] Running with database db.url = jdbc:hsqldb:mem:7e58cfd2-b9b6-47f9-bda1-6fe045e7a665;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:21,83] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 20:11:21,84] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 20:11:22,02] [info] Running with database db.url = jdbc:hsqldb:mem:e02f9206-cb15-468a-929a-82676a83a9b8;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 20:11:22,47] [info] Slf4jLogger started; [2017-12-05 20:11:22,67] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 20:11:22,68] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 20:11:22,69] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 20:11:22,71] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 20:11:23,78] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 20:11:23,82] [info] Workflow 159210e6-fa6a-4a99-b386-5931ae245324 submitted.; [2017-12-05 20:11:23,82] [info] SingleWorkflowRunnerActor: Workflow submitted 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,82] [info] 1 new workflows fetched; [2017-12-05 20:11:23,82] [info] WorkflowManagerActor Starting workflow 159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] WorkflowManagerActor Successfully started WorkflowActor-159210e6-fa6a-4a99-b386-5931ae245324; [2017-12-05 20:11:23,83] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-05 20:11:24,82] [error] WorkflowManagerActor Workflow 159210e6-fa6a-4a9",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406:1479,monitor,monitor,1479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992#issuecomment-349527406,1,['monitor'],['monitor']
Energy Efficiency,); at org.asynchttpclient.resolver.RequestHostnameResolver.resolve(RequestHostnameResolver.java:50); at org.asynchttpclient.netty.request.NettyRequestSender.resolveAddresses(NettyRequestSender.java:355); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithNewChannel(NettyRequestSender.java:298); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithCertainForceConnect(NettyRequestSender.java:140); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequestBuilder.execute(BoundRequestBuilder.java:35); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1(AsyncHttpClientBackend.scala:53); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1$adapted(AsyncHttpClientBackend.scala:42); at cats.effect.IO$.$anonfun$async$1(IO.scala:1042); at cats.effect.IO$.$anonfun$async$1$adapted(IO.scala:1040); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:329); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:118); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); at cats.effect.IO.unsafeRunAsync(IO.scala:269); at cats.effect.IO.unsafeToFuture(IO.scala:341); at cromwell.languages.util.ImportResolver$.$anonfun$httpResolverWithHeaders$1(ImportResolver.scala:92); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$2(package.scala:25); at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); at scala.collection.immutable.List.foldLeft(List.scala:86); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$1(package.scala:22); at cats.data.Kleisli.$anonf,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3977:2244,adapt,adapted,2244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977,1,['adapt'],['adapted']
Energy Efficiency,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032:141,adapt,adapters,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032,4,['adapt'],"['adapter', 'adapters']"
Energy Efficiency,"**Finalized Ticket**. For the `read_x()` functions, limit the size of data which can be read in. Check the file size prior to attempting to read as reading can also incur network charges. Any attempt to read an oversized file will result in a failure. See notes from Geraldine down below about how to phrase the error. `read_bool()` - 5 chars; `read_int()` - 19 chars; `read_float()` - 50 chars; `read_string()` - 128K; `read_lines()` - 128K; `read_json()` - 128K; `read_[tsv|map|object]()` - 1MB. **Original text for posterity**; Note to @kcibul - this is both a request for the WDL spec and Cromwell's implementation, not just the latter. Our read_X() functions (e.g. read_int, read_string, read_lines) have a flaw in that they'll dutifully read in the entire file. The problem with this is that a malicious and/or less than careful user could take down Cromwell or another WDL implementing engine (unless it was written in Erlang!) by reading in an enormous file. For instance a careless user might `read_string(SomeEnormousBam)`. The point of these functions are more of a convenience, if users are trying to sling around huge chunks of data they should be passing files around. In particular things like `read_boolean()` are particularly egregious as it will only interpret `true` or `false`. Similarly it seems unlikely that someone would have a valid use case to read in the first 9 billion digits of Pi into a `Float`. . I propose that we place a cap on how much data we will read to some reasonable amount of data (e.g. CWL uses 64 KiB, which seems a little excessively small to me).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762:179,charge,charges,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762,1,['charge'],['charges']
Energy Efficiency,"+1; we like to compose WDL workflows using import of many atomic tasks, some of these will be very simple like generating a UUID or calling mkdir, some will be long running computations. Being able to specify the short tasks to run locally and handing the longer ones off to the cluster would be efficient.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-229573997:296,efficient,efficient,296,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1086#issuecomment-229573997,1,['efficient'],['efficient']
Energy Efficiency,- Add dood to the ci docker compose used by jenkins.; - Run ci docker under user `hoggett`.; - Pass mysql creds to cromwell unit tests running in jenkins.; - Do not remove rendered creds via `sbt clean`.; - Reduce configs to `sbt test` and `sbt alltests:test`.; - Otherwise use the environment variable `CROMWEL_SBT_TEST_EXCLUDE_TAGS`.; - Tests don't need to be run in the directory `cromwell`.; - Only push artifacts on travis.; - Simplify skipping docker push.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4169:207,Reduce,Reduce,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4169,1,['Reduce'],['Reduce']
Energy Efficiency,- Additional wiring for the cromwell terminator.; - Reduced duplicate calls to ConfigFactory.load().; - Increased ability to pass around test configs.; - Provide names for more actors.; - Instrument failures in batch actors.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4785:52,Reduce,Reduced,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4785,1,['Reduce'],['Reduced']
Energy Efficiency,"- Allow a `0` value for CWL `outDirMin` and `tmpDirMin` resource attributes; - Adds an optional section to the language factory to define a command to run after the user's action that will return output files that can only be known at runtime; - Only defined for CWL for now, which will remove unnecessary pull of jq for WDL tasks on PAPI2; - Docker image and command can both be changed in the configuration; - The PAPI2 logic that handles delocalization of those file strips away some redundant pieces in the delocalized paths to reduce the overall length of the path",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4358:532,reduce,reduce,532,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4358,1,['reduce'],['reduce']
Energy Efficiency,"- Commit 1: Creates 2 subdirectories in the call directory. ```; cromwell-executions/globbinginput/; â””â”€â”€ 970c776f-3b9d-4c0e-a2ff-dbd76395b4ba; â”œâ”€â”€ call-globtask; â”‚Â Â  â”œâ”€â”€ execution; â”‚Â Â  â”‚Â Â  â”œâ”€â”€ outputfile1.txt; â”‚Â Â  â”‚Â Â  â”œâ”€â”€ outputfile2.txt; â”‚Â Â  â”‚Â Â  â”œâ”€â”€ rc; â”‚Â Â  â”‚Â Â  â”œâ”€â”€ script; â”‚Â Â  â”‚Â Â  â”œâ”€â”€ script.background; â”‚Â Â  â”‚Â Â  â”œâ”€â”€ script.submit; â”‚Â Â  â”‚Â Â  â”œâ”€â”€ stderr; â”‚Â Â  â”‚Â Â  â”œâ”€â”€ stderr.background; â”‚Â Â  â”‚Â Â  â”œâ”€â”€ stdout; â”‚Â Â  â”‚Â Â  â””â”€â”€ stdout.background; â”‚Â Â  â””â”€â”€ inputs; â”‚Â Â  â””â”€â”€ 90e8d77e2a99e99efa82c33e27365d71; â”‚Â Â  â””â”€â”€ somefile.txt; ```. This prevents globbing from matching input files as they're not in the same branch.; - Commit 2: Instead of recreating the whole directory structure underneath the call directory, hash the path and create a single directory named with this hash. ; - This reduces the depth of the call directory, and is hopefully a bit less confusing (people tend to be confused when they see the full path of their input file appended to the call directory); - The hash is only computed from the fullpath of the input's parent directory (not including the filename). This ensures that multiple files from the same directory will still end up in the same directory once localized). The hashing part is not necessary to fix this bug at all, and I remember the choice was made not to use hashes because it's unreadable and makes it hard to know where the inputs came from but I feel like recreating the full path underneath the call directory is even worse...; - Commit 3: Change the globbing pattern so it does NOT traverse recursively the hierarchy looking for matches by default. If that's the desired behaviour it should be reflected in the glob pattern in the wdl (with `**`). This is necessary to allow for this kind of usage https://github.com/broadinstitute/cromwell/issues/1245",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1415:775,reduce,reduces,775,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1415,1,['reduce'],['reduces']
Energy Efficiency,"- Replaced to-be-deprecated Credential (no 's') with Adapter around Credentials; - Removed dupe credentials adapting from PipelinesApiFactoryInterface; - Move service specific scopes (KMS, Genomics) out of GoogleAuthMode; - Changed credential creation methods to take scala collections",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5013:53,Adapt,Adapter,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5013,2,"['Adapt', 'adapt']","['Adapter', 'adapting']"
Energy Efficiency,"- Updated language for things that are no longer new, or don't seem to be happening; - Promote JIRA with a screenshot so users are confident there really is a promised land after they create an account; - Reduced exclamation. IntelliJ preview:. ![Screen Shot 2020-12-08 at 5 23 11 PM](https://user-images.githubusercontent.com/1087943/101548570-12416100-397a-11eb-804c-0dd586939272.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6130:205,Reduce,Reduced,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6130,1,['Reduce'],['Reduced']
Energy Efficiency,"- `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design the",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2270,schedul,scheduler,2270,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,1,['schedul'],['scheduler']
Energy Efficiency,- monitoring logs now delocalize; - publish monitoring script used as well as the monitoring logs to call-level metadata,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2529:2,monitor,monitoring,2,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2529,3,['monitor'],['monitoring']
Energy Efficiency,". This can be very powerful to quickly determine outlier tasks that could use optimization, without the need for any configuration or code (or any changes to the workflow). It's also much easier than the current state-of-the-art, i.e. parsing task-level monitoring logs. 2) Scripts can easily get aggregate statistics on resource utilization and could produce suggestions based on those. This could provide a path towards automatic runtime configuration based on the models trained with historical data. One could also detect situations like out-of-memory calls and automatically adjust resources according to those. It would also be pretty easy to add logic for estimation of task call-level cost based on the pricing of associated resources. This could provide a long-sought feature of real-time cost monitoring/control (thanks to @TimothyTickle for the suggestion). Monitoring is done using the new ""monitoring action"" for PAPIv2, which currently uses the hard-coded [quay.io/broadinstitute/cromwell-monitor](https://quay.io/repository/broadinstitute/cromwell-monitor) image, built from https://github.com/broadinstitute/cromwell-monitor (I wasn't sure if that code belonged here or in a separate repo). This is advantageous to just using it as a _monitoring_script_, because it removes all assumptions on the ""user"" Docker image (for the task itself). For example, we don't have to assume a particular distribution or presence of Python and its libraries. So it should work exactly the same for any task. Per @geoffjentry's suggestion, we've [consulted](https://groups.google.com/forum/#!topic/google-genomics-discuss/caYM7oHbfx0) with the Google Genomics team, and they don't see any apparent issues with the concept. We could expose this as a workflow option like `monitoring_image`, and allow configuring it at the Cromwell level, so e.g. any user of Terra (or any other hosted Cromwell with PAPIv2 backend) could get usage reports without having to configure anything. The metrics are reported",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510:1484,monitor,monitor,1484,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510,1,['monitor'],['monitor']
Energy Efficiency,".apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:282); 	at akka.dispatch.Mailbox.run(Mailbox.scala:223); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.do",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012:5955,monitor,monitoring,5955,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012,1,['monitor'],['monitoring']
Energy Efficiency,".exec(Mailbox.scala:234); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2018-03-09 15:52:58,29] [error] Failed to properly flush metadata to database; java.sql.SQLException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCUtil.sqlException(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSession.withPreparedStatement(JdbcBackend.scala:434); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl.preparedInsert(JdbcActionComponent.scala:502); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.run(JdbcActionC",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:11773,adapt,adapted,11773,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['adapt'],['adapted']
Energy Efficiency,.execute(ProxyPreparedStatement.java:44); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.execute(HikariProxyPreparedStatement.java); 	at slick.jdbc.StatementInvoker.results(StatementInvoker.scala:38); 	at slick.jdbc.StatementInvoker.iteratorTo(StatementInvoker.scala:21); 	at slick.jdbc.Invoker.foreach(Invoker.scala:47); 	at slick.jdbc.Invoker.foreach$(Invoker.scala:46); 	at slick.jdbc.StatementInvoker.foreach(StatementInvoker.scala:15); 	at slick.jdbc.StreamingInvokerAction.run(StreamingInvokerAction.scala:22); 	at slick.jdbc.StreamingInvokerAction.run$(StreamingInvokerAction.scala:20); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.jdbc.JdbcActionComponent$QueryActionExtensionMethodsImpl$$anon$1.run(JdbcActionComponent.scala:216); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:944); 	at scala.collection.Iterator.foreach$(Iterator.scala:944); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.dbio.SynchronousDatabaseAction$$anon$6.run(DBIOAction.scala:469); 	at slick.dbio.SynchronousDatabaseAction$$anon$10.run(DBIOAction.scala:561); 	at slick.dbio.SynchronousDatabaseAction$$anon$7.run(DBIOAction.scala:486); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.T,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5084:5480,adapt,adapted,5480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5084,1,['adapt'],['adapted']
Energy Efficiency,.google.cloud.storage.StorageException: 503 Service Unavailable; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:189); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:335); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:191); 	at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:188); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:94); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:54); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:188); 	at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:202); 	at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.readAttributes(CloudStorageFileSystemProvider.java:579); 	at java.nio.file.Files.readAttributes(Files.java:1737); 	at java.nio.file.Files.size(Files.java:2332); 	at better.files.File.$anonfun$size$1(File.scala:502); 	at better.files.File.$anonfun$size$1$adapted(File.scala:502); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:448); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:155); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1417); 	at scala.collection.TraversableOnce.sum(TraversableOnce.scala:216); 	at scala.collection.TraversableOnce.sum$(TraversableOnce.scala:216); 	at scala.collection.AbstractIterator.sum(Iterator.scala:1417); 	at better.files.File.size(File.scala:502); 	at cromwell.core.path.BetterFileMethods.size(BetterFileMethods.scala:323); 	at cromwell.core.path.BetterFileMethods.size$(BetterFileMethods.scala:323); 	at cromwell.filesystems.gcs.GcsPath.size(GcsPathBuilder.scala:179); 	at cromwell.backend.wdl.ReadLikeFu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576:4964,adapt,adapted,4964,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576,1,['adapt'],['adapted']
Energy Efficiency,".jprofiler.agent.InstrumentationCallee.exitFilteredMethod(Native Method); at com.jprofiler.agent.InstrumentationCallee.__ejt_filter_exitMethod(ejt:86); at akka.actor.LightArrayRevolverScheduler.clock(Scheduler.scala:213); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Redefined); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Redefined); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.actor.default-dispatcher-4"" #13 prio=5 os_prio=31 tid=0x00007fb76b38c000 nid=0x5f03 waiting on condition [0x000000012ac3d000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c002f9e0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(Redefined); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(Redefined); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java). ""cromwell-system-scheduler-1"" #10 prio=5 os_prio=31 tid=0x00007fb76b20f000 nid=0x5a07 runnable [0x000000012a0e1000]; java.lang.Thread.State: RUNNABLE; at com.jprofiler.agent.InstrumentationCallee.exitFilteredMethod(Native Method); at com.jprofiler.agent.InstrumentationCallee.__ejt_filter_exitMethod(ejt:86); at akka.dispatch.AbstractNodeQueue$Node.next(AbstractNodeQueue.java:124); at akka.dispatch.AbstractNodeQueue.pollNode(AbstractNodeQueue.java:86); at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:411); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Redefined); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Redefined); at java.lang.Thread.run(Redefined). ""Service Thread"" #9 daemon prio=9 os_prio=31 tid=0x00007fb76a82e000 nid=0x5103 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread3"" #8 daemon prio=9 os_prio=31 tid=0x00007fb76a060000 nid=0x4f03 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread2"" #7 daemon prio=9 os_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:47707,schedul,scheduler-,47707,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['schedul'],['scheduler-']
Energy Efficiency,.map(Traversable.scala:104) ~[cromwell.jar:0.19]; â€‚â€‚at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; â€‚â€‚at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; â€‚â€‚at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; â€‚â€‚at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; â€‚â€‚at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; â€‚â€‚at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; â€‚â€‚at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; â€‚â€‚at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/810:6233,Adapt,AdaptedForkJoinTask,6233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/810,1,['Adapt'],['AdaptedForkJoinTask']
Energy Efficiency,.scala:644); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:959); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); 	at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); 	at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Error evaluating ad hoc files:; <path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.backend.standard.St,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:3799,Schedul,Scheduler,3799,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['Schedul'],['Scheduler']
Energy Efficiency,"/gdc-dnaseq-cwl/blob/master/workflows/dnaseq/transform.cwl:. ```; $ java -jar ~/bin/womtool-31.1.jar womgraph transform.cwl; Exception in thread ""main"" scala.MatchError: WomMaybePopulatedFileType (of class wom.types.WomMaybePopulatedFileType$); 	at womtool.graph.WomGraph$.fakeInput(WomGraph.scala:222); 	at womtool.graph.WomGraph$.$anonfun$womExecutableFromCwl$2(WomGraph.scala:205); 	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); 	at scala.collection.immutable.HashMap$HashMap1.foreach(HashMap.scala:231); 	at scala.collection.immutable.HashMap$HashTrieMap.foreach(HashMap.scala:462); 	at scala.collection.TraversableLike.map(TraversableLike.scala:234); 	at scala.collection.TraversableLike.map$(TraversableLike.scala:227); 	at scala.collection.AbstractTraversable.map(Traversable.scala:104); 	at womtool.graph.WomGraph$.$anonfun$womExecutableFromCwl$1(WomGraph.scala:205); 	at scala.util.Either.map(Either.scala:350); 	at womtool.graph.WomGraph$.womExecutableFromCwl(WomGraph.scala:201); 	at womtool.graph.WomGraph$.fromFiles(WomGraph.scala:172); 	at womtool.Main$.$anonfun$womGraph$2(Main.scala:98); 	at womtool.Main$.continueIf(Main.scala:102); 	at womtool.Main$.womGraph(Main.scala:96); 	at womtool.Main$.dispatchCommand(Main.scala:38); 	at womtool.Main$.delayedEndpoint$womtool$Main$1(Main.scala:167); 	at womtool.Main$delayedInit$body.apply(Main.scala:12); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at womtool.Main$.main(Main.scala:12); 	at womtool.Main.main(Main.scala); ```. It would also be nice if the documentation included the fact that [`cwltool`](https://github.com/common-workflow-language/cwltool) needs to be installed.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4119#issuecomment-584388032:1798,adapt,adapted,1798,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4119#issuecomment-584388032,1,['adapt'],['adapted']
Energy Efficiency,"000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread3"" #8 daemon prio=9 os_prio=31 tid=0x00007fb76a060000 nid=0x4f03 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread2"" #7 daemon prio=9 os_prio=31 tid=0x00007fb76b011000 nid=0x4d03 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread1"" #6 daemon prio=9 os_prio=31 tid=0x00007fb76a815000 nid=0x4b03 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread0"" #5 daemon prio=9 os_prio=31 tid=0x00007fb76b810000 nid=0x4903 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""Signal Dispatcher"" #4 daemon prio=9 os_prio=31 tid=0x00007fb76b80c000 nid=0x3d0b waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""Finalizer"" #3 daemon prio=8 os_prio=31 tid=0x00007fb76a05e000 nid=0x3503 in Object.wait() [0x0000000126bc7000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x00000006c00371c0> (a java.lang.ref.ReferenceQueue$Lock); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164); at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209). ""Reference Handler"" #2 daemon prio=10 os_prio=31 tid=0x00007fb76b005800 nid=0x3303 in Object.wait() [0x0000000126ac4000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:157); - locked <0x00000006c00301d8> (a java.lang.ref.Reference$Lock). ""main"" #1 prio=5 os_prio=31 tid=0x00007fb76a803000 nid=0xf07 waiting on condition [0x000000010b088000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c061d308> (a scala.concurrent.impl.Promise$CompletionLatch); at ja",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:49505,monitor,monitor,49505,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['monitor'],['monitor']
Energy Efficiency,"000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread0"" #5 daemon prio=9 os_prio=31 tid=0x00007fb76b810000 nid=0x4903 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""Signal Dispatcher"" #4 daemon prio=9 os_prio=31 tid=0x00007fb76b80c000 nid=0x3d0b waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""Finalizer"" #3 daemon prio=8 os_prio=31 tid=0x00007fb76a05e000 nid=0x3503 in Object.wait() [0x0000000126bc7000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x00000006c00371c0> (a java.lang.ref.ReferenceQueue$Lock); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164); at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209). ""Reference Handler"" #2 daemon prio=10 os_prio=31 tid=0x00007fb76b005800 nid=0x3303 in Object.wait() [0x0000000126ac4000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:157); - locked <0x00000006c00301d8> (a java.lang.ref.Reference$Lock). ""main"" #1 prio=5 os_prio=31 tid=0x00007fb76a803000 nid=0xf07 waiting on condition [0x000000010b088000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c061d308> (a scala.concurrent.impl.Promise$CompletionLatch); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304); at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:49988,monitor,monitor,49988,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['monitor'],['monitor']
Energy Efficiency,"015-12-18 08:43:18,816] [error] SingleWorkflowRunnerActor: Ask timed out on [Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312]] after [5000 ms]; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312]] after [5000 ms]; at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:334); at akka.actor.Scheduler$$anon$7.run(Scheduler.scala:117); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:599); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:597); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467); at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375); at java.lang.Thread.run(Thread.java:745); [2015-12-18 08:43:19,174] [info] Message [cromwell.engine.workflow.WorkflowManagerActor$RestartWorkflows] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,180] [info] Message [akka.actor.Status$Failure] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,182] [error] WorkflowManagerActor: Workflow failed submission: cannot create children while terminati",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/334:1988,Schedul,Scheduler,1988,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/334,1,['Schedul'],['Scheduler']
Energy Efficiency,"0duB1nw. input fastq files can be retrieved here (they are small ~10000 reads each):. https://drive.google.com/file/d/1-c14Tja4zY3lyr6icFWT06stznR_-Zqr/view?usp=sharing; https://drive.google.com/file/d/1oJd_U9MjTllL0_kpNivw8I_LtSyvqpXH/view?usp=sharing. How can I solve this issue and make the workflow running smoothly?. ### Which backend are you running? ; I am running locally the workflow for now (because I am in the first phase of the development). ### Workflow is this:; ```; #workflow validated before running with: wdltool validate example.wdl and womtool validate scMeth_v2.wdl.sh -i scMeth_input_3.json. workflow scMeth {; # information for trimming the cell barcode; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName. # information for trimming the adapters and low quality reads; File file_format; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; call trimCellBarcode {; input:; sampleName=sampleName,; bases=bases,; input_fastq1=input_fastq1,; input_fastq2=input_fastq2,; command=command; }; call trimAdapters {; input:; file_format=file_format,; input_r1 = trimCellBarcode.fastq_debarcoded_R1,; input_r2 = trimCellBarcode.fastq_debarcoded_R2,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; TAG=TAG; }; }. task trimCellBarcode {; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName; command {; perl ${command} paired ${input_fastq1} ${input_fastq2} ${bases} ${sampleName}.R1.debarcoded.fq.gz ${sampleName}.R2.debarcoded.fq.gz; }; output {; File fastq_debarcoded_R1 = ""${sampleName}.R1.debarcoded.fq.gz""; File fastq_debarcoded_R2 = ""${sampleName}.R2.debarcoded.fq.gz""; }; }. task trimAdapters {; File file_format",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:1490,adapt,adapters,1490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['adapt'],['adapters']
Energy Efficiency,"1 20:01:06,50] [info] BackgroundConfigAsyncJobExecutionActor [132d7527test.t1:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2017-12-01 20:01:06,61] [error] WorkflowManagerActor Workflow 132d7527-a0af-4f08-8291-d935e7cd5632 failed (during ExecutingWorkflowState): Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; java.lang.RuntimeException: Could not evaluate t1.out = if select_first([flag1,false]) then glob(""test1.txt"")[0] else glob(""test2.txt"")[0]; at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:189); at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:188); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); at scala.util.Failure.recoverWith(Try.scala:232); at wdl4s.wdl.WdlTask.$anonfun$evaluateOutputs$2(WdlTask.scala:188); at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:157); at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:157); at scala.collection.Iterator.foreach(Iterator.scala:929); at scala.collection.Iterator.foreach$(Iterator.scala:929); at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:157); at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:155); at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); at wdl4s.wdl.WdlTask.evaluateOutputs(WdlTask.scala:181); at cromwell.backend.wdl.OutputEvaluator$.evaluateOutputs(OutputEvaluator.scala:15); at cromwell.backend.standard.StandardAsyncExecutionActor.evaluateOutputs(StandardAsyncExecutionActor.scala:406); at cromwell.backend.standard.StandardAsyncExecutionActor.evaluateOutputs$(StandardAsyncExecutionActor.scala:405)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2972:4067,adapt,adapted,4067,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2972,1,['adapt'],['adapted']
Energy Efficiency,"1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.C",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:3686,adapt,adapted,3686,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['adapt'],['adapted']
Energy Efficiency,"18 08:43:17,592] [info] WorkflowManagerActor Found no workflows to restart.; [2015-12-18 08:43:18,816] [error] SingleWorkflowRunnerActor: Ask timed out on [Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312]] after [5000 ms]; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312]] after [5000 ms]; at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:334); at akka.actor.Scheduler$$anon$7.run(Scheduler.scala:117); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:599); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:597); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467); at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375); at java.lang.Thread.run(Thread.java:745); [2015-12-18 08:43:19,174] [info] Message [cromwell.engine.workflow.WorkflowManagerActor$RestartWorkflows] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,180] [info] Message [akka.actor.Status$Failure] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,182] [error] WorkflowMa",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/334:1912,Schedul,Scheduler,1912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/334,1,['Schedul'],['Scheduler']
Energy Efficiency,"3-456789abcdef/call-main/d/y; TOTAL: 2 objects, 0 bytes (0 B); ```. The delocalization script is aware that `d` is directory:; ```; $ gsutil cat gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/gcs_delocalization.sh; source '/cromwell_root/gcs_transfer.sh'. timestamped_message 'Delocalization script execution started...'. # xxx; delocalize_6c578056c74a8d9a80724855ddac131c=(; ""mccarroll-mocha"" # project; ""3"" # max attempts; ""150M"" # parallel composite upload threshold, will not be used for directory types; ""file""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/memory_retry_rc""; ""/cromwell_root/memory_retry_rc""; ""optional""; ""text/plain; charset=UTF-8""; ""file""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/rc""; ""/cromwell_root/rc""; ""required""; ""text/plain; charset=UTF-8""; ""file""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/monitoring.log""; ""/cromwell_root/monitoring.log""; ""required""; ""text/plain; charset=UTF-8""; ""file""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/stdout""; ""/cromwell_root/stdout""; ""required""; ""text/plain; charset=UTF-8""; ""file""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/stderr""; ""/cromwell_root/stderr""; ""required""; ""text/plain; charset=UTF-8""; ""directory""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/d""; ""/cromwell_root/d""; ""required""; """"; ). delocalize ""${delocalize_6c578056c74a8d9a80724855ddac131c[@]}""; ; timestamped_message 'Delocalization script execution complete.'; ```. But somehow a new check was included in Cromwell 75 that wants `d` to be a file even if it is delocalized as a directory. This breaks the only [workaround](https://support.terra.bio/hc/en-us/community/posts/360071476431-Terra-fails-to-delocalize-files-listed-through-read-lines-) available in Cromwell to be able to delocalize a list",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6677:2200,monitor,monitoring,2200,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6677,1,['monitor'],['monitoring']
Energy Efficiency,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:2499,adapt,adapters,2499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,4,['adapt'],"['adapter', 'adapters']"
Energy Efficiency,"455FA72420237EB05902327"",; ""docker"": ""A84529F7A095541F1249576699F24AA1"",; ""continueOnReturnCode"": ""614DAABB2D7AAB5D41921614A49E4F92""; },; ""input count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""backend name"": ""50F66ECBC45488EE5826941BFBC50411"",; ""command template"": ""F41FEBA57D556A16A5F6C4EEF68ED1E0""; },; ""effectiveCallCachingMode"": ""ReadAndWriteCache""; },; ""inputs"": {},; ""backendLabels"": {; ""wdl-task-name"": ""fail-oom"",; ""cromwell-workflow-id"": ""cromwell-87492280-9828-4afa-b53e-bec675103c42""; },; ""labels"": {; ""wdl-task-name"": ""fail_oom"",; ""cromwell-workflow-id"": ""cromwell-87492280-9828-4afa-b53e-bec675103c42""; },; ""failures"": [; {; ""causedBy"": [],; ""message"": ""The compute backend terminated the job. If this termination is unexpected, examine likely causes such as preemption, running out of disk or memory on the compute instance, or exceeding the backend's maximum job duration.""; }; ],; ""jobId"": ""projects/99884963860/locations/us-central1/operations/1374639517116411519"",; ""monitoringLog"": ""gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/monitoring.log"",; ""backend"": ""gcp"",; ""end"": ""2020-08-29T00:04:05.346Z"",; ""stderr"": ""gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/stderr"",; ""callRoot"": ""gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom"",; ""attempt"": 1,; ""executionEvents"": [; {; ""description"": ""CallCacheReading"",; ""startTime"": ""2020-08-29T00:00:44.174Z"",; ""endTime"": ""2020-08-29T00:00:44.237Z""; },; {; ""startTime"": ""2020-08-29T00:00:42.044Z"",; ""description"": ""Pending"",; ""endTime"": ""2020-08-29T00:00:42.064Z""; },; {; ""description"": ""RunningJob"",; ""startTime"": ""2020-08-29T00:00:44.237Z"",; ""endTime"": ""2020-08-29T00:04:05.347Z""; },; {; ""startTime"": ""2020-08-29T00:00:42.531Z"",; ""endTime"": ""2020-08-29T00:00:44.174Z"",; ""description"": ""PreparingJob""; },; {; ""startTime"": ""2020-08-29T00:00:42.064Z"",; ""description"": ""R",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815:5320,monitor,monitoringLog,5320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815,2,['monitor'],"['monitoring', 'monitoringLog']"
Energy Efficiency,"5 Await.results removed for DSDEEPB-1549, fixes DSDEEPB-1631 for no extra charge.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/277:74,charge,charge,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/277,1,['charge'],['charge']
Energy Efficiency,"6835f0/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/api/BatchApiRequestManager.scala#L67)), putting an upper limit on the delay seems worth it, any thoughts?; 5. Do we need to get anything else for the job execution events? see below and [BatchRequestExecutor#getEventList](https://github.com/broadinstitute/cromwell/blob/a333f65b8e80ae37091a5629e0331c2105aeefeb/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/api/request/BatchRequestExecutor.scala#L196). <details>; <summary>Execution events details</summary>. What GCP provides:. ```; Event type=STATUS_CHANGED; time=seconds: 1712173852,nanos: 952604950; taskState=STATE_UNSPECIFIED,; description=Job state is set from QUEUED to SCHEDULED for job projects/392615380452/locations/us-south1/jobs/job-ba81bad8-82e9-4d95-8fc0-04dfbbd746da.; taskExecution.exitCode=0. Event type=STATUS_CHANGED,; time=seconds: 1712173947, nanos: 568998105; taskState=STATE_UNSPECIFIED; description=Job state is set from SCHEDULED to RUNNING for job projects/392615380452/locations/us-south1/jobs/job-ba81bad8-82e9-4d95-8fc0-04dfbbd746da.; taskExecution.exitCode=0. Event type=STATUS_CHANGED; time=seconds: 1712173989, nanos: 937816549; taskState=STATE_UNSPECIFIED; description=Job state is set from RUNNING to SUCCEEDED for job projects/392615380452/locations/us-south1/jobs/job-ba81bad8-82e9-4d95-8fc0-04dfbbd746da.; taskExecution.exitCode=0; ```. What we define as execution events:. ```; ExecutionEvent(Job state is set from QUEUED to SCHEDULED for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:10:01.704137839Z,None); ExecutionEvent(Job state is set from SCHEDULED to RUNNING for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:11:30.631264449Z,None); ExecutionEvent(Job state is set from RUNNING to SUCCEEDED for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412:4273,SCHEDUL,SCHEDULED,4273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412,1,['SCHEDUL'],['SCHEDULED']
Energy Efficiency,74); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$$anon$1.toWomBundle(FileElementToWomBundle.scala:30); at wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); at wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); at wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.convert(FileElementToWomBundle.scala:83); at wdl.draft3.transforms.wdlom2wom.package$.$anonfun$fileElementToWomBundle$1(package.scala:13); at scala.util.Either$RightProjection.flatMap(Either.scala:702); at cats.instances.EitherInstances$$anon$1.flatMap(either.scala:36); at cats.instances.EitherInstances$$anon$1.flatMap(either.scala:32); at cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); at languages.wdl.draft3.WdlDraft3LanguageFactory.getWomBundle(WdlDraft3LanguageFactory.scala:50); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:49); at scala.util.Either.flatMap(Either.scala:338); at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:40); at womtool.input.WomGraphMaker$.getBundle(WomGraphMaker.scala:22); at womtool.validate.Validate$.validate(Validate.scala:14); at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:44); at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:125); at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:130); at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:18); at scala.Function0.apply$mcV$sp(Function0.scala:34); at scala.Function0.apply$mcV$sp$(Function0.scala:34); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); at scala.App.$anonfun$main$1$adapted(App.scala:76); at scala.collection.immutable.List.foreach(List.scala:389); at scala.App.main(App.scala:76); at scala.App.main$(App.scala:74); at womtool.WomtoolMain$.main(WomtoolMain.scala:18); at womtool.WomtoolMain.main(WomtoolMain.scala); ```,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3977:6133,adapt,adapted,6133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977,1,['adapt'],['adapted']
Energy Efficiency,91b-46a7-b892-86454be067fd failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736:2531,adapt,adapted,2531,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736,1,['adapt'],['adapted']
Energy Efficiency,": Evaluating files_and_metadata_row[3] failed: Failed to find index Success(WomInteger(3)); on array:\n\nSuccess([\""SRR5395068\"", \""SRR5395068_1.fastq.gz\"", \""SRR5395068_2.fastq.gz\""])\n\n3"",; ""causedBy"": []; ```; How can i avoid this? Or is there a way to accomplish what I am trying to do?. ### Which backend are you running? ; Unix terminal within slurm scheduler. ### Example meta_data files:; 1) without barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz	; ```; 2) with barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz ATCGCT	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz ATCGGA; ```; ### Below my workflow:. workflow scMethTask3 {. #information about the monitoring scrip and the number of samples; File? monitoring_script; File meta_data. #information for trimming the cell barcode; File command; Int bases; ; #information for trimming the adapters and low quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memory_task1; Int memory_task2. #Start the call. Array[Array[String]] files_and_metadata = read_tsv(meta_data). scatter(files_and_metadata_row in files_and_metadata) {; String sampleName = files_and_metadata_row[0]; File f1 = files_and_metadata_row[1]; File f2 = files_and_metadata_row[2]; String? barcode = files_and_metadata_row[3]; #if the barcode is passed, proceed with it.; if (defined(barcode)) {; call trimCellBarcode {; input:; f1=f1,; f2=f2,; sampleName=sampleName,; barcode=barcode,; monitoring_script=monitoring_script,; command=command,; memory_task1=memory_task1,; bases=bases; }; }; #if the barcode is not passed, proceed with the trimming of the adapters only; if (!defined(barcode)) {; call trimAdaptersWithoutBarcodes{; input:; input_r1=f1,; input_r2=f2,; sampleName=sampleName,; low_quality",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:1755,adapt,adapters,1755,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,1,['adapt'],['adapters']
Energy Efficiency,":+1: It seems there's a potential to reduce some of the boilerplate around instances dealing with `ExpressionElement`. It seems like those instances exist to refine the type down to the ""leaf"" level where the more specific type can do its thing. I would try to eliminate one of these and see if you can parameterize the callers with a `[T]` or `[T <: ExpressionElement]` to save you from the trouble of specializing/refining/narrowing/casting (not sure the right word) the type yourself. [![Approved with PullApprove](https://img.shields.io/badge/one_reviewer-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/3413/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell) [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/3413/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3413#issuecomment-373823911:37,reduce,reduce,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3413#issuecomment-373823911,1,['reduce'],['reduce']
Energy Efficiency,:+1: feel free to merge once it goes green if I don't get to it first,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/109#issuecomment-123709570:37,green,green,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/109#issuecomment-123709570,1,['green'],['green']
Energy Efficiency,:+1: reaffirmed though it would be really nice to see green builds before merge ðŸ˜„,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-231114604:54,green,green,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1127#issuecomment-231114604,1,['green'],['green']
Energy Efficiency,:+1: you'll need to rebase on develop to get your builds to go green (zone issue). [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1403/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246718747:63,green,green,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246718747,1,['green'],['green']
Energy Efficiency,":latest\""\n }\n}\n\n"",; ""root"": """",; ""options"": ""{\n \""backend\"": \""gcp\"",\n \""default_runtime_attributes\"": {\n \""maxRetries\"": 1\n },\n \""monitoring_script\"": \""gs://caper-data/scripts/resource_monitor/resource_monitor.sh\""\n}"",; ""inputs"": ""{}"",; ""workflowUrl"": ""/mnt/data2/scratch/leepc12/test_wdl1_sub/test_mem_1.wdl"",; ""labels"": ""{\n \""caper-backend\"": \""gcp\"",\n \""caper-user\"": \""leepc12\""\n}""; },; ""calls"": {; ""mem_retry.fail_oom"": [; {; ""preemptible"": false,; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://encode-pipeline-test-runs/caper_out_10/mem_retry/87492280-9828-4afa-b53e-bec675103c42/call-fail_oom/stdout"",; ""backendStatus"": ""Success"",; ""compressedDockerSize"": 28591363,; ""commandLine"": ""set -e\n# This one-liner triggers 137 (SIGKILL due to OOM)\n# https://askubuntu.com/a/823798\ntail /dev/zero"",; ""shardIndex"": -1,; ""jes"": {; ""endpointUrl"": ""https://lifesciences.googleapis.com/"",; ""machineType"": ""custom-1-2048"",; ""googleProject"": ""encode-dcc-1016"",; ""monitoringScript"": ""gs://caper-data/scripts/resource_monitor/resource_monitor.sh"",; ""executionBucket"": ""gs://encode-pipeline-test-runs/caper_out_10"",; ""zone"": ""us-central1-b"",; ""instanceName"": ""google-pipelines-worker-ead27fbad8aa73b157bfc126cd63331f""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""[0,137]"",; ""docker"": ""ubuntu:latest"",; ""maxRetries"": ""1"",; ""cpu"": ""1"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-b"",; ""memoryMin"": ""2 GB"",; ""memory"": ""2 GB""; },; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""hashes"": {; ""output count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""runtime attribute"": {; ""failOnStderr"": ""68934A3E9455FA72420237EB05902327"",; ""docker"": ""A84529F7A095541F1249576699F24AA1"",; ""continueOnReturnCode"": ""614DAABB2D7AAB5D41921614A49E4F92""; },; ""input count"": ""CFCD208495D565EF66E7DFF9F98764DA"",; ""backend name"": ""5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5815:3545,monitor,monitoringScript,3545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5815,1,['monitor'],['monitoringScript']
Energy Efficiency,; 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomBundleMakers$$anon$1.toWomBundle(WdlDraft2WomBundleMakers.scala:19); 	at wdl.transforms.draft2.wdlom2wom.WdlDraft2WomBundleMakers$$anon$1.toWomBundle(WdlDraft2WomBundleMakers.scala:17); 	at wom.transforms.WomBundleMaker$Ops.toWomBundle(WomExecutableMaker.scala:16); 	at wom.transforms.WomBundleMaker$Ops.toWomBundle$(WomExecutableMaker.scala:16); 	at wom.transforms.WomBundleMaker$ops$$anon$2.toWomBundle(WomExecutableMaker.scala:16); 	at languages.wdl.draft2.WdlDraft2LanguageFactory.$anonfun$getWomBundle$3(WdlDraft2LanguageFactory.scala:120); 	at scala.util.Either.flatMap(Either.scala:338); 	at languages.wdl.draft2.WdlDraft2LanguageFactory.$anonfun$getWomBundle$1(WdlDraft2LanguageFactory.scala:119); 	at scala.util.Either.flatMap(Either.scala:338); 	at languages.wdl.draft2.WdlDraft2LanguageFactory.getWomBundle(WdlDraft2LanguageFactory.scala:118); 	at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:49); 	at scala.util.Either.flatMap(Either.scala:338); 	at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:40); 	at womtool.input.WomGraphMaker$.getBundle(WomGraphMaker.scala:22); 	at womtool.validate.Validate$.validate(Validate.scala:14); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:47); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:134); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:139); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:21); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:21); 	at womtool.WomtoolMain.main(WomtoolMain.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502:9545,adapt,adapted,9545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143#issuecomment-408976502,1,['adapt'],['adapted']
Energy Efficiency,; at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); at cats.effect.internals.Trampoline.execute(Trampoline.scala:43) ; at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); at cats.effect.internals.IOBracket$BracketStart.apply(IOBracket.scala:60); at cats.effect.internals.IOBracket$BracketStart.apply(IOBracket.scala:41); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:134); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); at cats.effect.internals.IOBracket$.$anonfun$apply$1(IOBracket.scala:36); at cats.effect.internals.IOBracket$.$anonfun$apply$1$adapted(IOBracket.scala:33); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:328); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:117); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); at cats.effect.IO.unsafeRunAsync(IO.scala:258); at cats.effect.IO.unsafeToFuture(IO.scala:345); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeAsync(AwsBatchAsyncBackendJobExecutionActor.scala:342); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:943); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:935); at cromwell.backend.impl.aws.AwsBatchAsyncBackendJobExecutionActor.executeOrRecover(AwsBatchAsyncBackendJobExecutionActor.scala:74); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.cor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303:5671,adapt,adapted,5671,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303,1,['adapt'],['adapted']
Energy Efficiency,"; java \; -Dconfig.file=my.conf \; -jar cromwell-34.jar \; run centaur/src/main/resources/standardTestCases/hello/hello.wdl \; -i centaur/src/main/resources/standardTestCases/hello/hello.inputs; ```. The accompanying `.test` file lists the CI expectations of the workflow run, ex: `centaur/src/main/resources/standardTestCases/hello.test`. > the stderr files are totally empty, and then the one stdout (without extension) shows those two mapping files â€¦ Let me know if this looks correct? What you are looking for? Completely off base?. Based on the WDL you linked to, this output looks like what was expected :+1:. > Also - any reason to have all capitals vs. lowercase for the backend examples? (e.g. SLURM vs slurm). no reasoN. ---. On a related note I personally would love to see cromwell+singularity running under our CI, so that we could all a) point others at the working example and b) be sure the examples continue to work in the future. Most Broadies I know are even greener on Singularity than CircleCI, but I would be keen to learn sometime. Google turned up your earlier work on installing (parts-of?) [Singularity on a Travis VM](https://github.com/singularityhub/singularity-ci). That combined with these commented out configs could be a fantastic starting point to getting singularity+cromwell regularly tested together. For a similar example, with cromwell+TES, here is where that CI script installs and runs `funnel`:. https://github.com/broadinstitute/cromwell/blob/9f33e2a867fe20924e4f24e0cba8774f7d6d3132/src/ci/bin/testCentaurTes.sh#L14-L36. A similar script that installs the singularity binaries plus a small cluster(?) and then uses a working config file to run our Centaur test suite would be amazing for users. After it's all working, users are being pointed to docs under https://cromwell.readthedocs.io/, such as https://cromwell.readthedocs.io/en/stable/backends/TES/. A similar entry should be added for a working/tested singularity setup. [![Approved with PullApprove]",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519:1406,green,greener,1406,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-416313519,1,['green'],['greener']
Energy Efficiency,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. Hi, . I wrote my first WDL (yay!) and troubleshot it locally using miniwdl. Now, I'm trying to get that WDL uploaded to Terra and the WOMtool validation step continues to pass me a fatal error that I can't seem to figure out. I've reduced the WDL to a single step that can reproduce this error and pasted below. I can't imagine I'm the first person to have this issue, but couldn't find evidence of it on the interwebs! In sum, I have a WDL that appears to be working fine (via miniwdl), but WOMtool (and Dockstore for that matter) finds a fatal error that prevents me from using it on Terra. Please help, thanks!!!. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; `ERROR: Unexpected symbol (line 6, col 5) when parsing 'setter'. Expected equal, got ""String"". String bam_to_reads_mem_size ^ $setter = :equal $e -> $1 `. <!-- Which backend are you running? -->; `womtool v61`; `miniwdl v1.5.2`. <!-- Paste/Attach your workflow if possible: -->; ```; version 1.0 . #WORKFLOW DEFINITION; workflow StripReadsFromBam {; String bam_to_reads_disk_size; String bam_to_reads_mem_size. #converts BAM to FASTQ (R1 + R2); call BamToReads {; 	input:; 	disk_size = bam_to_reads_disk_size,; 	mem_size = bam_to_reads_mem_size; }. #Outputs single reads file; output {; File outputReads = BamToReads.outputReads; }; }. #Task Definitions; task BamToReads {; File InputBam; String SampleName; String disk_size; String mem_size. #Calls samtools view to do the conversion; command {; #Set -e and -o says if any command I run fails in this script, make sure to return a failure; set -e; set -o pipefai",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6767:597,reduce,reduced,597,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6767,1,['reduce'],['reduced']
Energy Efficiency,"=0x9e8 in Object.wait() [0x00007fdb8d2d9000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b4175f0> (a java.lang.ref.ReferenceQueue$Lock); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164); at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209). ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007fdbcc279000 nid=0x9e7 in Object.wait() [0x00007fdb8d3da000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at java.lang.ref.Reference.tryHandlePending(Reference.java:191); - locked <0x000000015b4177a8> (a java.lang.ref.Reference$Lock); at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153). ""main"" #1 prio=5 os_prio=0 tid=0x00007fdbcc00a000 nid=0x9d7 in Object.wait() [0x00007fdbd452c000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Thread.join(Thread.java:1245); - locked <0x000000015d89d070> (a scala.sys.ShutdownHookThread$$anon$1); at java.lang.Thread.join(Thread.java:1319); at java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:106); at java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:46); at java.lang.Shutdown.runHooks(Shutdown.java:123); at java.lang.Shutdown.sequence(Shutdown.java:167); at java.lang.Shutdown.exit(Shutdown.java:212); - locked <0x000000015b6815a8> (a java.lang.Class for java.lang.Shutdown); at java.lang.Runtime.exit(Runtime.java:109); at java.lang.System.exit(System.java:971); at scala.sys.package$.exit(package.scala:40); at cromwell.Main$.waitAndExit(Main.scala:92); at cromwell.Main$.runWorkflow(Main.scala:77); at cromwell.Main$.delayedEndpoint$cromwell$Main$1(Main.scala:25); at cromwell.Main$delayedInit$body.apply(Main.scala:15); at scala.Function0$class.apply$mcV$sp(Function0.scal",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:9530,monitor,monitor,9530,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['monitor'],['monitor']
Energy Efficiency,"> 1. It looks like the perf tests were run on version `""cromwellVersion"": ""48-e0cee74-SNAP"",`, but I don't see that hash in the commit history here.; > ; > I just want to check that was the version you were expecting them to run against, since I would expect it to be a `49-...` hash (you presumably had to rebase onto develop to undo all of the not-quite-summarizer-fix changes)?. @cjllanwarne this is the proper version. I actually took your initial `cjl_summarization_queue` branch and made updates in it. Then I built it locally and pushed to my personal Dockerhub.; I only merged develop branch into this one before creating the PR. >I think we could make this process more efficient by only writing the IDs into the summary queue in the first place if we know we'll actually want to summarize them later on. Do you mean write only those IDs which have certain metadata key value? I'm not sure if this would give us some additional performance boost, since we'll need to check each record for matching our criteria.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436:679,efficient,efficient,679,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5409#issuecomment-584879436,1,['efficient'],['efficient']
Energy Efficiency,"> > I think they are true - I didn't write any tests; > ; > I was hoping there were already some... ðŸ˜¬ Perhaps not as part of this ticket since this seems to be concerned with making the lookup more efficient, but it seems there really should be some Centaur tests for the overall feature. There's actually a Centaur tests which verifies that the whole VPC-thing works, so I guess that might be enough.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700239033:198,efficient,efficient,198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700239033,1,['efficient'],['efficient']
Energy Efficiency,"> @grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`. Since it has been decided to keep support for older v2alpha1 version in addition to newer v2beta, this is no longer an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147:34,adapt,adapter,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147,1,['adapt'],['adapter']
Energy Efficiency,"> @pshapiro4broad just want to be sure you'll be adding the Centaur tests?. It turned out that we (green) didn't need this feature after all, and I've been a bit busy to run the required test. Maybe a red person could take this over, as I think it may be generally useful for other users?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-465747227:99,green,green,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-465747227,1,['green'],['green']
Energy Efficiency,"> Codecov isn't a mandatory check, we just don't have a way to tell Github to display the number without a red or green check. Cool, so pending one more approval and then merge? (also finger cross on other tests)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887940650:114,green,green,114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887940650,1,['green'],['green']
Energy Efficiency,"> Do we have a lot of users with UGER, external and internal?. I can only speak for internal: More and more ""Broad"" users are using Grid Engine, especially [UGER](https://intranet.broadinstitute.org/bits/service-catalog/compute-resources/high-performance-computing-uger), but not with cromwell, yet. DSP-methods have been big users of Grid Engine over the past few years, but mostly the older Sun Grid Engine, and with GATK-Queue. Meanwhile BITS is trying to get them to move over to a Univa Grid Engine installation BITS has called UGER. Unlike the older unlimited setup, UGER's setup has hard limits on the number of concurrent jobs that can be tracked by the Grid Engine scheduler, previously 100, now 1000. With the relaxing of the limit, plus the cromwell's `concurrent-job-limit` feature, this ticket a lower priority imho. Still, it ""would be nice"" if just like we submit in batches to JES, we also submitted in batches to other systems that support it including GridEngine/SLURM/PBS/etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984:674,schedul,scheduler,674,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-332393984,1,['schedul'],['scheduler']
Energy Efficiency,"> Engine support:. For streaming to/from the data storage system, the Arvados Keep data system means that the Arvados Crunch workflow manager doesn't have to wait for input files to be staged (copied) in. The Arvados Keep FUSE plugin only downloads data as the tool requests access to a particular offset. I don't think they co-schedule tasks (either on the same system or ""nearby"" nodes) for direct streaming yet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-860446694:328,schedul,schedule,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-860446694,1,['schedul'],['schedule']
Energy Efficiency,"> Fixes for this will be available in the next Cromwell release, no ETA yet. If you need the fixes immediately and are comfortable building from the `develop` branch, that is also an option. BTW, I really miss the logging you eliminated. With 87 I see the machine type allocated, with 88 I don't. Is there some command line option for turning all the logging back on?. Thank you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627:269,allocate,allocated,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2402959627,1,['allocate'],['allocated']
Energy Efficiency,"> Futures are fine, just not in the code which the actor itself directly controls. Hmm pretty much all our relevant code lives in actors so if the only place where we can use `Future`s is outside of actors, then I'd say `Future`'s days are counted xD. The _burden_ of verifying that new code doesn't break anything is part of the review process anyway IMHO, and adding N more actors instead of `Future`s with twice as many more messages, states, transitions, and classes doesn't reduce the burden of checking that everything is wired correctly, as far as I'm concerned. . Also, about futures being dangerous in actors because they can mutate state, this can only happen in a true `Actor` where your state pretty much has to be `var`s if you want to be able to mutate it, or in an `FSM` where you would also store some state as mutable `var`s inside the actor, instead of using the FSM data.; If you're using an FSM and all your mutable data is contained in the FSM data, then I don't see how creating futures would ever lead to mutating your state (and by state I mean mutable data, not FSM state) asynchronously. The FSM data is only updatable when the actor receives a message and decides to change state or stay in the same. A future `onComplete` could never force the FSM to mutate its data, which is why all we do is always send a message to someone when the future completes. And I don't see what difference it makes, from an actor perspective, if a message comes from a `Future` or another actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218595592:479,reduce,reduce,479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218595592,1,['reduce'],['reduce']
Energy Efficiency,"> Have you thought about where you'd like to place the conversion from ""quota timestamp is recent"" to Boolean ""quota is exhausted""?. @aednichols I was thinking probably in the `/database/slick/GroupMetricsSlickDatabase.scala` file or maybe 1 level higher. But yeah that will be included in the next PR which will actually use the values from the table to decide where to allocate new tokens to this group or not.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7501#issuecomment-2302802927:371,allocate,allocate,371,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7501#issuecomment-2302802927,1,['allocate'],['allocate']
Energy Efficiency,"> Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:; > ; > `ln -s /usr/bin/podman /usr/bin/docker`; > ; > > Probably you should check where is your podman binary with `which podman` and adapt the above command.; > ; > I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working.; > ; > `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs.; > ; > [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt). Hi, yes actually that was the first thing I tried but for some reasons (I already do not remember the exact error) it failed. I think that to configure backend would be cleaner way I think I'll return to it a bit later",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424:333,adapt,adapt,333,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1038781424,1,['adapt'],['adapt']
Energy Efficiency,"> However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?. I am not part of the cromwell team, so it is not up to me whether this gets merged or not. However, allowing softlinks in containers will give errors for a lot of people who are not aware of the implementation details. Those people *will* post bug reports on the cromwell bug tracker. If this were to work, I guess the best way is to allow a config override ""allow-softlinking-in-containers"" with a huge warning in the documentation. That way the unaware will not get caught by surprise as active action needs to be taken to run into this error. > Reducing the number of threads would also reduce the task throughput and limit performance. Offtopic: This is not necessarily always the case. Cromwell uses a very large number of threads by default if the server has a lot of cores. Even with the soft-linking strategy I would recommend playing with that setting a little. More threads is not necessarily better. Task and context switching are expensive operations too, not too mention the ability of the filesystem to handle multiple requests at once.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957:776,reduce,reduce,776,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1046559957,1,['reduce'],['reduce']
Energy Efficiency,"> I think they are true - I didn't write any tests. I was hoping there were already some... ðŸ˜¬ Perhaps not as part of this ticket since this seems to be concerned with making the lookup more efficient, but it seems there really should be some Centaur tests for the overall feature.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700234400:190,efficient,efficient,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5894#issuecomment-700234400,1,['efficient'],['efficient']
Energy Efficiency,"> I'm starting to wonder if it would be easier for me to just write out every CREATE statement to generate the current tables. I'd prefer to use liquibase syntax as much as possible, versus [custom crafted SQL](https://www.liquibase.org/documentation/changes/sql.html). > do you have a preference for 1) trying to make the current migrations work for Postgres too (without breaking the MD5s), or 2) make all existing migrations non-Postgres and add a single comprehensive Postgres-specific migration?. Of the two, I think it would be fantastic if we could do ""1)"". Minimum requirements are that existing MySQL users can startup cromwell w/o a liquibase error. Ultimately, if you can get updated changelogs that actually don't cause collisions with existing MD5s for those populated databases that's one avenue that might work. If not, and ""2)"" is uglier but doesn't break things for MySQL, then so be it. Side note: I suspect the existing Java/Scala changelogs can be a no-op / skip, assuming that anyone using Postgres will not need to migrate data for those specific changes. I believe we skipped those Java/Scala migrations for the in-memory HSQLDB instances. Also, you didn't ask, but in my dream world Cromwell would have changelogs that:; - Use liquibase syntax vs. sql as much as possible; - Work for a new database; - Work for all old/populated databases; - Work for HSQLDB + MySQL + PostgreSQL + MariaDB; - Can be updated to add other databases if/when our [Slick](http://slick.lightbend.com/doc/3.2.3/supported-databases.html) calls work or cromwell switches to another SQL adapter. To get to that last point I've wondered how one would best handle the liquibase MD5 issue in the future, either suppressing the warnings and / or resetting the MD5s as needed. **TL;DR Try 1), but as long as populated MySQL databases still startup with cromwell you're good!**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475371156:1584,adapt,adapter,1584,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475371156,1,['adapt'],['adapter']
Energy Efficiency,"> Is it a huge overhead/burden to also turn on the draft-3 versions?. Whenever the originals get updated, these should (in theory) be kept in sync. The point of the CRON tests is to run as close as possible what the real world workflows are running. As many of the originals run in FC, I believe they should be draft-2 for now. If one wanted to additionally clone draft-3/1.0 versions I think that would be fine. ToL: A better version of the CRON tests would just point-to/reference the originals from the source with smaller inputs, instead of having clones in this git repo. EDIT: More specifically re: burden-- this PR is just trying to get the tests green and then move on. I personally don't know enough about ""what's an input, what's an input-with-defaults, what's a non-input-but-calculated-from-an-input"" to go through the hundreds of lines for a ""quick"" convert.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3700#issuecomment-395052458:654,green,green,654,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3700#issuecomment-395052458,1,['green'],['green']
Energy Efficiency,"> Is this run on all jobs? If so Is it potentially something a user would want to turn off?. Yes, it runs on all jobs by default. I'm not sure if users would explicitly want to turn it off, since it doesn't interfere with anything as far as I can tell, and the pricing issue is virtually non-existent. > Also did you produce that graph manually? Is there a way to generate it easily for a workflow?. Yep, the graph can be easily produced through Stackdriver monitoring console with a few clicks, or a link to it can be constructed programmatically and exposed to the user. The graph is interactive, so there's no need to ""pre-render"" it - it is constructed dynamically by the monitoring console, based on user inputs and/or the link. > Can you include your monitor python/image code in this PR? Would be easier to maintain that way. Sure! Is there a folder path you'd prefer to keep it at? Perhaps I could put it under `supportedBackends/google/pipelines/v2alpha1/src/monitoring`?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451492862:458,monitor,monitoring,458,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451492862,4,['monitor'],"['monitor', 'monitoring']"
Energy Efficiency,"> Looks good to me, once @cjllanwarne's comments are addressed. Thank you!; > Out of curiosity, what kind of cluster/tooling/data processing work are you using at your site?. Actually, we're trying to run gene sequencing tasks using cromwell, meanwhile adopting Volcano as scheduler for our kubernetes clusters.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5184#issuecomment-534574382:273,schedul,scheduler,273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5184#issuecomment-534574382,1,['schedul'],['scheduler']
Energy Efficiency,"> May I get a review on the design, but not the scala?. Wow thanks for adding this. This is exactly what we need on our cluster. Sometimes the scheduler aggressively kills jobs based on VMEM (instead of actual mem). So retrying with upping the memory requirements is a nice way to circumvent this annoying issue. (Instead of using insane memory requirements to make sure it passes in 99% of the cases). As for the design. I would add; 1. A number of attempts configurable parameter in the config. A sane default would be 1. Meaning that this feature will not be used by default, for reasons elaborated on later.; 2. A factor with which the memory is increased on each attempt. So if the factor is 1.5 > Attempt 1 will be 1.5^0 = 1 times the memory, Attempt 2 1.5^1 = 1.5, Attempt 3 = 1.5^2 = 2.25. A sane default here would be 2 I guess. As for @cjllanwarne's concerns:. > The spec defines a maximum memory value, above which Cromwell will never go; ; I think the spec just states the value that should be given to whatever backend. But semantics aside, I guess that means the same as saying it is the maximum. > Cromwell will be able to run WDLs which will not run anywhere else... and thus we would have to be very strict in policing our ""best practices"" WDL to makes sure it can be run on other engines. This is a very good reason to not enable this feature by default. But since there are very good reasons for having this feature, having it as a configurable option will be very very nice. Let the user decide how they want to treat their memory requirements. That is the most user-friendly way to do. This is why I think a sane default for the number of attempts should be 1 (i.e. no retries).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499374511:143,schedul,scheduler,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5017#issuecomment-499374511,1,['schedul'],['scheduler']
Energy Efficiency,"> Should we include a backwards compatibility adapter so that existing configuration still work? It would be sad if someone upgraded Cromwell without reading the release noted and suddenly they have no (recognized) backends configured. I'm not sure about that, since with `v2beta` there was introduced an additional mandatory parameter: `location` (currently supported `us-central1` and `europe-west2`). If we decide to coerce configured v2alpha1 actor factory to v2beta, then we'll also have choose some default value for `location`, which I'd be cautious to do due to possible issues (e.g., someone runs Cromwell in Europe and doesn't want their data to leave Europe)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715:46,adapt,adapter,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715,1,['adapt'],['adapter']
Energy Efficiency,"> This is related to CI Updates PR #4169?. Yes. Cromwell's various libraries and executables are only pushed on develop & hotfix branches, well after one has merged changes in a PR. A number of times PR have been unknowingly breaking the develop/hotfix builds. After I confirmed that #4169 helped develop's ""sbt"" build go green, I submitted this #4181 PR to repair the `34_hotfix` branch. #4180 is a similar PR for `35_hotfix`. Meanwhile, #4179 is a couple of regression tests targeted at future `develop` PRs. During any `push` the ""sbt"" build will ensure that credentials for artifactory exist on disk, and that a docker hub repository exists for to-be-pushed executables.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4181#issuecomment-425737133:322,green,green,322,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4181#issuecomment-425737133,1,['green'],['green']
Energy Efficiency,"> Would it be easy to set it for all text outputs? The other one I'm thinking about is monitoring.log. It's pretty much a per-file thing, so each file needs to be considered on its own. gsutil has some logic to infer file type from extension so a `.txt` file should have the correct content type already. It does look like the monitoring file isn't text/plain and it would be easy to add. @mcovarr what do you think?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518329395:87,monitor,monitoring,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518329395,2,['monitor'],['monitoring']
Energy Efficiency,"> it sounded like it isn't a huge deal, just that there's some nuance to it. - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for `rc` files; - On restart if the `rc` file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job; - Thousands of jobs should NOT ping a scheduler for GridEngine/SLURM/LSF/etc. or it will be overloaded<sup>1</sup>; - It's ok to hit the filesystem [every second](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/supportedBackends/sfs/src/main/scala/cromwell/backend/sfs/SharedFileSystemAsyncJobExecutionActor.scala#L55) for thousands of jobs; - The current `SharedFileSystemJobExecutionActorSpec` looks for `rc` files [for up to ten seconds](https://github.com/broadinstitute/cromwell/blob/d9be2ce0993c21c209c8596f55d1295bc93d1974/backend/src/test/scala/cromwell/backend/BackendSpec.scala#L18-L20). All this can be likely be reconciled by having the tests behave differently from the main code. Ideally, the pseduo-backend running tests should quickly test if the job is done. The ""main"" code could look for the `rc` files every 30s or so, and every once in a while ping the GridEngine/SLURM/LSF/etc. master to check if the job is still alive. ---. <sup>1</sup> It would also be possible to cut down on overloading the scheduler masters by batching requests, as we now do with JES/PAPI.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929:267,schedul,scheduler,267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929,3,['schedul'],['scheduler']
Energy Efficiency,> not sure why this was showing all green with only one review ðŸ¤”. PullApprove audits are always available via the `code-review/pullapprove` [Details](https://pullapprove.com/broadinstitute/cromwell/pull-request/3691/) links. In this case the change fell into `groups.one_reviewer` because of `groups.two_reviewers.conditions.files.exclude: centaur/*`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3691#issuecomment-392093770:36,green,green,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3691#issuecomment-392093770,1,['green'],['green']
Energy Efficiency,"@EvanTheB I think I can answer that for you... much like `check-alive`, the `run-in-background` config point is only relevant for aborts and restarts; cromwell identifies what it needs to kill or restart based on the PID instead of the scheduler job id. The only way that cromwell knows whether a job is done or not is by checking for the existence of the `rc` file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715:236,schedul,scheduler,236,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-380681715,1,['schedul'],['scheduler']
Energy Efficiency,@EvanTheB We use SGE. The problem is our configuration. SGE checks on VMEM instead of actual memory used. This means that a lot of java tools will exceed the memory limits and be killed by the scheduler. In that case there is no RC file. That is why qstat -j should be checked as well. > The problem with just increasing this value is that it also slows checking for the rc file. Maybe we can do this in a more elegant way. I will have a look at your script and also at the cromwell code. It should be trivial to decouple the RC file checking from the check-alive checking. Maybe my colleague @DavyCats has some suggestions as well? Also I know that @cpavanrun uses a similar backend and makes use of this feature. Maybe he also has some suggestions.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488991546:193,schedul,scheduler,193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4905#issuecomment-488991546,1,['schedul'],['scheduler']
Energy Efficiency,@Ghost-in-a-Jar Also take a look at the green `alpha` label at [https://pact-broker.dsp-eng-tools.broadinstitute.org/matrix?q%5B%5Dpacticipant=cromwell-consumer&q%5B%5Dpacticipant=drshub-provider&latest=&mainBranch=&latestby=cvpv&limit=100](https://pact-broker.dsp-eng-tools.broadinstitute.org/matrix?q%5B%5Dpacticipant=cromwell-consumer&q%5B%5Dpacticipant=drshub-provider&latest=&mainBranch=&latestby=cvpv&limit=100),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7147#issuecomment-1577685577:40,green,green,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7147#issuecomment-1577685577,1,['green'],['green']
Energy Efficiency,"@Horneth oh right. Yeah, lets do it as in person unless no one is interested in which case disregard. If it's too much of a hassle to schedule around me feel free to not bother and I'll attend if I can.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2056#issuecomment-285151638:134,schedul,schedule,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2056#issuecomment-285151638,1,['schedul'],['schedule']
Energy Efficiency,"@LeeTL1220 my `reference.conf` database section looks correct:. ```; database {; # hsql default; profile = ""slick.jdbc.HsqldbProfile$""; db {; driver = ""org.hsqldb.jdbcDriver""; url = ""jdbc:hsqldb:mem:${uniqueSchema};shutdown=false;hsqldb.tx=mvcc""; connectionTimeout = 3000; }. # mysql example; #driver = ""slick.driver.MySQLDriver$""; #db {; # driver = ""com.mysql.jdbc.Driver""; # url = ""jdbc:mysql://host/cromwell?rewriteBatchedStatements=true""; # user = ""user""; # password = ""pass""; # connectionTimeout = 5000; #}. # For batch inserts the number of inserts to send to the DB at a time; # insert-batch-size = 2000. migration {; # For databases with a very large number of symbols, selecting all the rows at once can generate a variety of; # problems. In order to avoid any issue, the selection is paginated. This value sets how many rows should be; # retrieved and processed at a time, before asking for the next chunk.; read-batch-size = 100000. # Because a symbol row can contain any arbitrary wdl value, the amount of metadata rows to insert from a single; # symbol row can vary from 1 to several thousands (or more). To keep the size of the insert batch from growing out; # of control we monitor its size and execute/commit when it reaches or exceeds writeBatchSize.; write-batch-size = 100000; }; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016:1189,monitor,monitor,1189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217#issuecomment-298110016,1,['monitor'],['monitor']
Energy Efficiency,"@abaumann said that they're having a serious issue (due to BBQ) with workflows which have become stuck in non-terminal states. Set up some monitor which runs at a configurable period to look through all non-terminal workflows/calls and make sure that things are actually non-terminal, switching to terminal states as appropriate. This solution is viewed as a relatively painless (vs fixing it For Real) way of solving the problem, but if one sees an easier way of doing it that's even better.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/940:139,monitor,monitor,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/940,1,['monitor'],['monitor']
Energy Efficiency,"@aednichols I agree with your point regarding Google. However, I feel like there is a huge conflict of interest here: how can Google motivate itself to fix something that could potentially allow them to make a lot of money? How does Google suggest users should fix this problem? It seems a huge financial risk to include docker images in `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` as the corresponding buckets need to be public and cannot be set as Requester Pays, so anybody can download them at will. Do you have advice for how to best reach out to them to advocate for this?. Replicating images across regions is currently not very sustainable as it would rely on users' good will and understanding of this complicated problem, as Cromwell does not have a framework to automatically understand within a workflow which docker images it should pull. If Google does not get their act together, I suppose that ultimately the Cromwell team has to come to terms with the fact that the `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io` repository solutions are not sustainable and an alternative will need to be engineered and provided to those writing WDL pipelines. Not sure what the easiest solution would be though. Cromwell currently has some framework for dealing deferentially with Files with optional localization when a WDL is run on Google Cloud. Could something be included in Cromwell to allow the WDL to know in which Google cloud the tasks are being run so that at least the best repository could be automatically selected?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364:636,sustainab,sustainable,636,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884342364,2,['sustainab'],['sustainable']
Energy Efficiency,"@aednichols this PR is currently blocked, as I have a few questions Iâ€™m hoping to resolve with your teamâ€™s help, once you have time. The PR Jeff merged is very welcome, but it only solves a tiny portion of this one. As such, Iâ€™ve reverted the scope commit from it, so its only concern now is being able to pass additional information to the monitoring task. If youâ€™d prefer the questions to be raised here instead, please let me know.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-505654233:341,monitor,monitoring,341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-505654233,1,['monitor'],['monitoring']
Energy Efficiency,"@alexfrieden - I just ran this successfully. With your first error, the fact that you are getting an HTTP response indicates that it's not necessarily a networking issue on the AWS side, and as @Horneth stated something going on with DockerHub. For debugging in that scenario, I would try launching a t2.micro based on the CustomAMI you created with the same Batch instance profile in the VPC used by the Batch compute environment and try `docker pull <image>`. As for your second issue, did your tasks eventually transition from RUNNABLE? I've noticed that sometimes it takes about 5-10min for the Batch scheduler to ""re-warm"" after scaling down instances that were used for previous tasks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-435547443:605,schedul,scheduler,605,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4345#issuecomment-435547443,1,['schedul'],['scheduler']
Energy Efficiency,@alexwaldrop NB that I don't work there anymore and sadly haven't had the energy to actively contribute. Perhaps @aednichols can chime in,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-582136461:74,energy,energy,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-582136461,1,['energy'],['energy']
Energy Efficiency,"@antonkulaga I am reviewing the backlog systematically over the next few months so I am using the PO Cleanup label to keep track of Github issues that I am planning to review next or have already reviewed. It's not a reflection of it being fixed or scheduled, it's just for my tracking purposes. . I haven't reviewed this issue yet, good to know that it is still not functioning as intended. . @geoffjentry Is there a workaround to using Docker-Compose?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2284#issuecomment-345821084:249,schedul,scheduled,249,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2284#issuecomment-345821084,1,['schedul'],['scheduled']
Energy Efficiency,"@cjllanwarne ; I'm not sure I fully understand what you mean here. `checkalive` does run/schedule when `isAlive` is called.; Al tough I think what I use now is almost as you did explain. Only difference is that I first do isAlive instead of checking the exitcode. In f30c2be I did switch this. (first exitcode, then isAlive). Before I did had a default timeout of 120 seconds but I did remove that because of earlier comments. I can bring that back in if you want?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423934018:89,schedul,schedule,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423934018,1,['schedul'],['schedule']
Energy Efficiency,@cjllanwarne I asked @Horneth if we could use this during one of the upcoming lunch & learns which i need to schedule,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/505#issuecomment-194339115:109,schedul,schedule,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/505#issuecomment-194339115,1,['schedul'],['schedule']
Energy Efficiency,"@cjllanwarne I'm going to take this the complete opposite direction but I'll give you all the credit as it was what you said that led me here. Specifically it started with your statement "" a WDL author can decide when they want their globs to be exploded ..."" my reaction was ""hell no!"" as the point here is system sanctity and not user desire. If the point is to not allow a large glob to take down a cromwell server then we should not allow users to be the ones deciding what's getting processed in a more/less efficient manner. To bend an old gem of wisdom a bit beyond its original meaning, ""never trust the client"". That got me thinking that I think all of this (including my original post) is coming at this all wrong. To the WDL user they should only ever have to think in terms of `File` and `Array[File]`, but that should imply no specific implementation under the hood. I think some of this goes to how tightly coupled WDL is to implementation in Cromwell and how that fact tends to guide our thinking in certain directions. If instead we *always* treated `Array[File]` as a FOFN behind the scenes in a completely invisible to the user manner we'd be able to keep the simplistic sugar of WDL but go even beyond glob situations when it comes to memory savings. That way both `File` and `Array[File]` are internally managed as just a single file path. For obvious reasons (including your last statement and others that we both made) that wouldn't be a tiny change but I don't think it'd be monumental either.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066:513,efficient,efficient,513,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1777#issuecomment-268795066,1,['efficient'],['efficient']
Energy Efficiency,"@cjllanwarne Something went wrong with the latest release of the womtool.; The cromwell.jar does properly display the CLI usage when run.; ```; $ java -jar womtool-53.jar ; Exception in thread ""main"" java.lang.NoClassDefFoundError: scala/concurrent/duration/Duration; 	at scopt.Read$.liftedTree1$1(options.scala:67); 	at scopt.Read$.<init>(options.scala:66); 	at scopt.Read$.<clinit>(options.scala); 	at womtool.cmdline.WomtoolCommandLineParser.<init>(WomtoolCommandLineParser.scala:30); 	at womtool.cmdline.WomtoolCommandLineParser$.instance$lzycompute(WomtoolCommandLineParser.scala:13); 	at womtool.cmdline.WomtoolCommandLineParser$.instance(WomtoolCommandLineParser.scala:13); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:158); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:166); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:27); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:27); 	at womtool.WomtoolMain.main(WomtoolMain.scala); Caused by: java.lang.ClassNotFoundException: scala.concurrent.duration.Duration; 	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581); 	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178); 	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522); 	... 18 more; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5808:1115,adapt,adapted,1115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5808,1,['adapt'],['adapted']
Energy Efficiency,"@cjllanwarne Thanks for the clarification. I was already wondering why you would negate your own well-written namespace code with a single line... Anyway I created a pull request on the spec here: https://github.com/openwdl/wdl/pull/347, your feedback would be much appreciated. Here's to hoping that it gets unanimously approved :crossed_fingers: . @geoffjentry yes, the Cromwell team has a lot of influence on the spec by implementing or not implementing things. I can understand the temptation to use this for ""the greater good"" :wink: . But I am quite happy that the Cromwell developers chose to be in touch with the community and aggressively implement the development spec in the development version of Cromwell. This allows us to see how certain spec changes turn out *before* they get implemented in production. In this case I came across this when I was testing the code for #5312 and found that I could not set my resource requirements for BWA anymore (in BioWDL all tasks default to the least number of cores needed, and sometimes you want to override this for more power). Since BWA was nested in a subworkflow this turned out not to be possible. So now we can fix the spec and Cromwell before this ever gets into a release. I think it is great work by the Cromwell team. It can't always be easy to follow the spec that closely.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176:1077,power,power,1077,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5317#issuecomment-564426176,1,['power'],['power']
Energy Efficiency,"@cjllanwarne Thanks for the explanation! Exactly right, this is what the script says: ; #!/bin/bash; export _JAVA_OPTIONS=-Djava.io.tmpdir=/cromwell_root/tmp; export TMPDIR=/cromwell_root/tmp; cd /cromwell_root. echo ""Hello foobar!"" && exit 1; echo $? > job.rc.txt. @pgrosu The exit 1 was the purpose here - I was modifying the basic hello.wdl test, trying to do a lightweight test simulating the failure of the binary being called. Turns out, as @cjllanwarne explains, that exit 1 is not a good way to simulate that, because it defeats Cromwell's return-code monitoring. Here's a better test, which does work as expected: . task hello {; String addressee; command {; echo ""Hello ${addressee}!"" && head nonexistent; }; output {; String salutation = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; continueOnReturnCode: true; }; }. workflow w {; call hello; }",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-175739792:560,monitor,monitoring,560,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-175739792,1,['monitor'],['monitoring']
Energy Efficiency,"@cjllanwarne Things in this backend long existed before sfs backend came into being, and we didn't look into it yet. Good point though, I think we might try to adapt this to the sfs backend some time in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246858765:160,adapt,adapt,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1403#issuecomment-246858765,1,['adapt'],['adapt']
Energy Efficiency,"@cjllanwarne commented on [Mon May 22 2017](https://github.com/broadinstitute/wdl4s/issues/112). This problem presents itself when using `wdltool` but it looks like it's a match error coming from inside `WDL4S`. null.wdl:; ```; task empty{; command {}; output {; File out = ""${output}""; }; }; ```. On validate:; ```; $ java -jar target/scala-2.12/wdltool-0.11.jar validate ~/myWorkflows/null.wdl; null; ```. We can see more details when we try to graph it:; ```; $ java -jar target/scala-2.12/wdltool-0.11.jar graph ~/myWorkflows/null.wdl; Exception in thread ""main"" scala.MatchError: null; 	at wdl4s.expression.ValueEvaluator.evaluate(ValueEvaluator.scala:44); 	at wdl4s.WdlExpression$.evaluate(WdlExpression.scala:85); 	at wdl4s.WdlExpression.evaluate(WdlExpression.scala:161); 	at wdl4s.expression.ValueEvaluator.replaceInterpolationTag(ValueEvaluator.scala:20); 	at wdl4s.expression.ValueEvaluator.$anonfun$interpolate$2(ValueEvaluator.scala:33); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:157); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2703:1093,adapt,adapted,1093,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2703,1,['adapt'],['adapted']
Energy Efficiency,"@cjllanwarne you are totally right, if read/write_json worked it would not be such a pain, I would simple write everything to json, give it to a task with some Scala (or whatever language I want) script that return json and then read it to a cromwell Map. >All WDL values are immutable as an early design choice for the language. I do not mind it, I am used to it in Scala, but in Scala I have powerful filter/map/flatMap/foldLeft are you going to give any of them to WDL?. >You can get something similar by using the implicit gather on a scatter. eg I could map over an array to calculate the ""values plus one"" array like this:. Thanks, I did not know that such thing is possible, I thought that all variables declared inside loops/scatter are not visible from outside",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367449569:394,power,powerful,394,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367449569,1,['power'],['powerful']
Energy Efficiency,@danbills we do still need pullapprove to go green prior to merging,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4532#issuecomment-452746834:45,green,green,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4532#issuecomment-452746834,1,['green'],['green']
Energy Efficiency,"@danbills yes IMO the ""powers of 2"" you have here is the usual and expected form of exponential backoff. The other way is technically exponential but doesn't slow down nearly as quickly.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4076#issuecomment-419973403:23,power,powers,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4076#issuecomment-419973403,1,['power'],['powers']
Energy Efficiency,"@dformoso I checked in with the team and this issue is scheduled to be fixed by the time Cromwell releases support for WDL 2.0. At that time, `version development` will be promoted to an officially supported version; before then, `development` should be used with caution & probably not in production.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671439046:55,schedul,scheduled,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5737#issuecomment-671439046,1,['schedul'],['scheduled']
Energy Efficiency,"@dinvlad I just ran the monitoring script test using this ""doctered"" version of your function:; ```; def monitoringTerminationAction(): Action = {; val result = cloudSdkAction; .withCommand(s""/bin/sh"", ""-c"", s""kill -TERM -1 && sleep $monitoringTerminationGraceTime""); .withFlags(List(ActionFlag.AlwaysRun)); .setPidNamespace(monitoringPidNamespace). println(""result""). throw new Exception(result.toPrettyString); }; ```. ... and to my surprise... nothing got printed out and everything seemed to work fine. . Are you sure this function is actually being called from anywhere? If so, what are you doing that the test case is not? And can we engineer a test somehow to test this line of code?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082:24,monitor,monitoring,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555688082,4,['monitor'],"['monitoring', 'monitoringPidNamespace', 'monitoringTerminationAction', 'monitoringTerminationGraceTime']"
Energy Efficiency,"@dtenenba - the space on the scratch mount point (for cromwell it is `/cormwell_root`) is managed by a monitoring tool `ebs-autoscale` that is installed when creating a custom AMI configured for Cromwell, and then referencing that AMI when creating Batch compute environments. Running out of space points to one or more of the following:. * the monitor is not installed; * the monitor is looking at the wrong location in the filesystem. If you've created a custom AMI, I suggest launching an instance with it and checking that the monitor is watching the correct location. Do this by checking the log: `/var/log/ebs-autoscale.log`. If it's not, you'll need to recreate both the AMI and the Batch Compute Environment, and associate the new CE with your Job Queue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468794942:103,monitor,monitoring,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-468794942,4,['monitor'],"['monitor', 'monitoring']"
Energy Efficiency,"@ffinfo As I read it:. - If state is `None`, go to `Running` state; - If state is `Running`, the first thing we do is run `isAlive` (which I don't want to ever do!). That means that I have no way to opt-out of ever running `isAlive` (which is the thing I want before approving this PR). ---. What I was suggesting is (but there are many other ways):; - If I set `pollForAliveness: ""1 minute""` in the config file:; - Use `context.system.scheduler.scheduleOnce` to run an `isAlive` 1 minute in the future (completely separate from `pollStatus`).; - If that `isAlive` is true, schedule again another 1 minute in the future; - If not, record the time at which the job was not alive; - `pollStatus` continues on a different schedule:; - If the job is no longer alive, the `pollStatus` switches to `WaitingForReturnCode`; - If a time limit is set for the `WaitingForReturnCode` state, honor it; - If I set `pollForAliveness: false` in the config file:; - Go straight to `WaitingForReturnCode`; - No time limits for `WaitingForReturnCode`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424371053:436,schedul,scheduler,436,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-424371053,4,['schedul'],"['schedule', 'scheduleOnce', 'scheduler']"
Energy Efficiency,"@ffinfo Hi Peter - apologies for taking so long, the release I mentioned ended up taking a while longer than we thought. I talked to our PO this morning about this pull request and his take was that if this could be hooked up in a way which keeps the tests green (as much as they ever are) and doesn't add noticeable latency in the system for other users (and/or the behavior change is put behind a config option) that he'd be good with this concept. . It's been a month now so it's entirely possible you've already moved on with life or perhaps you have no interest for other reasons so I'll leave it up to you on how to proceed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442:257,green,green,257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-249220442,1,['green'],['green']
Energy Efficiency,"@ffinfo my concern was more that the `isAlive` should be opt-in, not that that timeout should be opt in. . if I'm reading this right (EDIT: I think I got it a bit wrong first time):. - The job enters the `Running` state; - The first time we poll for it, we *always* check whether it's alive; - While it still is, we keep running `isAlive` every time we get polled; - Otherwise we enter the `WaitingForReturnCode`; - After the job is no longer alive, we abandon it after a given timeout and declare it failed; - If no timeout is configured, we keep waiting forever. I think this shouldn't be too much of a refactor:. - The job enters the `WaitingForReturnCode` state; - We immediately schedule an `CheckAlive` message to the actor at the configurable time; - If the cadence is not set, we never send that message (this would be the default); - When that CheckAlive arrives we can run `isAlive` and remember the result (and if we're still alive, schedule another `CheckAlive` again after the same delay); - If the `isAlive` failed, the next poll would return `Failed`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423569265:684,schedul,schedule,684,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423569265,2,['schedul'],['schedule']
Energy Efficiency,"@geoffjentry @cjllanwarne do you think your swagger is in good enough shape now for codegen to work well? Green was hoping to use your client in our next project instead of rolling yet-another-of-our-own, but some of the endpoints we need (top-level query, and labels patch) haven't been implemented.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1855#issuecomment-400669179:106,Green,Green,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1855#issuecomment-400669179,1,['Green'],['Green']
Energy Efficiency,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:778,efficient,efficient-whole-exome-data-processing-using-workflows-on-the-cloud,778,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956,1,['efficient'],['efficient-whole-exome-data-processing-using-workflows-on-the-cloud']
Energy Efficiency,@geoffjentry Did you do anything related to dispatcher tooling for your recent Health monitoring pre-gull PR?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1747#issuecomment-326420760:86,monitor,monitoring,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1747#issuecomment-326420760,1,['monitor'],['monitoring']
Energy Efficiency,"@geoffjentry I removed retry in two spots which hopefully could be added back per this scheme as part of #808. Note that neither of those spots corresponds to GCS auth upload, which should be happening in the initialization actor and is the subject of #806. I actually thought this ticket was meant for the hotfix branch to deal with problems the Greenies had seen, but there doesn't seem to be any more detail or labeling to confirm or refute that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/834#issuecomment-219795863:347,Green,Greenies,347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/834#issuecomment-219795863,1,['Green'],['Greenies']
Energy Efficiency,"@geoffjentry I totally agree with having a singleton actor for load balancing / supervision / monitoring etc.. but I think the actual validation work itself is better handled by a one-shot do-and-die actor than by a singleton actor. I don't think the actor that is responsible for load balancing, error handling etc.. should also be responsible for doing the work it's supervising.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195401589:94,monitor,monitoring,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195401589,1,['monitor'],['monitoring']
Energy Efficiency,"@geoffjentry I understand re: egress charges. In my use case these aren't an issue, so a flag option would still help. Maybe, make egress cost a config option of a filesystem, and only reuse results if the egress cost would be under some user-specified value? You can also drop the requirement of specifying one engine/filesystem for all tasks. You could then return a cached result from any filesystem where it exists, without needing to copy it to a target filesystem. You could then also let workflow inputs point to files on different filesystems, and automatically choose the engine for each job based on where its inputs are.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4616#issuecomment-461576286:37,charge,charges,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4616#issuecomment-461576286,1,['charge'],['charges']
Energy Efficiency,"@geoffjentry I want to resort to authority and say ""Zen of Akka""... . Reasons for my gut feeling: A mutable val makes it look and act more like a state machine, and reduces the risk of accidentally leaking the variable pointer to other threads which may update it out of band. Obviously not likely in this case, but as a muscle memory thing a-la `Some(constant)`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1044#issuecomment-227570413:165,reduce,reduces,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1044#issuecomment-227570413,1,['reduce'],['reduces']
Energy Efficiency,"@geoffjentry Why do we need to reduce the scope of EJEA? What's the current problem? ; We do have the i/o actor, does that help?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-324468923:31,reduce,reduce,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-324468923,1,['reduce'],['reduce']
Energy Efficiency,"@geoffjentry Yeah, I have also hit my share of obscure errors over time in my applications, though by that time the failure-recovery rules usually kicked in to keep the system in a running state, with the periodic subsequent log monitoring and analysis in case certain edge-cases become more prevalent. It is great to hear about the shift towards scaling being explored for the near future, but I think you might have made things unnecessarily hard for yourself. Usually it is much easier to have scaling be built-in from the start into the application, and then tuning through metric-based scaling policies the application-triggered scaling rules, which can be bounded by appropriate upper limits before, or interactively after application deployment. This way one has the benefits of both worlds - controlling costs with scalability capabilities for satisfying possible capacity/performance requirements - but I am sure you are already aware of that as well :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235:229,monitor,monitoring,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-262130235,1,['monitor'],['monitoring']
Energy Efficiency,"@geoffjentry if the use case is really intended for validation criteria on objects which the user sets, I feel the same as you, that this is an abstraction that should not be handled withing wdl/cromwell. . While I understand the use case (we also have toyed around with the idea of this as a feature request) it adds unnecessary boundaries to object types that should be handled at the level of execution and not job submition. . I think what might be of use in these instances, for users (like myself) is using the parameter meta more efficiently to define in writing what constitutes valid entries. . Going back to the idea of objects as typed key Value pairs, I still think this is a valid idea, that has real use cases and purposes. In many cases data must be paired with other corresponding datasets and values. In a scatter operation having these types of structured objects would greatly simplify how we can group data together",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330323656:537,efficient,efficiently,537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2283#issuecomment-330323656,1,['efficient'],['efficiently']
Energy Efficiency,"@geoffjentry my labmates prefer TSV-s with headers and I had to adapt, so I tried loops as a solution. Going through Array[Array[String]] and turning it into Array[Map[String, String]] in a loop looked like the best solution for me. I tried both while loops and scatters and both of them could not change the array announced before the loop.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367444645:64,adapt,adapt,64,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3305#issuecomment-367444645,1,['adapt'],['adapt']
Energy Efficiency,@geoffjentry the travis build is failing because some error handling has changed and so 2 refresh token centaur tests are failing--they should go green once you're rebased onto develop.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248945902:146,green,green,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1438#issuecomment-248945902,1,['green'],['green']
Energy Efficiency,"@grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948:32,adapt,adapter,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948,1,['adapt'],['adapter']
Energy Efficiency,"@horneth Talk to miguel as he was the last to look at this and would have the most up to date information. IIRC where this was left off is that there were a handful of known hotspots, all of the low hanging fruit had been picked and at this point we're talking fundamental changes to how things are being stored/tracked. As you can see from the numbers used it'll vary by the oomph your computer has but you'll want to find a number closer to the ""it starts happening here"" point vs some arbitrarily huge number as IIRC some of the initial chokepoints were less bad than deeper ones but at huge numbers were still horrible to wait through. You'll definitely want to become friends with JProfiler if you're not already friends with it. The lightbend monitor could also be helpful here (although I'm less sure of that for these purposes) but it's probably worth waiting until I'm back for that.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288457149:749,monitor,monitor,749,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-288457149,1,['monitor'],['monitor']
Energy Efficiency,"@jeremiahsavage thank you again for reporting, and in particular for your excellent repro case. The fix for this bug has merged to develop and is scheduled to go out in Cromwell 40.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-482213725:146,schedul,scheduled,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-482213725,1,['schedul'],['scheduled']
Energy Efficiency,"@jsotobroad I believe the integration tests you set up for Green are covered by this ticket, do you agree?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247:59,Green,Green,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247,1,['Green'],['Green']
Energy Efficiency,"@katevoss @geoffjentry @kcibul @dshiga . The HCA has no current need for prioritization of workflows, but has a strong need for the ability to submit jobs without starting them (in a queue) and then start them later on. Our entire infrastructure design relies on this feature existing. This ticket is framed by Kristian above as though that functionality already exists, but from my understanding it does not? . I believe it is @ktibbett and the green team / gp production who really need the prioritization feature, so I'll tag her here to add in their use cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327938812:446,green,green,446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1566#issuecomment-327938812,1,['green'],['green']
Energy Efficiency,"@katevoss Hi Kate. I think there are two aspects to the issue worth considering - the first being how often we hit this problem in practice (I'll get back with you after I ask the production team) and the second being whether the underlying cause has been addressed - which is that relying only on the creation of a file to detect task completion is not robust at least for SGE/PBS type backends where jobs may be killed by the scheduler out-of-process without creating a file. Based only on the release changelog I suspect that the answer to the second is no. I suggest re-using the ""check-alive"" configuration value that's documented as currently used only on cromwell restart, for periodic (but infrequent) polling of the scheduler.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557:428,schedul,scheduler,428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-325070557,2,['schedul'],['scheduler']
Energy Efficiency,"@katevoss This is actually really important, not just for @droazen and @lbergelson ... This issue has cost the Broad $$$ and analysts a lot of time. Not just the people on this issue. And putting retry code into the GATK (or any task for that matter) is bit arduous and actually a more expensive solution, especially when some random code path is missed. Also, retry on memory should do a lot for us to be able to reduce costs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-316791748:414,reduce,reduce,414,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-316791748,1,['reduce'],['reduce']
Energy Efficiency,@kbergin Having this feature will help us remove all of our private docker images from pipeline-tools and will make it much easier to write adapter workflows in the future,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4236#issuecomment-429117857:140,adapt,adapter,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4236#issuecomment-429117857,1,['adapt'],['adapter']
Energy Efficiency,@kcibul I created tickets related to this and scheduled a meeting on Monday to hash them down.; Let me know if that covers this ticket and if so I'll close it.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-272209802:46,schedul,scheduled,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-272209802,1,['schedul'],['scheduled']
Energy Efficiency,"@kcibul now that even have a proposal doc to help reduce GOTC failure modes, it seems this spike/investigation is complete. Closing it for now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-273797012:50,reduce,reduce,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820#issuecomment-273797012,1,['reduce'],['reduce']
Energy Efficiency,"@kshakir One simple possibility for batching that would work for LSF and SLURM (not sure about other schedulers) would be to query the scheduler for all user jobs that are currently running, then compare this to the expected running jobs. The output for multiple jobs is very similar to that for a single job, so parsing should not be much harder. . - On LSF, ~~`check-alive = ""bjobs ${job_id}""`~~ would be replaced by `check-alive = ""bjobs""`.; - On SLURM, ~~`check-alive = ""squeue -j ${job_id}""`~~ would be replaced by `check-alive = ""squeue -u ${user}""`. This scales better but would remove the ability to test for single jobs, but it sounds like this isn't used anyway.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482:101,schedul,schedulers,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482,2,['schedul'],"['scheduler', 'schedulers']"
Energy Efficiency,@ktibbett and @jsotobroad does the Green team use this API? Will you need it in order to adopt Cromwell 25?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-289828222:35,Green,Green,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1324#issuecomment-289828222,1,['Green'],['Green']
Energy Efficiency,@ktibbett is green team using /stats for anything meaningful?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-393918710:13,green,green,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-393918710,1,['green'],['green']
Energy Efficiency,"@mcovarr Thanks for your response. The documentation can be somewhat unclear. I've updated the localization and have kept this inline with my main config for GCP Batch. I am using Cromwell v87. However, while running a job, Iâ€™m encountering issues when Cromwell is attempting to mount my files to a local mount. I have been monitoring the VM and job, it seems Cromwell is unsure of how to handle this: For instance:. **Error 1:**; ```; severity: ""DEFAULT""; textPayload: ""umount: /mnt/2d49bcb009113835140d638a10b535af: no mount point specified.""; timestamp: ""2024-09-26T14:07:54.88114; ```. **Error 2:**; ```; severity: ""ERROR""; textPayload: ""Copying gs://test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai to file:///mnt/disks/cromwell_root/test-cromwell-genomics-resources/references/hg38/v0/Homo_sapiens_assembly38.fasta.fai""; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124:324,monitor,monitoring,324,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7502#issuecomment-2376149124,1,['monitor'],['monitoring']
Energy Efficiency,@mcovarr The ontology change is related to the workflows that need to be run for the demo. It works without but they reduce the parsing time from 10 minutes to less than 30 seconds,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4168#issuecomment-425478951:117,reduce,reduce,117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4168#issuecomment-425478951,1,['reduce'],['reduce']
Energy Efficiency,@mcovarr it was ðŸ’¯ green before i pushed the PR so i'm gonna blame our very stable test suite for now,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3989#issuecomment-411623224:18,green,green,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3989#issuecomment-411623224,1,['green'],['green']
Energy Efficiency,@myazinn hey - the more i look at the rest of my week the more i'm thinking we might as well schedule the call :) Can you email me - `jgentry` with hostname `broadinstitute.org`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-511602109:93,schedul,schedule,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-511602109,1,['schedul'],['schedule']
Energy Efficiency,@nrockweiler requester pays buckets can be used in Cromwell when the Cromwell 32 is released. It is scheduled for this month,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2916#issuecomment-390666394:100,schedul,scheduled,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2916#issuecomment-390666394,1,['schedul'],['scheduled']
Energy Efficiency,"@pshapiro4broad commented on [Tue Aug 01 2017](https://github.com/broadinstitute/wdltool/issues/36). We would like to automate the process of validating a JSON input file against our WDL. Using the `inputs` command is helpful but it would be even easier to use for validation if `wdltool` had a command that generates the JSON schema for the inputs. The format for a JSON schema is here: http://json-schema.org/. ---. @geoffjentry commented on [Tue Aug 01 2017](https://github.com/broadinstitute/wdltool/issues/36#issuecomment-319471781). Not a comment on the actual topic but just a heads up that the `wdltool` repo is one of the dustiest corners in terms of developer attention :). Also since `wdltool` is really just a command line wrapper around `wdl4s`, really any functionality request would involve a ticket there, might be more efficient to cut out the middle man and go there w/ these requests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2882:836,efficient,efficient,836,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2882,1,['efficient'],['efficient']
Energy Efficiency,@pshapiro4broad you flatter me. But yeah we should probably come up w/ a centaur test that makes sure it is actually working as intended. The good news is that our travis PR tests conduct a whole battery of papi v2 tests and those pass so this didn't break anything :smile:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-460273110:196,battery,battery,196,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4609#issuecomment-460273110,1,['battery'],['battery']
Energy Efficiency,"@ruchim @Horneth @aednichols I'm seeing this error pop up running cromwell-35 on SGE, except the timeout is at 60 seconds rather than 10. The error gets repeated a number of times (in the latest log it appears 9 times). The output in question is a glob and there are 80 calls to the task producing it. 2 fastqs get chucked into 20 chunks each, so 40 total. FastQC is run for these chunks once before adapter clipping and once after, so 80 total. There's a bunch of other jobs being run as well, but I'm only seeing this error for this specifc output (`Fastqc.images`). ```; [2018-10-11 13:48:43,66] [error] WorkflowManagerActor Workflow 0a20b0d2-8ad2-43b1-ba92-49e1c39d6578 failed (during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'Fastqc.images': Futures timed out after [60 seconds]; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:858); at scala.util.Success.$anonfun$map$1(Try.scala:251); at scala.util.Success.map(Try.scala:209); at scala.concurrent.Future.$anonfun$map$1(Future.scala:288); at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:29); at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:29); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(Fo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379:400,adapt,adapter,400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-428948379,1,['adapt'],['adapter']
Energy Efficiency,"@ruchim I think @geoffjentry is spot on - adding a scope by itself won't change existing behavior. It's only when the user sets `monitoring_image` to `quay.io/broadinstitute/cromwell-monitor-bigquery`, _then_ it will fail if the SA for the task doesn't have `bigquery.tables.updateData` permission on the monitoring dataset. So existing users on Terra won't be affected, unless we start routinely adding that option to all workflows and don't adjust the IAM permissions on pets.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502242348:183,monitor,monitor-bigquery,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502242348,2,['monitor'],"['monitor-bigquery', 'monitoring']"
Energy Efficiency,"@ruchim I'm not sure about the details, we have a monitor script (https://github.com/HumanCellAtlas/pipeline-tools/blob/c6c11a20c91aa360fcd7ca7c28de14b281cabd7b/adapter_pipelines/ss2_single_sample/options.json#L2) running as workflow options besides the actual RSEM tool, which is monitoring the disk space. it outputs:; ```; /cromwell_root/monitoring.sh: line 15: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 17: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 19: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 13: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 15: echo: write error: No space left on device; /cromwell_root/monitoring.sh: line 17: echo: write error: No space left on device; ``` ; but not exit codes. Do you think it's possible to add some error handling to that bash script to let cromwell know the out of space error during the runtime? Even if it's practical to do that, it may still not as safe as the exit code throw by the actual tool. so wait for @jishuxu's response.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417695517:50,monitor,monitor,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417695517,8,['monitor'],"['monitor', 'monitoring']"
Energy Efficiency,"@scottfrazer So the reason I'm asking about the required functionality and JES (and asked if the main issue was the eventual annoying rebase if this isn't merged) is that my concern is that this is a hefty change mid-sprint when we're already concerned w/ the hairiness of our actual sprint goals. For instance what if this causes some unforeseen issue which causes the s/g to not be complete this sprint. We can handwave all we want about what is truly important or not but the only official metric of importance is what's in our sprint and if this disrupts that's no bueno - and regardless of our confidence level there _is_ a risk here. I suppose we could back it out but that'd still likely end up having been a big time disruption at that point. I would feel a lot more comfortable if a large body of WDL was run against JES backend (and Local too, really - though that's less worrisome) - it'd have been nice if someone decided the integration test battery was important enough to work on the side ;) If people have actually been listening to my requests to paste their interesting WDLs on that ticket that'd be a good start, but double check with @cjllanwarne as he wrote a WDL to exercise all the various functionality we supported at the time. . Actually what'd be really awesome is if you could run the WDL they're using for the demo as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134756661:955,battery,battery,955,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/145#issuecomment-134756661,1,['battery'],['battery']
Energy Efficiency,"@seandavi - The scratch mount point is passed to a daemon that monitors the disk space and attaches more EBS volumes as needed. `/scratch` is the default value for the scratch mount point when creating the AMI. However, for Cromwell you need to specify that this be `/cromwell_root`. See [here](https://docs.opendata.aws/genomics-workflows/cromwell/cromwell-aws-batch/#custom-ami-with-cromwell-additions) for Cromwell specific instructions.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4322#issuecomment-434849219:63,monitor,monitors,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4322#issuecomment-434849219,1,['monitor'],['monitors']
Energy Efficiency,"@wleepang @markjschreiber . I also ran into this issue on several workflows that each ran for 28 hours before failing. Similar to XLuyu, it was in a scattered task. I can't access the logs for the server which failed because Batch terminated it. I suspect that something happened while provisioning the server... through the UserData: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Under that assumption, the fetch_and_run script would have never been installed to the correct location, but the job continued to execute. I see that in some places, you have checks for things such as when the awscli fails to install, then the machine is shutdown. https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/templates/gwfcore/gwfcore-launch-template.template.yaml#L127. Perhaps there should be a validation step to ensure that the machine is correctly provisioned? Alternatively, is it possible to `set -e` directly in the UserData runcmd? I see that `set -e` is set within some scripts, such as `provision.sh`: https://github.com/aws-samples/aws-genomics-workflows/blob/master/src/ecs-additions/provision.sh#L3. Another thought... I see that in the UserData script, there are some calls out to the network. Would it make sense to set AWS_RETRY_MODE=adaptive in such cases to help protect against random network failures?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341:1344,adapt,adaptive,1344,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5872#issuecomment-730119341,1,['adapt'],['adaptive']
Energy Efficiency,A big thank you to @cjllanwarne for his Centaur magic that greened the Travis builds on this PR! ðŸ‡¬ðŸ‡§,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2282#issuecomment-302831033:59,green,greened,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2282#issuecomment-302831033,1,['green'],['greened']
Energy Efficiency,"A bit more info on this. The job mentioned above ran out of disk space. The monitoring.log is full of ""out of space"" errors. However, the job ran to completion and the output directory has an rc file containing 0, so Cromwell considered it a success. But the output files were truncated to zero bytes, presumably due to the disk space issue. Normally we get a hard failure when we run out of disk space but not in this case for some reason.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417449997:76,monitor,monitoring,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4006#issuecomment-417449997,1,['monitor'],['monitoring']
Energy Efficiency,"A rougher GiB-in-integer measure of total memory on the GCE VM due to unexplained but not particularly interesting fluctuations. diffs in the monitoring script:; ```; cromwell mcovarr$ diff <(gsutil cat gs://cloud-cromwell-dev/some/simple_script.sh) <(gsutil cat gs://cloud-cromwell-dev/some/rounding_script.sh); 3,4c3,4; < echo Total Memory: $(free -h | grep Mem | awk '{ print $2 }'); < echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); \ No newline at end of file; ---; > cat /proc/meminfo | grep MemTotal | sed -E 's/[^0-9]+([0-9]+).*/\1/' | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; > echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); ```. the new bits in action (bash arithmetic is integer only, hence the ""19 ... / 10"" and ""20 .. / 10""):; ```; # 1.9 GiB; cromwell mcovarr$ echo $((19 * 1024 * 1024 / 10)) | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; Total Memory: 2G; # 2.0 GiB; cromwell mcovarr$ echo $((20 * 1024 * 1024 / 10)) | awk '{printf ""Total Memory: %1.0fG\n"", $0 / (1024 * 1024)}'; Total Memory: 2G; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5228:142,monitor,monitoring,142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5228,1,['monitor'],['monitoring']
Energy Efficiency,"A significant amount of GotC failures are due to out of memory / disk errors.; Design a mechanism that allow to specify custom retry strategies that can modify runtime parameters based on failure modes. For example, â€œRetry on return code X with double the amount of memory and / or diskâ€. Thoughts:; - `currentAttempt()` wdl function to be used as a variable in a memory / disk formula; - monitor the job (monitoring script ?) to detect disk / memory overflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1847:389,monitor,monitor,389,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1847,2,['monitor'],"['monitor', 'monitoring']"
Energy Efficiency,"A system event monitoring and interrupt system seems way complex to me than implementing some conventions that leverage a standard task definition. At the point of submitting a process to Batch, you should already have all of the information needed to run a task, and can pass this along in a standard way to the standard task definition. This is the same information that is used to create the current job wrapping script. . FYI - the Funnel worker that is wrapped in `batch-task-runner` is modified to run stand-alone from a input JSON file, as opposed to communicating back to a Funnel server for task scheduling and distribution. The modified worker consumes the return codes passes the result back through it's own process and back to Batch. In this way, the sibling process is not hidden from Batch (or Cromwell).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405795490:15,monitor,monitoring,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-405795490,2,"['monitor', 'schedul']","['monitoring', 'scheduling']"
Energy Efficiency,Actually I think @vivster7 was the person to tag from green,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1213#issuecomment-235719562:54,green,green,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1213#issuecomment-235719562,1,['green'],['green']
Energy Efficiency,"Actually it seems that the config I was using already had that line in it, set to 16. I've reduced the concurrent-job-limit to 8, and I'll see how it goes",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-434295499:91,reduce,reduced,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-434295499,1,['reduce'],['reduced']
Energy Efficiency,Adapt engine upgrade Centaur test for horicromtal,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4786:0,Adapt,Adapt,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4786,1,['Adapt'],['Adapt']
Energy Efficiency,Add automatic monitoring/instrumentation of underlying job,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2507:14,monitor,monitoring,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507,1,['monitor'],['monitoring']
Energy Efficiency,Add stuck workflow monitor to 0.19_hotfix,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/940:19,monitor,monitor,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/940,1,['monitor'],['monitor']
Energy Efficiency,Added options to reduce /metadata content. Closes #972,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1014:17,reduce,reduce,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1014,1,['reduce'],['reduce']
Energy Efficiency,"Additionally, I really miss the logging 88 eliminated. With 87 I see the machine type allocated, with 88 I don't. Is there some command line option for turning all the logging back on?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7568#issuecomment-2405624738:86,allocate,allocated,86,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7568#issuecomment-2405624738,1,['allocate'],['allocated']
Energy Efficiency,"Addresses [WX-1306](https://broadworkbench.atlassian.net/browse/WX-1306), [WX-1307](https://broadworkbench.atlassian.net/browse/WX-1307), [WX-1308](https://broadworkbench.atlassian.net/browse/WX-1308), [WX-983](https://broadworkbench.atlassian.net/browse/WX-983). PR creates a GHA (currently runs on dispatch, can be updated to run on schedule of choice) that creates a billing project and BEE, attaches the BEE to a static landing zone, creates a workspace and provisions an app within the BEE, submits a workflow (basic hello world) to Cromwell, and performs app/workspace/billing project cleanup afterwards. BEE template is flagged by Janitor for post workflow cleanup to ensure no lingering resources. Workspace deletion and billing project deletion are finicky due to invariable timing of the deletion itself (can be either extremely short or longer than 12 minutes), so those two steps are handled by either an exception block (workspace deletion) or `continue-on-error` (billing project) to ensure that failures there do not reflect a failure on the test against Cromwell. Workspace provisioning and app creation are necessary for running tests against Cromwell, so a failure there will be reported as a failure on the Cromwell test. (As an aside, this could be rectified by having a static testing app that's always running in a dedicated testing environment. Test could be updated to run submissions against it so as long as that app is kept up to date. [WX-1306]: https://broadworkbench.atlassian.net/browse/WX-1306?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-1307]: https://broadworkbench.atlassian.net/browse/WX-1307?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-1308]: https://broadworkbench.atlassian.net/browse/WX-1308?atlOrigin=eyJpIjoiNWRkNTljNzYxNjVmNDY3MDlhMDU5Y2ZhYzA5YTRkZjUiLCJwIjoiZ2l0aHViLWNvbS1KU1cifQ; [WX-983]: https://broadworkbench.atlassian.net/browse/WX-983?atlOri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7236:335,schedul,schedule,335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7236,1,['schedul'],['schedule']
Energy Efficiency,"Ah, if it's a local sregistry, then the build limits are not the same, and Singularity Hub isn't incurring any charges on Google Cloud :) . Carry on!. Are you running a local sregistry? I put in a PR today to add a keystore, in case you want to test it out :) https://github.com/singularityhub/sregistry/pull/235",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243005:111,charge,charges,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-537243005,1,['charge'],['charges']
Energy Efficiency,All workflow tasks should be monitored until they reach a final state,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861:29,monitor,monitored,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861,1,['monitor'],['monitored']
Energy Efficiency,Allocate more memory for MarkDuplicates task of PreProcessingForVariantDiscovery_GATK4 workflow for test running in AWS [BA-6598],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5869:0,Allocate,Allocate,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5869,1,['Allocate'],['Allocate']
Energy Efficiency,Allocate more vCPUs than specified in WDL file,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6216:0,Allocate,Allocate,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6216,1,['Allocate'],['Allocate']
Energy Efficiency,"Alright, would you prefer to expose it as a workflow (or Cromwell) option? Something like `monitoring_image` and if it's not defined, then the action is skipped. This way, one can also use an alternative image with their custom logic (incl. other monitoring APIs).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451511789:247,monitor,monitoring,247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451511789,1,['monitor'],['monitoring']
Energy Efficiency,Also :+1: once green. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2141/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-294218303:15,green,green,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2141#issuecomment-294218303,1,['green'],['green']
Energy Efficiency,"Also yellow is kind of a ""warny"" color, maybe a happier green? :frog:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/461#issuecomment-186290162:56,green,green,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/461#issuecomment-186290162,1,['green'],['green']
Energy Efficiency,"Also, from a user perspective, it might be nice to include the monitoring script file path in the workflow metadata.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226287321:63,monitor,monitoring,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226287321,1,['monitor'],['monitoring']
Energy Efficiency,"Also, note that Google PD's can be expanded on the fly in seconds, even while the VM is still running under load. I've done this manually on non-FC VMs via the script below. Using this approach combined with a disk space monitoring process (and a size cap!) would allow the job to pass the first time, avoiding a retry. And... if it was also during the algorithm, not just data download, this could eradicate both disk space errors and disk over-provisioning. . https://github.com/broadinstitute/firecloud_developer_toolkit/blob/master/gce/expand_disk.sh. Unfortunately I don't know of a way to hot-swap RAM into the VM.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902:221,monitor,monitoring,221,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1991#issuecomment-325727902,1,['monitor'],['monitoring']
Energy Efficiency,"An addendum to this and #1456 is that a) I should formalize (at least a bit) what I was going a few weeks ago w/ spamming cromwell, monitoring, etc and b) we should go over it as a group such that everyone feels comfortable doing such things",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254583482:132,monitor,monitoring,132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445#issuecomment-254583482,1,['monitor'],['monitoring']
Energy Efficiency,"An early look at the hog factor:. Adds:; - A hog factor in the configuration file representing ""how many greedy users would it take to use up all our resources""; - Higher values protect Cromwell's resources to keep them available for small-scale users; - Lower values let power users get their stuff done as fast as possible; - 1 is equivalent to ""no hog factor"" (and is the default); - Idea: it would be awesome to be able to dynamically scale this up and down based on ""we need to run a workflow"" or ""person X really needs to run their stuff at *full* speed for the next 4 hours""; - The ability to identify a workflow option as indicating hog group ; - A hog group is assigned to every `BackendWorkflowDescriptor` (using the specified workflow option if possible, or 'root workflow ID' if the specified option is not provided); - An update to the `TokenPool` to make it hog-group aware. TODOs:; - [x] Enhance `RoundRobinQueueIteratorSpec` with hog-limit tests; - [x] Enhance `TokenQueueSpec` with hog-limit tests; - [ ] Add full-system tests demonstrating the hog limit in action",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4013:272,power,power,272,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4013,1,['power'],['power']
Energy Efficiency,Any suggestions or plans to reduce the verbosity?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3919#issuecomment-407124690:28,reduce,reduce,28,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3919#issuecomment-407124690,1,['reduce'],['reduce']
Energy Efficiency,"Any thoughts on how a general task retry policy may interact with [#1499](https://github.com/broadinstitute/cromwell/issues/1499) when the job scheduler (or user) kills a job on sge and the server is restarted?. I'm envisioning the ""check-alive"" result showing no job on sge invoking one of the task retries that a user specifies (whereas currently it'd be marked as failed). Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-391433515:143,schedul,scheduler,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161#issuecomment-391433515,1,['schedul'],['scheduler']
Energy Efficiency,"Applying the ""do not merge"" label for now, just in case turning off the awkward tests makes this go green... ðŸ¤”",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5211#issuecomment-540820505:100,green,green,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5211#issuecomment-540820505,1,['green'],['green']
Energy Efficiency,"As a **user running workflows on an HPC cluster**, I want **Cromwell to periodically check that my jobs are still running**, so that I can **know when jobs are alive versus when they reach the runtime limits and are killed by the backend**.; - Workaround: **Yes**; - from @delocalizer ; > The hacky non-async solution I have been using...was to have two check cycles, a frequent cheap one to see if rc existed and occasional expensive one to [poll the scheduler itself](https://github.com/delocalizer/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/backend/pbs/PbsBackend.scala#L128-L166); - Effort: **Small**; - Risk: **Small**; - Business value: **Small to Medium**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328207932:452,schedul,scheduler,452,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328207932,1,['schedul'],['scheduler']
Energy Efficiency,"As a **workflow runner running on Local**, I want **Cromwell to localize one copy of a file from the cloud when I use it multiple times in my workflow**, so that I **am only charged egress to local once**.; - Effort: **?** @geoffjentry ; - Risk: **?** @geoffjentry ; - Be careful that inputs aren't being modified in place before allowing them to be used again; - Business value: **TBD**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-328201762:174,charge,charged,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1348#issuecomment-328201762,1,['charge'],['charged']
Energy Efficiency,"As discussed in https://github.com/broadinstitute/cromwell/issues/6235, developers of workflows for GCP who store their images in Google Container Repositories can be exposed to large Google GCS egress charges when users attempt to run workflows in different continental regions, resulting in many trans-continental container pulls. There currently does not seem to be a satisfactory way to guard against this:. - We can't make our image repositories private because we want to make the workflows available to the public via Terra.; - We can't make the repositories requester-pays because the pipelines API does not support pulling images from requester-pays repositories.; - We can mirror our repositories to different regions, but we are still dependent on our users to configure their workflows to point to the right region and take good-faith extra steps to help us avoid these charges. Some possible ideas were suggested by @freeseek in https://github.com/broadinstitute/cromwell/issues/6235:. - Convince Google to support requester-pays buckets for container pulls in PAPI.; - Modify some combination of Cromwell/PAPI to cache images rather than pulling them for each task that is run.; - Develop infrastructure within Cromwell to know what region the workflow is running in and automatically select the right GCR mirror to pull from.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442:202,charge,charges,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442,2,['charge'],['charges']
Energy Efficiency,"At the moment there's a concept of a submission whitelist for users in CromIAM. Allow for this to be optional for situations where all users are allowed to submit workflows, this will reduce traffic between CromIAM & Sam. Although this CromIAM will be exposed to the internet, the Cromwell service will still be protected from drive-by whitelist-not-specified submissions by the Google authentication. This authentication will be enforced by the proxy.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4475:184,reduce,reduce,184,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4475,1,['reduce'],['reduce']
Energy Efficiency,BW-1222 Reduced test framework dependencies,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6735:8,Reduce,Reduced,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6735,1,['Reduce'],['Reduced']
Energy Efficiency,"Backend: I'm testing out PAPI v2 by running on the cromwell 34 and 36 methods servers.; Problem: inside the docker it looks like /cromwell_root is mounted on /dev/disk/by-id/google-local-disk (checking df -h, /proc/mounts, or /etc/mtab) but that device does not exist (in fact there is no /dev/disk directory).; Background: This task requests a persistent HDD and runs inside a docker. This problem does not exist on cromwell 30 (with jes backend). /cromwell_root is almost certainly actually mounted at /dev/sdb (that device exists, does not appear to be used anywhere, has the appropriate size as checked in /sys/block/sdb/size, and is typically what's listed as the filesystem in cromwell 30). . I know it's weird to even care about that, so to explain, my cromwell monitoring script looks at the block device corresponding to /cromwell_root in order to measure disk IO, which can potentially be a source of problems for some of the SV algorithms we're trying to debug/string together. the .wdl file; ```; workflow GetSystemInfo {; call get_system_info_docker; }. task get_system_info_docker {; command <<<; echo ""**** df -h""; df -h; ; echo; echo ""**** /""; ls -l /; ; echo; echo ""**** /mnt""; ls -l /mnt; ; echo; echo ""**** /dev""; ls -l /dev; ; if [ -d /dev/disk ]; then; echo; echo ""**** /dev/disk""; ls /dev/disk; fi; ; echo; echo ""**** /proc/mounts""; cat /proc/mounts; ; echo; echo ""**** /etc/mtab""; cat /etc/mtab; ; echo; echo ""**** /sys/block""; find -L /sys/block -maxdepth 2; ; echo; echo ""**** /sys/block/sdb/size (converted to integer GB)""; echo ""$(($(cat /sys/block/sdb/size) * 512 / 2**30))""; ; echo; echo ""**** /sys/devices""; find -L /sys/devices -maxdepth 3; >>>; ; runtime {; docker: ""talkowski/delly""; memory: ""1.7 GB""; cpu: ""1""; disks: ""local-disk 250 HDD""; preemptible: 3; }; }; ```; Snips of relevant output from cromwell 36 (edited for brevity):; ```; **** df -h; Filesystem Size Used Available Use% Mounted on; /dev/disk/by-id/google-local-disk; 245.1G 60.0M 245.0G 0% /cromwell_ro",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388:769,monitor,monitoring,769,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388,1,['monitor'],['monitoring']
Energy Efficiency,"Based on extensive log-examination, I believe this change has somehow broken the ability of a CWL workflow to survive restarts. Some suites pass some of the time due to a confluence of (1) getting lucky and avoiding a restart and (2) retries. It is very often the case that a test case only succeeds on the second or third try. Because I don't want anything to slip through the cracks due to probability, I'm not personally going to call this green until I see zero `Could not read from gs://cloud-cromwell-dev/cromwell_execution/travis/` messages.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4203#issuecomment-428253066:443,green,green,443,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4203#issuecomment-428253066,1,['green'],['green']
Energy Efficiency,"Based on the documentation it (at least seems like) there are ways to get this working. For example:. > if the docker image uses a hash, all call caching settings apply normally. You could try a full uri WITH a hash, e.g,. ```; library/ubuntu:latest@sha256:868fd30a0e47b8d8ac485df174795b5e2fe8a6c8f056cc707b232d65b8a1ab68; ```; Then in the case that you provide a tag (e.g., library/ubuntu:14.04):. > Cromwell will attempt to look up the immutable digest of the image with this floating tag. Upon success it will pass both the floating tag and this digest value to the backend. So if you try providing the example above, does it still reduce it to `ubuntu@sha256:868fd30a0e47b8d8ac485df174795b5e2fe8a6c8f056cc707b232d65b8a1ab68`?. > If Cromwell fails to lookup the digest (for instance an unsupported docker registry, wrong credentials, it will run the job with the user provided floating tag. So couldn't you just give it something you know will fail, and then have it use the tag you did provide?. If you wanted udocker to work, it seems more like a bug that cromwell is considering an incomplete uri (just ubuntu and the hash) as ""the correct way."" That tells us nothing about the registry, the start of the namespace (you could consider this like the collection, library) or a tag. Minimally the entire namespace should be honored, and the hash can still be looked up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454577692:635,reduce,reduce,635,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454577692,1,['reduce'],['reduce']
Energy Efficiency,"Below is a timing diagram from a run of this WDL on develop Cromwell. The large turquoise blobs are `BackendIsCopyingCachedOutputs` and the small bright green blobs that immediately follow are `UpdatingCallCache` (the medium sized forest green blobs that follow that are `UpdatingJobStore` and are the subject of a [separate ticket](https://github.com/broadinstitute/cromwell/issues/2085)). For this particular WDL the former consumes slightly over 6 minutes and the latter about 9 seconds, but everything seems to happen in parallel for all shards. . Does this seem acceptable or should this get more attention?. <img width=""1062"" alt=""screen shot 2017-04-21 at 3 11 54 pm"" src=""https://cloud.githubusercontent.com/assets/10790523/25292867/160548a0-26a6-11e7-8573-f6dff2948df3.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/901#issuecomment-296289162:153,green,green,153,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/901#issuecomment-296289162,2,['green'],['green']
Energy Efficiency,"Both test failures appear to be bogus, once @kshakir's work to tag the breaking tests as integration is complete the Travis builds should go green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/354#issuecomment-169402572:141,green,green,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/354#issuecomment-169402572,1,['green'],['green']
Energy Efficiency,"Building docker with `docker build .` generates warnings like. `; [warn] Error extracting zip entry 'scalaz/syntax/ApplicativeBuilder$ApplicativeBuilder3$ApplicativeBuilder4$ApplicativeBuilder5$ApplicativeBuilder6$ApplicativeBuilder7$ApplicativeBuilder8$ApplicativeBuilder9$ApplicativeBuilder10$ApplicativeBuilder11$ApplicativeBuilder12$$anonfun$tupled$11.class' to '/cromwell/filesystems/gcs/target/streams/$global/assemblyOption/$global/streams/assembly/f51a334150f68ddb35ece4ef3954cb923f3f7ed9_8c5a159afa2afdeb4a64f13d1087eb8c913e47ea_da39a3ee5e6b4b0d3255bfef95601890afd80709/scalaz/syntax/ApplicativeBuilder$ApplicativeBuilder3$ApplicativeBuilder4$ApplicativeBuilder5$ApplicativeBuilder6$ApplicativeBuilder7$ApplicativeBuilder8$ApplicativeBuilder9$ApplicativeBuilder10$ApplicativeBuilder11$ApplicativeBuilder12$$anonfun$tupled$11.class': java.io.FileNotFoundException: /cromwell/filesystems/gcs/target/streams/$global/assemblyOption/$global/streams/assembly/f51a334150f68ddb35ece4ef3954cb923f3f7ed9_8c5a159afa2afdeb4a64f13d1087eb8c913e47ea_da39a3ee5e6b4b0d3255bfef95601890afd80709/scalaz/syntax/ApplicativeBuilder$ApplicativeBuilder3$ApplicativeBuilder4$ApplicativeBuilder5$ApplicativeBuilder6$ApplicativeBuilder7$ApplicativeBuilder8$ApplicativeBuilder9$ApplicativeBuilder10$ApplicativeBuilder11$ApplicativeBuilder12$$anonfun$tupled$11.class (File name too long); `. It appears this is because the max filename under docker is ~242 characters, but the sbt default for max generated class name is ~254/255. See https://github.com/docker/docker/issues/1413. The fix is to reduce this as described here . http://stackoverflow.com/questions/28565837/filename-too-long-sbt",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1428:1574,reduce,reduce,1574,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1428,1,['reduce'],['reduce']
Energy Efficiency,Bump file read timeout to maybe reduce test failures,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4036:32,reduce,reduce,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4036,1,['reduce'],['reduce']
Energy Efficiency,CROM-6718: FR: Call Caching - Add flag for minimizing chance of GCP cross-region network egress charges being incurred,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6324:96,charge,charges,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6324,3,['charge'],['charges']
Energy Efficiency,"Can you rebase and see if centaur on travis-ci/push is happier? I suspect your branch contains stale develop code, since travis-ci/pr is green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1904#issuecomment-276699905:137,green,green,137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1904#issuecomment-276699905,1,['green'],['green']
Energy Efficiency,"Centaur test failure was in an unrelated test (see below, https://broadworkbench.atlassian.net/browse/CROM-6744). The tests that I modified passed, and I am rerunning the failed stage. Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: a4d90928-3f33-4be6-87d7-a3d01a0b73[...]); 19:12:57.605 [daemonpool-thread-43] INFO centaur.api.CentaurCromwellClient$ - Submitting cwl_relative_imports_url returned workflow id 39a765a7-4294-4da4-a059-5204d29a5abf; 19:13:02.771 [pool-3-thread-1] ERROR centaur.reporting.Slf4jReporter - Test 'successfully run cwl_relative_imports_url' failed on attempt 3 of 3 with workflow id '39a765a7-4294-4da4-a059-5204d29a5abf' ; centaur.test.CentaurTestException: Unexpected terminal status Failed while waiting for one of [Running, Succeeded] (workflow ID: 39a765a7-4294-4da4-a059-5204d29a5abf); 	at centaur.test.CentaurTestException$.apply(CentaurTestException.scala:34); 	at centaur.test.Operations$$anon$15.$anonfun$status$2(Test.scala:294); 	at centaur.test.Operations$$anon$15.$anonfun$status$2$adapted(Test.scala:292)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499:1069,adapt,adapted,1069,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6557#issuecomment-961386499,1,['adapt'],['adapted']
Energy Efficiency,Closing for now because CWL support in WaaS / Womtool is now scheduled to land some time after the essential WaaS functionality for WDL has shipped. See https://github.com/broadinstitute/cromwell/issues/4333,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4285#issuecomment-442099426:61,schedul,scheduled,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4285#issuecomment-442099426,1,['schedul'],['scheduled']
Energy Efficiency,Closing this given the fact that the latest QA testing framework in the form of a Jenkins build is now green: https://fc-jenkins.dsp-techops.broadinstitute.org/view/CromIAM-Testing/job/Taurus-Gatling-Test-Pipeline/237/,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4268#issuecomment-450773870:103,green,green,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4268#issuecomment-450773870,1,['green'],['green']
Energy Efficiency,"Closing, since we've implemented Cromwell metadata export via a Cloud Function. For details, see https://github.com/broadinstitute/cromwell-task-monitor-bq#metadata-upload",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-528065027:145,monitor,monitor-bq,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-528065027,1,['monitor'],['monitor-bq']
Energy Efficiency,"Codecov isn't a mandatory check, we just don't have a way to tell Github to display the number without a red or green check.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887936453:112,green,green,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7355#issuecomment-1887936453,1,['green'],['green']
Energy Efficiency,Compilation included at no extra charge in #2649,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2648#issuecomment-331915147:33,charge,charge,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2648#issuecomment-331915147,1,['charge'],['charge']
Energy Efficiency,"Concoct an SQL table schema which can handle efficiently looking up any information currently provided by the Cromwell endpoints which read from the metadata service. This might get hairy with query and/or metadata, but see what can be done. This schema will be realized in a CloudSQL database. NB: Google PubSub does not guarantee ordering of events. We should already be robust to this via the CRDT structures and such, but take that into account w/ this table structure. I could imagine there being situations where e.g. â€œtimestamp from the payload of last event I processed which led to this stateâ€ might also be good to track or something like that. If you find yourself looking for ideas on how to handle ordering issues, [this Google doc](https://cloud.google.com/pubsub/docs/ordering) provides some examples for all of the situations we have",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3242:45,efficient,efficiently,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3242,1,['efficient'],['efficiently']
Energy Efficiency,"Cool. 59 is scheduled to go out before March 26. I'll edit the changelog once I figure out if the lab wants to:; - wait for us to finish fixing our CI and patch 58, or; - take a 59-SNAP and this fix may not be hotfixed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800644653:12,schedul,scheduled,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800644653,1,['schedul'],['scheduled']
Energy Efficiency,"Created in collaboration with: @TMiguelT. The OpenWDL spec states when interpolating a string in the command block:; > Within interpolations, string concatenation with the + operator has special typing properties to facilitate formulation of command-line flags. [...] If either operand has an optional type, then the concatenation has type String?, and the runtime result is None if either operand is None. Ie:; - `string + null + string -> null`:. That is, if `str` is not defined (`null`), the following should resolve to null and empty:; ```; ~{'""--prefix"" ""' + str + '""'}; ```. Currently, it's resolving to `""` (a single double-quote). Eg: In the task:. ```wdl; version development; task quotetest {. input {; String? str; }. command <<<; echo ~{'""--prefix"" ""' + str + '""'}; >>>. output {; String out = read_string(stdout()); }; }; ```. A fix to this would reduce our usages of `if defined(name) then """" else """"`.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5460:861,reduce,reduce,861,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5460,1,['reduce'],['reduce']
Energy Efficiency,Cromwell Logs: reduce and redirect,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1012:15,reduce,reduce,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1012,1,['reduce'],['reduce']
Energy Efficiency,"Cromwell engine with Google Cloud backend provides support for so-called *monitoring script* that can be used to monitor virtual machine / container stats while cromwell task command is being executed. The monitoring script launches asynchronously right before the task command and ends right after the command has finished. The monitoring scripts does help a lot in the monitoring processes but it cannot be used to add some common initialization for all cromwell tasks as long as it is launched asynchronously. Nevertheless a possibility to have support for some common initialization logic for all cromwell tasks can be of help. For example, if most of the workflow tasks uses filesystem mounts then their initialization can be either specified in the beginning of each task or it can be specified in a single place, so-called *initialization script*. The support for *initialization script* is inspired totally by the *monitoring script* and the implementation is pretty the same. Initialization script can be specified using the `init_script` workflow option.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5342:74,monitor,monitoring,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5342,6,['monitor'],"['monitor', 'monitoring']"
Energy Efficiency,"Cromwell has REST api to query the workflow status,'RUNNING','Aborted' etc. Does Cromwell has any event system to subscribe these events? With these events, outside apps can monitor the workflow in real time.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6756:174,monitor,monitor,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756,1,['monitor'],['monitor']
Energy Efficiency,"Cromwell is a scheduler at heart and wants to dispatch jobs to a backend that it doesn't manage, like cloud or HPC. You may have better results with [MiniWDL](https://github.com/chanzuckerberg/miniwdl) for the jobs-on-laptop use case.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416447360:14,schedul,scheduler,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6998#issuecomment-1416447360,1,['schedul'],['scheduler']
Energy Efficiency,"Cromwell will need to provide an endpoint called status which will return a 200 with other information on subsystems. For examples of how this was implemented in Rawls, see their [StatusService](https://github.com/broadinstitute/rawls/blob/eccaae155ef7a7bba7cbd7403405beb155a723a9/core/src/main/scala/org/broadinstitute/dsde/rawls/status/StatusService.scala) and [HealthMonitor](https://github.com/broadinstitute/rawls/blob/eccaae155ef7a7bba7cbd7403405beb155a723a9/core/src/main/scala/org/broadinstitute/dsde/rawls/monitor/HealthMonitor.scala). Subsystems to monitor include the CloudSQL database, Dockerhub, GCS and PAPI. If you can think of something else which seems useful feel free to add it in. This endpoint should also be exposed via CromIAM w/o needing authZ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2432:515,monitor,monitor,515,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2432,2,['monitor'],['monitor']
Energy Efficiency,"Current known issues (feel free to add/remove/edit):; - [ ] Today the existing BatchCompute cluster consists of 1 pre-allocated machine slowing down parallel CI tests (run `bcs c` to see the current size). `OnDemand` clusters are available but take time to spin up the VM instance even [without docker](https://github.com/broadinstitute/cromwell/issues/3518).; - [ ] Like all integration tests there may be intermittent failures/timeouts connecting to external resources. While retry support could be copied out of the PAPI backends and into each backend, once [retries are available across all backends](https://github.com/broadinstitute/cromwell/issues/3161) the CI should be setup to retry failures.; - [ ] The BCS backend is leaking _some_ finished and failed jobs, hitting the job quota after a day or two. It's possible a [nightly cron job](https://github.com/broadinstitute/cromwell/issues/3555) could clean out the leaked jobs but for users this issue should really be fixed elsewhere.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3554:118,allocate,allocated,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3554,1,['allocate'],['allocated']
Energy Efficiency,"Currently we have a lot of information that gets logged by Cromwell and some of it seems like it may not be useful to our customers. For example, every actor state transition gets logged, but it's unclear who is using that information. . We tech talked today an the suggestion was to meet with some key customer's, figure out if there are aspects of our logging they don't need to help reduce, and instead just redirect the unwanted logging to debug level, so that it can still be used for debugging purposes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1012:386,reduce,reduce,386,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1012,1,['reduce'],['reduce']
Energy Efficiency,"Currently when a user issues a request to abort a workflow, the process is purely in-memory. This means that there's no way to manually trigger an abort other than the web endpoint. Instead, do something similar to the workflow store. Record the request in a table, and have cromwell monitor that table looking for workflows it is running, and when it sees such a thing use that to trigger the abort. . It is **not** a bad state if there's a workflow in the table which Cromwell doesn't know about. Make sure any updates to this table are [locked](https://github.com/broadinstitute/cromwell/issues/3342)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3344:284,monitor,monitor,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3344,1,['monitor'],['monitor']
Energy Efficiency,"Currently, monitoring only functions on google VMs and not on local cromwell runs (the documentation is not clear on that). . The mint team is developing pipelines where we would like to use a monitoring script to judge ram and disk usage to better estimate runtime specifications. It would be helpful if it could be enabled for local use as well as VM usage (I hear this isn't hard). . Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2642:11,monitor,monitoring,11,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2642,2,['monitor'],['monitoring']
Energy Efficiency,"DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; â€‚â€‚at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; â€‚â€‚at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; â€‚â€‚at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; â€‚â€‚at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; â€‚â€‚at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; â€‚â€‚at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 2016-05-10 11:38:08,737 cromwell-system-akka.actor.default-dispatcher-3 INFOâ€‚â€‚- WorkflowActor [UUID(972b838f)]: persisting status of CollectUnsortedReadgroupBamQualityMetrics:10 to Failed.; 2016-05-10 11:38:08,738 cromwell-system-akka.actor.default-dispatcher-3 ERROR - WorkflowActor [UUID(972b838f)]: Read timed out",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/826:8003,Adapt,AdaptedForkJoinTask,8003,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/826,1,['Adapt'],['AdaptedForkJoinTask']
Energy Efficiency,"Dear Cromwell dev team,. This is an enhancement suggestion. . When using the google backend for resources allocation, one can specify `gpuCount` and `gpuType` to request for specific resources. I am currently trying to design a task that optionally needs to access a GPU (function of input/parameters). I tried different approach to dynamically schedule GPUs, but `gpuCount` seems constrain to a non-null positive integer. https://github.com/broadinstitute/cromwell/blob/bfef756ca35b46570dff3fda57f77dd4b2b0d25c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L190 . https://github.com/broadinstitute/cromwell/blob/bfef756ca35b46570dff3fda57f77dd4b2b0d25c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/GpuValidation.scala#L28-L40. To allow for dynamic access to GPUs, I propose to extend `gpuCount` type to allow for a null value, and to check for a non-null value for resource allocation. https://github.com/broadinstitute/cromwell/blob/bfef756ca35b46570dff3fda57f77dd4b2b0d25c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/PipelinesApiRuntimeAttributes.scala#L193. Please let me know if such a feature is not desired for any reason. . \* I tried accessing the Jira tracker but `doesn't have access to Jira on broadworkbench.atlassian.net.`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6679:345,schedul,schedule,345,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6679,1,['schedul'],['schedule']
Energy Efficiency,"Dear WDL team,. I am using joint-discovery-gatk4.wdl for running jointcalling on 6000 plus samples using LSF as scheduler. I have lsf configuration file which was working with small set of samples. When I tried running Jointcalling on 6000+ samples using WDL, it is giving the below error.; ; **[2019-12-04 10:59:03,89] [ESC[38;5;220mwarnESC[0m] JobExecutionTokenDispenser - High load alert. Freeze token distribution.**; ; And I changed the concurrent-job-limit = 5000 in lsf configuration, now it is giving; ; **[2019-12-01 07:08:46,91] [info] WorkflowExecutionActor-b2c84d70-611d-4dad-bb18-78a6648e4113 [^[[38;5;2mb2c84d70^[[0m]: Starting JointGenotyping.ImportGVCFs (195 shards); [2019-12-01 07:15:32,84] [^[[38;5;1merror^[[0m] Failed to summarize metadata; java.sql.SQLException: java.lang.OutOfMemoryError: GC overhead limit exceeded**. Could you please help me to proceed further. Thanks In Advance; Fazulur Rehaman",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5305:112,schedul,scheduler,112,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5305,1,['schedul'],['scheduler']
Energy Efficiency,"Deleting some comments due to being interspersed with untrue things. @LeeTL1220 . As per #1762 the intention was to have spec mandated minimums and implementation level maximums. The former never happened so technically it's not part of the spec at all. And as I noted, Cromwell team is no longer in charge of the WDL spec, so ... That said, it's tunable. You can increase it if you want. I wouldn't recommend going all that high unless you're willing to really jam a lot of memory in there. As per your iterator comment, I go back to the Cromwell team doesn't control WDL anymore and there's no WDL construct which would allow that. There's been chatter about things which might help but they're unlikely to arrive until after WDL 1.0",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338350853:300,charge,charge,300,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338350853,1,['charge'],['charge']
Energy Efficiency,"Depending on what your monitoring script does and how long your command takes to run it's possible that the task finishes before the monitoring script had time to write / flush anything into the monitoring log.; I ran this and got an empty log . ```; task t {; command {; echo ""hey""; }. runtime {; docker: ""ubuntu:latest""; }; }; workflow w {; call t; }; ```. but this gave me a non-empty one . ```; task t {; command {; sleep 5; }. runtime {; docker: ""ubuntu:latest""; }; }; workflow w {; call t; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226331034:23,monitor,monitoring,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226331034,3,['monitor'],['monitoring']
Energy Efficiency,Differences of job scheduling between using cromwell+PBS backend and using PBS alone,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6339:19,schedul,scheduling,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6339,1,['schedul'],['scheduling']
Energy Efficiency,"Documenting my review process here. For dependency updates, I like to review the changelog. In this case, we're going from `0.61.0-alpha` to `0.124.8` which is a large jump, but that doesn't tell the whole story. * This looks like a lot of releases to check. For sure, checking every release manually is not practical; we'll have to rely on their release notes.; * Until `0.120.0`, this library used to be included in a [monorepo-ish repo of Java libraries](https://github.com/googleapis/google-cloud-java) which appears to have had a regular 2-week release cycle. Not every release had changes to the `google-storage-nio` library. In fact, browsing the release notes, I found only a handful that mentioned changes to storage NIO. These all looked very innocent to me.; * After `0.120.0`, the library code moved to its own [repo](https://github.com/googleapis/java-storage-nio). Releases there have been less frequent and more irregularly scheduled, but still largely consist of dependency updates. (It's possible that _those_ dependency updates introduce unexpected behaviors in `java-storage-nio`, but there's only so much we can audit).; * Cromwell was briefly running with `0.123.8` until the bug mentioned here was discovered. Not knowing when that bug was introduced, we rolled all the way back. Now, we are pretty confident that it was introduced in [`0.122.0`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.122.0) and fixed in [`0.123.13`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.13).; * Again looking at releases that are not just dependency updates, nearly all of the changes look very innocent to me. In fact, updating to at least [`0.123.23`](https://github.com/googleapis/java-storage-nio/releases/tag/v0.123.23) will give us the benefit of a [fix](https://github.com/googleapis/java-storage-nio/pull/841) to a requester-pays problem that we encountered ourselves.; * There's only one other post-`0.120.0` [change](https://github.com/googleapis",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452:939,schedul,scheduled,939,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6804#issuecomment-1184386452,1,['schedul'],['scheduled']
Energy Efficiency,Does anyone know what is wrong with codecov? My changes shouldn't have reduced test coverage that much.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6072#issuecomment-731197096:71,reduce,reduced,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6072#issuecomment-731197096,1,['reduce'],['reduced']
Energy Efficiency,Don't bother reviewing this - once travis goes green i'm going to push it,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1792:47,green,green,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1792,1,['green'],['green']
Energy Efficiency,Don't let awkward scheduled actions wreck test cases,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4898:18,schedul,scheduled,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4898,1,['schedul'],['scheduled']
Energy Efficiency,Double summarization to reduce skipping rows,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4672:24,reduce,reduce,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4672,1,['reduce'],['reduce']
Energy Efficiency,"EDIT: Changed A/C to use default sentry style configuration, instead of wiring custom HOCON configs. **Issue:**; Whenever Cromwell generates a warning or error message an additional message is emitted from `raven-logback` about a ""suitable DSN"". ```; [2018-05-18 21:17:10,79] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-05-18 21:17:10,80] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; ```. This appears to be because `raven-logback` is activated in logback.xml but is not configured by default in Cromwell. **Background:**; [Sentry](https://sentry.io/) describes itself as:. > Open-source error tracking that helps developers monitor and fix crashes in real time. Cromwell is using an deprecated version of the Sentry java bindings for logback called `raven-logback`. The current bindings are called `sentry-logback`. Additionally, the cromwell docs currently mention that sentry can be setup via the ""configuration value"" `sentry.dsn`. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Logging.md#L48. https://github.com/broadinstitute/cromwell/blob/b8d3d2fd4a583d3e46394efb104005c12cdf182d/docs/Configuring.md#L345-L355. This is not correct as `raven-logback` nor its underlying library `raven` use Typesafe Config. Instead for `raven` the value must be set as a system property, or alternatively as a different environment variable. However the latest `sentry` library (and transitively `sentry-logback`) do allow code configuration via `Sentry.init`. **A/C:**; - Replace `raven-logback` dependency with `sentry-logback`; - ~Allow setting a `cromwell.sentry.*` stanza with Cromwell specific sentry configuration. Alternative namespaces could be `sentry.*` or `system.sentry.*`, but both namespaces may collide with other library/application configurations in the future!~; - ~Wire the `cromwell.sentry.*` HOCON fields into `Sentry.init`~; - ~Default the sentry DSN in `reference.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3657:700,monitor,monitor,700,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3657,1,['monitor'],['monitor']
Energy Efficiency,"Edit (by @cjllanwarne) in light of #4806:. Following #4806 we will be able to read Google project metadata to specify a VPC network and subnet. Therefore what will remain for ***this*** ticket is making the same functionality available on a per-workflow basis... eg an ability to supply the same network/subnetwork information via workflow-options?. ---. ### Original issue text:. https://cloud.google.com/vpc/docs/vpc -- for a primer on GCP Subnets. Users should be able to tell Cromwell to launch nodes into a subnet. For environments like Firecloud, we should have some mechanism (like maybe SAM) to make sure the user actually has the right to use a particular subnet. . The main reason to do this is https://cloud.google.com/vpc/docs/using-flow-logs -- we want to be able to monitor traffic in and out of the network for more significant audited environments. So the driver is ultimately ""compliance"". But it's probably a good idea anyhow. After this is done, please work with FC team to make sure they can take advantage of this. I'm not sure who to tag to make sure this cross-team work is done.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4070:780,monitor,monitor,780,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4070,1,['monitor'],['monitor']
Energy Efficiency,Enable WB health monitor to report status on PAPIV1 + PAPIV2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4206:17,monitor,monitor,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4206,1,['monitor'],['monitor']
Energy Efficiency,"Er, yes that was in a fork that I appear to have deleted... :flushed: although it wouldn't be useful as-is (was?) anymore because it was from back in the day when all the different sharedfilesystem backends were implemented in code, not defined in configuration as they are now. Last comment of @kshakir [above](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929) summarizes the situation perfectly for a within-cromwell solution. If I were going to work around this now I would `cron` up a simple script that:. 1. Makes API call to query the cromwell service for running jobs; 2. Finds all the corresponding `stdout.submit` files in the cromwell job task call execution directories to get scheduler job ids for the cromwell job; 3. Asks the scheduler for the alive-or-dead status of those scheduler job ids and if not alive, aborts the cromwell job via API call",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578:717,schedul,scheduler,717,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-360318578,3,['schedul'],['scheduler']
Energy Efficiency,"Exactly. When you use Akka's scheduler it's not blocking a thread, which leads to much happiness.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1279#issuecomment-238992929:29,schedul,scheduler,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1279#issuecomment-238992929,1,['schedul'],['scheduler']
Energy Efficiency,Excellent! The build failures are due to the MySQL quirks addressed by #160; once that's merged to scatter-gather and this is rebased I expect the build will go green.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/161#issuecomment-135916384:161,green,green,161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/161#issuecomment-135916384,1,['green'],['green']
Energy Efficiency,"Extending mcovarr's work in #6366 . Big shoutout to mcovarr!!!. [Per @mbookman]; This pull request is an initial update to address:. CROM-6718: FR: Add flag for minimizing chance of GCP cross-region network egress charges being incurred. This PR specifically focuses on the risks of egress charges incurred due to call caching. The framing of the approach here, which is a bit broader than originally noted in CROM-6718, is:; Make call caching location-aware, prioritizing copies that minimize egress charges.; Add a workflow option enabling control of what egress charges can be incurred for call cache copying.; The new workflow option would be:. call_cache_egress: [none, continental, global]. where the values affect whether call cache copies can incur egress charges:; none: only within-region copies are allowed, which generate no egress charges; continental: within content copies are allowed; within-content copies have reduced costs, such as $0.01 / GB in the US; global: copies across all regions are allowed. Cross-content egress charges can be much higher (ranging from $0.08 / GB up to $0.23 / GB). ### CURRENT STATUS OF PR:; With the changes in this PR, Cromwell successfully checks the location of the source and destination file to be copied, compares the location, and makes a decision of whether or not it should be copied based on the call_cache_egress option. If it should be copied, the files are copied as normal. If it should not be copied, the cache attempt fails and the workflow runs instead.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6369:214,charge,charges,214,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6369,16,"['charge', 'reduce']","['charges', 'reduced']"
Energy Efficiency,FR: Localization - Add flag for minimizing chance of GCP cross-region network egress charges being incurred [CROM-6764],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6332:85,charge,charges,85,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6332,1,['charge'],['charges']
Energy Efficiency,Failed attempt to allocate memory only when run via cromwell,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4465:18,allocate,allocate,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4465,1,['allocate'],['allocate']
Energy Efficiency,"Feature request from @gsaksena:. Often times a job is preempted in under 10 minutes, it would be nice to retry those jobs more than the runtime attribute for preemptible. Users are not charged within the first 10 minutes, so it is a cheap way to try to save costs while only taking a bit more time. . From Google, it is likely that if a machine will be preempted, it will be within the first 10-20 minutes.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2167:185,charge,charged,185,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2167,1,['charge'],['charged']
Energy Efficiency,Feel free to merge as soon as we get 4 green checkboxes. I know I'd love to get this in my other branch.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1206#issuecomment-235476027:39,green,green,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1206#issuecomment-235476027,1,['green'],['green']
Energy Efficiency,"Figuring that out is what this ticket is for :); Right now it involves at least having a cromwell available, a github access token, running the release WDL, monitoring that everything goes well and that the WDL succeeds. This could be automated using jenkins for example.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328:157,monitor,monitoring,157,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2403#issuecomment-333242328,1,['monitor'],['monitoring']
Energy Efficiency,"Fingerprint just uses 10MB. And you can set it lower if you like. There is a fingerprint-size option. ; Strange that just the fingerprinting alone already gives so much load on the filesystem. . Did you try limiting the threads on your cromwell instance? You can set them like this:; ```; akka {; actor.default-dispatcher.fork-join-executor {; # Number of threads = min(parallelism-factor * cpus, parallelism-max); # Below are the default values set by Akka, uncomment to tune these. #parallelism-factor = 3.0; parallelism-max = 3; }; }; ```; This will limit the amount of threads to 3. So cromwell can only handle 3 files at the same time. That should massively reduce the load on your filestorage server.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040396102:663,reduce,reduce,663,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1040396102,1,['reduce'],['reduce']
Energy Efficiency,Fix flaky health monitor test,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259:17,monitor,monitor,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259,1,['monitor'],['monitor']
Energy Efficiency,Fix polling when jobs are killed by HPC scheduler,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499:40,schedul,scheduler,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499,1,['schedul'],['scheduler']
Energy Efficiency,"Fixed by https://github.com/broadinstitute/cromwell/pull/2808, although other jobs are not stopped when a job fails, they're left as is and are being monitored until they reach completion.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2029#issuecomment-342580733:150,monitor,monitored,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2029#issuecomment-342580733,1,['monitor'],['monitored']
Energy Efficiency,Fixup for monitoring logs,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2529:10,monitor,monitoring,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2529,1,['monitor'],['monitoring']
Energy Efficiency,"Flexible date times no longer work in the cromwell 0.20. (e.g. ""2016-06-20"" is no longer a valid date). Either update the README and notify customers of this change or revert to the flexible date time parser. On behalf of green team, Vivek votes for keeping the stricter date time parser.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1064:222,green,green,222,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1064,1,['green'],['green']
Energy Efficiency,"Follow up on https://github.com/broadinstitute/cromwell/pull/4112. This will reduce the load on the JVM a lot. I did indeed a stress test on our system with 50.000 async qsub/qstat jobs but this was outside the jvm. Inside the jvm this ends up in blocking threads to cromwell. When the timeout is set to 120 seconds, `isAlive` will only run once each 120 seconds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4220:77,reduce,reduce,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4220,1,['reduce'],['reduce']
Energy Efficiency,"Followed up with @ruchim via email and we poked around the codebase this afternoon and think we have a good idea on how to add this functionality. First though we wanted to run the general plan by you guys and had a couple of questions. . It seems like the place to staple this in would be as an additional `Action` in `GenomicsFactory` (as was alluded to above). We weren't sure if it made sense to staple it in as an additional `monitoringAction` or `userAction` as they are both handled slightly differently than this would be. Neither fits perfectly, but it seems like it could potentially be made to work in either place. . Another alternative (maybe the cleanest one?) would be to just introduce a `remoteAccessAction` or similar helper that builds up an action just for this purpose. We wouldn't want to change the default behavior and/or have this `ssh` server always running, so it seems to make sense to make it configurable. We figured the PAPI section of the conf file would be a reasonable place to add this new option. Does that generally sound in line with how you guys would approach this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-494561050:431,monitor,monitoringAction,431,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-494561050,1,['monitor'],['monitoringAction']
Energy Efficiency,"Following on from the conversation at #4039, I've started a ""Getting started with Containers"" on the Cromwell docs. My thoughts are it might be a good place to put thoughts about containers, how they can be used and any caveats about them. This tutorial follows the [user goal](https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-330341279) (from #2177):; > As a **user with images in Singularity**, I want **Cromwell to support using Singularity images (either via Singularity Hub and the command line, or connecting via API)**, so that I can **use Singularity images and not have to duplicate them in Docker**. However, without any code changes, just purely through configuration. . I think it's worth touching on using these container technologies with job schedulers, so I've included the ConfigurationFile and the HPCIntro tutorials as prerequisites.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4635:776,schedul,schedulers,776,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4635,1,['schedul'],['schedulers']
Energy Efficiency,For JES and Local backends:; - Workflow root; - Call root. JES backend only:; - Google project; - Execution bucket; - Endpoint URL; - GCS path to the monitoring log,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1020#issuecomment-227234967:150,monitor,monitoring,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1020#issuecomment-227234967,1,['monitor'],['monitoring']
Energy Efficiency,"For now at least, the implementation isnâ€™t that big of a deal as long as itâ€™s pushbutton GCP stuff. The goal is to store the JSON events coming out of PubSub as-is in a fashion that we can access them in the future as necessary. We donâ€™t need efficient querying of these events but we do need the ability to easily get all the events associated with a workflow. Cloud Datastore seems like itâ€™d work (for instance, kind being workflow ID and entity being the metadatum) but I donâ€™t really know. We could also just do something like store these in a google bucket. Letâ€™s not get too complicated here, the idea is to find something simple and verify that itâ€™s feasible over time. Once this is squared away, enhance the application from #3244 to also dump the events **as-is** into this event store w/o modification.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3246:243,efficient,efficient,243,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3246,1,['efficient'],['efficient']
Energy Efficiency,"From forum post https://gatkforums.broadinstitute.org/wdl/discussion/12361/continue-on-sigterm-code#latest. The situation:; * Local backend; * The python script spawns a monitor which will SIGTERM it when the task completes; * The `128+SIGTERM` exit code was specified as valid in the runtime attributes; * However, Cromwell has already assumed that the job was aborted before it checks against the `continueOnReturnCode` values, the workflow fails instead of continuing.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3896:170,monitor,monitor,170,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3896,1,['monitor'],['monitor']
Energy Efficiency,"From my investigation, those were due to transient failures of ontology parsing, which this should reduce now: https://github.com/broadinstitute/cromwell/pull/4210",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4232#issuecomment-429867750:99,reduce,reduce,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4232#issuecomment-429867750,1,['reduce'],['reduce']
Energy Efficiency,"From the point of view of the wider user community (which reaches far outside the Broad's walls), it would be difficult to justify (not to mention communicate and support) an implicit retry mechanism that would effectively override the request stated by the user in their WDL. So we would have to expose that second setting, but then that increases the technical complexity that we need to maintain and support as well. Additionally, this would be vulnerable to business decisions by Google -- for example, what if they change the no-charge duration cutoff in response to a sudden dramatic increase of retries on early-preemption jobs? . Generally speaking I believe the best thing we can do for the user community is provide a transparent way for people to understand what are the odds and tradeoff of preemption, and to control the setting depending on their time & cost constraints (ie how much they're willing to gamble).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293700592:534,charge,charge,534,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2167#issuecomment-293700592,1,['charge'],['charge']
Energy Efficiency,"GOTC keeps track of very workflow failure in a spreadsheet, and summarizes. @bradtaylor has shown it to me before, so he might be the place to start. The task here is to review that spreadsheet, provide a summary of the top failure modes and schedule a tech/design meeting to define the tickets that would have prevented these failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1820:242,schedul,schedule,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1820,1,['schedul'],['schedule']
Energy Efficiency,"Given that:; - The WDL spec today is un-opinionated on the size of an `Int`; - The WDL `Int` is defined as 64 bit in the upcoming https://github.com/openwdl/wdl/pull/250; - NB: swap the red and green diffs in your head because Jeff merged it too soon and this is a placeholder ""if we for some reason needed to revert"" PR; - And assuming that people are not currently writing workflows specifically relying on an explosion if their Ints are above 2^16. I suggest you just default to making anything WDL produce`WomLong`s in all new situations, since we'll probably sweep through and update everything else at some point soon",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4489#issuecomment-446668372:194,green,green,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4489#issuecomment-446668372,1,['green'],['green']
Energy Efficiency,"Given the current Tyburn bent toward JES workflows with GCS inputs and Docker images, the standard battery isn't really compatible with stress testing these changes. But I can make some three-step and scatter WDLs and do some custom posthumous execution.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/233#issuecomment-147788296:99,battery,battery,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/233#issuecomment-147788296,1,['battery'],['battery']
Energy Efficiency,Good news: I think if you rebase this on develop the builds may go green ðŸ¤ž,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548045062:67,green,green,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5237#issuecomment-548045062,1,['green'],['green']
Energy Efficiency,Good refactor to reduce future headaches/bugs/effort,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-229630583:17,reduce,reduce,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1076#issuecomment-229630583,1,['reduce'],['reduce']
Energy Efficiency,Green integration testing with Centaur,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2336:0,Green,Green,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2336,1,['Green'],['Green']
Energy Efficiency,Green integration testing with Tyburn,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2337:0,Green,Green,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337,1,['Green'],['Green']
Energy Efficiency,"Green team is seeing many errors detailed [here](https://partnerissuetracker.corp.google.com/issues/122571609):. The workaround is:. >... cromwell should migrate to pipelines-io, but in the meantime, if you want to make a quick fix for this you can do:. ` rm -f $HOME/.config/gcloud/gce`. > immediately before invoking gsutil inside your retry loop. This would save approximately 10% of the failures being seen in Green Team efforts per @tbl3rd and the bug",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4641:0,Green,Green,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4641,2,['Green'],['Green']
Energy Efficiency,GreenTeam saw 3 406 errors when polling our workflows for status. These occurred on 5/3/2016. Error hitting REST API: https://<REDACTED>/api/workflows/v1/ca25d78b-3d4c-4336-b9b8-c64e8ea0dc43/status => Unexpected response code: 406; Error hitting REST API: https://<REDACTED>/api/workflows/v1/ca25d78b-3d4c-4336-b9b8-c64e8ea0dc43/status => Unexpected response code: 406; Error hitting REST API: https://<REDACTED>/api/workflows/v1/ca25d78b-3d4c-4336-b9b8-c64e8ea0dc43/status => Unexpected response code: 406,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775:0,Green,GreenTeam,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775,1,['Green'],['GreenTeam']
Energy Efficiency,Greening develop for newly added conformance tests.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3213:0,Green,Greening,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3213,1,['Green'],['Greening']
Energy Efficiency,"Had a brief chat w/ @dshiga - I was wondering if all slowness was directly tied to reading from metadata or general slowness and it was the latter (not that it couldn't be _caused_ by reading from MD, but it's across the board slowness). Three thoughts:; - We were talking yesterday about how someone not named me should try setting up typesafe monitor and use it to debug/profile/analyze. No time like the present!; - I can't find the ticket (@kcibul - do you know the right string to search for?) but we should look into the thread pools/dispatchers. This could be a case where something is bringing down the whole system but bulkheading would keep everything else responsive; - A while back we talked about streamlining submission such that WF submissions get a ""Submitted"" status but aren't necessarily immediately launched, and the system would pull them - allowing us to tune the rate at which we pull. Perhaps time to resurrect this one?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772:345,monitor,monitor,345,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228412772,1,['monitor'],['monitor']
Energy Efficiency,"Has there been any further discussion about this issue? Our team was also recently hit by a large egress charge for inter-continent docker image pulls by Cromwell -- we'd really like to be able set our image repositories to requester-pays to prevent that. . Having Cromwell/PAPI cache images would also really help to mitigate the problem -- similarly to @freeseek our workflow is structured to scatter some steps quite widely, so one relatively small workflow run can currently result in hundreds of docker pulls of the same image.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884262789:105,charge,charge,105,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-884262789,1,['charge'],['charge']
Energy Efficiency,Health monitor: Flock of Pregulls,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2582:7,monitor,monitor,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2582,1,['monitor'],['monitor']
Energy Efficiency,"Hello @ruchim :-). The Spark job will run just as good in Yarn as in Mesos, I am pretty sure about that. Main difference is that Mesos is much more advanced than Yarn. It is more scalable, both in terms of nr of nodes, nr of jobs, and types of jobs and applications. . In Mesos, you can run both normal applications (web apps etc.), like you do in Kubernetes, and you can run compute / Big Data processing jobs in the same cluster. You can schedule both cpu and memory usage, not only memory usage as in Yarn. Mesos creates a virtual operating system on top of your cluster, kind of. Yarn is not capable of that as I know it. You can even run Yarn and Kubernetes on top of Mesos etc. Choosing Mesos over Yarn, will therefor make sense for many companies, because you get one system to rule them all. It might add more complexity also though ... I am a bit dated on this, Yarn might have evolved since I looked at it. This article is good at explaining the difference:. https://www.oreilly.com/ideas/a-tale-of-two-clusters-mesos-and-yarn. Here is a nice summary of the main differences:; https://data-flair.training/blogs/comparison-between-apache-mesos-vs-hadoop-yarn/. Hope this give some answers :-)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-417230977:440,schedul,schedule,440,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-417230977,1,['schedul'],['schedule']
Energy Efficiency,"Hello @ruchim! Thanks for looking into the issue. The idea behind the init script was to reduce code duplication between all Cromwell tasks that use [recently added](https://github.com/broadinstitute/cromwell/pull/5343) `enable_fuse` flag as much as possible. Otherwise mounts have to be manually configured for each and every Cromwell task in order to take advantage of the fuse capabilities. Definitely the decision either to use or not use such init script highly depends on a workflow. From my point of view if some of workflow tasks use fuse capabilities then most of them probably do the same. Therefore the usage of init script is not required but can be helpful in such cases. As an example let's look at the following workflow. It just calculates number of files in some of the mounted directories. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. task references {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/references/ | wc -l > number; >>>; }. task samples {; output {; Int NUMBER = read_int(""number""); }. command <<<; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ls -lh /mount-point/samples/ | wc -l > number; >>>; }; ```. As long as we have some common initialization in both tasks we can extract it to the init script which will be executed right before each task command. If we perform such optimization then we have to upload `init_script.sh` to google cloud and enable it in the workflow properties. *init_script.sh*; ```bash; mkdir -p /mount-point; mount 8.8.8.8:/data /mount-point; ```. *workflow_options.json*; ```bash; {; ""init_script"": ""gs://storage/init_script.sh""; }; ```. *count.wdl*; ```wdl; version 1.0. workflow count {; output {; Int REFERENCES = references.NUMBER; Int SAMPLES = samples.NUMBER; }. call references { }; call samples { }; }. ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988:89,reduce,reduce,89,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5342#issuecomment-597303988,1,['reduce'],['reduce']
Energy Efficiency,"Hello Jeffâ€”would you mind testing if this would break Terra or Green team prod? Meaning, if the SAs donâ€™t have BQ scope before being sent to Cromwell, and then Cromwell adds the BQ scopeâ€”does the task fail to run?. <sub>Sent with <a href=""http://githawk.com"">GitHawk</a></sub>",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5023#issuecomment-501312553:63,Green,Green,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5023#issuecomment-501312553,1,['Green'],['Green']
Energy Efficiency,"Hello and welcome to the Cromwell repo. Three minutes is about what I would expect from personal experience, as a minimum time to run any task. Consider that Life Sciences allocates, starts, and pulls Docker on a dedicated VM just to print ""hello world"". It will never look favorable for small tasks whose execution time is short compared to VM setup time.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462#issuecomment-892901814:172,allocate,allocates,172,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462#issuecomment-892901814,1,['allocate'],['allocates']
Energy Efficiency,"Hello! I'm not from the Cromwell developers team, but I've already tried to run Cromwell using Podman. Have you tried just to create a symbolic link named 'docker' in your `/usr/bin`? For example:. `ln -s /usr/bin/podman /usr/bin/docker`. > Probably you should check where is your podman binary with `which podman` and adapt the above command. I ran it without changing Cromwell defaults and the workflow execution has finished successfully (as you can see in the attached log.txt file). I used [this sample workflow](https://github.com/lmtani/cromwell-cli/blob/main/sample/wf.wdl) to see it working. `java -jar cromwell-75.jar server`, and then submit the WDL and its inputs. [log.txt](https://github.com/broadinstitute/cromwell/files/8050279/log.txt)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1036412053:319,adapt,adapt,319,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6660#issuecomment-1036412053,1,['adapt'],['adapt']
Energy Efficiency,"Hello, does anyone tried to implement a Job Array with LSF scheduler within Cromwell?. I am adapting my pipeline (that uses job array and the LSF scheduler) written in bash for Cromwell. To be sure that every task runs smoothly I have used only 1 sample and everything seems to work fine. Now I would like to create a job array with multiple samples, e.g. given a file with a list of unique ID (as I usually do in bash with LSF scheduler). Can anyone share his/her own experience?. Thanks in advance!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087:59,schedul,scheduler,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491#issuecomment-513844087,4,"['adapt', 'schedul']","['adapting', 'scheduler']"
Energy Efficiency,"Hello,. I posted a couple of issues on the bcbio-nextgen tracker and I thought I'd link them here as well, in case anyone can help, as these seem to be Cromwell-specific errors. Running bcbio-nextgen with IPython parallel instead of Cromwell works fine for the scenario I've set up. In summary, I'm trying to run variant calling with bcbio-nextgen using Cromwell and 100 small FASTQ files (50-100 reads each), for testing, on an SGE cluster. The main issue seems to be that the master node is coming up under heavy load, for some reason, and, at some point, the Cromwell jobs stop getting scheduled. . This is described in more detail here: https://github.com/bcbio/bcbio-nextgen/issues/2721. A second issue, which might be related, is with the file `cromwell_work/persist/metadata.lobs`. It gets quite big, at 36 GB, for the scenario that I've described earlier, and I'm wondering if this is normal. . I'm also seeing some error messages (`java.sql.BatchUpdateException: lob is no longer valid`) when running bcbio-nextgen with Cromwell, in the same scenario as above. Could this somehow be related to the high cluster load issues as well? . More details are here: https://github.com/bcbio/bcbio-nextgen/issues/2722.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4732:589,schedul,scheduled,589,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4732,1,['schedul'],['scheduled']
Energy Efficiency,"Hello,. I wonder if it is possible to specify the GCP Batch task scheduling policy via Cromwell configuration. In specific, can I set `taskCountPerNode` to be `1` to enforce one job per VM (as in https://cloud.google.com/batch/docs/reference/rest/v1/projects.locations.jobs#taskgroup)?. Sincerely,; Yiming. <!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7521:65,schedul,scheduling,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7521,1,['schedul'],['scheduling']
Energy Efficiency,Help Travis Go Green,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4582:15,Green,Green,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4582,1,['Green'],['Green']
Energy Efficiency,"Hey @Shenglai,. Although this isn't well documented, but for each scheduler based backend (such as SLURM), on has to configure the backend with the memory units required for the scheduler. And without this conversion, Cromwell doesn't assume what needs to be sent to the scheduler. I believe we just need to improve the documentation, but I do believe the intended behavior is that Cromwell always converts declared memory into bytes by default, and backends can be configured to use other units. . Let me know if you have any questions, and I'll be closing this issue (and replacing it with an issue to improve documentation) if there are no concerns. Thanks",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4080#issuecomment-439166849:66,schedul,scheduler,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4080#issuecomment-439166849,3,['schedul'],['scheduler']
Energy Efficiency,"Hey @benjamincarlin,. The call caching feature is really designed with the focus on not having to recomputing outputs. In the case of the monitoring script, it really was meant to be treated as a debugging tool and not a true output from a task. Can you explain why you need to the monitoring log for cached jobs, especially as its not new information? Is the motivation to be able to access all monitoring logs under one workflow uuid directory?. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-444621805:138,monitor,monitoring,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-444621805,3,['monitor'],['monitoring']
Energy Efficiency,"Hey @bkohrn, just checking up on old threads:. - running a MySQL database will reduce the memory usage of Cromwell for larger workflows.; - ie: memory(mySQL + cromwell) < memory(cromwell w/ NO MySQL); - CPU can be mitigated by turning off call-caching, or at least the ""File"" strategy for call-caching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-572796376:79,reduce,reduce,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-572796376,1,['reduce'],['reduce']
Energy Efficiency,"Hey @cjllanwarne yes, I regularly run it with `monitoring_image` and it does terminate the monitoring container gracefully every time, as can be seen via the PAPIv2 operation log. This is the whole reason for its existence - to send SIGTERM to the monitoring container, otherwise PAPIv2 sends a SIGKILL and the last time points don't get reported..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714119:91,monitor,monitoring,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287#issuecomment-555714119,2,['monitor'],['monitoring']
Energy Efficiency,"Hey @indraniel -- this isn't yet supported --but the plan is to add it eventually. For now, it may be helpful to add something like an rsync at the top of your task, to see what logs are being produced by your task, or if things have gone quiet. It's also possible to add a monitoring script to run in the background that includes info about cpu/memory usage ; https://cromwell.readthedocs.io/en/stable/wf_options/Google/#google-pipelines-api-workflow-options (see ""monitoring_script"")",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-491980358:274,monitor,monitoring,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4966#issuecomment-491980358,1,['monitor'],['monitoring']
Energy Efficiency,"Hey @jsotobroad -- if you think you need this fix for sometime in the near future then we should label it GreenFriends so it gets the deserved attention. Otherwise, this sounds like a bug--I would consider it a regression as I believe we expect all WF's to produce metadata.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/719#issuecomment-212041006:106,Green,GreenFriends,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/719#issuecomment-212041006,1,['Green'],['GreenFriends']
Energy Efficiency,"Hey Denis -- I'll pose the same question as I did in another open [PR](https://github.com/broadinstitute/cromwell/pull/5023). Would you mind testing if this change is going to break for Terra and/or Green team prod? Meaning, if the user service Accounts donâ€™t have BQ permission before setting those scopes in Cromwell, does the task fail to run?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502094173:199,Green,Green,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502094173,1,['Green'],['Green']
Energy Efficiency,"Hi !; Just to make sure I understand, are you saying that the monitoring log is not copied over when a call is being cached ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-433937512:62,monitor,monitoring,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-433937512,1,['monitor'],['monitoring']
Energy Efficiency,"Hi , I'm new to cromwell/wdl. I tried to run a workflow using a SGE backend. wget -O SGE.conf ""https://raw.githubusercontent.com/broadinstitute/cromwell/develop/cromwell.example.backends/SGE.conf"". ```; $ java -Dconfig.file=${PWD}/SGE.conf -jar ${CROMWELL_JAR} run test.wdl --inputs input.json ; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1(App.scala:76); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563); 	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:926); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /home/lindenbaum-p/notebook/2023/20230208.wdl/SGE.conf: 60: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); ```. adding an extra `}` at the end of SGE.conf fixed the problem. I'll submit a PR if I'm not wrong.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7007:843,adapt,adapted,843,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7007,1,['adapt'],['adapted']
Energy Efficiency,"Hi - If I submit ~100 or so jobs to a cromwell server, is there a way I can limit the number of workflows that get fully executed at a time? . I have it restricted as 'concurrent-job-limit = 10' for the backend, and even though cromwell is only allowing 10 jobs to be running at any point, it seems to be going through all hundred jobs and running parts of each at a time rather than getting through 10 complete workflows. So at this point, I've got a hundred partially completed workflows rather than any number of fully completed workflows. I'd prefer to get some number of completed workflows through before dispatching new jobs. Anything I can do here other than having my own dispatcher to the cromwell server that proactively meters the jobs?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5503:732,meter,meters,732,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5503,1,['meter'],['meters']
Energy Efficiency,"Hi @azzaea,. The AWS backend for Cromwell integrates with AWS Batch for job scheduling and execution. As such it pretty much only uses tasks that use Docker containers. My understanding is that Cromwell can be configured with multiple backends (e.g. AWS and FilesystemLocal) and that tasks can be parameterized via inputs to the workflow to choose which backend it runs on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502847475:76,schedul,scheduling,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4997#issuecomment-502847475,1,['schedul'],['scheduling']
Energy Efficiency,"Hi @cjllanwarne @mcovarr @ruchim !; I've again rebased my branch against the latest develop and Travis jobs are all green. So I want to repeat my last question about failing test: **was it removed, or is everything OK now?**; Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-558154031:116,green,green,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-558154031,1,['green'],['green']
Energy Efficiency,"Hi @cjllanwarne @ruchim @mcovarr !; I've finally rebased against the latest develop and resolved conflicts, and CI is all green. So it a good sign? :); I still could try to reproduce a `test with space` scenario, if it was turned off on CI. Should I?; Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-555097592:122,green,green,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5110#issuecomment-555097592,1,['green'],['green']
Energy Efficiency,"Hi @dspeck1 ,. Thanks for your reply. The is the cromwell outputs when things happened:. ```; =======================log start============. status_events {; description: ""Job state is set from QUEUED to SCHEDULED for job projects/A_JOB_ID.""; event_time {; seconds: 1713287682; nanos: 566509009; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from SCHEDULED to RUNNING for job projects/A_JOB_ID.""; event_time {; seconds: 1713287919; nanos: 96623968; }; type: ""STATUS_CHANGED""; }; status_events {; description: ""Job state is set from RUNNING to FAILED for job projects/A_JOB_ID. Job failed due to task failures; . For example, task with index 0 failed, failed task event description is Task state is updated from RUNNING to FAILED on zones/A_INSTANCE_ID due to Spot VM; preemption with exit code 50001.""; event_time {; seconds: 1713288624; nanos: 767597866; }; type: ""STATUS_CHANGED""; }. task_groups {; key: ""group0""; value {; counts {; key: ""FAILED""; value: 1; }; instances {; machine_type: ""e2-standard-2""; provisioning_model: SPOT; task_pack: 1; boot_disk {; type: ""pd-balanced""; size_gb: 30; image: ""projects/batch-custom-image/global/images/batch-cos-stable-official-20240320-01-p00""; }; }; }; }; run_duration {; seconds: 705; nanos: 670973898; }. 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GcpBatchAsyncBackendJobExecutionActor [UUID(0c7363b7)Test.mergeTest:NA:1]: Status change fr; om Running to Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - isTerminal match terminal run status with Failed; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.backend-dispatcher-2485 INFO - GCP batch job unsuccessful matched isDone; 2024-04-16 17:30:25 cromwell-system-akka.dispatchers.engine-dispatcher-2358 INFO - WorkflowManagerActor: Workflow 0c7363b7-6b8f-48cf-8f38-f66d127b305f failed (during ExecutingWorkflowSta; te): java.lang.RuntimeException: Task Test.mergeTest:NA:1 failed for un",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630:203,SCHEDUL,SCHEDULED,203,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7407#issuecomment-2061445630,2,['SCHEDUL'],['SCHEDULED']
Energy Efficiency,"Hi @ffinfo. I originally thought this was going to be a patch to just lower the rc polling verbosity, but you've begun tackling a much bigger issue. Thanks a bunch for your work so far!. We've also got a bunch of other work we're juggling at the moment, so it's unlikely I or others on the team will have time to look at this issue of disappearing SGE jobs in depth for at least a couple weeks. For now, here's a brain dump of notes. After a short bit of review, I'd perhaps try a different approach.; - On execute or recover, `scheduleOnce` a message back to `self` to later check if a job is alive.; - When the message is received check if the job is alive.; - If the job is alive `scheduleOnce` a message to check if the job is alive again.; - If the job is not alive write an rc file with `143` (or other code, see notes on configuration below).; - An instance of `cromwell.core.retry.Backoff` should travel inside the scheduled messages. Each time the message is to be scheduled, get the next time. As for the existing code, here are a few notes.; - Use `java.time` instead of `java.util`. `java.time.Instant` and `java.time.Duration` may be used to calculate the amount of time between two instants.; - `IsAliveCash.cash` should be `.cache`.; - `.map(_.cache).getOrElse(true)` should be `.forall(_.cache)`, however...; - `.cache` always appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra check",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:528,schedul,scheduleOnce,528,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,4,['schedul'],"['scheduleOnce', 'scheduled']"
Energy Efficiency,"Hi @illusional, we put this PR into our schedule to review, could you please make a version of it inside the repo as you did for https://github.com/broadinstitute/cromwell/pull/5573?. Thanks,; Adam",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5464#issuecomment-661918359:40,schedul,schedule,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5464#issuecomment-661918359,1,['schedul'],['schedule']
Energy Efficiency,"Hi @myazinn and @Kirvolque - sorry for the delay here, the conversation last week wound up with Chris & I chatting w/ @wleepang from AWS, this looks like another case where the required bits of information are split across multiple people. I'm going to **try** to figure out what needs figuring out this week, but we'll see. If that winds up not working out, one thought Lee & I had was that we could set up a 3-way call next week. Both he & I will be in Basel next week which seemed like it'd be easier to schedule with you all instead of finding a time which worked for him on the west coast of the US and you all",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-511458740:507,schedul,schedule,507,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-511458740,1,['schedul'],['schedule']
Energy Efficiency,"Hi @myazinn, ~can you please rebase with develop? We just merged a fix for a bug that was causing our tests to fail. Once you rebase I can run the tests again. Thank you!~; Please disregard my comment. Actually we have some mechanism in place where our tests pick up the changes from develop and then run the tests. Since it has gone green I will merge this PR. Thank you for the contribution! :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-535962720:334,green,green,334,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097#issuecomment-535962720,1,['green'],['green']
Energy Efficiency,Hi @seandavi - you'll want to use the `concurrent-job-limit` field in your backend config (see the `reference.conf` for more details) which was put in for exactly this reason. It's a crude implement but the local backend was intended more as a debug/noodling around backend and not a full on scheduler.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-309215718:292,schedul,scheduler,292,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-309215718,1,['schedul'],['scheduler']
Energy Efficiency,"Hi @vanajasmy and thanks for your contribution. Codecov is a nice-to-have, we report it as a useful indicator but don't mandate that every single PR continue a monotonic march towards 100%. The real measure we care about is a matter of judgment - i.e. does all functionality have reasonable tests, and does critical functionality have exhaustive tests. In order to set expectations, it may be a bit before we have cycles to review this PR. Reviewing does take a substantial team effort and has to be included into the schedule alongside other tasks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-511475292:518,schedul,schedule,518,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-511475292,1,['schedul'],['schedule']
Energy Efficiency,"Hi Will,. I see, but unfortunately I still get an error - I ran the your updated workflow without the Docker portion and specifically sent an `SIGINT`, and it looks like it ended with an error. Below are the steps - hope this does not affect the launch schedule:. ``` Bash; $ cat error_continue.wdl; task hello {; String addressee; command {; echo ""Hello ${addressee}!"" && kill -SIGINT $BASHPID; }; output {; String salutation = read_string(stdout()); }; runtime {; continueOnReturnCode: true; }; }. workflow w {; call hello; }; $; $ java -jar cromwell.jar inputs error_continue.wdl; This functionality is deprecated and will be removed in 0.18. Please use wdltool: https://github.com/broadinstitute/wdltool; {; ""w.hello.addressee"": ""String""; }; $; $ java -jar cromwell.jar inputs error_continue.wdl > error_continue.json; This functionality is deprecated and will be removed in 0.18. Please use wdltool: https://github.com/broadinstitute/wdltool; $; $ java -jar cromwell.jar run error_continue.wdl error_continue.json; [2016-01-31 16:37:25,449] [info] RUN sub-command; [2016-01-31 16:37:25,469] [info] WDL file: error_continue.wdl; [2016-01-31 16:37:25,471] [info] Inputs: error_continue.json; [2016-01-31 16:37:25,989] [info] Slf4jLogger started; [2016-01-31 16:37:26,86] [info] SingleWorkflowRunnerActor: launching workflow; [2016-01-31 16:37:27,345] [info] Running with database db.url = jdbc:hsqldb:mem:748afb13-e3af-4e9d-af14-5c2b3bd209a9;shutdown=false;hsqldb.tx=mvcc; [2016-01-31 16:37:28,247] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 2a89a995-aa89-4172-a5e1-1054cbccd9e0; [2016-01-31 16:37:28,291] [info] WorkflowManagerActor Found no workflows to restart.; [2016-01-31 16:37:28,660] [info] WorkflowActor [2a89a995]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#-896492658])) message received; [2016-01-31 16:37:28,788] [info] WorkflowActor [2a89a995]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwell-system/user/SingleWorkfl",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:253,schedul,schedule,253,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,1,['schedul'],['schedule']
Energy Efficiency,"Hi all,. As discussed with @TimothyTickle, @ruchim, @benjamincarlin, @gsaksena, @abaumann, @kshakir, @geoffjentry, and others at the Broad retreat and DSP holiday hackathon, we're putting a proposal for a new feature that reports task call resource utilization metrics to Stackdriver Monitoring API. This serves 2 important goals:. 1) Users can easily plot real-time resource usage statistics across all tasks in a workflow, or for a single task call across many workflow runs, etc. This can be very powerful to quickly determine outlier tasks that could use optimization, without the need for any configuration or code (or any changes to the workflow). It's also much easier than the current state-of-the-art, i.e. parsing task-level monitoring logs. 2) Scripts can easily get aggregate statistics on resource utilization and could produce suggestions based on those. This could provide a path towards automatic runtime configuration based on the models trained with historical data. One could also detect situations like out-of-memory calls and automatically adjust resources according to those. It would also be pretty easy to add logic for estimation of task call-level cost based on the pricing of associated resources. This could provide a long-sought feature of real-time cost monitoring/control (thanks to @TimothyTickle for the suggestion). Monitoring is done using the new ""monitoring action"" for PAPIv2, which currently uses the hard-coded [quay.io/broadinstitute/cromwell-monitor](https://quay.io/repository/broadinstitute/cromwell-monitor) image, built from https://github.com/broadinstitute/cromwell-monitor (I wasn't sure if that code belonged here or in a separate repo). This is advantageous to just using it as a _monitoring_script_, because it removes all assumptions on the ""user"" Docker image (for the task itself). For example, we don't have to assume a particular distribution or presence of Python and its libraries. So it should work exactly the same for any task. Per @geoffj",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510:284,Monitor,Monitoring,284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510,3,"['Monitor', 'monitor', 'power']","['Monitoring', 'monitoring', 'powerful']"
Energy Efficiency,"Hi everyone, . When I use cromwell + PBS backend I found the way that jobs are scheduled has some differences from using PBS alone. For example, I have a pipeline of 2 steps, A-B, where B depends on A. Now I want to submit this pipeline for 2 times which will generate 4 jobs A1 B1 A2 B2. Let's assume the cluster only have resource to run one of the jobs at a time. . When I use PBS alone all of the 4 jobs will be in the queue, at the beginning A1 gets to run and the others waiting. When A1 is done B1, A2 both have a chance to run depending on the priority PBS assigns to them. So the order of the four jobs might be A1-B1-A2-B2 or A1-A2-B1-B2. When I use cromwell + PBS backend cromwell will first send A1 and A2 to the queue without B1 and B2 since they won't be ready to run until A1 and A2 are done. When A1 is done A2 gets to run because it's the only job in the queue while B1 is on its way to the queue. So in this case the order of these jobs can only be A1-A2-B1-B2. This is not big issue when there are only a few pipelines to run. However, when I have, say 100 such pipelines, B1 has to wait until A100 to finish since when A1 finishes A2-A99 are already in the queue waiting and B1 has just set off. This means the finishing time of pipeline1(A1-B1) will be affected a lot by the total number of pipelines submitted to cromwell engine. . Is there any way for cromwell to change this situation (like sending all jobs to the backend without blocking any of them)? I really don't want to wait until all ""A""s to finish to get the first result of submitted pipelines. Hope I have made this problem clear. I have read the documents of cromwell and googled quit a bit but didn't find any solution. . Any help would be appreciated!. Yue",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6339:79,schedul,scheduled,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6339,1,['schedul'],['scheduled']
Energy Efficiency,"Hi there,. Biocontainers are incredibly popular tools in bioinformatics. Each time a [Bioconda recipe](https://bioconda.github.io/recipes.html) is made, a Biocontainer is automatically generated. This creates a collection of Dockerized tools immediately available to everybody. To reduce footprint, Biocontainers run on BusyBox, a Linux version tailored to embedded systems. Some tools therefore do not expose the full set of options of matching GNU tools on Ubuntu, CentOS, etc. Given the popularity of Biocontainers, it would be great if Cromwell could support them fully. There are a couple of small bugs related to `find` and `xargs` that should be easy to fix when one explores the file `stderr.background` for a task run from a Biocontainer:. ```; find: unrecognized: -empty; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: find [-HL] [PATH]... [OPTIONS] [ACTIONS]. Search for files and perform actions on them.; First failed action stops processing of current file.; Defaults: PATH is current directory, action is '-print'. 	-L,-follow	Follow symlinks; 	-H		...on command line only; 	-xdev		Don't descend directories on other filesystems; 	-maxdepth N	Descend at most N levels. -maxdepth 0 applies; 			actions to command line arguments only; 	-mindepth N	Don't act on first N levels; 	-depth		Act on directory *after* traversing it. Actions:; 	( ACTIONS )	Group actions for -o / -a; 	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4607:281,reduce,reduce,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607,1,['reduce'],['reduce']
Energy Efficiency,"Hi! A task-level caching control has been introduced recently in the Cromwell 49. That's cool, thanks for the feature!. https://cromwell.readthedocs.io/en/stable/optimizations/VolatileTasks/; The example is using wdl specification 1.0 and it differs from draft-2 when tackling the meta section:. - draft-2 waits for a $string as a metadata value; - 1.0 waits for $meta_value = $string | $number | $boolean | 'null' | $meta_object | $meta_array. https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#metadata-section; https://github.com/openwdl/wdl/blob/master/versions/draft-2/SPEC.md#metadata-section. Hence, a `volatile: true` is not valid for draft-2, because a boolean is not a string. Is it possible to adapt the meta section for draft-2 specification too? In example: `volatile: ""true""`. The majority of our workflows still stick to the draft-2 and their translation to 1.0 will be painful.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-607310823:718,adapt,adapt,718,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1695#issuecomment-607310823,1,['adapt'],['adapt']
Energy Efficiency,"Hi! A task-level caching control has been introduced recently in the Cromwell 49. That's cool, thanks for the feature!. https://cromwell.readthedocs.io/en/stable/optimizations/VolatileTasks/; The example is using wdl specification 1.0 and it differs from draft-2 when tackling the meta section:. - draft-2 waits for a $string as a metadata value; - 1.0 waits for $meta_value = $string | $number | $boolean | 'null' | $meta_object | $meta_array. https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#metadata-section; https://github.com/openwdl/wdl/blob/master/versions/draft-2/SPEC.md#metadata-section. Hence, a `volatile: true` is not valid for draft-2, because a boolean is not a string. Is it possible to adapt the meta section for draft-2 specification too? In example: `volatile: ""true""`. The majority of our workflows still stick to the draft-2 and their translation to 1.0 will be painful. The issue is related to https://github.com/broadinstitute/cromwell/issues/1695",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5476:718,adapt,adapt,718,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5476,1,['adapt'],['adapt']
Energy Efficiency,"Hi, ; I run 2 cromwell jobs, and then I find each job have lots of file handle, I use command ""lsof | grep uername | awk '{print $1""\t""$2}' | sort | uniq -c | sort -nr"" to see it, if I run 50 jobs, I will not be able to log into that Linux through shell, how to reduce the number of file handle in each job? thanks. ```; 6533 cromwell- 4751; 5687 cromwell- 2381; 940 pool-6-th 4751; 940 pool-6-th 2381; 940 pool-5-th 2381; 705 pool-5-th 4751; 611 GC 4751; 611 GC 2381; 470 pool-9-th 4751; 470 pool-9-th 2381; 470 pool-8-th 4751; 470 pool-8-th 2381; 470 pool-7-th 4751; 470 pool-7-th 2381; 470 pool-10-t 4751; 470 pool-10-t 2381; 282 G1 4751; 282 G1 2381; 188 blaze-tic 4751; 188 blaze-tic 2381; 94 VM 4751; 94 VM 2381; 94 java 4751; 94 java 2381; 94 db-9 4751; 94 db-9 2381; 94 db-8 4751; 94 db-8 2381; 94 db-7 4751; 94 db-7 2381; 94 db-6 4751; 94 db-6 2381; 94 db-5 4751; 94 db-5 2381; 94 db 4751; 94 db-4 4751; 94 db-4 2381; 94 db-3 4751; 94 db-3 2381; 94 db-2 4751; 94 db 2381; 94 db-2 2381; 94 db-20 4751; 94 db-20 2381; 94 db-19 4751; 94 db-19 2381; 94 db-18 4751; 94 db-18 2381; 94 db-17 4751; 94 db-17 2381; 94 db-16 4751; 94 db-16 2381; 94 db-15 4751; 94 db-15 2381; 94 db-1 4751 ...; ```. this is my java command; ```{shell}; java -Xms10M -Xmx125M -Dconfig.file=SGE.conf -jar cromwell-86.jar run xxx.wdl --inputs xxx.json; ```. SGE.conf file:; ```; # Documentation:; # https://cromwell.readthedocs.io/en/stable/backends/SGE. backend {; default = SGE. providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {. # Limits the number of concurrent jobs; concurrent-job-limit = 5. # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. exit-code-timeout-seconds = 120. run",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7571:262,reduce,reduce,262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7571,1,['reduce'],['reduce']
Energy Efficiency,"Hi, I've been running Cromwell with the file-based DB fine for a few weeks, but today had a seemingly unrelated problem and this seems to have corrupted the DB. Potentially unrelated error:; ```; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/cromwell-service/WorkflowStoreCoo; rdinatedAccessActor#1289452983]] after [60000 ms]. Message of type [cromwell.engine.workflow.workflowstore.WorkflowStor; eCoordinatedAccessActor$FetchStartableWorkflows]. A typical reason for `AskTimeoutException` is that the recipient acto; r didn't send a reply.; at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:675); at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:696); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:202); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:875); at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:113); at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:107); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:334); at akka.actor.LightArrayRevolverScheduler$$anon$3.executeBucket$1(LightArrayRevolverScheduler.scala:285); at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:289); at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:241); at java.base/java.lang.Thread.run(Thread.java:834); ```. Error that I receive now when I try to start Cromwell:. ```; 2020-05-05 15:31:33,773 INFO - dataFileCache commit start; 2020-05-05 15:33:32,400 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 121641ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.h",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649:754,Schedul,Scheduler,754,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-623865649,2,['Schedul'],['Scheduler']
Energy Efficiency,"Hi,. Are there any plans to support a scheduler that can submit jobs to a mesos cluster (e.g. Chronos or whatever?). Thanks; M",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3461:38,schedul,scheduler,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3461,1,['schedul'],['scheduler']
Energy Efficiency,"Hi,. Sorry for submitting an issue here but I'm consistently getting a ""Something has gone wrong"" error trying to log in to your Jira. I'm hoping someone can offer some guidance for an issue I'm having running a CWL workflow with Cromwell on GCP. I'm using bcbio to generate CWL to do joint calling. This worked fine when I tested it with a single sample to shake out any issues with the pipeline. However when scaling up to a 20 sample batch there's an issue with the get_parallel_regions_jointvc step. This step appears to be localizing multiple copies of the reference genome data (one for each sample) to the same disk. This really blows up the storage requirements as the number of samples increase and ends up exhausting the storage allocated to the worker instance. Is this expected behaviour or is there some kind of configuration I'm missing that would avoid this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5131:739,allocate,allocated,739,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5131,1,['allocate'],['allocated']
Energy Efficiency,"Hi,; ### TL;DR. **Cromwell should allow for the configuration of Docker resource / environment flags at run-time.**. ---. I have a use-case where I'd like to run Cromwell jobs in a cluster environment via Docker Swarm. Since Swarm doesn't require any additional configuration outside of standard `docker run` commands, it's trivial to distribute Cromwell jobs across Swarm nodes. However, Swarm provides a series of [filters](https://github.com/docker/swarm/tree/master/scheduler/filter) and constrains that control how the scheduler distributes containers to nodes. For example, I might be interested in limiting the execution of a Cromwell job to a specific region / datacenter. This requires you to specify filters in the `docker run` command with the environment flag, `-e`. For example, to run a container on Swarm nodes that run in the `us-east` region:. ```; â€º docker run -d --name my_image -e constraint:region!=us-east* my_container; ```. Obviously, this configuration should _not_ be managed in the WDL document. Instead, it would be great for the Cromwell command-line tool and REST API to support additional runtime options for specifying Docker environment variables. For example:. ```; â€º cromwell run --docker-env ""constraint:region!=us-east*"" my_workflow.wdl -; ```. > Hint: Docker supports daemon labels. In the above case, the workflow would; > execute on a Swarm node whose Docker daemon that was started with:; > ; > ```; > docker daemon --label region=us-east; > ```. As for the API, the POST action to `/api/workflows/:version` would allow for multiple Docker env strings. The other feature I would like to request is translating `memory` and `cpu` configuration options (at the task level) to Docker via `--memory` and `--cpuset-cpus` `docker run` flags, respectively. These options are currently only used for the JES backend, but it seems as though they can also be used for the Local backend if Docker is specified. So, to summarize:; 1. Allow Docker `-e` flags to be specifie",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/375:470,schedul,scheduler,470,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/375,2,['schedul'],['scheduler']
Energy Efficiency,"Hi,; Is there any update on this PR? Can you give me an approximate timeline; for looking into this PR for review and integration?. Thanks,; Vanaja. On Mon, Jul 15, 2019 at 9:34 AM Adam Nichols <notifications@github.com>; wrote:. > Hi @vanajasmy <https://github.com/vanajasmy> and thanks for your; > contribution.; >; > Codecov is a nice-to-have, we report it as a useful indicator but don't; > mandate that every single PR continue a monotonic march towards 100%. The; > real measure we care about is a matter of judgment - i.e. does all; > functionality have reasonable tests, and does critical functionality have; > exhaustive tests.; >; > In order to set expectations, it may be a bit before we have cycles to; > review this PR. Reviewing does take a substantial team effort and has to be; > included into the schedule alongside other tasks.; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5070?email_source=notifications&email_token=ALILATS73U3ASS2XEBHYMJLP7SRI3A5CNFSM4IBORPI2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODZ6H4XA#issuecomment-511475292>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ALILATQDJNZOYLMFIMT5C5TP7SRI3ANCNFSM4IBORPIQ>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852:814,schedul,schedule,814,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5070#issuecomment-519218852,1,['schedul'],['schedule']
Energy Efficiency,HipChat chooses @scottfrazer and @Horneth because they show as green,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/376#issuecomment-171254307:63,green,green,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/376#issuecomment-171254307,1,['green'],['green']
Energy Efficiency,"Hmm, another moment is the Cromwell SA used by the GCE instances will need to have `Monitoring Metric Writer` role to be able to write metrics. I'll verify what happens when they don't. This is not an issue if the SA is Default Compute Engine account though.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451516304:84,Monitor,Monitoring,84,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451516304,1,['Monitor'],['Monitoring']
Energy Efficiency,Hopefully addresses issues with missed summarizations by filtering on the client. This appears to have the same 3 PAPI v2 failures that are on develop related to GPUs and slightly less memory than expected in a monitoring log assertion. ðŸ¤·â€â™‚,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5125:211,monitor,monitoring,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5125,1,['monitor'],['monitoring']
Energy Efficiency,How to set task scheduling policy for GCP Batch backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7521:16,schedul,scheduling,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7521,1,['schedul'],['scheduling']
Energy Efficiency,"I am reading your tutorial on [HPC](https://cromwell.readthedocs.io/en/stable/backends/HPC/#:~:text=FileSystems-,Shared%20FileSystem,providers.) and I have a question that could be very uneducated. The shared filesystem section talks about the localization strategies for inputs, which is certainly an issue, but the outputs are not mentioned. Let's say I have several nodes in a cluster and a single shared volume between them, either physical or software one (like Lustre). I am using Slurm backend and any node can end up running any task based on internal Slurm scheduling. Ideally I would want each task to copy the inputs from the shared volume to a local folder, create outputs, and then copy outputs to the shared volume. I know one can output final outputs anywhere, but how can one control what happens to intermediate files? The problem would arise if the subsequent tasks in the workflow are done on different nodes, but is enforcing (one node)/(one wf execution) even possible? Even if it is it beats the point of scheduling resources by availability. The solution I can see is running Cromwell FROM the shared volume, but then everything would happen there and tiny inputs and outputs would choke the job and possibly cause wear on hardware. Unless I can set a temp directory while the outputs are written?. I am asking because I am not experienced and would like to know if there are solutions I am missing before I end up doing development on my own. ; Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5802:566,schedul,scheduling,566,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5802,2,['schedul'],['scheduling']
Energy Efficiency,"I am trying to adapt a WDL workflow originally developed for DNAnexus, to work in AWS Batch. I am running from ""develop"" branch on Mac, server mode. The workflow seems to run on AWS, but then fails when checking for output logs in S3... inputs: [demux_plus_inputs.json.txt](https://github.com/broadinstitute/cromwell/files/2099495/demux_plus_inputs.json.txt); workflow: [demux_only.wdl.txt](https://github.com/broadinstitute/cromwell/files/2099496/demux_only.wdl.txt); resource file: [workflows.zip](https://github.com/broadinstitute/cromwell/files/2099470/workflows.zip); config file, which shows some attempts to add the local filesystem, since I get an error about that: ; [aws.conf.txt](https://github.com/broadinstitute/cromwell/files/2099528/aws.conf.txt). ```; 2018-06-13 14:29:27,796 cromwell-system-akka.dispatchers.api-dispatcher-72 INFO - Unspecified type (Unspecified version) workflow a67833cb-b894-4790-872f-9f3104cab60c submitted; 2018-06-13 14:29:44,760 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - 1 new workflows fetched; 2018-06-13 14:29:44,761 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Starting workflow UUID(a67833cb-b894-4790-872f-9f3104cab60c); 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - WorkflowManagerActor Successfully started WorkflowActor-a67833cb-b894-4790-872f-9f3104cab60c; 2018-06-13 14:29:44,765 cromwell-system-akka.dispatchers.engine-dispatcher-5 INFO - Retrieved 1 workflows from the WorkflowStoreActor; 2018-06-13 14:29:44,774 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; 2018-06-13 14:29:45,255 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDescriptorActor [UUID(a67833cb)]: Parsing workflow as WDL draft-2; 2018-06-13 14:29:46,004 cromwell-system-akka.dispatchers.engine-dispatcher-32 INFO - MaterializeWorkflowDe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3774:15,adapt,adapt,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3774,1,['adapt'],['adapt']
Energy Efficiency,"I asked your question to PAPI and here is the response:. > This detail is not something that should be counted on in a containerized environment.; That said: the /dev/disk/by-id/* system is simply a convenient alias. The underlying block storage doesn't change (eg, /dev/disk/by-id/google-local-disk is a symlink to a block device, in this case, /dev/sdb). So they should be able to continue monitoring if they want, it will just be harder to recover the mapping.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230:392,monitor,monitoring,392,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-439092230,1,['monitor'],['monitoring']
Energy Efficiency,"I can understand. This solution is not ideal. On the upside: it is only activated for those who willingly put ""cached-copy"" in their configs. The rest of the cromwell users are **not** affected by the lock mechanism. By default this does **not** affect anyone. I could adapt this PR and plaster the words: `WARNING: EXPERIMENTAL` all over it if that helps. EDIT: While I mention it ""is not ideal"", the only situation where the locks might not be effective is when using multiple cromwell processes, that do use the same execution folder. Does this happen often in practice? Is this even a supported use case?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-498211225:269,adapt,adapt,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-498211225,1,['adapt'],['adapt']
Energy Efficiency,"I don't have the power to invite people, but the signup link is here: https://sylabs.io/singularity/slack/",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509871160:17,power,power,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-509871160,1,['power'],['power']
Energy Efficiency,"I exposed this as `monitoring_image` workflow option, and included the Dockerfile & script in `supportedBackends/google/pipelines/v2alpha1/src/main/resources/cromwell-monitor/`. Should we also add a CI script that builds the image?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451601217:167,monitor,monitor,167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451601217,1,['monitor'],['monitor']
Energy Efficiency,"I feel like overall WDL developers need to have some way to control that docker images be cached within the WDL. Tasks that are not scattered are likely not relevant here, as one single download is unlikely to incur large egress charges. On the other hand for scattered tasks there should be a way for the WDL developer to demand caching, rather than relying on the user to do the right thing. Ideally this should all be handled by Google and container images, when downloaded, should be cached for a pre-determined amount of time. There is something called `mirror.gcr.io` but I did not fully understand how it works and whether it could be part of the solution here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788:229,charge,charges,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-888389788,1,['charge'],['charges']
Energy Efficiency,"I find 18 occurrences. `./backend/src/main/scala/cromwell/backend/ExecutionHash.scala: // TODO: PBE: ideally hashes should be deterministic; ./backend/src/test/scala/cromwell/backend/caching/CachingConfigSpec.scala:// TODO PBE Adapt to how new caching works, but the test logic should not need much change; ./database/src/test/scala/cromwell/database/slick/SlickDatabaseSpec.scala: // TODO PBE get rid of this after the migration of #789 has run.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO PBE Is this the right place for startTime ?; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowActor.scala: // TODO: PBE: some of the x-es have an actually execution & output stores.; ./engine/src/main/scala/cromwell/engine/workflow/WorkflowManagerActor.scala: // TODO PBE Restart: to be verified after restart is implemented but these WorkflowSucceededResponse/WorkflowFailedResponse seem useless; ./engine/src/main/scala/cromwell/webservice/CromwellApiService.scala: // TODO: PBE: Certainly want to do something for this! But probably not to the WMA; ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO PBE: this should be done by MWDA (ticket #1076); ./engine/src/test/scala/cromwell/engine/workflow/MaterializeWorkflowDescriptorActorSpec.scala: // TODO: PBE: Re-enable (ticket #1063); ./engine/src/test/scala/cromwell/engine/WorkflowManagerActorSpec.scala: // TODO PBE: Restart workflows tests: re-add (but somewhere else?) in 0.21; ./project/Settings.scala: //""-deprecation"", // TODO: PBE: Re-enable deprecation warnings; ./services/src/main/scala/cromwell/services/metadata/MetadataService.scala: /* TODO: PBE: No MetadataServiceActor.props until circular dependencies fixed.; ./supportedBackends/jes/src/main/scala/cromwell/backend/impl/jes/JesAsyncBackendJobExecutionActor.scala: // TODO: PBE: Trace callers of ""new CallContext()"". Seems to be multiple places in JES, etc. For now:; ./supportedBackends/jes/sr",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479:227,Adapt,Adapt,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1221#issuecomment-240175479,1,['Adapt'],['Adapt']
Energy Efficiency,"I fixed the regex. It turned out that this also fixed any issues. > Thanks for finding and fixing this!. Thank you for trusting me with push access on this repository. It makes it easier for me as all tests run immediately, also the tests that need private variables. Also I can restart jobs on travis now that looks like they are failed due to some intermittent connection error. I had to restart one for this PR, and it indeed turned green on the retry. This makes it easier for me to fix any bugs I find. The trust is much appreciated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836:436,green,green,436,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5437#issuecomment-594475836,1,['green'],['green']
Energy Efficiency,"I have adapted @delocalizer's script for an LSF backend: https://github.com/wtsi-hgi/olly-maersk/blob/master/zombie-killer.sh It's currently untested, but it shouldn't be far off from what's required.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-475176014:7,adapt,adapted,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-475176014,1,['adapt'],['adapted']
Energy Efficiency,"I honestly have not really pulled docker images without Cromwell before, other than on my laptop for minimal testing. If I try to pull a docker manually I do get the same error, as you suggested, even if the Google VM and the GCR bucket are both running on the same Google Cloud network. Isn't this a bad design from Google though? How do I make my dockers available for my WDLs and on Terra while at the same time preventing actors running the same WDLs in Google Clouds in other continents from forcing me to incur egress charges? I must be missing something. I see two possible alternative partial solutions for this issue:. (i) is there a way to write a WDL so that it automatically detects whether it should use `us.gcr.io`, or `eu.gcr.io` or `asia.gcr.io` and so that it would automatically select the one that is closer (and free)? I suppose not, as this would be outside the specification of WDL. Curios what you think though. (ii) is there a way to prevent Cromwell running with PAPIv2 from having to pull a docker image multiple time? I wrote WDLs that run on large cohorts (biobank size) and they can scatter task arrays with ~1,000 shards. If this resulted in pulling a docker once, absorbing the cost would likely still be scalable, but as it is now it is very inefficient and it makes the cost of running the WDL almost dominated by the pulling of the dockers if egress costs are involved. [Notice also that someone from the VA run my WDL but I think that, since the computation was performed on an LSF HPC cluster, the docker image was pulled only once and then reused within the LSF HPC cluster, as I did not notice any significant egress costs when this happened]. @cjllanwarne thank you for reaching out to Google. I hope this spurs a broader discussion. I am not in urgent need for a fix, but I very much hope a solution is available in the long term.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702:524,charge,charges,524,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6235#issuecomment-814160702,1,['charge'],['charges']
Energy Efficiency,I just ran into the problem of an SGE node crash that we didn't find out until we wondered why certain jobs took so long to run. Having the `is-alive` command and check run at a configurable poll interval would be really usefull for me. Having a poll interval of 0 by default (i.e. turned off) and the value in hours/days could leave it configurable without overloading the queque masters. But as Uncle Ben said: With great power...,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-413172161:424,power,power,424,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-413172161,1,['power'],['power']
Energy Efficiency,I know green requested this so they'll be happy but should also let blue know as well,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1133#issuecomment-231094276:7,green,green,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1133#issuecomment-231094276,1,['green'],['green']
Energy Efficiency,"I recognize the bind to /data, I had discussion with Seth about taking this approach! I think @leepc12 has been actively working and testing and can give quick feedback? If it works, it works, there is enough change coming to singularity wrt services that I wouldnâ€™t put too much energy into iterating over this if itâ€™s working.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424665338:280,energy,energy,280,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-424665338,1,['energy'],['energy']
Energy Efficiency,"I reproduced the problem locally. The problem is our default health monitor `StandardHealthMonitorServiceActor` which periodically tries to ping Docker Hub. I'm not sure what changed between 36 and 37 that this now manifests when previously it did not. There is a `cromwell.services.healthmonitor.impl.noop.NoopHealthMonitorServiceActor` that one should be able to swap in to turn off health monitoring, but unfortunately there's a bug in the 37 version of this class that causes it to crash during initialization. As you've noticed the workflow still runs successfully albeit with a lot of noise, we'll try to come up with a more satisfactory resolution.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462415697:68,monitor,monitor,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4626#issuecomment-462415697,2,['monitor'],"['monitor', 'monitoring']"
Energy Efficiency,"I set up a heavily scattered (~1000x) workflow to run overnight on my local machine (Mac OS Catalina, Intel hardware). The machine is set up to never sleep and was connected to AC power. My Cromwell config is set to only run one task at a time, ie, only one shard of a scattered task runs at a time. The workflow stopped processing on shard 885, which was the 233rd shard to start (shards appear to start in a random order, that's not an issue). It looks like the Docker container in question is getting created, but not used. The container is not running according to Docker Desktop and the Docker CLI tools (see output below). ### Workflow; I've seen this happen with a few workflows, but this time around it's this one (failure is occurring on second task): https://github.com/aofarrel/SRANWRP/blob/bioproject_stuff/workflows/is_this_tuberculosis.wdl. ### Ruled out; * Running tasks concurrently/Cromwell config not being respected: The workflow would have either hung Docker or tasks would have returned 137; * Docker application (not the container, the entire application) hanging, like what happens when trying to run tasks concurrently on a local machine: `docker run -it` works in a new terminal window; * IP getting blocked: This would cause error output, and I can still ping SRA from the same IP without issue; * Loss of internet: This would cause error output (`curl command failed`); * Control-S: Control-Q doesn't unfreeze it; * No more disk space: There's about 50 GB free and each instance of the scattered task uses less than a GB. ### Docker container logs; `docker logs cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528` gives no output. ### Entering the container; `docker exec -it cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528 /bin/sh` returns; `Error response from daemon: Container cf6f4828adc61eacf06337ce3caf2c110df6cc04937530a90bbfb0843acbb528 is not running`. ### Docker inspect; ```; >docker inspect cf6f4828adc61eacf06337ce3caf2c110df6cc0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6946:180,power,power,180,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6946,1,['power'],['power']
Energy Efficiency,I suggest also allow specifying walltime and queue; walltime in particular as it enables most efficient performance of the scheduler and is often a required resource request in traditional HPC cluster setups.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/583#issuecomment-207641967:94,efficient,efficient,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/583#issuecomment-207641967,2,"['efficient', 'schedul']","['efficient', 'scheduler']"
Energy Efficiency,"I talked to the greens a little bit today and I'm thinking the way to go about this is per-function limits. `read_bool()` - 5 chars (""false""); `read_int()` - 19 chars (if I did my sleuthing & counting correct for MAX_INT); `read_float()` - 50 chars (made up, but maxint plus a lot of decimal places); `read_string()` - 128K (because it's 2x what CWL gives you ;) ). Where it gets tricky are the larger objects. I'm less certain on what to do here:. `read_lines()` - I'm thinking this should be the same as `read_string()`; `read_json()` - Same? I think?; `read_[tsv|map|object]()` - No idea but from talking to the greens there are certainly reasonable use cases which would be much larger than the above. It sounded like if we capped this at 1MB it'd easily cover everything they did. From a workbench safety-from-users perspective this seems like the class of functions most likely to be abused as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-266913447:16,green,greens,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1762#issuecomment-266913447,2,['green'],['greens']
Energy Efficiency,"I think technically these could be collapsed into one class, but from your description maybe it's preferable to keep this structure with the roles of the classes more clearly articulated? The outer actor has the responsibility of implementing the `BackendJobExecutionActor` trait, but can be implemented as a simple adapter to any backend-specific means of executing jobs, which here just happens to be an FSM. Not sure what the best nomenclature would be for this distinction.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214527720:316,adapt,adapter,316,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/734#issuecomment-214527720,1,['adapt'],['adapter']
Energy Efficiency,"I think the effect is fine. We often tell people that once a job is runnable that Cromwell fires it off, but that's always used as a way to help them understand that Cromwell isn't partaking in true scheduling (ie resource based negotiation via SGE, PAPI, etc). IMO it's absolutely ok for jobs which are runnable to have not started if that's the limitation the system imposes. Further, I think it's a good move in the resiliency front. Infinite scalability is a great goal, but from a practical perspective a limit is always going to be reached, so finding ways to make the system manage to keep on ticking ok when that happens is a good thing. That's generally going to involve slowing things down.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370621740:199,schedul,scheduling,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370621740,1,['schedul'],['scheduling']
Energy Efficiency,"I think this should be fine, but @Horneth brought it up during an unrelated code review. . Determine if we desire determinism here, and fix it (via `reduceLeft`?) if appropriate",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2417:149,reduce,reduceLeft,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2417,1,['reduce'],['reduceLeft']
Energy Efficiency,"I want reduce the length of bash command line when perform the joint-genotyping on thousands samples, so that I'm using InitialWorkDirRequirement to create a file path list. Here's the example CWL:; ```; #!/usr/bin/env cwl-runner. cwlVersion: v1.0. class: CommandLineTool. requirements:; - class: InlineJavascriptRequirement; - class: DockerRequirement; dockerPull: quay.io/shenglai/alpine_with_bash:1.0; - class: InitialWorkDirRequirement; listing:; - entryname: ""files.json""; entry: $(inputs.files.basename); inputs:; files: File. outputs:; output:; type: File; outputBinding:; glob: ""files.json"". baseCommand: []; ```; the input is; ```; {; ""files"": {""class"": ""File"", ""path"": ""/mnt/glusterfs/a.file""}; }; ```; the cwltool output `files.json` is. ```; {""basename"":""a.file"",""nameroot"":""a"",""nameext"":"".file"",""location"":""file:///mnt/glusterfs/a.file"",""path"":""/var/lib/cwl/stg1a204114-8318-47ee-b5d0-7454ab87f7df/a.file"",""dirname"":""/var/lib/cwl/stg1a204114-8318-47ee-b5d0-7454ab87f7df"",""class"":""File"",""size"":0}; ```. the cromwell one is; ```; {""nameext"":"".file"",""location"":""/mnt/glusterfs/a.file"",""path"":""/mnt/glusterfs/a.file"",""size"":0,""dirname"":""/mnt/glusterfs"",""secondaryFiles"":[],""basename"":""a.file"",""class"":""File"",""nameroot"":""a""}; ```. As you can see, from cwltool, it actually returns a mapped path under `path`, so the later docker command can pick that up.; On the other hand, cromwell only returns the host path, which can not be found in docker. I only tested the piece with SLURM. I would assume it also fails on local run.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4533:7,reduce,reduce,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4533,1,['reduce'],['reduce']
Energy Efficiency,"I want to comment on the fact that there are hidden dangers in using the function `Float size(Array[File])` as shown in a Terra community forum [post](https://support.terra.bio/hc/en-us/community/posts/360071583412-PreparingJob-state-consumes-most-of-a-task-running-time-how-to-avoid-). For large arrays, this can cause tasks to take forever to start in Google Cloud. Although this is not a WDL specification issue, developers need somehow to be made aware of this, especially in cases where the array contains files that are known in advance to have similar sizes, in which case the following code:; ```; input {; Array[File]+ files; }; Float arr_size = size(files, ""GiB""); ```; Could be replaced by:; ```; input {; Array[File]+ files; }; Float arr_size = length(files) * size(files[0], ""GiB""); ```; And be significantly more efficient.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3183#issuecomment-662042665:827,efficient,efficient,827,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3183#issuecomment-662042665,1,['efficient'],['efficient']
Energy Efficiency,I was trying to pick something we haven't already used and we already have green. The colors they have are here:; https://github.com/demhydraz/badge-collection/blob/master/licenses/bsd3clause.md. We could try to unify all of the colors but that might not be possible on some badges,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/461#issuecomment-186291377:75,green,green,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/461#issuecomment-186291377,1,['green'],['green']
Energy Efficiency,"I'd argue that the arbitrary KV thing was one of the largest mistakes from Original WDL (i.e. what came out of the 2 weeks of us locking ourselves in a room and figuring it all out) as it destroys portability unless one adds *some* structure to it. We've doubled down on it over the years by adding what IMO should be Cromwell workflow options into the runtime block which means that a WDL can now have important control information on it which might ruin the portability of that workflow. The worst example I can think of is `backend` which is a purely Cromwell concept - what happens when that workflow goes to run on DNAnexus? What happens when `backend` is *also* a concept in another engine but it means something else? What happens when one engine interprets `cpu` to mean ""at least this much"" and another ""exactly this much""? What happens when in the former case the user gets charged more money than they thought because more memory than they needed was allocated? What happens when one engine assumes `mem` is just a number representing GB and can't parse a string w/ units?. (admittedly `mem` is a bad example as it's one of the very few things in `runtime` the spec is actually opinionated about, but you get the point). If the goal is to decouple Cromwell from WDL, the most obvious target is the `runtime` section. If people need more control over their Cromwell experience the answer is to a) provide that information in a way which doesn't destroy workflow portability of the WDL and b) expose that via Firecloud if those users need Firecloud. FWIW one of my primary goals for WDL 1.0 is a massive redo of `runtime` including removing the arbitrariness of it. If things are implementation specific they can be passed in to that engine separately, which would also help maintain the portability of the workflow itself.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2990#issuecomment-349459099:884,charge,charged,884,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2990#issuecomment-349459099,2,"['allocate', 'charge']","['allocated', 'charged']"
Energy Efficiency,"I'd estimate Travis/Dockering Grid Engine as medium effort, as others have already done it. Example links for the inspired:; - Google result [example](https://github.com/gawbul/docker-sge/blob/ff400b613f5bb1eae28f16e6a47d8bb1116e9617/Dockerfile) of docker+sge (crazy number of docker layers though!); - https://support.univa.com/ would probably help us (we have a license somewhere, and can probably run it similar to how we only run JES for Broad users); - help@broad would probably help create an installer script as well. For example, years ago there was a script that installed Sun's Grid Engine via CloudFormation. Speaking of Sun Microsystems, SGE is [dead](http://www.softpanorama.net/HPC/Grid_engine/Implementations/index.shtml), as well as its successors [OGE](http://www.oracle.com/technetwork/oem/grid-engine-166852.html) and an attempted-then-abandoned FOSS fork [OGS](http://gridscheduler.sourceforge.net/). Long live [SoGE](https://arc.liv.ac.uk/SGE/), and [UGE](http://www.univa.com/products/#service2). It's fine to use ""SGE"", just like we use the term ""JES"", but we'll likely need to target specifically UGE for Broadies and/or SoGE for the rest of the Grid Engine world. > Outside of Broad we probably have more LSF users than SGE users. True, there are lots of [popular](https://trends.google.com/trends/explore?date=today%205-y&q=Grid%20Engine,%2Fm%2F082f3d,%2Fm%2F0cmb2ky,%2Fm%2F04n7lk2) grid [schedulers](https://en.wikipedia.org/wiki/Job_scheduler#Batch_queuing_for_HPC_clusters). I'd be more than happy to run yet-another-travis-job for whatever scheduler, if someone contribs the docker image / setup script like we have for Funnel. > I don't know how well our SGE stuff works with UGER so perhaps not. Cromwell works great on BITS' newer UGE cluster named ""UGER"". I use cromwell frequently with `concurrent-job-limit` set to 900 due to our resource caps. **TL;DR Getting grid engine test support setup for a Broad-like environment is possible, just hasn't been a priority.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356:1811,schedul,schedulers,1811,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1180#issuecomment-324116356,2,['schedul'],"['scheduler', 'schedulers']"
Energy Efficiency,"I'd like to bump this, we are running into this issue with cromwell-41 (and I am about to check cromwell-46) that when we have a workflow failure, the failure message appears in the server logs but is never copied to the workflow log. . Eg., ; Workflow Log (empty):; > cat workflow.5a34cc05-9f9a-40a0-8691-2b0eb49cdbc3.log. Server Log:; > grep -A3 5a34cc05-9f9a-40a0-8691-2b0eb49cdbc3 cromwell-2019-09-17.7566.log; 2019-09-25 15:59:21,689 cromwell-system-akka.dispatchers.engine-dispatcher-26816 ERROR - WorkflowManagerActor Workflow 5a34cc05-9f9a-40a0-8691-2b0eb49cdbc3 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Invalid value for File input 'GermlineMasterWF.trimseq.TRIMSEQ_paired.Adapters': empty value; Invalid value for File input 'GermlineMasterWF.trimseq.TRIMSEQ_single.Adapters': empty value; 	at cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$workflowInitializationFailed(MaterializeWorkflowDescriptorActor.scala:215)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-535552697:825,Adapt,Adapters,825,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4310#issuecomment-535552697,2,['Adapt'],['Adapters']
Energy Efficiency,"I'm concerned that the awesomeness of this will be lost if there's not a dead-simple way to get the graph. But I can accept that as a feature request on this already nice PR. Put the code in `supportedBackends/google/pipelines/v2alpha1/src/main/resources/cromwell-monitor/`, please and thank you!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451494152:264,monitor,monitor,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451494152,1,['monitor'],['monitor']
Energy Efficiency,"I'm curious - did we need to ensure that SAs had support for `monitoring.write` when you all added that in #4562? My bet is that this would have the same behavior, although perhaps not as that scope seems to be a little hidden",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502204552:62,monitor,monitoring,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502204552,1,['monitor'],['monitoring']
Energy Efficiency,"I'm getting an error from Cromwell `Failed to coerce one or more keys or values for creating a Map[String, Int?]` when defining an object as an input to a task that consists of a string, file and int. I would expect the Int to get converted into a string, but the error makes it seem as if Cromwell is trying to convert all of the values into integers. Here is the part of the WDL that's throwing the error: https://github.com/HumanCellAtlas/pipeline-tools/blob/c949cb5ffa2df8f2a7fc7d7a4c34478e8eadbf34/adapter_pipelines/cellranger/adapter.wdl#L222",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4131:532,adapt,adapter,532,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4131,1,['adapt'],['adapter']
Energy Efficiency,"I'm getting an error when trying to run the following WDL, which is using an `Object` type for the output of one of the tasks: https://github.com/HumanCellAtlas/pipeline-tools/blob/master/adapter_pipelines/smart_seq2/adapter.wdl#L46. This WDL previously worked in Cromwell 29. The WDL fails immediately on validation with this error:; ```; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Some([Declaration type=Object name=prep.inputs expr=Some(prep.inputs)]) (of class scala.Some)""; }; ],; ""message"": ""Workflow input processing failed""; }; ]; ```. We're relying on objects in our HCA pipelines so it would be great if this could get fixed soon!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3060:217,adapt,adapter,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3060,1,['adapt'],['adapter']
Energy Efficiency,"I'm guessing that the ""people aren't around"" thing isn't going to change at 5pm on a Friday, so .... I'm thinking now that instead, perhaps the thing to do is to also have the actor know if read is on and if write is on. When the actor spins up, it ...; - If read is on, just starts determining the cache hit status w/o being asked; - If write is on, starts hashing everything; - If both are on, starts hashing everything but is checking for cache hit until a miss occurs, at which point it's just generating hashes. The actor could then receive two messages, one is ""are you a cache hit"" and the other is ""please now persist to the store"" (if you asked for cache hit status before it was done, perhaps it could send back a ""come back later"" message - that way you can avoid returning the Futures, although perhaps people like that). We know for a fact that if read is on that we'll be checking the cache, so might as well start that ASAP. If write is on, we're potentially wasting energy - e.g. if a job fails, but presumably (hopefully!) job successes are more common than job failures and we can get a jumpstart on the process.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880:982,energy,energy,982,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1225#issuecomment-236289880,1,['energy'],['energy']
Energy Efficiency,"I'm late to the party on this, but:. > Then chains of tasks could effectively become one task. I don't think merging of tasks works if you have certain resource or software dependencies, eg: inside a docker container. From a software engineering POV, is it easy / possible to detect and facilitate streaming between tasks like this, especially if they're scheduled as completely separate jobs? To me it sounds super difficult, like you'd have like a ""fuzzy"" dependency graph, and you could end up streaming your result data between nodes or tasks (and even worse if you're running on the cloud). (@mr-c, you've talked about this a [few times](https://github.com/common-workflow-language/cwltool/issues/644#issuecomment-366719563)). > parallel, rather than sequentially. Mostly, but what happens if two of the inputs are technically streamable, or even more complicated how would `stdin` fit into this. The [CWL documentation](https://www.commonwl.org/v1.0/CommandLineTool.html#CommandLineTool) says that it requires the path (eg: [`$(inputs.stdinRef.path)`](; https://www.biostars.org/p/258614/#290536)) which to me sounds like it isn't exactly streamable, but `stdout` [implicitly is?](https://www.commonwl.org/v1.0/CommandLineTool.html#stdout) WDL in the version [1.0 spec](https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#language-specification) doesn't include any reference to 'stream', so I'm surprised to see the DNAnexus adding a separate tagging mechanism for this optimisation. _Late edit: reformatting for clarity_; Engine support:. - Cromwell (not supported) [[source](https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455542734)]; - CWLTool (not supported) [[source](https://github.com/common-workflow-language/cwltool/issues/644)]; - Toil (not supported) [[source](https://github.com/common-workflow-language/cwltool/issues/644#issuecomment-366719563)]. But piping (named and anonymous) is super easy in WDL because you have a command line, and in CWL yo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455367417:355,schedul,scheduled,355,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3454#issuecomment-455367417,1,['schedul'],['scheduled']
Energy Efficiency,I'm looking at ways to reduce that,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-295373442:23,reduce,reduce,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-295373442,1,['reduce'],['reduce']
Energy Efficiency,I'm not actually going to wait for :+1: but will wait for it to go green. My claim will be out of safety but really it's because I'm lazy and will do it later.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2664:67,green,green,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2664,1,['green'],['green']
Energy Efficiency,"I'm not even sure if this is feasible (maybe it's a crazy idea at all), but it will be really useful to drop the metadata of a workflow/subworkflow/task into its execution bucket. An use-case could be: right now, if we want to get information of another subworkflow, from within another workflow, we have to include the credentials(service-account keys/HTTPBasic Auth u/p info) for talking to Cromwell(Cromwell-as-a-service) in the latter workflow's docker image and make the docker image private, which is really tricky (if you have a variety of environments) and makes the workflows less portable. Having an external broker reduces the overhead of managing credentials to some extent, but introduces another external dependency for the workflows. So in this particular case what would be appreciated is a way to get the metadata(even parts of it) of previous workflows from another workflow, where all workflows are living under the same ""parent workflow umbrella"". . I realize that a handful teams are doing similar things like us(querying Cromwell(which in this case, is an external service to the workflow) to get the workflow metadata from another workflow), so having this feature could make a lot of lives easier :)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4236:626,reduce,reduces,626,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4236,1,['reduce'],['reduces']
Energy Efficiency,I'm proposing here the configuration I use for [TORQUE](https://www.adaptivecomputing.com/products/torque/).,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5082:68,adapt,adaptivecomputing,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5082,1,['adapt'],['adaptivecomputing']
Energy Efficiency,"I'm using this in a fully automated setting, as part of this script:; https://github.com/broadinstitute/dsp-scripts/blob/master/cromwell/methods/cromwell_monitoring_script.sh. So for me it won't work if the combination of google VM / docker / cromwell results in the disk *not* being mounted on /dev/sdb for any reason. This could happen if the user requests disks to be mounted in a specific place (https://cromwell.readthedocs.io/en/stable/RuntimeAttributes/), requests more than one disk but would prefer the second disk be monitored, or if cromwell starts using /dev/sdb for some other resource and the disks get pushed to /dev/sdc. I guess it would be more precise to say, ""I'm not aware of this happening, but lots of people use this script and I have no idea if any of them would complain to me if it didn't work"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-529532328:527,monitor,monitored,527,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-529532328,1,['monitor'],['monitored']
Energy Efficiency,"I've been complaining that monitoring doesn't provide any insight into dispatchers. I was getting ready to inquire about this and noticed this in the docs:; https://developer.lightbend.com/docs/monitoring/latest/instrumentations/akka/dispatchers.html. I believe this is (relatively) new, see what's going on here",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1747:27,monitor,monitoring,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1747,2,['monitor'],['monitoring']
Energy Efficiency,"I've been working on this issue in parallel. My general recommendation would be to **not use GCR for _public_ images** as there is a perpetual risk of egress charges to the image owner. GCR is great for keeping more of your workflow infrastructure inside Google Cloud, but egress charges will be billed to you as the owner of the storage bucket. [Artifact registry seems to have its own set of egress charges](https://cloud.google.com/artifact-registry/pricing), similar to GCS egress charges. Alternatively, services like [quay.io](https://quay.io/plans/) offer unlimited storage and serving of public repos. The consequence of not using GCR is that docker images are now hosted outside of Google Cloud, meaning workflows will need to make an external network call to download the image. The external call will require VMs to have an external IP address. Large parallel workflows will need quota for several external IP addresses, and may run into quota limits. . To alleviate this, workflow runners could be instructed to make a copy of the image into their own GCR. This also has the advantage that the workflow runner can use a repository in the same location as their VMs. Workflow publishers should include instructions on how to upload the image to a private GCR. . How does that sound as a set of guidelines for the community?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238:158,charge,charges,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902091238,4,['charge'],['charges']
Energy Efficiency,"I've worked out what happened, but I don't know if I can resolve this next problem. . I had call-caching turned on for SFS, and this was MD5 hash was being calculated by Cromwell on the login node, however for 2x 100GB BAM files at each step this was (obviously in retrospect) a resource drain. This was only applicable to backends that use the Local Filesystem (GCS and S3 file systems probably use their blob / object id). If you come across this issue, you might have a couple of solutions:; - Turn off call-caching, might not matter to you.; - If you're not using containers, you might be able to get away with the [path+modtime caching strategy](https://cromwell.readthedocs.io/en/stable/Configuring/#local-filesystem-options), requires you to use the [`soft-link` copying strategy](https://cromwell.readthedocs.io/en/stable/backends/HPC/#shared-filesystem).; - If you **are** using containers, you're out of luck unfortunately.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-507482774:288,drain,drain,288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4945#issuecomment-507482774,1,['drain'],['drain']
Energy Efficiency,"If it can be specified by a workflow option, then everyone is happy. And; we don't have to wait for 1.0. On Oct 20, 2017 20:30, ""Jeff Gentry"" <notifications@github.com> wrote:. > Deleting some comments due to being interspersed with untrue things.; >; > @LeeTL1220 <https://github.com/leetl1220>; >; > As per #1762 <https://github.com/broadinstitute/cromwell/issues/1762> the; > intention was to have spec mandated minimums and implementation level; > maximums. The former never happened so technically it's not part of the; > spec at all. And as I noted, Cromwell team is no longer in charge of the; > WDL spec, so ...; >; > That said, it's tunable. You can increase it if you want. I wouldn't; > recommend going all that high unless you're willing to really jam a lot of; > memory in there.; >; > As per your iterator comment, I go back to the Cromwell team doesn't; > control WDL anymore and there's no WDL construct which would allow that.; > There's been chatter about things which might help but they're unlikely to; > arrive until after WDL 1.0; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338350853>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/ACDXkyBD9ZS-tfUwjFaEVS_i9Gro7EOUks5suTs2gaJpZM4QBFpH>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338398224:586,charge,charge,586,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338398224,1,['charge'],['charge']
Energy Efficiency,"If my question isnt clear enough, let me expand some. I need to set AWS_BATCH_JOB_ATTEMPTS because AWS will terminate my jobs in the middle of them if my spot instance request is outbid (usually the only higher bids are on demand prices). AWS_BATCH_JOB_ATTEMPTS will allow me to tell aws that when I job is stopped for that reason, it will restart the job automatically without me needing to continually monitor it, Is there any way to do this?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-632668244:404,monitor,monitor,404,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5511#issuecomment-632668244,1,['monitor'],['monitor']
Energy Efficiency,"Ignore my redundant self-approval of this PR. I intended to click ""merge"" but apparently the green ""Approve"" button was too tempting for me not to click on first",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6374#issuecomment-868768959:93,green,green,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6374#issuecomment-868768959,1,['green'],['green']
Energy Efficiency,"In a sense yes, but it's upstream of it.; Currently if you scatter 10 million wide, Cromwell still creates 10 million EJEAs that are going to ask for a token.; This stops it from even creating EJEAs if it knows they won't be able to run anyway (yet).; To be perfectly honest I just wanted to kinda float the idea as I haven't gotten to precisely measure if it indeed reduces cpu / memory load (I'm strongly guessing yes though since it's less work being done but...).; I wanted to get people's opinion and if it's a no go already I won't bother measuring.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370564846:367,reduce,reduces,367,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356#issuecomment-370564846,1,['reduce'],['reduces']
Energy Efficiency,"In the following code snippet:; ```WDL; scatter(centrifuge in centrifugeList){; call centrifugeDownload {; input:; centrifugeOutput= centrifuge.outputDir,; domain=centrifuge.domain,; database=if defined(centrifuge.database) then centrifuge.database else ""refseq""; }; }; }; ```; The `centrifugeList` is a list of dictionaries. The resulting `centrifuge` `object` may or may not have a key database. . ## Expected behaviour:; `database` defaults to `""refseq""` if no `database` key is present in the dictionary. It will use the database key if it exists. ## Observed behaviour:; ```; java.lang.RuntimeException: Evaluating if defined(centrifuge.database) then centrifuge.database else ""refseq"" failed: Could not find key database in WdlObject; at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2(ExpressionKey.scala:36); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.$anonfun$processRunnable$2$adapted(ExpressionKey.scala:31); at scala.Function1.$anonfun$andThen$1(Function1.scala:52); at cats.data.Validated.fold(Validated.scala:14); at cats.data.Validated.bimap(Validated.scala:109); at cats.data.Validated.map(Validated.scala:152); at cromwell.engine.workflow.lifecycle.execution.keys.ExpressionKey.processRunnable(ExpressionKey.scala:31); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$4(WorkflowExecutionActor.scala:452); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$2(WorkflowExecutionActor.scala:449); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.$anonfun$processRunnableTaskCallInputExpression$1(WorkflowExecutionActor.scala:448); at scala.util.Either.flatMap(Either.scala:338); at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.processRunnable",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3093:954,adapt,adapted,954,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3093,1,['adapt'],['adapted']
Energy Efficiency,"In theory it shouldn't make a difference.* Cromwell isn't scheduling anything, it's merely seeing what is runnable and launching it. Thus the time delta should be miniscule. However, in the real world there are things such as quotas on the backend and that could make a difference, yes. But keep in mind that backends will themselves process job requests differently and aren't necessarily going to honor the order we send them in anyways. Thus, in my opinion this sort of general optimization is going to be folly as we can never guarantee the underlying behavior anyways. To your primary point, I get what you're saying although my experience has been that every time someone has stated that a behavior should be X as it matches the real world I find someone telling me that the opposite behavior matches the real world. :) This is one of those cases. This one is more complex in that we also hear differing opinions on the whole workflow level (ie should workflows be processed as many at once as possible or optimizing for throughput for any individual workflow). At the end of the day the limiting factor is going to be the backend set up and whatever quotas are in place. * Yes, there is a global job limit but this can be tweaked as high as one would like, so effectively not an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2736#issuecomment-336137269:58,schedul,scheduling,58,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2736#issuecomment-336137269,1,['schedul'],['scheduling']
Energy Efficiency,Includes:. * Saloni's fixes for the monitoring test breakage; * Dan's fixes for GPU test breakages; * A revert of external contribution #5113 that was never run through full CI before merge to develop and causes PAPI v2 builds to hang,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5133:36,monitor,monitoring,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5133,1,['monitor'],['monitoring']
Energy Efficiency,Initial health monitor support,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2653:15,monitor,monitor,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2653,2,['monitor'],['monitor']
Energy Efficiency,"Input:. 1. Cromwell commit [`53c4693a0508518e9a6f957fbba2b1afc7dc90b5`](https://github.com/broadinstitute/cromwell/pull/6739/commits/53c4693a0508518e9a6f957fbba2b1afc7dc90b5); - I ran the job on my own branch for testing, but I updated the job to point to `develop` by PR time. Outputs:. 1. Commit to `main` in Cromwhelm: https://github.com/broadinstitute/cromwhelm/commit/0218cf0ccad9a7e9a74c785c2101ecb77ec56a13. 2. [Image in Docker Hub](https://hub.docker.com/layers/cromwell/broadinstitute/cromwell/79-cba6c97-SNAP/images/sha256-9cd6b3e404efbd1a8610b45ae606dde056dd172e6f1a83fbae36e340fb0103ea?context=explore). 3. [Complete log output of action.](https://github.com/broadinstitute/cromwell/runs/6097432895?check_suite_focus=true). ---. The action runs on self-hosted instances created by devops (`runs-on: self-hosted`) which are 2x as powerful as Travis. The whole build takes ~7 minutes, including `sbt server/docker`. My high spec Broad laptop is just a little faster at ~5 minutes. ~There is a prequisite PR https://github.com/broadinstitute/terraform-ap-deployments/pull/616 that is very close pending a naming discussion.~ Merged. ---. Now we are cool like the other repos that deploy continuously. This is near and dear to my heart:. <img width=""1159"" alt=""Screen Shot 2022-04-20 at 11 58 40 AM"" src=""https://user-images.githubusercontent.com/1087943/164273206-134f1b88-d1bb-447e-b56e-7d01c2e1cada.png"">",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6739:841,power,powerful,841,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6739,1,['power'],['powerful']
Energy Efficiency,"Instead of failing fast on database `Exception`s (NOTE: not `Error`s), the actors/fsms should schedule a retry. Example from [`WriteMetadataActor`](https://github.com/broadinstitute/cromwell/blob/9a734c4f36ae122153736004e8d54fc44af8fbde/services/src/main/scala/cromwell/services/metadata/impl/WriteMetadataActor.scala#L45-L52):; ```scala; case Event(FlushBatchToDb, HasEvents(e)) =>; log.debug(""Flushing {} metadata events to the DB"", e.length); addMetadataEvents(e.toVector) onComplete {; case Success(_) => self ! DbWriteComplete; case Failure(regerts) =>; log.error(""Failed to properly flush metadata to database"", regerts); self ! DbWriteComplete; }; stay using NoEvents; ```. On an exception, the `e` events could be rescheduled.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2016:94,schedul,schedule,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2016,1,['schedul'],['schedule']
Energy Efficiency,"Instrumentation was scheduled on a timer out of band of the actor's thread, and in some cases accessing non thread safe state inside the actor (in a read only fashion but still it can cause incoherent values: https://stackoverflow.com/questions/37690525/multiple-threads-checking-map-size-and-conccurency); Instead use messages to self to schedule instrumentation on the actor's thread",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4402:20,schedul,scheduled,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4402,2,['schedul'],"['schedule', 'scheduled']"
Energy Efficiency,Investigate dispatcher tooling in lightbend monitoring,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1747:44,monitor,monitoring,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1747,1,['monitor'],['monitoring']
Energy Efficiency,"It allows a user to understand that they're requesting to abort something which is already terminal. Whether or not that's useful is a matter of debate. Clearly someone does because it was put in by someone. I say that it's ""useful"" but am using quotes because I doubt anyone is ever going to act on knowledge. I don't recall if it returns 200 or just a generic error (I could dig it up if it matters, but not now as I don't have the energy and I don't think we should keep it)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379:434,energy,energy,434,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2415#issuecomment-332683379,1,['energy'],['energy']
Energy Efficiency,"It is not about cromwell subscribing its own events. As u already said, cromwell has exposed restful api for external integration, so it is your job to monitor workflow status as u want such as maintaining an event system like influxdb. Say, you can setup a telegraf exec plugin for polling cromwell server periodically and streaming status into infuxdb, then use influxdb as an event system and trigger all downstream actions once status is changed, you can even setup a grafana as dashboard of workflows monitor system. Or if your crowmwell server can be accessed via internet, the easier way is to poll it from AWS lambda and put workflow status to aws SQS or SNS.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833:152,monitor,monitor,152,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6756#issuecomment-1158965833,2,['monitor'],['monitor']
Energy Efficiency,"It seems to me very cumbersome to ask users to think this through. Currently my WDLs have a variable:; ```; String docker_registry = ""us.gcr.io""; ```; But I have dockers uploaded on each of `us.gcr.io`, `eu.gcr.io`, and `asia.gcr.io`. The main issue is that the user has to take initiative to fill that variable with the correct GCR. It would be great if there was some sort of environmental variable that could let the WDL know which docker registry is the closest (and which one has no egress charges).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902128358:495,charge,charges,495,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-902128358,1,['charge'],['charges']
Energy Efficiency,"It would be nice to have a rest Endpoint which would cause Cromwell to stop launching any new Jobs, wait for a terminal state in all of the current jobs, then gracefully shutdown after a set delay. The use case for this is an attempt to scale cromwell. We run a large number of workflows are and monitor the number of workflows per machine, sometimes we need to increase the number of cromwells to accomodate the load, but then want to shut them off later",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2595:296,monitor,monitor,296,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2595,1,['monitor'],['monitor']
Energy Efficiency,"It would be very nice to support Mesos, as it is a very nice framework for in house cloud computing systems. Not everyone is able to launch their jobs into the cloud. I see you already have support for Yarn here: . http://cromwell.readthedocs.io/en/develop/backends/Spark/. And we also have this:; ```; A not so widely known fact is that Spark has its root in Mesos: it was ; initially developed at the AMPLab as a proof-of-concept Mesos ; framework to demonstrate how easy and fast ; it is to develop a distributed platform on top of Mesos; ```; taken from here : ; * https://mesosphere.com/blog/spark-mesos-shared-history-and-future-mesosphere-hackweek/. Spark and Mesos was really closely integrated, though I see that Spark has created their own scheduler, Mesos is still a very good way of running Spark jobs. It would be a very nice addition to the Chromwell framework! . Mesos is used in many other Big Data cloud environments outside of the Bioinformatics pipelines.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-388333576:750,schedul,scheduler,750,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3461#issuecomment-388333576,1,['schedul'],['scheduler']
Energy Efficiency,"It's unclear how your environment is set up:. 1. You're using Cromwell to schedule jobs to GoogleCloud.; 2. You're Cromwell on a Google cloud instance and not scheduling any jobs out. ## Cromwell scheduling to GCloud. - Cromwell runs out of memory and dies; - Your task is running out of memory, this would cause your workflow to fail. . ### Cromwell OOM. Use MySQL and connect it to Cromwell: https://cromwell.readthedocs.io/en/stable/Configuring/#database. Cromwell + MySQL uses less memory and you get durability:. ### Task. It seems that your task is running out of memory, and not Cromwell. . I'll direct you towards the WDL spec: https://github.com/openwdl/wdl/blob/master/versions/1.0/SPEC.md#memory. But basically, within your task you need to place the block or with more memory. ```; task mytask {; ...other stuff; runtime {; memory: ""2GB""; }; ```. ## Running Cromwell on a GC instance. Restart your instance with more memory.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-572850228:74,schedul,schedule,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5347#issuecomment-572850228,3,['schedul'],"['schedule', 'scheduling']"
Energy Efficiency,"Iâ€™ve got a workflow in the 900 genomes run that appears to be an example of Cromwell losing a job. One of the haplotype scatters, which should take a day, instead has been going for 4 days. JES thinks itâ€™s done. [This is the workflow link in FireCloud](https://portal.firecloud.org/#workspaces/engle-macarthur-ccdd%3Agenomes-reprocessing/Monitor/d2237443-0fd7-45fa-99c7-ae25ae5e3c68). The workflow id is 9d16d8cf-7470-422d-8329-0a55a89f6411. The operation id is ELnDoOadKxj3jPfVxujN5Vwgy_-Z2IQbKg9wcm9kdWN0aW9uUXVldWU. Attached are the cromwell logs and the operations id info. [operation-metadata.txt](https://github.com/broadinstitute/cromwell/files/739430/operation-metadata.txt); [cromwell-logs-9d16d8cf-7470-422d-8329-0a55a89f6411.txt](https://github.com/broadinstitute/cromwell/files/739431/cromwell-logs-9d16d8cf-7470-422d-8329-0a55a89f6411.txt)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1928:338,Monitor,Monitor,338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1928,1,['Monitor'],['Monitor']
Energy Efficiency,"JhbHBoYTEvYXBpL0xvY2FsaXphdGlvbi5zY2FsYQ==) | `0% <0%> (Ã¸)` | :arrow_up: |; | [...google/pipelines/common/PipelinesApiJobPaths.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlKb2JQYXRocy5zY2FsYQ==) | `100% <0%> (Ã¸)` | :arrow_up: |; | [...on/PipelinesApiAsyncBackendJobExecutionActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvZ29vZ2xlL3BpcGVsaW5lcy9jb21tb24vc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvYmFja2VuZC9nb29nbGUvcGlwZWxpbmVzL2NvbW1vbi9QaXBlbGluZXNBcGlBc3luY0JhY2tlbmRKb2JFeGVjdXRpb25BY3Rvci5zY2FsYQ==) | `26.96% <0%> (+0.39%)` | :arrow_up: |; | [...n/scala/cromwell/core/path/BetterFileMethods.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y29yZS9zcmMvbWFpbi9zY2FsYS9jcm9td2VsbC9jb3JlL3BhdGgvQmV0dGVyRmlsZU1ldGhvZHMuc2NhbGE=) | `30.1% <0%> (+1.02%)` | :arrow_up: |; | [...wl/src/main/scala/cwl/ExpressionInterpolator.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | ... and [301 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5113/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=footer). Last update [242206f...e9ad3d7](https://codecov.io/gh/broadinstitute/cromwell/pull/5113?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842:4262,Power,Powered,4262,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5113#issuecomment-521975842,1,['Power'],['Powered']
Energy Efficiency,"Jira: https://broadworkbench.atlassian.net/browse/WX-1670. ### Description. Cromwell will use information provided in the new GROUP_METRICS_ENTRY table to allocate new tokens for job requests whose hog group is not experiencing any cloud quota exhaustion. Note that this will be applied to jobs seeking ""execution"" tokens. Jobs seeking ""restart"" tokens are not affected by this change. . TODO:; - [x] test changes in BEE; - [x] fix unit tests; - [x] add new unit tests; - [x] update Changelog?. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7520:155,allocate,allocate,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7520,1,['allocate'],['allocate']
Energy Efficiency,Js green integration,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2374:3,green,green,3,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2374,1,['green'],['green']
Energy Efficiency,"Just a heads up, after talking with @Horneth and @cjllanwarne I'm planning to merge this before Thibault's PR if/when these builds go green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1340#issuecomment-243482423:134,green,green,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1340#issuecomment-243482423,1,['green'],['green']
Energy Efficiency,Kills workflow if HPC Scheduler kills job due to out of memory error,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5107:22,Schedul,Scheduler,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107,1,['Schedul'],['Scheduler']
Energy Efficiency,LGTM when travis is green ðŸ‘ . [![Approved with PullApprove](https://img.shields.io/badge/reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2775/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2775#issuecomment-338986251:20,green,green,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2775#issuecomment-338986251,1,['green'],['green']
Energy Efficiency,"Look at code in existing but closed PRs in cromwell-backend/cromwell. This will need to be greatly adapted for develop, but should be doable.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/487:99,adapt,adapted,99,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/487,1,['adapt'],['adapted']
Energy Efficiency,"Looking at it, and chewing my own words, the changes that I'm seeing has a few round-trips to the database (e.g. getting workflows in a particular state). Given that we are trying to reduce DB calls from the engine, do you guys think it to be a good idea?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201018255:183,reduce,reduce,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201018255,1,['reduce'],['reduce']
Energy Efficiency,Make Papi V1 CRON Green Again,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3731:18,Green,Green,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3731,1,['Green'],['Green']
Energy Efficiency,Make Papi V2 CRON Green Again,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3846:18,Green,Green,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3846,1,['Green'],['Green']
Energy Efficiency,Merging as this is all green and there were no unresolved comments,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4865#issuecomment-485612460:23,green,green,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4865#issuecomment-485612460,1,['green'],['green']
Energy Efficiency,"Merging this as it's green and has no requested cleanups. @cjllanwarne I'm not deleting the branch, will leave that to you",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4853#issuecomment-485502251:21,green,green,21,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4853#issuecomment-485502251,1,['green'],['green']
Energy Efficiency,"Merging this for @aednichols as he's out, this is green, and there are no outstanding things described to clean up.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4858#issuecomment-485501748:50,green,green,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4858#issuecomment-485501748,1,['green'],['green']
Energy Efficiency,Missing Functionality: JES Monitoring Script,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1006:27,Monitor,Monitoring,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1006,1,['Monitor'],['Monitoring']
Energy Efficiency,Monitor imminence of database fullness [BA-6248],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5455:0,Monitor,Monitor,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5455,1,['Monitor'],['Monitor']
Energy Efficiency,Monitoring script doesn't always appear to run,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1158:0,Monitor,Monitoring,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158,1,['Monitor'],['Monitoring']
Energy Efficiency,"Monitoring.log file is created when monitoring_script WF option is used, but the log file is completely empty. It's possible that the correct outputs aren't copied over to the log file.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226281925:0,Monitor,Monitoring,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226281925,1,['Monitor'],['Monitoring']
Energy Efficiency,"Much like our operation polling the creation of new runs in jes winds up being a problem when lots of jobs are created at once as we wind up with a ton of threads gumming up the works but still can only submit at the rate of the genomics api qps. It'd be way more fun to use those threads for things like DB access, mining bitcoins, skynet, etc. Transform the JesPollingActor & friends such that that structure manages both create and get operations (the batch API allows for heterogenous requests) are going through the same structure metered by the QPS supplied by the config file.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1798:536,meter,metered,536,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1798,1,['meter'],['metered']
Energy Efficiency,My main concern here is relying on it as a proof of validity until it's guaranteed that it won't go green if it's not picking up workflows in flight.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2164#issuecomment-293645068:100,green,green,100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2164#issuecomment-293645068,1,['green'],['green']
Energy Efficiency,"NOTE: if the PAPI v2 upgrade happens in all environments before horizontaling, this ticket becomes unnecessary and should be closed as a noop. OTHERWISE:. Adapt the existing Centaur test for PAPI v1 to PAPI v2 upgrade for a horizontal Cromwell configuration.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4801:155,Adapt,Adapt,155,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4801,1,['Adapt'],['Adapt']
Energy Efficiency,"No. The feature is due to a script that monitors disk storage and mounts; new disks into a btrfs filesystem. This isnâ€™t standard for ECS. On Tue, Jul 21, 2020 at 4:08 PM Richard Davison <notifications@github.com>; wrote:. > @markjschreiber <https://github.com/markjschreiber>; >; > The EC2 workers contain a script that automatically expands that mount; > users don't need to set that up. No custom AMI is required, in theory any; > AMI that can work with ECS could be used.; >; > Is this a new feature of all new ECS Optimized Amazon Linux instances?; >; > â€”; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662080007>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AF2E6EM4MI3WWXI2AFZAK2LR4XYUHANCNFSM4LW5UP3A>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662153993:40,monitor,monitors,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5468#issuecomment-662153993,1,['monitor'],['monitors']
Energy Efficiency,"Nope as far as I know, it's turned on by default (and is used by GCP to monitor most resources): https://cloud.google.com/service-usage/docs/enabled-service#default",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451496022:72,monitor,monitor,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451496022,1,['monitor'],['monitor']
Energy Efficiency,"Not necessarily the problem, but even non-obvious GCP quotas can limit how many workers are scheduled. Specifically, some things to look out for:. - Preemtible specific resources, like CPUs and Memory (if you're running preemtibles); - If you are using preemtibles, there may not be enough available instances; - Local SSD (GB); - Internal IP addresses; - In-use IP addresses; - List requests per 100 seconds. I thought mine were high enough, but from this page (replace `$region` with your region) you can click the ""Current Usage"" to sort by in-demand resources:. - https://console.cloud.google.com/iam-admin/quotas?project=portable-pipeline-project&location=$region. ![image](https://user-images.githubusercontent.com/22381693/72295508-d2132900-36ab-11ea-8380-256c2ad381b4.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305:92,schedul,scheduled,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5352#issuecomment-573891305,1,['schedul'],['scheduled']
Energy Efficiency,"Not really, I have a work-around (if /sys/block/sdb/ is a directory and /dev/sdb is mounted in mtab, use /sys/block/sdb/); ```; function findBlockDevice() {; MOUNT_POINT=$1; FILESYSTEM=$(grep -E ""$MOUNT_POINT\s"" /proc/self/mounts \; | awk '{print $1}'); DEVICE_NAME=$(basename ""$FILESYSTEM""); FS_IN_BLOCK=$(find -L /sys/block/ -mindepth 2 -maxdepth 2 -type d \; -name ""$DEVICE_NAME""); if [ -n ""$FS_IN_BLOCK"" ]; then; # found path to the filesystem in the block devices. get the; # block device as the parent dir; dirname ""$FS_IN_BLOCK""; elif [ -d ""/sys/block/$DEVICE_NAME"" ]; then; # the device is itself a block device; echo ""/sys/block/$DEVICE_NAME""; else; # couldn't find, possibly mounted by mapper.; # look for block device that is just the name of the symlinked; # original file. if not found, echo empty string (no device found); BLOCK_DEVICE=$(ls -l ""$FILESYSTEM"" 2>/dev/null \; | cut -d'>' -f2 \; | xargs basename 2>/dev/null \; || echo); if [[ -z ""$BLOCK_DEVICE"" ]]; then; 1>&2 echo ""Unable to find block device for filesystem $FILESYSTEM.""; if [[ -d /sys/block/sdb ]] && ! grep -qE ""^/dev/sdb"" /etc/mtab; then; 1>&2 echo ""Guessing present but unused sdb is the correct block device.""; echo ""/sys/block/sdb""; else ; 1>&2 echo ""Disk IO will not be monitored.""; fi; fi; fi; }; ```. I am not sure if this is a google VM problem, a docker problem, or a problem with how cromwell specifies volumes to docker; but I took their response to be ""we don't care and won't fix it"". Fortunately for me the work-around nearly always works for cromwell jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-529486322:1257,monitor,monitored,1257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4388#issuecomment-529486322,1,['monitor'],['monitored']
Energy Efficiency,Not sure I have the powers to push a dev image to Docker Hub though (let's discuss offline).,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5382#issuecomment-577760343:20,power,powers,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5382#issuecomment-577760343,1,['power'],['powers']
Energy Efficiency,Note that there's no documentation nor testing for this atm. Yes I'm aware of that :). Initial health monitor support; o Provide health monitor infrastructure to provide ability to attach checks for underlying subsystems; o Provide status endpoint which will query current contents of health monitor implementation; o Moved some google code to a new cloudSupport project; o Moved some general docker code to core from the dockerHashing project,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2582:102,monitor,monitor,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2582,3,['monitor'],['monitor']
Energy Efficiency,Note: This PR breaks the cromwell-as-a-git-submodle functionality. But I've got verbal confirmation from @hjfbynara that Green is no longer using cromwell this way. This is the error that one sees with the `sbt-git` used in this PR plus a git submodule:. ```java; fatal: Invalid gitfile format: /Users/kshakir/src/cromwell/.git; [error] java.util.NoSuchElementException: head of empty stream; [error] 	at scala.collection.immutable.Stream$Empty$.head(Stream.scala:1104); [error] 	at scala.collection.immutable.Stream$Empty$.head(Stream.scala:1102); [error] 	at com.typesafe.sbt.SbtGit$.$anonfun$buildSettings$21(SbtGit.scala:138); [error] 	at sbt.internal.util.Init$Value.$anonfun$apply$3(Settings.scala:804); [error] 	at sbt.internal.util.EvaluateSettings.$anonfun$constant$1(INode.scala:197); [error] 	at sbt.internal.util.EvaluateSettings$MixedNode.evaluate0(INode.scala:214); [error] 	at sbt.internal.util.EvaluateSettings$INode.evaluate(INode.scala:159); [error] 	at sbt.internal.util.EvaluateSettings.$anonfun$submitEvaluate$1(INode.scala:82); [error] 	at sbt.internal.util.EvaluateSettings.sbt$internal$util$EvaluateSettings$$run0(INode.scala:93); [error] 	at sbt.internal.util.EvaluateSettings$$anon$3.run(INode.scala:89); [error] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); [error] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); [error] 	at java.lang.Thread.run(Thread.java:745); [error] java.util.NoSuchElementException: head of empty stream; ```. cc https://github.com/broadinstitute/cromwell/issues/644,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2785#issuecomment-339382680:121,Green,Green,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2785#issuecomment-339382680,1,['Green'],['Green']
Energy Efficiency,"Now that we've updated our WDLs to 1.0, we've found that `womtool graph` no longer works. It looks like it only supports draft2 and earlier WDL. `test.wdl`:; ```; version 1.0. workflow Test { }; ```. ```; $ java -jar womtool-35.jar graph /tmp/test.wdl ; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: ERROR: Finished parsing without consuming all tokens. version 1.0; ^; ; 	at wdl.draft2.parser.WdlParser.parse(WdlParser.java:2330); 	at wdl.draft2.parser.WdlParser.parse(WdlParser.java:2335); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:266); 	at wdl.draft2.model.WdlNamespace$.$anonfun$load$1(WdlNamespace.scala:160); 	at scala.util.Try$.apply(Try.scala:209); 	at wdl.draft2.model.WdlNamespace$.load(WdlNamespace.scala:160); 	at wdl.draft2.model.WdlNamespace$.loadUsingSource(WdlNamespace.scala:156); 	at wdl.draft2.model.WdlNamespaceWithWorkflow$.load(WdlNamespace.scala:571); 	at womtool.graph.GraphPrint$.generateWorkflowDigraph(GraphPrint.scala:19); 	at womtool.WomtoolMain$.graph(WomtoolMain.scala:94); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:48); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:125); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:130); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:18); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:18); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```; ; The `womgraph` command still works, but the output from that command is so verbose it's unusable for viewing our workflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4234:1537,adapt,adapted,1537,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4234,1,['adapt'],['adapted']
Energy Efficiency,"OK I think the TDD world is fucking wit me as code cov gave me a red mark with a 0% diff. Everyhing else is green, merging.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2906#issuecomment-344821296:108,green,green,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2906#issuecomment-344821296,1,['green'],['green']
Energy Efficiency,"ONITOR_MOUNT_POINT:-""/cromwell_root""}; SLEEP_TIME=${SLEEP_TIME:-""10""}. function getCpuUsage() {; # get cpu info just by grep-ing from /proc/stat. Use awk to convert to %; grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$4+$5)} END {printf ""%.1f%%"", usage}'; }. function getMem() {; # get desired memory value from /proc/meminfo, in GiB, and also; # as a percentage of total; # argument is the label of the desired memory value; cat /proc/meminfo \; | awk -v MEM_FIELD=""$1"" '{; f[substr($1, 1, length($1)-1)] = $2; } END {; printf ""%.2f GiB"", f[MEM_FIELD] / 1048576; }' ; }. function getMemUnavailable() {; # get unavailable memory from /proc/meminfo, in GiB; cat /proc/meminfo \; | awk '{; f[substr($1, 1, length($1)-1)] = $2; } END {; ; if(""MemAvailable"" in f) {; mem_available = f[""MemAvailable""]; } else {; mem_available = f[""MemFree""] + f[""Buffers""] + f[""Cached""]; }; mem_in_use = f[""MemTotal""] - mem_available; printf ""%.2f Gib %.1f%%"", mem_in_use / 1048576, 100 * mem_in_use / f[""MemTotal""] ; }' ; }. function getDisk() {; # get information about disk usage from ""df"" command.; DISK_COLUMN=$(echo ""$1"" | awk '{print tolower($1)}'); MOUNT_POINT=$2; df -h ""$MOUNT_POINT"" \; | sed 's/Mounted on/Mounted-on/' \; | awk -v DISK_COLUMN=$DISK_COLUMN '; FNR==1 {; for(i=1; i<=NF; i++) { f[tolower($i)]=NF-i }; }; FNR>1 {; FIELD_NUM=NF-f[DISK_COLUMN]; if(FIELD_NUM > 0) {; print $(FIELD_NUM); }; }'; }. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(getCpuUsage); echo \* Memory usage: $(getMemUnavailable); echo \* Disk usage: $(getDisk Used $MONITOR_MOUNT_POINT) $(getDisk Use% $MONITOR_MOUNT_POINT); }. echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(getMem MemTotal); echo Total Disk space: $(getDisk Size $MONITOR_MOUNT_POINT); echo ; echo --- Runtime Information ---. while true; do; runtimeInfo; sleep $SLEEP_TIME; done; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027:3106,MONITOR,MONITORING,3106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507#issuecomment-436035027,1,['MONITOR'],['MONITORING']
Energy Efficiency,"Ok thanks, would you take charge of that?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5276#issuecomment-553168102:26,charge,charge,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5276#issuecomment-553168102,1,['charge'],['charge']
Energy Efficiency,"Okay. So initially, I was passing the `ServiceRegistryActor` reference via props down the chain of actors, and each of the actors needed to follow a pattern of steps to insert into the metadata (e.g. create a `MetadataEvent` with a `MetadataPutAction` within which the `Metadata[Key, Value]` resided, send this message over to the the service registry, and handle failures, if any. It did not look good by any stretch IMO. All the metadata generating entities needed to be aware of the `MetadataService` (the erstwhile dataAcess). **Edit:** Using the above design in-fact now. ~~Currently, what I've done below is create a single instance of the `ServiceRegistryActor` in the `WorkflowManagerActor`and then create a `WorkflowProfilerActor` (one per workflow) which is supposed to handle all the metadata information coming from the engine side for a particular workflow. The way it happens is based on the presumption that almost all the information that we needed was present in the `StateName` and `StateData` of our FSMs. Unfortunately, with Akka's `SubscribeTransitionalCallback` we can only monitor the FSM states, and not the data. So I've created a trait (which the Engine's FSMs can extend from) which provide the semantics of wrapping up the state and data of the FSM in a message, and publish it into Akka's event stream. The ProfilerActor is the listener of these events and handles them appropriately. With this, I was able to make the FSMs unaware of the MetadataServices, and simply publish it's state and data in the event stream while performing any transitions.~~. ~~Let me know if you guys have any (other?) ideas / suggestions.~~. Contents added to metadata with this PR:; - [x] workflowName; - [ ] calls (To come from the backends); - [x] outputs ; - [x] id; - [x] inputs; - [x] submission; - [x] status; - [x] end; - [x] start",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829:1096,monitor,monitor,1096,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829,1,['monitor'],['monitor']
Energy Efficiency,"Okay. So initially, I was passing the `ServiceRegistryActor` reference via props down the chain of actors, and each of the actors needed to follow a pattern of steps to insert into the metadata (e.g. create a `MetadataEvent` with a `MetadataPutAction` within which the `Metadata[Key, Value]` resided, send this message over to the the service registry, and handle failures, if any. It did not look good by any stretch IMO. All the metadata generating entities needed to be aware of the `MetadataService` (the erstwhile dataAcess). Currently, what I've done below is create a single instance of the `ServiceRegistryActor` in the `WorkflowManagerActor`and then create a `WorkflowProfilerActor` (one per workflow) which is supposed to handle all the metadata information coming from the engine side for a particular workflow. The way it happens is based on the presumption that almost all the information that we needed was present in the `StateName` and `StateData` of our FSMs. Unfortunately, with Akka's `SubscribeTransitionalCallback` we can only monitor the FSM states, and not the data. So I've created a trait (which the Engine's FSMs can extend from) which provide the semantics of wrapping up the state and data of the FSM in a message, and publish it into Akka's event stream. The ProfilerActor is the listener of these events and handles them appropriately. With this, I was able to make the FSMs unaware of the MetadataServices, and simply publish it's state and data in the event stream while performing any transitions. . Let me know if you guys have any (other?) ideas / suggestions. Edit: Names of new actors might be pretty bad IMO. Please suggest better ones if you have any. _P.S. : Currently based out of the Chris's branch since his metadata changes were needed._; _P.P.S. : Still a WIP for improving some stuff._",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/822:1048,monitor,monitor,1048,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/822,1,['monitor'],['monitor']
Energy Efficiency,"Or, ""unflakify `WriteMetadataSpec`"". This test's assertions that metadata write batching and retrying would happen in the way it was expecting them to was upset if a scheduled metadata flush occurred before the first batch was full. This was leading to a non-trivial rate of sbt tests failing with messages like ""expected 50 database writes but got 52""",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4898:166,schedul,scheduled,166,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4898,1,['schedul'],['scheduled']
Energy Efficiency,"Or, now that I think I understand the original scheme a little better:. - The job enters the `Running` state; - We immediately schedule an `CheckAlive` message to the actor at the configurable time; - If the cadence is not set, we never send that message (this would be the default); - When that CheckAlive arrives we can run `isAlive` and remember the result (and if we're still alive, schedule another `CheckAlive` again after the same delay); - To decide the new status:; - If we see a return code file, we use it; - Otherwise if `isAlive` is false and we've waited too long (could be configurable but I think we can set a sensible default) then we fail. I think that way the behavior is identical to today by default, but if we schedule `isAlive`s, then they behave as your scheme would imply. Do you think that would work?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423571735:127,schedul,schedule,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423571735,3,['schedul'],['schedule']
Energy Efficiency,"Our Green friends are regularly seeing cases where Cromwell reports a workflow's `status` as `On Hold` but follow-on calls to `releaseHold` return a 403 HTTP status. This appears to be happen because `status` queries go to the potentially laggy workflow summary table but `releaseHold` goes straight to the workflow store. Cromwell [returns 403](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/webservice/routes/CromwellApiService.scala#L148) if the workflow for which `releaseHold` is invoked is [in the workflow store but not in `On Hold` or `Submitted`](https://github.com/broadinstitute/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/workflow/workflowstore/SqlWorkflowStore.scala#L140) status. . 403 seems like a strange HTTP status to return in this case. If the thought was ""the workflow has to be on hold to be able to release the hold"" then we would also return 403 for workflows found in `Submitted`, but we don't; those get a 200. Since the status queries will always be laggy to some degree it seems to make more sense to interpret the endpoint as ""put this workflow in a runnable state"", where finding the workflow already in a `Running` state would not be cause for alarm.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4983:4,Green,Green,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4983,1,['Green'],['Green']
Energy Efficiency,PAPI v2 monitoring action,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4562:8,monitor,monitoring,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4562,1,['monitor'],['monitoring']
Energy Efficiency,"PR 1 of 3:; 1. Remove cromwell-core dependency from cloud-support; 2. Run jes centaur on travis; 3. Generate coverage for integration tests. ---. Instead of credentials requiring WorkflowOptions, any String => String will do, including Map[String, String].; Retrieving credentials only requires actorSystem/executionContext when retrying.; Moved logback dependencies from common library over to testing.; Added mockito to all artifact tests.; Fixed akka-stream-testkit dependency appearing in core's main instead of test.; Split confusingly named baseDependencies into configDependencies ++ catsDependencies.; Other dependency cleanup to reduce duplicates and extra transitive dependencies.; Log stderr from centaur'ed cromwell failures.; The total attempt time to connect to cromwell for a test is now longer than the timeout of a cromwell restart.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2938:638,reduce,reduce,638,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2938,1,['reduce'],['reduce']
Energy Efficiency,"PR Documentation describing the SparkBackend added to Cromwell. This PR adds support for execution of spark jobs as task in a workflow using the existing wdl format, with restrictions on the environment like having a local file system, a shared file system or a network file system when running a spark job in the spark standalone cluster mode. This implementation takes a wdl with the backend configuration specified as ""Spark"" and then generates the appropriate spark commands and monitoring process to ensure the job runs to completion. Meaning, details of the spark internals are completely abstracted from the user provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.â€‚â€‚. Current deploy modes supported for any spark job:; â€‚â€‚a - Client deploy mode using the spark standalone cluster manager; â€‚â€‚b - Cluster deploy mode using the spark standalone cluster manager; â€‚â€‚c - Client deploy mode using Yarn resource manager; â€‚â€‚d - Cluster deploy mode using Yarn resource manager; â€‚â€‚; Future PR Plans:; â€‚â€‚In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).; â€‚â€‚This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation. â€‚â€‚In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily bec",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1339:483,monitor,monitoring,483,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339,1,['monitor'],['monitoring']
Energy Efficiency,"Perf testing has shown that removing this query improves CC time and reduces DB load (see last row in CC google doc); Unclear if it's worth keeping it as a configurable thing ?; This keeps storing the individual hashes, it just stops using them for ""fast"" cache miss detection.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4121:69,reduce,reduces,69,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4121,1,['reduce'],['reduces']
Energy Efficiency,Potential hotfix candidate but would be nice to know why these empty queues are appearing in the first place. An attempt to recover from (though probably not fix the underlying cause of) tokens going missing due to stack traces like:; ```; [cromwell-system-akka.actor.default-dispatcher-1158] ERROR akka.actor.OneForOneStrategy - dequeue on empty queue; java.util.NoSuchElementException: dequeue on empty queue; 	at scala.collection.immutable.Queue.dequeue(Queue.scala:155); 	at cromwell.engine.workflow.tokens.TokenQueue.recursingDequeue(TokenQueue.scala:63); 	at cromwell.engine.workflow.tokens.TokenQueue.dequeue(TokenQueue.scala:50); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1(RoundRobinQueueIterator.scala:46); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.$anonfun$findFirst$1$adapted(RoundRobinQueueIterator.scala:46); 	at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:415); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1169); 	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1159); 	at scala.collection.immutable.StreamIterator.$anonfun$next$1(Stream.scala:1058); 	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1047); 	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1052); 	at scala.collection.TraversableOnce.collectFirst(TraversableOnce.scala:144); 	at scala.collection.TraversableOnce.collectFirst$(TraversableOnce.scala:132); 	at scala.collection.AbstractTraversable.collectFirst(Traversable.scala:104); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.findFirst(RoundRobinQueueIterator.scala:48); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:32); 	at cromwell.engine.workflow.tokens.RoundRobinQueueIterator.next(RoundRobinQueueIterator.scala:10); 	at scala.collection.Iterator$SliceIterator.next(Itera,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4909:835,adapt,adapted,835,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4909,1,['adapt'],['adapted']
Energy Efficiency,Power of 2s !,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4489:0,Power,Power,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4489,1,['Power'],['Power']
Energy Efficiency,Power through non-findable terminals in error messages,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3048:0,Power,Power,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3048,1,['Power'],['Power']
Energy Efficiency,"Probably, depending on what you mean exactly. The real impetus is for monitoring tools though",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/148#issuecomment-134176327:70,monitor,monitoring,70,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/148#issuecomment-134176327,1,['monitor'],['monitoring']
Energy Efficiency,"Quoth Dave: Green Team launched 50 workflows and could initially query the API despite some slowness. After theyâ€™ve been running for 45 mins or so, hitting the API is only intermittently successful:. ```; https://cromwell.gotc-staging.broadinstitute.org/api/workflows/v1/query; Ooops! The server was not able to produce a timely response to your request.; Please try again in a short while!; ```. ```; Unexpected error while awaiting Cromwell Workflow completion: Error hitting REST API: https://cromwell.gotc-staging.broadinstitute.org/api/workflows/v1/5296889b-8b88-41db-a5fa-d1071ac22a... => Unexpected response code: 502; ```. Just trying to get to the swagger page takes a couple of minutes to load or fails to load altogether. This is highly variable - about 2 hours in we still have 50 workflows running and API queries are coming back very quickly. In production using Cromwell 0.19, GotC routinely runs ~200-500 workflows simultaneously.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228405075:12,Green,Green,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1065#issuecomment-228405075,1,['Green'],['Green']
Energy Efficiency,"Rather than shoving this required, one-per-entry information as a runtime-required value in detritus, why not add a not-nullable column in the call caching metainfo?. This would also let us reduce the size of the detritus table by optionally dropping the constant callRootPath prefix from every path",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1485:190,reduce,reduce,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1485,1,['reduce'],['reduce']
Energy Efficiency,"Re this PR, I'm re-working it to pass all of the task call-specific monitoring info (workflow name/id, task call name/index/attempt, `inputs`, `disks`, and the opaque image-specific `monitoringConfig` string) through a JSON config file on GCS (thanks @kshakir for the idea!). This way, we no longer have to use environmental variables, and can pass much more data than can be held in a variable (looking at you, `inputs`!). That still has a potential problem of running against the API quota for GCS, but since we already access multiple files from that execution bucket inside the task, the assumption is that accessing one extra file won't hurt. However, I'm unsure if we should just get rid of all of the environmental variables in favor of one that points to the GCS URL of the JSON config, or should we keep them (and thus duplicate information) for backwards compatibility. AFAIK, hardly anyone used this feature yet, especially given that the current ""official"" `monitoring_image` has been essentially broken, because Google decided to start failing Stackdriver Monitoring requests if they are submitted more than 1/minute, for a given time series. We should fix the `monitoring_image` separately (or replace it with the BigQuery version), but the question remains whether we still want to keep those env vars around for that previous use case (for anyone who might've been using a custom `monitoring_image` already). Alternatively, we could pass only the `inputs` and `monitoringConfig` through a JSON blob on GCS, and continue with the rest as-is via env vars (but that's messy, IMO).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502266400:68,monitor,monitoring,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028#issuecomment-502266400,4,"['Monitor', 'monitor']","['Monitoring', 'monitoring', 'monitoringConfig']"
Energy Efficiency,Reduce Scope of EJEA,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1413:0,Reduce,Reduce,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1413,1,['Reduce'],['Reduce']
Energy Efficiency,Reduce hash cost of WdlSyntaxErrorFormatter,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3965:0,Reduce,Reduce,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3965,1,['Reduce'],['Reduce']
Energy Efficiency,Reduce hashcode work in file hashing actor,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2555:0,Reduce,Reduce,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2555,1,['Reduce'],['Reduce']
Energy Efficiency,Reduce hashing closes #1603,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1613:0,Reduce,Reduce,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1613,1,['Reduce'],['Reduce']
Energy Efficiency,Reduce likelihood of DRS hogging IoActor NIO threads [BT-341],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6439:0,Reduce,Reduce,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6439,1,['Reduce'],['Reduce']
Energy Efficiency,Reduce number of ExecutionStore updates generated by large scatters [BA-6612 trial],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5881:0,Reduce,Reduce,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5881,1,['Reduce'],['Reduce']
Energy Efficiency,"Reduce number of logged ""unhandled"" messages",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4260:0,Reduce,Reduce,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4260,1,['Reduce'],['Reduce']
Energy Efficiency,Reduce number of parallel tests running on TES backend [BT-101],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6174:0,Reduce,Reduce,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6174,1,['Reduce'],['Reduce']
Energy Efficiency,Reduce quantity of scary logging,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1421:0,Reduce,Reduce,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1421,1,['Reduce'],['Reduce']
Energy Efficiency,"Reduce quantity of scary logging (and don't rely on scary logging in the tests) Closes 1311, Closes 1422",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1422:0,Reduce,Reduce,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1422,1,['Reduce'],['Reduce']
Energy Efficiency,Reduce the reliance on lenthall,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1211:0,Reduce,Reduce,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1211,1,['Reduce'],['Reduce']
Energy Efficiency,Reduced test framework dependencies,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6474:0,Reduce,Reduced,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6474,1,['Reduce'],['Reduced']
Energy Efficiency,"Regardless of where the task is executing though, that `sync` is expensive, and the more cores and memory you have on execution nodes, and the less work the `command` is actually doing, the worse things become when lots of tasks get allocated to a given node.; Simple ""echo Hello, World"" tasks occupy one of our 56-core execution nodes for about 30 seconds when 56 of them are allocated to a node, nearly the whole time spent in sync. ; The answer could well be ""then stop doing that"", but I hope not!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284765336:233,allocate,allocated,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284765336,2,['allocate'],['allocated']
Energy Efficiency,"Reproduced the assembly failure in Travis on a different branch that was green last night. i.e., the problem isn't here, so I'm merging this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/582#issuecomment-202836710:73,green,green,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/582#issuecomment-202836710,1,['green'],['green']
Energy Efficiency,"Right now if a workflow fails and Cromwell is in fail fast mode, the workflow gets marked as failed and monitoring stops. What this means is if one task fails, but another is ""Running"", that other task will always be in a ""Running"" state. This leads to many user questions (e.g. ""Is this still running and costing me money?""). . Every task should be monitored to its final state rather than only workflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861:104,monitor,monitoring,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861,2,['monitor'],"['monitored', 'monitoring']"
Energy Efficiency,"Running 3step workflow on JES with a workflow options file that looks like this. ``` json; {; ""monitoring_script"": ""gs://sfrazer-dev/monitor.sh""; }; ```. the `monitor.sh` script contains:. ``` bash; echo this; echo is; echo a; echo test; ```. It seems that for my three invocations of this workflow, they all had the correct data in `monitoring.log` for the `ps` sub-command, but not the other two commands.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1158:133,monitor,monitor,133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158,3,['monitor'],"['monitor', 'monitoring']"
Energy Efficiency,Running locally: https://github.com/common-workflow-language/common-workflow-language/blob/master/CONFORMANCE_TESTS.md. Then get it on their master one: http://ci.commonwl.org/. Don't wire it into our travis yet as we won't be green for a while.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2590:227,green,green,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2590,1,['green'],['green']
Energy Efficiency,"SCHEDULED for job projects/392615380452/locations/us-south1/jobs/job-ba81bad8-82e9-4d95-8fc0-04dfbbd746da.; taskExecution.exitCode=0. Event type=STATUS_CHANGED,; time=seconds: 1712173947, nanos: 568998105; taskState=STATE_UNSPECIFIED; description=Job state is set from SCHEDULED to RUNNING for job projects/392615380452/locations/us-south1/jobs/job-ba81bad8-82e9-4d95-8fc0-04dfbbd746da.; taskExecution.exitCode=0. Event type=STATUS_CHANGED; time=seconds: 1712173989, nanos: 937816549; taskState=STATE_UNSPECIFIED; description=Job state is set from RUNNING to SUCCEEDED for job projects/392615380452/locations/us-south1/jobs/job-ba81bad8-82e9-4d95-8fc0-04dfbbd746da.; taskExecution.exitCode=0; ```. What we define as execution events:. ```; ExecutionEvent(Job state is set from QUEUED to SCHEDULED for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:10:01.704137839Z,None); ExecutionEvent(Job state is set from SCHEDULED to RUNNING for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:11:30.631264449Z,None); ExecutionEvent(Job state is set from RUNNING to SUCCEEDED for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:12:16.898798407Z,None); ```. </details>; </details>. ## Load test results. We have executed many load tests, this is the latest one involving 14k jobs. Data / Backend | Batch with Mysql | PAPIv2 with Mysql; ------------- | -------------|---------; Jobs | 14400 | 14400; Execution time | 20936 seconds | 24451 seconds. Overall, all our tests indicate that Batch finishes executing the jobs faster than PAPIv2. <details>; <summary>Load tests settings</summary>. We have ran Cromwell in server mode with the following settings:. - request-timeout: 10m; - idle-timeout: 10m; - job-rate-control: jobs = 20, per = 10 seconds; - max-workflow-launch-count: 50; - new-workflow-poll-rate: 1; - database: MySQL; - vir",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412:4973,SCHEDUL,SCHEDULED,4973,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412,1,['SCHEDUL'],['SCHEDULED']
Energy Efficiency,"Scala steward updates [BW-930]. sbt-assembly, google-cloud-monitoring, guava, ficus, akka-http-circe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6571:59,monitor,monitoring,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6571,1,['monitor'],['monitoring']
Energy Efficiency,Scala-Steward: Update google-cloud-monitoring from 1.100.1 to 2.0.0,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5742:35,monitor,monitoring,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5742,1,['monitor'],['monitoring']
Energy Efficiency,Scala-Steward: Update google-cloud-monitoring from 1.100.1 to 2.0.1,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5764:35,monitor,monitoring,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5764,1,['monitor'],['monitoring']
Energy Efficiency,Scala-Steward: Update google-cloud-monitoring from 2.0.1 to 2.0.3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5884:35,monitor,monitoring,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5884,1,['monitor'],['monitoring']
Energy Efficiency,Scala-Steward: Update google-cloud-monitoring from 2.0.1 to 2.0.4,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5926:35,monitor,monitoring,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5926,1,['monitor'],['monitoring']
Energy Efficiency,Scala-Steward: Update google-cloud-monitoring from 2.0.14 to 2.3.3,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6400:35,monitor,monitoring,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6400,1,['monitor'],['monitoring']
Energy Efficiency,Scala-Steward: Update google-cloud-monitoring from 2.0.4 to 2.0.5,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5962:35,monitor,monitoring,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5962,1,['monitor'],['monitoring']
Energy Efficiency,Scala-Steward: Update google-cloud-monitoring from 2.0.4 to 2.0.6,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5971:35,monitor,monitoring,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5971,1,['monitor'],['monitoring']
Energy Efficiency,Scala-Steward: Update google-cloud-monitoring from 2.0.4 to 2.0.7,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6011:35,monitor,monitoring,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6011,1,['monitor'],['monitoring']
Energy Efficiency,Scala-Steward: Update google-cloud-monitoring from 2.0.4 to 2.0.8,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6063:35,monitor,monitoring,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6063,1,['monitor'],['monitoring']
Energy Efficiency,Scala-Steward: Update google-cloud-monitoring from 2.0.8 to 2.0.14,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6253:35,monitor,monitoring,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6253,1,['monitor'],['monitoring']
Energy Efficiency,Scala-Steward: Update google-cloud-monitoring from 3.0.2 to 3.0.6,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6518:35,monitor,monitoring,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6518,1,['monitor'],['monitoring']
Energy Efficiency,Scala-Steward: Update google-cloud-monitoring from 3.0.6 to 3.0.8,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6610:35,monitor,monitoring,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6610,1,['monitor'],['monitoring']
Energy Efficiency,Scala-Steward: Update google-cloud-monitoring from 3.2.5 to 3.2.10,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7041:35,monitor,monitoring,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7041,1,['monitor'],['monitoring']
Energy Efficiency,Scala-Steward: Update google-cloud-monitoring from 3.2.5 to 3.2.9,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6859:35,monitor,monitoring,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6859,1,['monitor'],['monitoring']
Energy Efficiency,Schedule a retry after database exceptions,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2016:0,Schedul,Schedule,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2016,1,['Schedul'],['Schedule']
Energy Efficiency,"See @Horneth 's laptop, test the pieces of the Green team's pipeline. Do what the Green team (or workbench team) is doing. Capture it.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2112:47,Green,Green,47,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2112,2,['Green'],['Green']
Energy Efficiency,"See https://github.com/broadinstitute/cromwell/pull/4022 - originally closed because I wasn't sure it was the right thing, and I didn't see a way to test it. I'm now more certain it's worth trying. Even if this doesn't actually fix the deadlock and we need to use one of the other solutions, I think this may still be worth having because it will save the RDBMS a lot of work coordinating transactions. [`autocommit` documentation](https://dev.mysql.com/doc/refman/8.0/en/innodb-autocommit-commit-rollback.html); [Slick transactions and pinned sessions](http://slick.lightbend.com/doc/3.2.0/dbio.html#transactions-and-pinned-sessions). Acceptance criteria:; - [ ] Not a performance regression - heartbeats are still efficiently written to the DB in batches; - [ ] Prevents the deadlock",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4249:716,efficient,efficiently,716,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4249,1,['efficient'],['efficiently']
Energy Efficiency,"See linked epic for related postmortem. * happens If # of VMs > maximum number of IPs available; * throttling of API calls to GCE prevents destruction of finished VMs (DB: t sure ; * Finished VMs are holding IP addresses, preventing new calls from obtaining them. # IP Exhaustion. * New networks are in /20 CIDR block, allowing 2^12 = 4096 IP addresses; * PAPI v1 is limited to default network; * PAPI v2 can specify network per project (TODO: confirm per project? per call?); * PAPI v2 non-default networks can use /16 and thus 65K IP addresses. # Context. * Can only occur when quota increase is requested to put max # cpus > available IPs. # Mechanics. * Pass in network name as workflow option from rawls -> cromwell. # Questions. * What caused the API throttling in the first place? We allocated too many VM's and just generally sent too much traffic to GCE?; * Does PAPI v2 address the IP exhaustion situation? Otherwise we have to manage the resources for it.; * Migration of old projects to use PAPI v2 ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3665:791,allocate,allocated,791,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3665,1,['allocate'],['allocated']
Energy Efficiency,"See the various commit test timings to see whether tests did or did not run. Idea:; * One green light is as good as two since in any real situation the 'push' tests would be restarted if the 'PR' tests are green.; * We can still test on push if we really want to, using `FORCETEST` in the commit message; * We won't have so many ""background"" tests running at any given time, hopefully reducing the queue-time for PRs. In other words:; * Should significantly *increase* cycle time for PRs and *reduce* developer time shepherding PRs through two sets of identical tests",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4839:90,green,green,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4839,3,"['green', 'reduce']","['green', 'reduce']"
Energy Efficiency,"Seen in FC alpha, two workflow ids and were still listed as `Running` even though the summarizer had already passed the `Succeeded` entries written in for their metadata both at `22:14:47`.; - `c5b6ee46-1f09-4830-91a8-fd814866d664`; - `c5d48405-06ca-4207-9d1a-5923a9eb0fcd`. `Succeeded` entries written seconds before and seconds after were summarized correctly. ```; $ docker [redacted] -e alpha; Welcome to the MariaDB monitor. Commands end with ; or \g.; Your MySQL connection id is 2786095; Server version: 5.6.36-google-log (Google). Copyright (c) 2000, 2016, Oracle, MariaDB Corporation Ab and others. Type 'help;' or '\h' for help. Type '\c' to clear the current input statement. MySQL [cromwell]> select * from SUMMARY_STATUS_ENTRY;; +-------------------------+---------------------------------+-----------------------+------------+; | SUMMARY_STATUS_ENTRY_ID | SUMMARY_TABLE_NAME | SUMMARIZED_TABLE_NAME | MAXIMUM_ID |; +-------------------------+---------------------------------+-----------------------+------------+; | 1 | WORKFLOW_METADATA_SUMMARY_ENTRY | METADATA_ENTRY | 2993666495 |; +-------------------------+---------------------------------+-----------------------+------------+; 1 row in set (0.05 sec). MySQL [cromwell]> select * from METADATA_ENTRY where METADATA_JOURNAL_ID >= 2993666495;; +---------------------+--------------------------------------+--------------+----------+-------------------+-------------------+---------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------+---------------------+; | METADATA_JOURNAL_ID | WORKFLOW_EXECUTION_UUID | METADATA_KEY | CALL_FQN | JOB_SCATTER_INDEX | JOB_RETRY_ATTEMPT | METADATA_VALUE | METADATA_TIMESTAMP | METADATA_VALUE_TYPE |; +---------------------+--------------------------------------+--------------+----------+-------------------+-------------------+-----------------------------------------------------",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4511:421,monitor,monitor,421,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4511,1,['monitor'],['monitor']
Energy Efficiency,Sequence DBIOActions to reduce the number of transactions.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/364:24,reduce,reduce,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/364,1,['reduce'],['reduce']
Energy Efficiency,"Should help with debugging occasional failures like this in travis by replacing ""not found"" with ""this looks similar"":. ```; StatsDInstrumentationServiceActorSpec:; StatsDInstrumentationServiceActor; - should increment counters (1 second, 193 milliseconds); - should add count (1 second, 9 milliseconds); - should set gauges (1 second, 10 milliseconds); info- should set timings *** FAILED *** (3 seconds, 701 milliseconds); info Missing packet: prefix_value.cromwell.test_prefix.test.metric.bucket.timing.stddev:0.00|g (StatsDInstrumentationServiceActorSpec.scala:83); info org.scalatest.exceptions.TestFailedException:; info ...[0m[0m; info at cromwell.services.instrumentation.impl.statsd.StatsDInstrumentationServiceActorSpec.$anonfun$new$4(StatsDInstrumentationServiceActorSpec.scala:83); info at cromwell.services.instrumentation.impl.statsd.StatsDInstrumentationServiceActorSpec.$anonfun$new$4$adapted(StatsDInstrumentationServiceActorSpec.scala:83); info at scala.collection.immutable.Set$Set4.foreach(Set.scala:206); info at cromwell.services.instrumentation.impl.statsd.StatsDInstrumentationServiceActorSpec.$anonfun$new$2(StatsDInstrumentationServiceActorSpec.scala:83); info at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); info at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); info at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); info ...; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4387:910,adapt,adapted,910,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4387,1,['adapt'],['adapted']
Energy Efficiency,"Similar to #6324 , but with localization instead of call caching. . The framing of the approach here is:; Make localization of inputs location-aware.; Add a workflow option enabling control of what egress charges can be incurred for input localization.; The new workflow option would be:; localization_egress: [none, continental, global]. where the values affect whether localization of inputs can incur egress charges:; none: only within-region copies are allowed, which generate no egress charges; continental: within content copies are allowed; within-content copies have reduced costs, such as $0.01 / GB in the US; global: copies across all regions are allowed. Cross-content egress charges can be much higher (ranging from $0.08 / GB up to $0.23 / GB). There may also be cases where we have access to get and list objects in the bucket, but can't view the metadata of the bucket. This prevents us from figuring out the location of the bucket. To aid with this, this PR also adds the workflow option:; localization_egress_strict: [true, false] ; If localization_egress_strict is true, then the workflow will fail if we are unable to determine the location of the buckets. ### CURRENT STATUS OF PR:; The idea for this approach is to add an action right before localization, called egressCheck. In this action, we compare the bucket location of each input file with the location of the VM. If we determine there will be egress charge, we will exit appropriately (depending on workflow option).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6332:205,charge,charges,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6332,6,"['charge', 'reduce']","['charge', 'charges', 'reduced']"
Energy Efficiency,"Since Jeff is probably busy, this is the ticket where hopefully @Horneth can take his place. I'll switch out the assignments to reduce confusion...sorry @geoffjentry!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/792#issuecomment-217962409:128,reduce,reduce,128,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/792#issuecomment-217962409,1,['reduce'],['reduce']
Energy Efficiency,Since it told me to merge #80 first I did so. If this flips back green afterwards before I leave I'll merge this too,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/79#issuecomment-118390659:65,green,green,65,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/79#issuecomment-118390659,1,['green'],['green']
Energy Efficiency,"Since the `monitoring_log` test checks whether the monitoring script is working normally, which it is, for the time being fix the memory expectation to unblock other PRs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5132:51,monitor,monitoring,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5132,1,['monitor'],['monitoring']
Energy Efficiency,"Since this is fairly backend-specific I've implemented as a `cromwell.backend.google.pipelines.common.api.RunStatus` instead of a `cromwell.core.ExecutionStatus`. It also reduces the scope of changes. Feedback welcome. Looks like this in metadata requests:; ```; ""calls"": {; ""sleepy_sleep.sleep"": [; {; ""preemptible"": false,; ""executionStatus"": ""Running"",; ""stdout"": ""gs://cloud-cromwell-dev-self-cleaning/cromwell_execution/ci/sleepy_sleep/058bff35-4a55-4c0f-9113-0885f4119cd9/call-sleep/stdout"",; ""backendStatus"": ""AwaitingCloudQuota"",; ""compressedDockerSize"": 28566425,; ""commandLine"": ""sleep 180;\nls -la"",; ""shardIndex"": -1,; ""jes"": {; ""executionBucket"": ""gs://cloud-cromwell-dev-self-cleaning/cromwell_execution/ci"",; ""endpointUrl"": ""https://lifesciences.googleapis.com/"",; ""googleProject"": ""broad-dsde-cromwell-dev""; },; ```; The Terra UI workflow dashboard currently makes calls that look like; ```; https://rawls.dsde-dev.broadinstitute.org/api/workspaces/general-dev-billing-account/anichols-post-ppw/submissions/a7cfb487-9c30-4fd7-b10f-19d6f3c1d192/workflows/f52e4e4b-0299-44b7-a16f-904d2ee3f1e9; ?includeKey=end; &includeKey=executionStatus; &includeKey=failures; &includeKey=start; &includeKey=status; &includeKey=submittedFiles:workflow; &includeKey=workflowLog; &includeKey=workflowName; &includeKey=callCaching:result; &includeKey=callCaching:effectiveCallCachingMode; ```; so it would be easy to add a `&includeKey=backendStatus` and logic to evaluate something like; ```; if (executionStatus == ""Running"" && backendStatus == ""AwaitingCloudQuota"") {; msg = ""Trying to run but needs quota""; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6655:171,reduce,reduces,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6655,1,['reduce'],['reduces']
Energy Efficiency,"So, quick thoughts on this. +1 on @delagoya's sidecar container concept (with some concerns I'll mention below). When implementing #3835 I had this issue in mind as well, along with some thoughts on implementation. My thought was to have a sidecar always running, using the [system events](https://docs.docker.com/engine/api/v1.37/#operation/SystemEvents) api to monitor for containers that have exited. At that time, it can [inspect the container](https://docs.docker.com/engine/api/v1.37/#operation/ContainerInspect), make sure it's a cromwell container, and use the volume information (in conjunction with the TASK_ID environment variable I'm setting) to find the local files and copy them out to s3. Something to consider that I don't see discussed yet on this thread: A sidecar approach requires the sidecar to have fairly permissive access to S3. On the positive side, this does alleviate the need for S3 permissions on the container running the task.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-401398435:363,monitor,monitor,363,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-401398435,1,['monitor'],['monitor']
Energy Efficiency,Some updates to support migrating null outputs to the job store simpleton table. Also cherry picked a TES commit to get the build green.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2140:130,green,green,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2140,1,['green'],['green']
Energy Efficiency,"Sorry for the PR bombardment. I came across issue #5271 and noticed this was an issue we also had, and have partially solved already. This is the documentation of our solution. It is not as elegant and efficient as SQLite, but it gets the job done. I have heard of a center that has a separate cromwell server (with database) running **for each user** to get around filesystem permissions and privacy/access concerns. This is a bit unwieldy to say the least. . A file-based database solves these problems by allowing each user to run `cromwell `on the command line and automatically creating a file-based database that is tied to the filesystem permissions.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5320:202,efficient,efficient,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5320,1,['efficient'],['efficient']
Energy Efficiency,"Starting last summer/fall, MySQL adopted a new release schedule that includes faster cycles with breaking changes [0]. We should stick to the 8.0.xx branch [1] as this is now considered LTS and will be best supported by tools like TestContainers and CloudSQL. [0] https://dev.mysql.com/blog-archive/introducing-mysql-innovation-and-long-term-support-lts-versions/; [1] https://github.com/testcontainers/testcontainers-java/pull/8131",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7360:55,schedul,schedule,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7360,1,['schedul'],['schedule']
Energy Efficiency,"Starting to spider out the proofing of concept for a google-y metadata system into something which is actually storing events as well as providing a not horribly inefficient read access for the current set of metadata-y endpoints. A high level description: Stream metadata events out of Cromwell via Google PubSub, and store them in two locations. The first is a permanent event store which will be storing these events in an immutable fashion, which will allow us to be flexible with downstream presentation w/o information loss. The second will be a set of SQL tables which have been designed to provide efficient results for all of the standard Cromwell metadata endpoints such as metadata, status, outputs, etc. For Broad folks, more information is available [here](https://docs.google.com/document/d/1F5WsEAKvYx6njdF-yJZ4LHvT39KcErCdBpCOySq-NoQ/edit)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3243:606,efficient,efficient,606,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3243,1,['efficient'],['efficient']
Energy Efficiency,"Still working on getting full test suite to pass, but DBMS test is green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7160#issuecomment-1591837281:67,green,green,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7160#issuecomment-1591837281,1,['green'],['green']
Energy Efficiency,"Submitting for discussion. This change should fix some of the errors of this type we're seeing by clearing cache for both relevant workflows instead of just one. However, it will only do so in the specific case where the initial test failure happens when checking cache behavior, because that's the only time we have easy access to the id of the associated workflow. My assumption is that this will reduce the likelihood of this error but not eliminate it. . Before going back and making a larger change to pass an object containing all relevant workflow ids through a bunch of different test code, to ensure it can always be part of `CentaurTestException`, I wanted to get some initial feedback. Is this (adding additional workflow id(s) to `CentaurTestException` so that we can easily clear their cache hits from the database in `tryTryAgain`) the right direction to fix this problem? It feels wrong to update the signatures of all these unrelated methods just to populate the exception. I also thought about trying to update `TestFormulas.runWorkflowTwiceExpectingCaching` and other similar methods to capture the raised `CentaurTestException`, add the additional workflow id(s), and rethrow, but didn't want to mess with the location the error is thrown from.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134:399,reduce,reduce,399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6654#issuecomment-1016819134,1,['reduce'],['reduce']
Energy Efficiency,"Submitting jobs to UGER should use an array:. > You are limited to 100 job in the cluster at once. This is to encourage the use of Task Arrays. Task arrays use the variable $SGE_TASK_ID which can be called inside the job script.""qsub -t 1-1000 /path/to/jobScript"". via: https://intranet.broadinstitute.org/bits/service-catalog/research-support/job-scheduling",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1491:348,schedul,scheduling,348,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1491,1,['schedul'],['scheduling']
Energy Efficiency,Support for monitoring script in JES via workflow options,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/281:12,monitor,monitoring,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/281,1,['monitor'],['monitoring']
Energy Efficiency,"Sure. What's the ticket number? The issue this user posted is about both submitted workflows and aborted workflows getting stuck. I asked him/her to abort the submitted ones so that the workspace would stop showing as Running, but that didn't work. Can we confirm that they aren't getting charged for machines not aborting that they have requested to abort?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335536766:289,charge,charged,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1861#issuecomment-335536766,1,['charge'],['charged']
Energy Efficiency,TEST Scalability/Performance: Pieces of the Green team,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2112:44,Green,Green,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2112,1,['Green'],['Green']
Energy Efficiency,"TL;DR the monitoring script actually does work, but Cromwell and Centaur need a bit of tweaking for this to be testable. I added a `sleep` to the Centaur test per @Horneth's suggestion and the monitoring log does get written. However Cromwell never creates a metadata event with the path to the monitoring log. Also the test needs to know where to expect the monitoring log, which is actually a function of the workflow root which Centaur currently doesn't know. I'd like to fix this by publishing the workflow root as a metadata event and then making Centaur wise to it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226891562:10,monitor,monitoring,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1006#issuecomment-226891562,4,['monitor'],['monitoring']
Energy Efficiency,"Talked to @jsotobroad in person.; - Kamon was removed in develop, but no hotfix needed because; - Green team changed their `java ...` script to not invoke the kamon code. No longer seems to be an issue. Closing and removing my branch that, in parallel, tested the kamon removal against a stale version of 0.19_hotfix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/735#issuecomment-217288661:98,Green,Green,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/735#issuecomment-217288661,1,['Green'],['Green']
Energy Efficiency,Task monitoring action for PAPIv2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510:5,monitor,monitoring,5,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510,1,['monitor'],['monitoring']
Energy Efficiency,"Test the Cromwell 27 upgrade for GOTC. Also, let's talk about what Green needs for integration testing in general, and what they need.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2301:67,Green,Green,67,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2301,1,['Green'],['Green']
Energy Efficiency,"Tests are in, waiting for green!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3725#issuecomment-394841642:26,green,green,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3725#issuecomment-394841642,1,['green'],['green']
Energy Efficiency,Tests are now green! Had to double the amount of changes from of the original commit to fix some logging issues.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3030#issuecomment-350478429:14,green,green,14,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3030#issuecomment-350478429,1,['green'],['green']
Energy Efficiency,"Thank you!. A related question is - could the existing APIs be used to make custom implementations of ""task call""?; In particular, AWS ECS offers a very simple way to create custom task schedulers, which could process an API call with relative ease. One could then schedule a task on ECS instead of JES based on that API call.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239593121:186,schedul,schedulers,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239593121,2,['schedul'],"['schedule', 'schedulers']"
Energy Efficiency,Thanks ! I squashed the commits and will merge when travis is green if there is nothing else.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/170#issuecomment-137745619:62,green,green,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/170#issuecomment-137745619,1,['green'],['green']
Energy Efficiency,"Thanks @aednichols, green tick against SBT now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656470426:20,green,green,20,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5494#issuecomment-656470426,1,['green'],['green']
Energy Efficiency,Thanks @illusional! I dont think will be an option for us. Given the variable nature of the number of genomes we are going to be dealing with we are eventually going to hit this again even if we did reduce our reliance on the read functions. It feels like it needs an option in the config to increase the timeout.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-823930311:199,reduce,reduce,199,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-823930311,1,['reduce'],['reduce']
Energy Efficiency,"Thanks @kshakir, that's awesome! Swagger's expressive power didn't seem able to capture the shape of the returned data, so having the example restored will I'm sure be much appreciated by Blue. :smile:",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/211#issuecomment-145079262:54,power,power,54,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/211#issuecomment-145079262,1,['power'],['power']
Energy Efficiency,"Thanks @rhpvorderman for the suggestions. . We are running wdl pipelines for single cell workloads that have thousands of concurrent tasks working on a dozen files each. Just the filesystem metadata operations alone are an issue for the filesystem, let alone whether the amount of data fetched is small. We were already hitting a wall in job submission speed due to this issue, we've been running cromwell with these changes in production now without issues. Reducing the number of threads would also reduce the task throughput and limit performance. #4900 is not what we need because we dont want to waste time copying when we can just soft-link. I have little doubt this solution is the most optimal for our team. However, I understand your concerns about docker. We are happy to do a little extra work to make this PR palatable to your team, perhaps by adding warnings in the appropriate places?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1045223132:501,reduce,reduce,501,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6676#issuecomment-1045223132,1,['reduce'],['reduce']
Energy Efficiency,"Thanks for pointing out the Cromwell/PAPI v2 beta support for docker image caches, I wasn't aware of this feature. . After reading the documentation at https://cromwell.readthedocs.io/en/develop/backends/Google/#docker-image-cache-support, I have a few additional questions about how this could help with the problem described in this issue. . It looks like docker image caching needs to be configured at the Cromwell server level with a manifest file indicating the location of images. Are the images listed in the manifest truly global (as indicated in their paths, ie `projects/broad-dsde-cromwell-dev/global/images/..`), so they won't incur egress to use them to create VMs in different regions? Can they be made public for use by users in other projects?. For an external user already running their own cromwell instance in a different region who wants to run our pipeline, we'd need to publish an image caching configuration stanza and manifest file along with our workflow, and the administrator of the cromwell server would need to modify their server configuration to point to our manifest file and restart the server -- is that correct? If their cromwell server was already configured with a different image cache manifest, is there a way to add a second manifest, or would they have to edit their own cache manifest to include the entries in ours?. If I'm right in my understanding (please correct me if I'm not) it seems like this solution could help, but is quite cumbersome and requires very savvy external users who are willing to take extra steps to help prevent saddling us with egress charges. It would be great if a cache manifest could be configured at the workflow level -- perhaps in the wdl or the workflow options.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738:1603,charge,charges,1603,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6442#issuecomment-887774738,1,['charge'],['charges']
Energy Efficiency,"Thanks for reviewing this, so far I don't expect considerable code changes to be made. I have saw that you approved a few other PRs, it could be a good idea to wait for them to get merged before reviewing this one to reduce the diff. > 1. -e is for exclude. There is a papi_v2beta_gcsa.test that should be running under Batch still. I believe the Beta version is a fairly essential test because it checks the configuration Terra uses for auth (though I'm not 100% sure on this one). I have not familiarized myself yet with Centaur which is why I preferred asking. > 2. We use Codecov as an advisory thing, if the human developers think the tests are solid, they're solid. I was thinking that there could be a wrong setup, perhaps running the wrong tests which would explain why code with tests was marked with no coverage, still, I have the impression that the behavior improved since #7418 was merged. > 3. I think I'd need to know more detail about what the test checks for and how it fails. Looking at the comment in StandardAsyncExecutionActor#requestsAbortAndDiesImmediately, it does seem like we may want the false behavior because it's responsible for some finalization activities around the job. Agree, do you have any insights on how can I possibly test this to save some research time?. > 4.. 100 workers seems like a lot? I think our default for PAPI is 3, and we stick to that default in Terra. Is the Batch behavior radically different, motivating 100? I totally agree that the formula could use re-evaluation (or maybe even elimination). Yes, they are quite a lot but we were testing different parameters and found this behavior, still, it seems worth to leave this for another PR. > 5. That seems fine. So long as we have some kind of handling for every case in com.google.cloud.batch.v1.JobStatus.State` we should be good. The execution events are designed to be tied closely to the implementation of the backend with minimal translation or invention of new states. Great.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874:217,reduce,reduce,217,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412#issuecomment-2110742874,1,['reduce'],['reduce']
Energy Efficiency,"Thanks for the comment Paul -- we've already started those discussions with the Google folks. We want this built deep into the Pipelines API as true notifications via pubsub which seems a reasonable feature. It's possible to do what you say yourself (using a GCE node(s) to query and then post) but with 100,000s of operations in flight it's much more efficient to be notified than poll... but I'm sure you agree with that ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260216282:352,efficient,efficient,352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-260216282,1,['efficient'],['efficient']
Energy Efficiency,"Thanks for your contribution!. I realize we didn't give much of an explanation for the ""hold"" label - we describe our process for external contributors here: https://github.com/broadinstitute/cromwell/blob/develop/CONTRIBUTING.md. In a nutshell, reviewing PRs is time-intensive and we have to work it into our team schedule alongside everything else.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4943#issuecomment-493122445:315,schedul,schedule,315,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4943#issuecomment-493122445,1,['schedul'],['schedule']
Energy Efficiency,Thanks to @ruchim both Green (single sample / arrays) workflows are integrated into the Jenkins [tyburn-cromwell-test](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/) job. This job runs daily at 8 am M-F currently and is governed by the Jenkins [tyburn-master-job](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-master-test/) The code is not checked into develop because of Tyburn history ðŸ˜¡ so all the integrated test code is located on the rm_startingPoint [branch](https://github.com/broadinstitute/tyburn/tree/rm_startingPoint). Hopefully it will be back in develop soon but it probably wont be before my sabbatical is over so this is the current state of things. In the [configure](https://gotc-jenkins.dsp-techops.broadinstitute.org/job/tyburn-cromwell-test/configure) of the jenkins tyburn-cromwell-test job there is a script section where we invoke the tyburn command we use to run the test. Here you can config values that change shape the test. ```; -Dexecutioners.workflowToInstance.singlesample=1 \; -Dexecutioners.workflowToInstance.arrays=1 \; -Dexecutioners.workflowToInstance.lots_of_inputs=1 \; ```. Each value tells Tyburn how many instances of each test to run. Currently it is set to 1 because we weren't sure what kind of load we were going for and how often the test will run.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377:23,Green,Green,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337#issuecomment-311365377,1,['Green'],['Green']
Energy Efficiency,"Thanks to a good tip from Devops [(Slack link)](https://broadinstitute.slack.com/archives/CADM7MZ35/p1664898679060499), Cromwell and CromIAM adopt the current recommended pattern for services that go to dev on merge. [Code adapted from how Workspace Manager does it](https://github.com/DataBiosphere/terra-workspace-manager/actions/runs/3176595248/workflow#L142-L149). Also, clean up a missed `-SNAP` reference to make things consistent.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6924:223,adapt,adapted,223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6924,1,['adapt'],['adapted']
Energy Efficiency,Thanks. Setting `cpuPlatform` in the runtime attributes is the only way to avoid scheduling to an e series machines through Cromwell. GCP Batch is limited to setting cpuPlatform or instance type. There is no preferred machine family type setting in GCP Batch. With that said the e series machine should not be that slow. Performance is supposed to be comparable to N1. If it repeatable open a support case with GCP.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256082242:81,schedul,scheduling,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7474#issuecomment-2256082242,1,['schedul'],['scheduling']
Energy Efficiency,"That's a cool use of this configuration to work around this issue !. I just want to give some context around it. This rate control was originally put in place to protect Cromwell against excessive load or a very large spike of jobs becoming runnable in a short period of time. Through this mechanism Cromwell can also stop starting new jobs altogether when under too heavy load.; While this achieve the desired effect of rate limiting how many submit requests are being sent to AWS batch in a period of time, I think a medium-term better fix is too implement something similar to the [PipelinesApiRequestManager](https://github.com/broadinstitute/cromwell/blob/develop/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/api/PipelinesApiRequestManager.scala) for the AWS backend.; The reason is that it acts as a coarse level of granularity which might have undesired side-effects:. 1) it is a system wide configuration, meaning in a multi backend Cromwell it might be too constraining for some backends and not enough for others; 2) It also rate limits starting jobs that might actually be call cached and incur 0 requests to AWS Batch, making it too conservative; 3) It only helps rate limiting the number of job creation requests to batch. Once a job is started, it issues status requests to monitor the job which aren't throttled. Not to say that this is a bad workaround, I think it's a *good* workaround, but still a workaround :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444253181:1341,monitor,monitor,1341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4303#issuecomment-444253181,1,['monitor'],['monitor']
Energy Efficiency,"The 10K call caching run revealed that we still spend a lot of time doing in this area, even with scheduled time based calls to `runnableCalls`. ![screen shot 2017-04-19 at 2 09 51 pm](https://cloud.githubusercontent.com/assets/2978948/25194952/1bd7ee2e-250a-11e7-9eff-cef3b3f78c4f.png)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-295373255:98,schedul,scheduled,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1602#issuecomment-295373255,1,['schedul'],['scheduled']
Energy Efficiency,"The OGE back end shell script that is submitted to the task scheduler on SGE/OGE back ends is potentially missing a sync statement. . `; (; cd /SingleSampleWF/ffc0dbea-a292-4e24-833a-d2010d373600/call-DeliverReports/execution; sync; ). mv /SingleSampleWF/ffc0dbea-a292-4e24-833a-d2010d373600/call-DeliverReports/execution/rc.tmp /SingleSampleWF/ffc0dbea-a292-4e24-833a-d2010d373600/call-DeliverReports/execution/rc; `. We've had cases where job completed successfully, OGE reports the job as exiting normally, but the rc file is written with a '79' (undocumented behavior until we located it in the source code). And yet on examination of the logs, there were no errors, and the rc file was moved from rc.tmp (which is no longer there). Our assumption is that the mv command is not getting flushed to our storage system, and if cromwell polls for zombie tasks before that happens we will see it as a job failure. . See https://github.com/broadinstitute/cromwell/blob/68949364c7ffab8450116baa8e2a4e276bb16d70/backend/src/main/scala/cromwell/backend/standard/StandardAsyncExecutionActor.scala#L465. which is not followed by SCRIPT_EPILOGUE that would add the required sync command after the mv.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6988:60,schedul,scheduler,60,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6988,1,['schedul'],['scheduler']
Energy Efficiency,"The build failures are in unrelated Swagger tests, rebasing on develop should get this green.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-341493941:87,green,green,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2807#issuecomment-341493941,1,['green'],['green']
Energy Efficiency,"The current de-localization logic for PAPIv2 (which goes into `gcp_transfer.sh`) de-localizes ""job"" outputs (the ones declared in the `output {}` section of a task) _first_. The problem with that is if the task command fails, then at least one of the ""job"" outputs never gets created, and so `gcp_transfer.sh` fails permanently (after a few unsuccessful retries to delocalize a ""job"" output), and never gets to de-localize ""helper"" files like `memory_retry_rc` (and `rc`) or `monitoring.log`. As a result, features like `memory-retry` or `monotiring_script` don't work when the task fails (for example, when it runs out of memory!), because they rely on the presence of those files in the GCS execution folder. I propose moving de-localization of ""helper"" files in front of the ""job"" outputs. This way, the ""helper"" files will always be de-localized first, because they're supposed to be always present, unlike declared ""outputs"" of the task, which depend on the success of the task command. JIRA reference: https://broadworkbench.atlassian.net/browse/BA-6112. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5274:476,monitor,monitoring,476,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5274,1,['monitor'],['monitoring']
Energy Efficiency,"The current de-localization logic for PAPIv2 (which goes into `gcp_transfer.sh`) de-localizes ""job"" outputs (the ones declared in the `output {}` section of a task) _first_. The problem with that is if the task command fails, then at least one of the ""job"" outputs never gets created, and so `gcp_transfer.sh` fails permanently (after a few unsuccessful retries to delocalize a ""job"" output), and never gets to de-localize ""helper"" files like `memory_retry_rc` (and `rc`) or `monitoring.log`. As a result, features like `memory-retry` or `monotiring_script` don't work when the task fails (for example, when it runs out of memory!), because they rely on the presence of those files in the GCS execution folder. I propose moving de-localization of ""helper"" files in front of the ""job"" outputs. This way, the ""helper"" files will always be de-localized first, because they're supposed to be always present, unlike declared ""outputs"" of the task, which depend on the success of the task command. JIRA reference: https://broadworkbench.atlassian.net/browse/BA-6112. Thanks!. Note: this is a re-submission of #5274",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5283:476,monitor,monitoring,476,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5283,1,['monitor'],['monitoring']
Energy Efficiency,"The current de-localization logic for PAPIv2 (which goes into `gcp_transfer.sh`) de-localizes ""job"" outputs (the ones declared in the `output {}` section of a task) _first_. The problem with that is if the task command fails, then at least one of the ""job"" outputs never gets created, and so `gcp_transfer.sh` fails permanently (after a few unsuccessful retries to delocalize a ""job"" output), and never gets to de-localize ""helper"" files like `memory_retry_rc` (and `rc`) or `monitoring.log`. As a result, features like `memory-retry` or `monotiring_script` don't work when the task fails (for example, when it runs out of memory!), because they rely on the presence of those files in the GCS execution folder. I propose moving de-localization of ""helper"" files in front of the ""job"" outputs. This way, the ""helper"" files will always be de-localized first, because they're supposed to be always present, unlike declared ""outputs"" of the task, which depend on the success of the task command. JIRA reference: https://broadworkbench.atlassian.net/browse/BA-6112. Thanks!. Note: this is a re-submission of #5274 and #5283 (to overcome Travis hurdles..)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5288:476,monitor,monitoring,476,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5288,1,['monitor'],['monitoring']
Energy Efficiency,"The easiest way to capture and compare metrics from performance runs is to push them into GCP Stackdriver Monitoring. We have an abstraction in the service registry for Instrumentation implementations, see the [StatsDInstrumentationServiceActor].(https://github.com/broadinstitute/cromwell/blob/develop/services/src/main/scala/cromwell/services/instrumentation/impl/statsd/StatsDInstrumentationServiceActor.scala). ## Library; `libraryDependencies += ""com.google.cloud"" % ""google-cloud-monitoring"" % ""1.66.0""`. ## Acceptance Criteria . * Metrics can be labeled from global config settings. Namely: ; * Global config setting for Role (reader, worker, summarizer); * Setting for Host name (e.g. ""gce-cromwell-perfX""); * Setting for stable identifier for the perf test (e.g. ""call-cache-whitelist"").; * Logger can be configured using Service Account from Config.; * Performance runs are visible in GCP Stackdriver graphs with all labels. # Reference. * [Configuring stackdriver agent, useful for service account creation](https://cloud.google.com/monitoring/agent/install-agent); * [v3 monitoring javadoc](https://googleapis.github.io/google-cloud-java/google-cloud-clients/apidocs/com/google/cloud/monitoring/v3/package-summary.html); * [GCP Client Libraries for monitoring](https://cloud.google.com/monitoring/docs/reference/libraries)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4788:106,Monitor,Monitoring,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4788,7,"['Monitor', 'monitor']","['Monitoring', 'monitoring']"
Energy Efficiency,"The essence of this issue is that:. ...I have a workflow that calls sub workflows; ...My Workflow validates and everything is swell; ...I want to now run this against a Cromwell server, and turns out I need a zip. ; ...I make a dependencies zip, but see that Cromwell doesn't seem to like it!. AC: Have there be an option in Womtool that packages the files referenced in my import statements as a valid dependencies zip to help reduce issues at runtime.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3501:428,reduce,reduce,428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3501,1,['reduce'],['reduce']
Energy Efficiency,"The failure rate was getting unreasonable so I figured someone should do something. It might take a bit longer, but at least it'll more likely return green. ---. Once build activity died down after everyone went home, I was able to isolate the imapct of conformance PapiV2 by running it on its own: virtually none (my run started at 7:10). <img width=""898"" alt=""screen shot 2018-10-12 at 7 39 48 pm"" src=""https://user-images.githubusercontent.com/1087943/46898326-a37a2300-ce56-11e8-9c0a-ff5f33ade931.png"">. It's still possible that other jobs suck up quota which in turn affects conformance PapiV2 even if it uses very little itself. I asked for more quota again, because my first request didn't give us enough breathing room (screenshot _after_ first increase):; <img width=""976"" alt=""screen shot 2018-10-12 at 6 24 21 pm"" src=""https://user-images.githubusercontent.com/1087943/46898377-15526c80-ce57-11e8-8af6-f7844e84b843.png"">",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4248:150,green,green,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4248,1,['green'],['green']
Energy Efficiency,"The first run of JG failed because an actor context happened to be null.; Specifically this happened when we use the actor system dispatcher as an Execution Context for the execution of futures in the Actor.; I only saw this once and the reason is unclear. Akka claims actor contexts can't be null.; One possible explanation (maybe?) is that the actor schedules some futures, then die for an unknown reason. By the time the futures become runnable and try to access their EC, the actor is already dead so the creation of the EC from the `context.dispatcher` fails. <img width=""1411"" alt=""screen shot 2017-04-10 at 5 07 02 pm"" src=""https://cloud.githubusercontent.com/assets/2978948/25280517/3f8fc75a-2678-11e7-966a-54340199c1e0.png"">",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2189:352,schedul,schedules,352,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2189,1,['schedul'],['schedules']
Energy Efficiency,"The first step towards having great continuous performance assertions is owning the existing performance testing. This PR brings the performance tests under the auspices of the Cromwell team with the intention that we can thereafter perform, adapt, add to and maintain these processes ourselves. EDIT: See the change as a rendered `png` here: https://github.com/broadinstitute/cromwell/tree/cjl_diy_perf_testing/processes/release_processes#performance-testing",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4996:242,adapt,adapt,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4996,1,['adapt'],['adapt']
Energy Efficiency,"The green step of red/green/refactor, but wow does this need refactoring.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/20:4,green,green,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20,2,['green'],['green']
Energy Efficiency,"The highlight subcommand of cromwell strips out the runtime, param_meta and meta information. . test.wdl:. ```; task runtime_meta {; String memory_mb; String sample_id; String param; String sample_id. command {; java -Xmx${memory_mb}M -jar task.jar -id ${sample_id} -param ${param} -out ${sample_id}.out; }; output {; File results = ""${sample_id}.out""; }; runtime {; docker: ""broadinstitute/baseimg""; }; parameter_meta {; memory_mb: ""Amount of memory to allocate to the JVM""; param: ""Some arbitrary parameter""; sample_id: ""The ID of the sample in format foo_bar_baz""; }; meta {; author: ""Joe Somebody""; email: ""joe@company.org""; }; }. workflow test {; call runtime_meta; }; ```. The command `$ java -jar cromwell-0.15.jar highlight test.wdl console` outputs:. ```; task runtime_meta {; String memory_mb; String sample_id; String param; String sample_id. command {; java -Xmx${memory_mb}M -jar task.jar -id ${sample_id} -param ${param} -out ${sample_id}.out; }; output {; File results = ""${sample_id}.out""; }. workflow test {; call runtime_meta; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/360:454,allocate,allocate,454,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/360,1,['allocate'],['allocate']
Energy Efficiency,"The ones I've added are draft-2, but will eventually update to draft-3!. This PR is contingent on [cron job](https://travis-ci.org/broadinstitute/cromwell/builds/385209234) going green.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3700:179,green,green,179,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3700,1,['green'],['green']
Energy Efficiency,The point of the setting is to protect the server from users. Putting the power directly in the hands of the users seems unwise,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338406950:74,power,power,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2768#issuecomment-338406950,1,['power'],['power']
Energy Efficiency,The power of laziness compels me! :P,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4002#issuecomment-412943323:4,power,power,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4002#issuecomment-412943323,1,['power'],['power']
Energy Efficiency,"The purpose of this PR is to fix issue #4586.; It turns out that Cromwell looks for the ad hoc files in the wrong location while using AWS. These files placed in the S3 bucket, while Cromwell expects them to be in the root execution directory. There were already two PRs from us ([1](https://github.com/broadinstitute/cromwell/pull/5064), [2](https://github.com/broadinstitute/cromwell/pull/5057)) aimed to solve this issue, but these were not the appropriate solutions.; This time we found what part of GCP backend handles these ad hoc files and implemented the same logic on AWS. Also, in order to reduce the amount of duplicate code, we made a small refactoring.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5097:600,reduce,reduce,600,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5097,1,['reduce'],['reduce']
Energy Efficiency,"The query endpoint responds with a 400 error when trying filter by ""On Hold"" status. curl -X GET ""https://[cromwell_url]/api/workflows/v1/query?status=On%20Hold"" -H ""accept: application/json"". 400; {; ""status"": ""fail"",; ""message"": ""Unrecognized status values: On Hold""; }. This makes it difficult to use On Hold for queuing because you can't find the On Hold workflows unless you query for everything and then filter the results by status on the client side, which is not very efficient.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3650:477,efficient,efficient,477,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3650,1,['efficient'],['efficient']
Energy Efficiency,"The timing diagram would be more powerful if the workflow status was included at the top of the chart. As it stands now, I need to check two places to understand the detailed state of my workflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1496:33,power,powerful,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1496,1,['power'],['powerful']
Energy Efficiency,"There have been many requests over time to provide data on CPU, memory, and disk usage. . In an ideal world this would be a rich time series of data, in a less ideal world this would be max with a couple of periodic data points, and in a much more practical world it'd just be max usage. The last one is the definition of done here but if one wants to get ambitious ....... The PAPI backend already allows for a custom script to be attached, we should be putting in something on our own for any backend which runs things via unix command lines (ie spark jobs and such likely don't make sense). At least one group is using the following in production, this is likely a good start if not completely AOK. ``````#!/bin/bash; echo ==================================; echo =========== MONITORING ===========; echo ==================================; echo --- General Information ---; echo \#CPU: $(nproc); echo Total Memory: $(free -h | grep Mem | awk '{ print $2 }'); echo Total Disk space: $(df -h | grep cromwell_root | awk '{ print $2}'); echo ; echo --- Runtime Information ---. function runtimeInfo() {; echo [$(date)]; echo \* CPU usage: $(top -bn 2 -d 0.01 | grep '^%Cpu' | tail -n 1 | awk '{print $2}')%; echo \* Memory usage: $(free -m | grep Mem | awk '{ OFMT=""%.0f""; print ($3/$2)*100; }')%; echo \* Disk usage: $(df | grep cromwell_root | awk '{ print $5 }'); }. while true; do runtimeInfo; sleep 300; done```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2507:779,MONITOR,MONITORING,779,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2507,1,['MONITOR'],['MONITORING']
Energy Efficiency,"There's another concern here that PAPIv2 doesn't ([currently](https://groups.google.com/forum/#!topic/google-genomics-discuss/newaE3R-cwY)) terminate background containers nicely once all non-background actions terminate. I.e. it uses SIGKILL, which cannot be caught in our monitoring container, and hence so far it has not been able to report the last timepoint. However, to work around that I'll add one more action, which will be _non-background_, and will run after the user action. This 2nd action will be assigned to the same `pidNamespace` as the monitoring action (which possible with PAPIv2), and hence will have access to the PID of the monitoring action and will then kill it ""nicely"" with SIGTERM, and wait for it to terminate - something like this: `killall python && wait`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-452087901:274,monitor,monitoring,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-452087901,3,['monitor'],['monitoring']
Energy Efficiency,"These endpoints are not being used by our major clients and represent a special case of data you can get from the metadata endpoint. If a more efficient mechanism is needed to get this data in the future, we can add that as reshaping of the metadata output (e.g. .../metadata/{call}). Make it Gone, including from the README and Swagger!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/984:143,efficient,efficient,143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/984,1,['efficient'],['efficient']
Energy Efficiency,"These light green ones don't seem to have any parents:. <img width=""881"" alt=""Screen Shot 2019-11-07 at 4 06 57 PM"" src=""https://user-images.githubusercontent.com/13006282/68427727-bbff5e80-0178-11ea-9f81-b47efeb616e9.png"">",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551264545:12,green,green,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5264#issuecomment-551264545,1,['green'],['green']
Energy Efficiency,This PR . - separates out the logic for getting the next workflow to archive and number of workflows left to archive; - adds a config option on how often we want to calculate number of workflows left to archive and schedules it ; - adds a comment for the reason behind naming convention of archived metadata files. Closes https://broadworkbench.atlassian.net/browse/BW-653; Closes https://broadworkbench.atlassian.net/browse/BW-658,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6325:215,schedul,schedules,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6325,1,['schedul'],['schedules']
Energy Efficiency,"This PR adds `WORKFLOW_NAME`, `TASK_INPUTS`, `TASK_DISKS`, and `MONITORING_CONFIG` environment variables for `MonitoringAction` in PAPIv2 backend. These variables are used to pass details about task inputs and disk mappings (both in JSON form), along with an image-specific config string (e.g. `project-id.dataset-id.table-id` for `quay.io/broadinstitute/cromwell-monitor-bigquery`), into the container specified through the existing `monitoring_image` option. It also adds `bigquery.insertdata` OAuth scope, to be used for streaming monitoring data into BigQuery (@adrazhi seems to approve scope extension).; ; This PR will enable us to:; - stream monitoring data at scale into BQ (much more so than was possible through Stackdriver),; - build detailed models for prediction of runtime resource utilization, using BQ or external tools (e.g. Looker); - easily detect runtime failure modes such as running OOM. (Please see https://github.com/broadinstitute/cromwell-task-monitor-bbq for more info on BQ use case). However, the proposed changes are not specific to BQ (apart from the scope), and could be used for other `monitoring_image` implementations in the future, thanks to the new `monitoring_config` option for PAPIv2 backend. **Please note**: this is an initial implementation that's **not yet ready for a merge**. For example, `TASK_INPUTS` are not serialized correctly yet. We intend to add more commits to implement it fully. However, we're soliciting early feedback and review. Interested parties: @kshakir, @benjamincarlin, @rexwangcc, @mohawkTrail, @ruchim, @abaumann",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5028:110,Monitor,MonitoringAction,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5028,5,"['Monitor', 'monitor']","['MonitoringAction', 'monitor-bbq', 'monitor-bigquery', 'monitoring']"
Energy Efficiency,"This PR changes Docker image for `monitoringTerminationAction()` in PAPIv2 backend, from `alpine` to `CloudSdkImage`. This is done to remove dependency on Docker Hub, and to re-use a Cloud SDK GCR image that's already present on the system, instead of fetching a fresh one. JIRA: https://broadworkbench.atlassian.net/browse/BA-6117. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5276:34,monitor,monitoringTerminationAction,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5276,1,['monitor'],['monitoringTerminationAction']
Energy Efficiency,"This PR changes Docker image for `monitoringTerminationAction()` in PAPIv2 backend, from `alpine` to `CloudSdkImage`. This is done to remove dependency on Docker Hub, and to re-use a Cloud SDK GCR image that's already present on the system, instead of fetching a fresh one. JIRA: https://broadworkbench.atlassian.net/browse/BA-6117. Thanks!. Note: re-submitted this PR in place of #5276",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5282:34,monitor,monitoringTerminationAction,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5282,1,['monitor'],['monitoringTerminationAction']
Energy Efficiency,"This PR changes Docker image for `monitoringTerminationAction()` in PAPIv2 backend, from `alpine` to `CloudSdkImage`. This is done to remove dependency on Docker Hub, and to re-use a Cloud SDK GCR image that's already present on the system, instead of fetching a fresh one. JIRA: https://broadworkbench.atlassian.net/browse/BA-6117. Thanks!. Note: re-submitted this PR in place of #5276, #5282 and #5286, to fix Travis flakiness.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5287:34,monitor,monitoringTerminationAction,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5287,1,['monitor'],['monitoringTerminationAction']
Energy Efficiency,"This PR changes Docker image for `monitoringTerminationAction()` in PAPIv2 backend, from `alpine` to `CloudSdkImage`. This is done to remove dependency on Docker Hub, and to re-use a Cloud SDK GCR image that's already present on the system, instead of fetching a fresh one. JIRA: https://broadworkbench.atlassian.net/browse/BA-6117. Thanks!. Note: re-submitted this PR in place of #5282 and #5276",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5286:34,monitor,monitoringTerminationAction,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5286,1,['monitor'],['monitoringTerminationAction']
Energy Efficiency,This PR is the output from an analysis made in order to see if SFS backend functionality can be re-used as part of HtCondor backend for CCC.; The end result was that most of the things can be re-used but caching since it's being centralized in someway by SFS.; The idea is to add this contribution to this repository since it reduces LOCs but in the CCC context the old version will still be supported.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2141:326,reduce,reduces,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2141,1,['reduce'],['reduces']
Energy Efficiency,This PR will need . ~~https://github.com/broadinstitute/centaur/pull/112~~; ~~and https://github.com/broadinstitute/wdl4s/pull/36~~. ~~merged before travis can go green~~,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1610:163,green,green,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1610,1,['green'],['green']
Energy Efficiency,"This branch was WIP to get BCBIO to work on PAPI but contains updates (namely retyring gsutil commands) that could be useful re: travis flakiness, and it's green so making a PR.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3813:156,green,green,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3813,1,['green'],['green']
Energy Efficiency,"This contains a feature of accepting business events like workflow execution events and other events through the event bus provided by Akka framework and delegate them to centralized logging like Logstash, Fluentd etc for further Monitoring/UI status. There is also a wrapper that encapsulate event bus subscription/publication behaviour.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/400:230,Monitor,Monitoring,230,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/400,1,['Monitor'],['Monitoring']
Energy Efficiency,"This draft PR is one half (WIP) of migrating the E2E workflow test over to the dsp-reusable-workflows repo. There is a sibling branch on that repo as well that also needs to be developed along side this (same branch name [`WX-1307-port`], but no PR made just yet.). The PR here simply reduces the workflow so that the run-script job simply generates a token and passes all required inputs to the test script workflow housed in the other repo. If this migration moves forward, the sibling branch must either be merged first or merged at the same time as this one to avoid having to worry about branch referencing in the workflow. If migrating is optional, then you can choose to move forward or drop it if other priorities pop up (since the vanilla `WX-1307` branch can be used to cover the e2e testing from the Cromwell repo).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7239:285,reduce,reduces,285,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7239,1,['reduce'],['reduces']
Energy Efficiency,This has the potential to reduce QPS errors enormously.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229210088:26,reduce,reduce,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1089#issuecomment-229210088,1,['reduce'],['reduce']
Energy Efficiency,"This implements the cromwell agent (process that monitors docker for Cromwell tasks) as part of #3804. I will include the sidecar process later as a separate PR. After building the agent and running the agent with the build-agent/run-agent scripts, a ""cromwell-agent"" container will be shown in docker ps. This agent will be reading the docker event stream from /events, filter for starts with grep, and pass any start events to the small go program ""spawnoninput"". This program simply spawns a new process when it sees input. In this case, the process is a shell script that pauses the container, checks to see if it matches the target condition, then either resumes/exits (when not meeting the target condition) or leaves the container paused and launches a sidecar, which is still in development. If the target condition does not match, this has an effect of slowing startup for the launched container of approximately 100ms for the inspection process. This image should ideally be under the Broad's account on Dockerhub or ECR - it's based on alpine Linux and is relatively small. It should also be run automatically in the AWS Batch/Cromwell AMI. More documentation will be forthcoming in a later PR, though some inline comments are in the code here.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3934:49,monitor,monitors,49,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3934,1,['monitor'],['monitors']
Energy Efficiency,This is a [Not Acceptable](http://stackoverflow.com/questions/14251851/what-is-406-not-acceptable-response-in-http) error. The content types that Green said it would accept in the response were not compatible with the type of response Cromwell produced. It may be that Cromwell experienced a real error (which was hopefully logged) and returned a text/plain error response which I don't think you'd accept.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216659028:146,Green,Green,146,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/775#issuecomment-216659028,1,['Green'],['Green']
Energy Efficiency,"This is a recent change which broke it if that's the case, which should be impossible as we only push when green",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4833#issuecomment-483295408:107,green,green,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4833#issuecomment-483295408,1,['green'],['green']
Energy Efficiency,"This is an issue to track the same problem that @ffinfo has raised on Gitter and PR #1346. The scenario for us is an HPC cluster environment with a scheduler (SGE or PBS) that strictly enforces walltimes; having jobs be killed by the scheduler (without producing rc file) when they exceed the requested walltime is, if not exactly common, not that rare either. The hacky non-async solution I have been using (up until the nice configurable backends of release 0.21 changed everything in backend-land) was to have two check cycles, a frequent cheap one to see if rc existed and occasional expensive one to poll the scheduler itself: https://github.com/delocalizer/cromwell/blob/develop/engine/src/main/scala/cromwell/engine/backend/pbs/PbsBackend.scala#L128-L166",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499:148,schedul,scheduler,148,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499,3,['schedul'],['scheduler']
Energy Efficiency,"This is green and actually does appear to work with a couple of mods to the three-step workflow, one to fake the `ps` for deterministic results, another to fix a bug in the wc command. But this seriously needs refactoring:; 1. Proper concurrency handling for store updates. The current code is awful, the mutable stores are just passed around the various concurrently executing actors. Probably the stores should be behind an actor which the CallActor and WorkflowActor would `ask` for queries or updates, and then compose a `pipeTo` with the `Future` response.; 2. While the potential parallelism of the cgrep and wc calls does appear to work as intended, the current test does not assert this.; 3. The call dependency determination only works for member access expressions and is kind of gross.; 4. The connection of inputs to outputs requires member access expressions and uses logic very much like point 3. Also the way outputs are copied to inputs in the symbol store seems clunky and wasteful; if the input expressions could be truly evaluated at call invocation it wouldn't be necessary to create copies of this data.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/20:8,green,green,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20,1,['green'],['green']
Energy Efficiency,"This might be a bit controversial so feel free to seagull. _What it does_: When looking for new nodes to run in a workflow, if there are more than 1000 call nodes in ""Queued"" state, don't start new call nodes (still execute other ones like ExpressionNodes etc...).; _Pro_: Reduce load by not starting too many jobs that won't be able to run for the moment anyway since there's already 1000+ queued (waiting for a token); _Con_: Jobs stay in `NotStarted` state longer (this status is sent to metadata and is visible by users), even if they could technically be started as far as dependencies are concerned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3356:273,Reduce,Reduce,273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3356,1,['Reduce'],['Reduce']
Energy Efficiency,"This reverts commit 7bb8d1f102560b625d260db0773f92981b05141d. I misremembered the Centaur `backendsMode` as defaulting to `any` when actually it defaults to `all`. Since there are no Cromwell server instances created in Cromwell CI with both PAPI and GCPBATCH backends, instead of turning on ~90 Centaur tests for GCPBATCH, my changes actually turned *off* ~90 Centaur tests for PAPIv2. ðŸ˜¬ . Current `develop` is green with this revert but I would strongly encourage rebasing any in-flight branches on these changes before merging!. ### Description. <!-- What is the purpose of this change? What should reviewers know? -->. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7512:412,green,green,412,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7512,1,['green'],['green']
Energy Efficiency,"This sets up a mechanism to:; 1) Collect information about ""load"" from various part of the system; 2) Summarize this information and calculate a global load; 3) Notify parts of the system of changes in the global load. Current implementation is simple:; Only 2 load levels: `NormalLoad` and `HighLoad`; Actors reporting their load are:; - WriteMetadataActor; - JobStoreReadActor; - JobStoreWriteActor; - CallCacheWriteActor; - CallCacheReadActor; - KeyValueReadActor; - KeyValueWriteActor; - IoActor; - JesAPIQueryManagerActor. Additionally free memory is also being monitored and will go to `HighLoad` if going below a certain threshold. Global load == max(all load levels). So if one actor or more say their load is high, the global load will be high, otherwise normal.; The only actor listening to changes on the global load is the job token dispenser. It will stop dispensing tokens when load is high and start again when load is back to normal. At the exception of the IoActor, all the above mentioned actors have a queue in which they store work to be done. Their load is determined by comparing the size of this queue to a threshold.; The IoActor's queue is not easily accessible because hidden in the stream implementation and its size cannot easily be known. However we know when its full because we can't add to it anymore (this is when backpressure messages are sent). When that happens the IO actor reports its load to be `High`. When it hasn't had to backpressure for 10 seconds, the load returns to normal. There are many ways this could be made smarter but it already yields improvements in terms of stability and robustness. TODO: . - [x] Add Changelog; - [x] Configuration ? Lots of thresholds and values in this PR that could be configurable, how much and how do we want to configure ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3366:567,monitor,monitored,567,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3366,1,['monitor'],['monitored']
Energy Efficiency,"This should go green soon and be good to merge. I'll submit the master patch tomorrow, unless someone else gets to it first.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213477862:15,green,green,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/728#issuecomment-213477862,1,['green'],['green']
Energy Efficiency,This was a clean cherry pick so i'm just going to wait for the tests to go green,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3163:75,green,green,75,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3163,1,['green'],['green']
Energy Efficiency,"This would greatly reduce my need to modify WDL scripts to start where I have data already processed. For example, if a script goes BAM-->coverage-->CNVs, if I have already collected coverage on my BAMs, I would like to be able to provide `coverage` to the same script and have Cromwell skip the tasks involving the BAM and run the remaining steps in the workflow, e.g. coverage-->CNVs. . I run WDLs using gcloud, within a VM and locally. I don't use FireCloud so my runs do not use call-caching. I want to take the boilerplate WDL scripts the GATK4 repo makes available to run processes. I am specifically looking at the latest somatic CNV workflow. If I have alreaded padded my intervals and/or collected counts on the BAMs, I'd like to still use the rest of the steps in the workflow by specifying in the INPUTS JSON an intermediate file. If the script is thus:; ```; call CNVTasks.PreprocessIntervals {; input:; intervals = intervals,; ref_fasta_dict = ref_fasta_dict,; gatk4_jar_override = gatk4_jar_override,; gatk_docker = gatk_docker; }. if (select_first([do_explicit_gc_correction, false])) {; call CNVTasks.AnnotateIntervals {; input:; intervals = PreprocessIntervals.preprocessed_intervals,; ref_fasta = ref_fasta,; ref_fasta_fai = ref_fasta_fai,; ref_fasta_dict = ref_fasta_dict,; gatk4_jar_override = gatk4_jar_override,; gatk_docker = gatk_docker; }; ```. In the inputs, instead of defining:; ```; ""CNVSomaticPanelWorkflow.intervals"": ""File"",; ```; I would like to be able to instead provide:; ```; ""CNVSomaticPanelWorkflow.PreprocessIntervals.preprocessed_intervals"": ""File"",; ```. And not have the run error due to the lack of the `CNVSomaticPanelWorkflow.intervals` file. . I would really appreciate such a feature as it saves me the time of having to rewrite WDL scripts for each tweaked subset workflow. Thanks.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2949:19,reduce,reduce,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2949,1,['reduce'],['reduce']
Energy Efficiency,"Three tests are still ignored-- not sure if they are worth keeping. Small changes to the structure of the case classes of KeyValueService. Hoping to reduce having to pass around scopes just to gather basic call information like fqn, index and attempt.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1278:149,reduce,reduce,149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1278,1,['reduce'],['reduce']
Energy Efficiency,"To add some more context, not having this ability makes it difficult for us to use the On Hold status for queuing in Mint without a lot of overhead. In more detail:. We have a new service called Falcon that periodically queries our workflow collection in CaaS and starts the oldest On Hold workflow. Right now we have no choice but to query for *all* on hold workflows in each cycle, which for scale testing and production will be thousands of workflows -- even though we just want the oldest one or a few of the oldest ones. We could try to paginate and use multiple requests to skip to the last page, but when several workflows per second are being submitted we can't reliably find the oldest on hold workflow that way. It would be much more efficient if we could ask Cromwell to reverse the order and get just the first page of say the 10 oldest On Hold workflows.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3947#issuecomment-410034522:744,efficient,efficient,744,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3947#issuecomment-410034522,1,['efficient'],['efficient']
Energy Efficiency,"To stick to our launch schedule for the GATK-aaS alpha, I'd really like to see this functionality (enable Cromwell server to treat SIGINT as abort) go in this week. . Is that going to be feasible? Who should David sync with to figure out how best to do it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175717937:23,schedul,schedule,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-175717937,1,['schedul'],['schedule']
Energy Efficiency,"Today, there's a max workflow concurrency limit and by design it helps restrict how much work is being taken on by a single instance of Cromwell. However, when a root level workflow spawns a lot of subworkflows, those workflows don't count against the max workflow concurrency limit, and that can nullify the work being done to reduce load by putting in a max concurrency limit on workflows in the first place. AC: From a performance perspective, it would be great if sub-workflows were accounted for in the max workflow concurrency limit. So when Cromwell sweeps for new workflows, sub-workflows are accounted for in the max concurrent workflow counter before starting any new root workflows.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4066:328,reduce,reduce,328,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4066,1,['reduce'],['reduce']
Energy Efficiency,"Totally valid workflow fails in the last stage in the latest development version of cromwell because of:; ```; Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types; ```; error.; I enclose both wdl and input. ; [quantification.zip](https://github.com/broadinstitute/cromwell/files/2761544/quantification.zip). The error is the following:. ```json; Workflow failed. WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:1:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2000), WomString(characteristics) -> WomString(number of donors -> 1;age -> 26 years old;tissue -> Kidney;vendor -> Biochain;isolate -> Lot no.: B106007;gender -> Male), WomString(series) -> WomString(GSE69360), WomString(organism) -> WomString(Homo sapiens), WomString(run) -> WomString(SRR2014240), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014240), WomString(name) -> WomString(Biochain_Adult_Kidney), WomString(gsm) -> WomString(GSM1698570), WomString(title) -> WomString(Biochain_Adult_Kidney))), WomString(run) -> WomString(SRR2014240), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/cal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555:445,monitor,monitoring,445,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555,1,['monitor'],['monitoring']
Energy Efficiency,Travis build fails but reports green/passing,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2788:31,green,green,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2788,1,['green'],['green']
Energy Efficiency,"Travis doesn't have ignored-test-results afaik, nor support for a dynamic yaml test matrix based on internal-vs.-external contributions. If one knows of a way to do what the ticket wants, ignore vs. pass, I'll happily incorporate that fix. Otherwise, this PR reduces red-fatigue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2045#issuecomment-283436987:259,reduce,reduces,259,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2045#issuecomment-283436987,1,['reduce'],['reduces']
Energy Efficiency,"Under load Cromwell seems to have a large number of engine dispatcher threads blocked with stack traces like the following:. ```; ""cromwell-system-akka.dispatchers.engine-dispatcher-107"" #455 prio=5 os_prio=0 tid=0x00007fa5b8075000 nid=0x4598 waiting for monitor entry [0x00007fa7999d8000]; java.lang.Thread.State: BLOCKED (on object monitor); at cromwell.core.logging.WorkflowLogger$.makeFileLogger(WorkflowLogger.scala:26); - waiting to lock <0x00000000ca4a8b40> (a cromwell.core.logging.WorkflowLogger$); at cromwell.core.logging.WorkflowLogger.<init>(WorkflowLogger.scala:108); at cromwell.core.logging.WorkflowLogging$class.workflowLogger(WorkflowLogger.scala:21); at cromwell.engine.workflow.WorkflowActor.workflowLogger$lzycompute(WorkflowActor.scala:151); - locked <0x00000000eceaf5f0> (a cromwell.engine.workflow.WorkflowActor); at cromwell.engine.workflow.WorkflowActor.workflowLogger(WorkflowActor.scala:151); at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:262); at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:260); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at akka.actor.FSM$$anonfun$handleTransition$1.apply(FSM.scala:606); at scala.collection.immutable.List.foreach(List.scala:381); at akka.actor.FSM$class.handleTransition(FSM.scala:606); at akka.actor.FSM$class.makeTransition(FSM.scala:688); at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:151); at akka.actor.FSM$class.applyState(FSM.scala:673); at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:151); at akka.actor.FSM$class.processEvent(FSM.scala:668); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1445:255,monitor,monitor,255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1445,2,['monitor'],['monitor']
Energy Efficiency,"Updates [com.google.cloud:google-cloud-monitoring](https://github.com/googleapis/java-monitoring) from 1.100.1 to 2.0.0.; [GitHub Release Notes](https://github.com/googleapis/java-monitoring/releases/tag/v2.0.0) - [Changelog](https://github.com/googleapis/java-monitoring/blob/master/CHANGELOG.md) - [Version Diff](https://github.com/googleapis/java-monitoring/compare/v1.100.1...v2.0.0). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/fthomas/scala-steward/blob/3208ffc94f1fcd586eda4a559961d1ef013f8952/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" } ]; ```; </details>. labels: library-update, semver-major",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5742:39,monitor,monitoring,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5742,6,['monitor'],['monitoring']
Energy Efficiency,"Updates [com.google.cloud:google-cloud-monitoring](https://github.com/googleapis/java-monitoring) from 1.100.1 to 2.0.1.; [GitHub Release Notes](https://github.com/googleapis/java-monitoring/releases/tag/v2.0.1) - [Changelog](https://github.com/googleapis/java-monitoring/blob/master/CHANGELOG.md) - [Version Diff](https://github.com/googleapis/java-monitoring/compare/v1.100.1...v2.0.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/fthomas/scala-steward/blob/e6e484297151f4295e46ab0ef4aeb2de13a91724/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" } ]; ```; </details>. labels: library-update, semver-major",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5764:39,monitor,monitoring,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5764,6,['monitor'],['monitoring']
Energy Efficiency,"Updates [com.google.cloud:google-cloud-monitoring](https://github.com/googleapis/java-monitoring) from 3.2.5 to 3.2.10.; [GitHub Release Notes](https://github.com/googleapis/java-monitoring/releases/tag/v3.2.10) - [Version Diff](https://github.com/googleapis/java-monitoring/compare/v3.2.5...v3.2.10). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7041:39,monitor,monitoring,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7041,6,['monitor'],['monitoring']
Energy Efficiency,"Updates [com.google.cloud:google-cloud-resourcemanager](https://github.com/googleapis/java-resourcemanager) from 1.1.2 to 1.1.4.; [GitHub Release Notes](https://github.com/googleapis/java-resourcemanager/releases/tag/v1.1.4) - [Version Diff](https://github.com/googleapis/java-resourcemanager/compare/v1.1.2...v1.1.4). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/00809e6249b134635f71919c17c1c81603beb22d/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.1.2).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/green/arrays/arrays.inputs; ```; </details>; <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-resourcemanager"" } ]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6611:1048,green,green,1048,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6611,1,['green'],['green']
Energy Efficiency,"Updates [org.mockftpserver:MockFtpServer](https://mockftpserver.org) from 3.0.0 to 3.1.0. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9367739b48d1bae5766674e547f8cdf8a82d318a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (3.0.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/green/arrays/arrays.options; database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mockftpserver"", artifactId = ""MockFtpServer"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.mockftpserver"", artifactId = ""MockFtpServer"" }; }]; ```; </details>. labels: test-library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7071:819,green,green,819,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7071,1,['green'],['green']
Energy Efficiency,"Updates com.google.cloud:google-cloud-monitoring from 2.0.1 to 2.0.3. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/fthomas/scala-steward/blob/6525ca66f9e66047fdcc2b5f4391921a1dec5c61/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" } ]; ```; </details>. labels: library-update, semver-patch",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5884:38,monitor,monitoring,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5884,2,['monitor'],['monitoring']
Energy Efficiency,"Updates com.google.cloud:google-cloud-monitoring from 2.0.1 to 2.0.4. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/fthomas/scala-steward/blob/0484b2e331da8203ec4270291416c96540bebe35/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" } ]; ```; </details>. labels: library-update, semver-patch",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5926:38,monitor,monitoring,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5926,2,['monitor'],['monitoring']
Energy Efficiency,"Updates com.google.cloud:google-cloud-monitoring from 2.0.14 to 2.3.3. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/92f43ce5f010fa03daeb2625a3f9c1b1be80cca5/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" } ]; ```; </details>. labels: library-update, semver-minor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6400:38,monitor,monitoring,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6400,2,['monitor'],['monitoring']
Energy Efficiency,"Updates com.google.cloud:google-cloud-monitoring from 2.0.4 to 2.0.5. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/fthomas/scala-steward/blob/f73b36c43171cc50fa4140329f0c50e5c1cd2f97/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" } ]; ```; </details>. labels: library-update, semver-patch",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5962:38,monitor,monitoring,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5962,2,['monitor'],['monitoring']
Energy Efficiency,"Updates com.google.cloud:google-cloud-monitoring from 2.0.4 to 2.0.6. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/fthomas/scala-steward/blob/34f2a22d62f874be4398bb76b5d5ddef377873ca/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" } ]; ```; </details>. labels: library-update, semver-patch",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5971:38,monitor,monitoring,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5971,2,['monitor'],['monitoring']
Energy Efficiency,"Updates com.google.cloud:google-cloud-monitoring from 2.0.4 to 2.0.7. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/b81ddda677fdf5b341d0edd6a6b1fb511472e7f0/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" } ]; ```; </details>. labels: library-update, semver-patch",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6011:38,monitor,monitoring,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6011,2,['monitor'],['monitoring']
Energy Efficiency,"Updates com.google.cloud:google-cloud-monitoring from 2.0.4 to 2.0.8. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/b0f7471dc9fc547e00d32ff7a800073ea1d1951d/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" } ]; ```; </details>. labels: library-update, semver-patch",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6063:38,monitor,monitoring,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6063,2,['monitor'],['monitoring']
Energy Efficiency,"Updates com.google.cloud:google-cloud-monitoring from 2.0.8 to 2.0.14. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/b48aba70ec793405c98788a322d160987ba51d3e/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" } ]; ```; </details>. labels: library-update, semver-patch",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6253:38,monitor,monitoring,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6253,2,['monitor'],['monitoring']
Energy Efficiency,"Updates com.google.cloud:google-cloud-monitoring from 3.0.2 to 3.0.6. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/6472b97b3365f2800f4202d1bf6b1d647bd2b0cc/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (3.0.2).; You might want to review and update them manually.; ```; project/Dependencies.scala; project/swagger2markup.sbt; src/ci/bin/test.inc.sh; ```; </details>; <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" } ]; ```; </details>. labels: library-update, semver-patch, old-version-remains",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6518:38,monitor,monitoring,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6518,2,['monitor'],['monitoring']
Energy Efficiency,"Updates com.google.cloud:google-cloud-monitoring from 3.0.6 to 3.0.8. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/00809e6249b134635f71919c17c1c81603beb22d/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" } ]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6610:38,monitor,monitoring,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6610,2,['monitor'],['monitoring']
Energy Efficiency,"Updates com.google.cloud:google-cloud-monitoring from 3.2.5 to 3.2.9. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (3.2.5).; You might want to review and update them manually.; ```; cwl/src/test/resources/cwl/ontology/EDAM.owl; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-monitoring"" }; }]; ```; </details>. labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6859:38,monitor,monitoring,38,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6859,3,['monitor'],['monitoring']
Energy Efficiency,Use Cloud SDK image for monitoringTerminationAction in PAPIv2 backend [BA-6117],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5276:24,monitor,monitoringTerminationAction,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5276,4,['monitor'],['monitoringTerminationAction']
Energy Efficiency,"Useful tidbit to note, [this is how you signal failure due to unimplemented functionality](https://github.com/common-workflow-language/common-workflow-language/pull/278/files#diff-ee814a9c027fc9750beb075c283a973cR49) - which I believe means that one can still be ""green"" on CI",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2590#issuecomment-333315542:264,green,green,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2590#issuecomment-333315542,1,['green'],['green']
Energy Efficiency,"Volcano is a batch system running high performance workloads on Kubernetes, which provides scheduling, job management, and other mechanisms k8s missed. This is an example configure for cromwell to run on volcano clusters.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5184:91,schedul,scheduling,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5184,1,['schedul'],['scheduling']
Energy Efficiency,"WDL file: hello.wdl; [2015-12-18 08:43:13,439] [info] Inputs: hello.json; [2015-12-18 08:43:13,560] [info] input: test.hello.name => ""world""; [2015-12-18 08:43:13,776] [info] SingleWorkflowRunnerActor: launching workflow; [2015-12-18 08:43:15,936] [info] Running with database db.url = jdbc:hsqldb:mem:86473284-494c-43d2-94fd-d00107a2a787;shutdown=false;hsqldb.tx=mvcc; [2015-12-18 08:43:17,516] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = e67af113-c3a7-41f4-9178-6640c1c652e9; [2015-12-18 08:43:17,592] [info] WorkflowManagerActor Found no workflows to restart.; [2015-12-18 08:43:18,816] [error] SingleWorkflowRunnerActor: Ask timed out on [Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312]] after [5000 ms]; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312]] after [5000 ms]; at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:334); at akka.actor.Scheduler$$anon$7.run(Scheduler.scala:117); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:599); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:597); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467); at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375); at java.lang.Thread.run(Thread.java:745); [2015-12-18 08:43:19,174] [info] Message [cromwell.engine.workflow.WorkflowManagerActor$RestartWorkflows] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/334:1385,Schedul,Scheduler,1385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/334,1,['Schedul'],['Scheduler']
Energy Efficiency,WX-1489 Hopefully reduce CI flakiness by modernizing deadlock test,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7376:18,reduce,reduce,18,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7376,1,['reduce'],['reduce']
Energy Efficiency,WX-751 Token refresh signal for monitoring,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6939:32,monitor,monitoring,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6939,1,['monitor'],['monitoring']
Energy Efficiency,WX-927 monitoring log fixes for GCP Batch backend,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7549:7,monitor,monitoring,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7549,1,['monitor'],['monitoring']
Energy Efficiency,Wait for Cromwell travis master build to go green before releasing to github,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2401:44,green,green,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401,1,['green'],['green']
Energy Efficiency,Waiting for the tests to turn green-- will take off the EarlyFeedbackRequested label at that point.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1394#issuecomment-246362005:30,green,green,30,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1394#issuecomment-246362005,1,['green'],['green']
Energy Efficiency,"We can make it happen. We should schedule the PR update. I suspect if one were to touch the `changelog.xml` in develop, it would cause a merge conflict here in this PR, preventing the two changes from being _accidentally_ merged. I would need to rebase this branch on top of any other changes. Specifically, the new liquibase file would need to be moved, along with updating the moved `changelog.xml`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-173437400:33,schedul,schedule,33,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/371#issuecomment-173437400,1,['schedul'],['schedule']
Energy Efficiency,"We had a situation where develop was broken (grab commit 0ff86c409d2e5dac4b766fceb89f47ba3c304f99). If you run ""sbt test"" it fails with a compilation error about HtCondorInitializationActorSpec.scala. However, Travis for this is green. Travis is running:. sbt -Dbackend.providers.Local.config.filesystems.local.localization.0=copy clean coverage nointegration:test coverageReport && sbt coverageAggregate && sbt assembly. Which if you run it locally yields a successful test. First thought was that it was because this test was flagged as ""nointegration"" but that's not even the case. However even if it were, we should be checking that things compile even if we don't run a certain class of tests.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/888:229,green,green,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/888,1,['green'],['green']
Energy Efficiency,"We have an issue with the centaurJes tests running for external users. . Assuming the tests go green for https://github.com/broadinstitute/cromwell/pull/1760, I think this one should be considered green too.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1757#issuecomment-265764232:95,green,green,95,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1757#issuecomment-265764232,2,['green'],['green']
Energy Efficiency,"We have the same issue; we use an OGE back-end that implements hard limits on resource utilization which will terminate jobs that exceed these limits without allowing them to create an rc file. Currently we are running local mode cromwell instances with a workaround of putting a time-limit on the cromwell task of 48 hours, but this is extremely wasteful of our back end resources (cromwell itself consumes a large amount of memory on our cluster nodes). Our current workarounds involve putting a soft-limit on each job, attempting to trap SIGUSR1 in advance of the job being killed by the queue manager, at which point they create an rc file with a non-zero error code, but there is no guarantee that we can catch every instance of this. . Ideally we would like for cromwell to query running jobs at a user configurable interval (optionally never, but not as often as the file system is pinged for rc files so as not to burden the scheduler) against its list of jobs that are both not finished, and running and at minimum trigger the equivalent error state of a non-zero rc file return code. . We experience this problem frequently (and expect it to increase as we move more pipelines to cromwell) because our pipelines can not reliably estimate the amount of memory they need for tasks apriori.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150:933,schedul,scheduler,933,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-359006150,1,['schedul'],['scheduler']
Energy Efficiency,"We just reduced our use of read functions, usually removing globs helped a lot.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-823928232:8,reduce,reduced,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4057#issuecomment-823928232,1,['reduce'],['reduced']
Energy Efficiency,"We randomly receive PKIX and RPC deadline errors on a small subset of GCP LS API and Batch runs, which always crash the main Cromwell java process. We have added local certs to our java install launching cromwell - this has reduced, but not eliminated PKIX errors. We have no remedy for these issues at the moment besides submitting again. Is there anything we can do as users to make cromwell more fault-tolerant of cloud-related issues like these?. PKIX error example:. Failed to query job status (projects/XXXXX/jobs/job-7638b0fb-XXXX-XXXX-81ca-9f609da4c664) from GCP; com.google.api.gax.rpc.UnavailableException: io.grpc.StatusRuntimeException: UNAVAILABLE: io exception; Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]; at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:112); at com.google.api.gax.rpc.ApiExceptionFactory.createException(ApiExceptionFactory.java:41); at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:86); at com.google.api.gax.grpc.GrpcApiExceptionFactory.create(GrpcApiExceptionFactory.java:66); at com.google.api.gax.grpc.GrpcExceptionCallable$ExceptionTransformingFuture.onFailure(GrpcExceptionCallable.java:97); at com.google.api.core.ApiFutures$1.onFailure(ApiFutures.java:84); at com.google.common.util.concurrent.Futures$CallbackListener.run(Futures.java:1133); at com.google.common.util.concurrent.DirectExecutor.execute(DirectExecutor.java:31); at com.google.common.util.concurrent.AbstractFuture.executeListener(AbstractFuture.java:1277); at com.google.common.util.concurrent.AbstractFuture.complete(AbstractFuture.java:1038); at com.google.common.util.concurrent.AbstractFuture.setException(AbstractFuture.java:808); at io.grpc.stub.ClientCalls$GrpcFuture.setException(ClientCalls.java:574); at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:544); at io.grpc.PartialForw",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7551:224,reduce,reduced,224,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7551,1,['reduce'],['reduced']
Energy Efficiency,"We want to run green workflows with slimmed down inputs to make sure we haven't broken any of the features they currently use with our changes. First try will be with using Travis and probably moving to Jenkins if it can't be done. This test could be run nightly/weekly, discuss with the team once things are set up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2336:15,green,green,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2336,1,['green'],['green']
Energy Efficiency,"We want to run green workflows with slimmed down inputs to make sure we haven't broken any of the features they currently use with our changes. The easiest first step would be to make this a part of the Tyburn daily test we already have on Jenkins. This test should probably run weekly, discuss with the team once things are set up.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2337:15,green,green,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2337,1,['green'],['green']
Energy Efficiency,"We've got an ongoing situation with Green where their Cromwell halts workflow processing with zips after a certain amount of time https://github.com/broadinstitute/cromwell/issues/4117. . Our current hypothesis is that they are running out of disk space for unzipped imports. Their Cromwell already had hundreds of MBs worth within a few hours after restart (which, because of the way they Docker, clears the disk). @ApChagi @tbl3rd @hjfbynara @tlangs",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4406#issuecomment-457345628:36,Green,Green,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4406#issuecomment-457345628,1,['Green'],['Green']
Energy Efficiency,"We've merged a few improvements based on the investigation (due in Cromwell 37), but we cannot guarantee this specific problem is fixed. The root cause is proving elusive and we find ourselves needing to pause for now due to schedule pressure.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-442192218:225,schedul,schedule,225,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4117#issuecomment-442192218,1,['schedul'],['schedule']
Energy Efficiency,"What @danbills said. It's a probabilistic data structure which can tell you either ""I've probably seen this before"" or ""I've definitely *not* seen this before"" in a very space & time efficient manner. So in a case like this where there are a lot of misses we could use it to do a quick ""should I even bother hitting the DB"" sanity test. . I'd want to see that this is actually a bottleneck before proceeding, this was just an idea I had that I wanted to keep around for memory's sake in case we do see it be a bottleneck.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332244385:183,efficient,efficient,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2248#issuecomment-332244385,1,['efficient'],['efficient']
Energy Efficiency,"What are the other 2 commands ? It's a known issue that if the commands are very fast to execute, the monitoring.log is not flushed before it's delocalized and the VM shuts down, and it ends up empty.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-232419608:102,monitor,monitoring,102,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1158#issuecomment-232419608,1,['monitor'],['monitoring']
Energy Efficiency,"What can we do to make sure this gets done? We need this for FC users (specifically) right now -- basically want to be able to capture VPC traffic. So FC and Compliance is the driver. Since this is cross-team, who owns the work? Who can get it on the schedule? FC PAPI nodes should be launched in a Subnet, is the main issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4070#issuecomment-434326020:251,schedul,schedule,251,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4070#issuecomment-434326020,1,['schedul'],['schedule']
Energy Efficiency,What is the affect of the current EJEA versus a reduced EJEA on Cromwell and the user? What do you mean by developer sanity? What would the effort be to reduce EJEA?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-324470912:48,reduce,reduced,48,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1413#issuecomment-324470912,2,['reduce'],"['reduce', 'reduced']"
Energy Efficiency,"What's here looks good, but our tech talk left me wondering if this is actually going to address the problems the ticket is looking to solve. i.e., if we tell people there's this new `sub()` function but when they try to write real-world WDLs that use it they can't because of existing language restrictions that aren't really related to the changes here. I'm thinking we might want to ask Green for samples of their workflows that have been `sub`bed and see if Cromwell can deal with them on this branch.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/605#issuecomment-200489510:390,Green,Green,390,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/605#issuecomment-200489510,1,['Green'],['Green']
Energy Efficiency,"When Cromwell is finding a successful call cache hit, it is failing to copy the file even though the user has access to it. This causes the workflow to be rerun rather than copying the cached results. . Inside Failures you can see the following error:; ```[Attempted 1 time(s)] - HttpResponseException: 400 Bad Request { ""error"": { ""errors"": [ { ""domain"": ""global"", ""reason"": ""required"", ""message"": ""Bucket is requester pays bucket but no user project provided."" } ], ""code"": 400, ""message"": ""Bucket is requester pays bucket but no user project provided."" } }```. https://portal.firecloud.org/#workspaces/dvoet-prod-test-20190305-3/dvoet_tutorial_requester_pays/monitor/9d516b3c-5b7f-4241-9929-99b54ef7e7e1/102e99b1-26b2-4bf4-80ec-fcc02c32136d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4756:662,monitor,monitor,662,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4756,1,['monitor'],['monitor']
Energy Efficiency,"When I first looked at this I thought it was not in our power to change because the standard said something different, but I believe the standard allows for `{}` and we simply need to update the parser in OpenWDL (and then Cromwell).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4501#issuecomment-477711210:56,power,power,56,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4501#issuecomment-477711210,1,['power'],['power']
Energy Efficiency,"When deploying Cromwell on our HPC which uses slurm as a scheduler, I use a wckey unique to a workflow and identical across tasks in that workflow - very handy for searching for failed jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-544098095:57,schedul,scheduler,57,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5235#issuecomment-544098095,1,['schedul'],['scheduler']
Energy Efficiency,"When investigating the GCS ls issue from the green folks in 0.19 I saw that it was using one of our old style blocking retries. I then assumed that there's no way we'd still be doing that and told everyone that we weren't. Then I went and double checked what I said and sadly I was wrong. GcsFileSystemProvider.withRetry blocks threads. With the new dispatcher bulkheading it should be better, but considering how many of these might be in flight at once in a joint genotyping sort of environment, this seems bad.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1279:45,green,green,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1279,1,['green'],['green']
Energy Efficiency,"When testing/adapting production WDLs for public use, I often run into the problem that some of the input files are in private buckets. Because there are many input files, it's tedious to check all of them. So I tend to miss one or two that eventually make my workflows fail, often in late stages of the workflow. This can also happen if you get something wrong in a file path (typo, forgot a subdirectory etc). . It would be incredibly helpful if there was an option to have Cromwell check that all the external input files (specified with full paths to eg a gs:// bucket) are available and reachable (meaning we have permission to read them) before kicking off the actual workflow, and fail with an informative error if it's not the case. This would save a fair amount of iteration on large workflows with many external inputs.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2163:13,adapt,adapting,13,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2163,1,['adapt'],['adapting']
Energy Efficiency,"When we interact with Google APIs, including JES and GCS sometimes things go awry for transient reasons. The belief is that retrying a bit would make it go green. We want to handle this automatically for the user. We shall:. - Retry up to N times via an exponential backoff. N is TBD (10?); - Upon each retry, emit a log message; - If N is reached the job will fail w/ the last error event as the failure reason. TBD: Design discussion on how to implement this",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1914:156,green,green,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1914,1,['green'],['green']
Energy Efficiency,"While exploring the idea of using a `monitoring_image` for this, I noticed it injects more or less the metadata I'd want into the monitoring container via environment variables already: . https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/v2beta/src/main/scala/cromwell/backend/google/pipelines/v2beta/api/MonitoringAction.scala#L36. https://github.com/broadinstitute/cromwell/blob/adb8d2ad87cba307e5b1eccd1a3e21857cc9b81c/supportedBackends/google/pipelines/common/src/main/scala/cromwell/backend/google/pipelines/common/monitoring/Env.scala#L18. Is there a reason this could not also be injected into UserActions, and would you accept a PR that does so? (As a side note, it seems the monitoring image could likely accomplish what we want as well, but using one on Terra, or setting any custom workflow options is not allowed as far as I know).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851:130,monitor,monitoring,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7137#issuecomment-1590042851,4,"['Monitor', 'monitor']","['MonitoringAction', 'monitoring']"
Energy Efficiency,"While playing around with some performance/scalability stuff today I saw various first hand examples just how costly logging can be in terms of throughput. . For instance, while submitting workflows in a tight loop removing the ""workflow X submitted"" logging consistently improved overall requests per second (as per apache bench) by roughly 20%. This example is likely the largest offender but we should start looking at stuff like this and think carefully if the logging is worth the drain.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1807:486,drain,drain,486,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1807,1,['drain'],['drain']
Energy Efficiency,"While this is investigated, you should be able to work around this by moving the constant string outside the `${}`s, eg ; ```; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032#issuecomment-350283048:195,adapt,adapters,195,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032#issuecomment-350283048,2,['adapt'],['adapters']
Energy Efficiency,Whilst looking into non-MySQL based solutions for CQRS read stores I saw what I thought was a neat idea. By tagging each event with a version it is possible to not do bulk migrations when one needs to modify event entity structure (aside: we need to find ways to reduce that) but rather to migrate them the first time they're accessed.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2356:263,reduce,reduce,263,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2356,1,['reduce'],['reduce']
Energy Efficiency,"Why not â€œUsing containers with Cromwellâ€? Have a section for singularity (and undocker), and have a subsection of that for singularity with job schedulers?. There are other Cromwell github threads that have the user goal as practically: I canâ€™t use docker, what do I do? Thatâ€™s definitely a part of my use case, and reproducibility is a big point as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-462488066:144,schedul,schedulers,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-462488066,1,['schedul'],['schedulers']
Energy Efficiency,"With your current version of Cromwell, the workflow does not terminate even if the underlying task is killed by the HPC scheduler due to out of memory error. This should be generalized to batch schedulers.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5107:120,schedul,scheduler,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5107,2,['schedul'],"['scheduler', 'schedulers']"
Energy Efficiency,Won't go green until https://github.com/broadinstitute/wdl4s/pull/67. Includes better error messages and halting failure for RuntimeExceptions during expression evaluation.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1863:9,green,green,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1863,1,['green'],['green']
Energy Efficiency,Would it be easy to set it for _all_ text outputs? The other one I'm thinking about is `monitoring.log`,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518311368:88,monitor,monitoring,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518311368,1,['monitor'],['monitoring']
Energy Efficiency,"Wow! Thanks @kshakir! This is great news! Thanks for putting your effort in this! ; I think I will start a few pipelines to see the effect on memory usage and storage space and compare that with HSQLdb. @aednichols ; > It seems that SQLite is a superset of what is supported by HSQL. It is also very broadly popular. > Could we replace HSQL with SQLite so that we can aren't maintaining two embedded database implementations?. Replacing HSQL with SQLite and making SQLite default has multiple advantages:; - HSQL uses a lot of memory for the in-memory database. SQLite is probably more efficient (testing required); - With SQLite a file-based database could be enabled by default (in `<<CROMWELL_ROOT>>/cromwell-db.sqlite`) which would enable call-caching by default for people using it from the command line with the run command.; - If the filebased database is enabled by default, there is no need for most users to get into the details of getting a proper database server running. However HSQL has the advantage that it is very well supported by slick and other java/scala practice (HSQL is itself written in Java) and that it requires minimal code to get it working. By contrast SQLite needs more code as shown in this PR. It is thus more costly to support SQLite.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734691039:586,efficient,efficient,586,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6091#issuecomment-734691039,1,['efficient'],['efficient']
Energy Efficiency,Yeah I recall Jeff tried a simpler setup like this first and then had to resort to nesting Cromwells when that didn't work. But clearly Cromwell has become that much more efficient so now it does work hooray!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2948#issuecomment-347844423:171,efficient,efficient,171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2948#issuecomment-347844423,1,['efficient'],['efficient']
Energy Efficiency,"Yeah weird, I don't have that much trust in coverall honestly, because although it's -0.02 it's also green..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1056#issuecomment-228054459:101,green,green,101,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1056#issuecomment-228054459,1,['green'],['green']
Energy Efficiency,"Yes, I am using Cromwell on SLURM. The significant changes since the completed pull request for the SLURM configuration have been; 1. addition of `script-epilogue = """"` to eliminate the sync behavior after completing tasks. Sync caused some jobs to wait beyond their runtime limits for the sync to complete. ; 2. `concurrent-job-limit` to limit number of jobs submitted to scheduler. The HMS staff will kill all jobs if you submit lots (many hundreds?) of jobs that complete too quickly (<1min) because this creates a bottleneck at the scheduler. `concurrent-job-limit` obviates the need to substantially rewrite the workflow, and in my case contention with other users usually limits the number of compute nodes available more than `concurrent-job-limit`. . #1499 was a problem during pipeline development, and as a result I use very generous runtime limits for all jobs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401:373,schedul,scheduler,373,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1750#issuecomment-328217401,2,['schedul'],['scheduler']
Energy Efficiency,"Yes. On Mon, Oct 29, 2018 at 10:45 AM Thib <notifications@github.com> wrote:. > Hi !; > Just to make sure I understand, are you saying that the monitoring log is; > not copied over when a call is being cached ?; >; > â€”; > You are receiving this because you authored the thread.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-433937512>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/Ao_T2PcOQlD_TvcUcH4sutd4vJ9OdN8mks5upxSngaJpZM4X_RGI>; > .; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-433948423:144,monitor,monitoring,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4330#issuecomment-433948423,1,['monitor'],['monitoring']
Energy Efficiency,"Yesterday there was a config change that targeted the /stats endpoint rather than /status to assess Cromwell vitality. Unfortunately this accidentally seems to have DOSed Cromwell and produced tons of messages like the following in the logs. Cromwell effectively locked up and needed a hard restart to recover. I don't think the rate at which /stats was called was excessively high. The counting mechanism is apparently sending messages around to the whole graph of execution actors when it seems like a more efficient means of answering the stats question should be possible. Even if turns out a more efficient calculation isn't possible, the current system doesn't appear to be taking sub workflow actors into account correctly and I don't even know how the MWDA and WIA got caught up in this. ```; WARN c.e.w.l.e.SubWorkflowExecutionActor - unhandled event JobCountQuery in state SubWorkflowRunningState; ```. ```; WARN c.e.w.l.m.MaterializeWorkflowDescriptorActor - MaterializeWorkflowDescriptorActor [UUID(XXXXX)]: received an unhandled message Event(JobCountQuery,()) in state MaterializingState; ```. ```; WARN c.e.w.l.i.WorkflowInitializationActor - WorkflowInitializationActor-XXXXX [UUID(XXXXX)]: received an unhandled message: Event(JobCountQuery,WorkflowLifecycleActorData(Set(Actor[XXXXX]),List(),Map(),List())); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3683:509,efficient,efficient,509,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3683,2,['efficient'],['efficient']
Energy Efficiency,"You could set `preemptible` very high to minimize the chance of preemption. I don't think there would be any issue setting it to 10 or even more. That said, it can be a bit of a false economy because failed attempts still cost real money. It may even be the case that falling back to non-preemptible saves money. Let's say preemptibles are $1 an hour and normal VMs are $3. If you run a 12 hour task that gets preempted 6 times at the 6 hour mark, that's 6 x 6 x $1 = $36 down the drain, a day and a half of wall clock time, and no results to show for it. Whereas a single non-preemptible run would be 12 x $3 = $36 and you'd have your results. Obviously this math will vary widely by use case and you will have to observe your preemption rates in practice to come up with the optimal balance. Thanks for an interesting discussion, I had never thought about the ""only preemptible"" use case before.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030186650:481,drain,drain,481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6666#issuecomment-1030186650,1,['drain'],['drain']
Energy Efficiency,[34 hotfix] Reduce hash cost of WdlSyntaxErrorFormatter (#3965),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3975:12,Reduce,Reduce,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3975,1,['Reduce'],['Reduce']
Energy Efficiency,[51 hotfix] Reduce blacklist cache metric spam [BA-6464],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5527:12,Reduce,Reduce,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5527,1,['Reduce'],['Reduce']
Energy Efficiency,[BA-6612 / Hotfix edition] Reduce number of ExecutionStore updates generated by large scatters,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5886:27,Reduce,Reduce,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5886,1,['Reduce'],['Reduce']
Energy Efficiency,"[Per @mbookman]; This pull request is an initial update to address:. CROM-6718: FR: Add flag for minimizing chance of GCP cross-region network egress charges being incurred. This PR specifically focuses on the risks of egress charges incurred due to call caching. The framing of the approach here, which is a bit broader than originally noted in CROM-6718, is:; Make call caching location-aware, prioritizing copies that minimize egress charges.; Add a workflow option enabling control of what egress charges can be incurred for call cache copying.; The new workflow option would be:. call_cache_egress: [none, continental, global]. where the values affect whether call cache copies can incur egress charges:; none: only within-region copies are allowed, which generate no egress charges; continental: within content copies are allowed; within-content copies have reduced costs, such as $0.01 / GB in the US; global: copies across all regions are allowed. Cross-content egress charges can be much higher (ranging from $0.08 / GB up to $0.23 / GB). ### CURRENT STATUS OF PR:; These first few commits are a WIP/request for feedback. I would love discussion on what the best approach would be. The idea for this initial approach is to raise an exception right before copying cached outputs if the bucket locations would cause an egress charge (depending on workflow option). The CallCacheJobActor continue attempting to copy outputs until it finds one that doesn't cause an egress charge (depending on workflow option), or until it determines cache miss. . If the above approach is reasonable then I would need coding advice on:; 1) How do I properly use GcsBatchCommandBuilder.locationCommand (or something similar) in the CacheHitCopyingActor?; 2) How do I properly get the WorkflowOption CallCacheEgress in the CacheHitCopyingActor?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6324:150,charge,charges,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6324,10,"['charge', 'reduce']","['charge', 'charges', 'reduced']"
Energy Efficiency,[WX-1670] Don't allocate job tokens for hog groups experiencing quota exhaustion,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7520:16,allocate,allocate,16,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7520,1,['allocate'],['allocate']
Energy Efficiency,[WX-1835] Scheduled logging for list of groups experiencing quota exhaustion,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7539:10,Schedul,Scheduled,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7539,1,['Schedul'],['Scheduled']
Energy Efficiency,[develop] Reduce blacklist cache metric spam [BA-6464],MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5528:10,Reduce,Reduce,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5528,1,['Reduce'],['Reduce']
Energy Efficiency,_Big picture:_ This PR introduces a `CallDescriptor` wrapper around some other stuff. How do you envision this to be of use in PBE? Do you see this as turning into a `TaskDescriptor`? Does it reduce any effort later on?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-191933148:192,reduce,reduce,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/502#issuecomment-191933148,1,['reduce'],['reduce']
Energy Efficiency,"_prio=31 tid=0x00007fb76b38c000 nid=0x5f03 waiting on condition [0x000000012ac3d000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c002f9e0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(Redefined); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(Redefined); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java). ""cromwell-system-scheduler-1"" #10 prio=5 os_prio=31 tid=0x00007fb76b20f000 nid=0x5a07 runnable [0x000000012a0e1000]; java.lang.Thread.State: RUNNABLE; at com.jprofiler.agent.InstrumentationCallee.exitFilteredMethod(Native Method); at com.jprofiler.agent.InstrumentationCallee.__ejt_filter_exitMethod(ejt:86); at akka.dispatch.AbstractNodeQueue$Node.next(AbstractNodeQueue.java:124); at akka.dispatch.AbstractNodeQueue.pollNode(AbstractNodeQueue.java:86); at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:411); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Redefined); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Redefined); at java.lang.Thread.run(Redefined). ""Service Thread"" #9 daemon prio=9 os_prio=31 tid=0x00007fb76a82e000 nid=0x5103 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread3"" #8 daemon prio=9 os_prio=31 tid=0x00007fb76a060000 nid=0x4f03 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread2"" #7 daemon prio=9 os_prio=31 tid=0x00007fb76b011000 nid=0x4d03 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread1"" #6 daemon prio=9 os_prio=31 tid=0x00007fb76a815000 nid=0x4b03 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread0"" #5 daemon prio=9 os_prio=31 tid=0x00007fb76b810000 nid=0x4903 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""Signal Dispatcher",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:48211,Schedul,Scheduler,48211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Schedul'],['Scheduler']
Energy Efficiency,"` also increases the worker's delay to pull work, for example, setting this value to `100` or above causes would cause the delay to become ~18m which seems insane (see [BatchApiRequestManager.scala](https://github.com/broadinstitute/cromwell/blob/cee36d98755d2163f279600786bd60d6226835f0/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/api/BatchApiRequestManager.scala#L67)), putting an upper limit on the delay seems worth it, any thoughts?; 5. Do we need to get anything else for the job execution events? see below and [BatchRequestExecutor#getEventList](https://github.com/broadinstitute/cromwell/blob/a333f65b8e80ae37091a5629e0331c2105aeefeb/supportedBackends/google/batch/src/main/scala/cromwell/backend/google/batch/api/request/BatchRequestExecutor.scala#L196). <details>; <summary>Execution events details</summary>. What GCP provides:. ```; Event type=STATUS_CHANGED; time=seconds: 1712173852,nanos: 952604950; taskState=STATE_UNSPECIFIED,; description=Job state is set from QUEUED to SCHEDULED for job projects/392615380452/locations/us-south1/jobs/job-ba81bad8-82e9-4d95-8fc0-04dfbbd746da.; taskExecution.exitCode=0. Event type=STATUS_CHANGED,; time=seconds: 1712173947, nanos: 568998105; taskState=STATE_UNSPECIFIED; description=Job state is set from SCHEDULED to RUNNING for job projects/392615380452/locations/us-south1/jobs/job-ba81bad8-82e9-4d95-8fc0-04dfbbd746da.; taskExecution.exitCode=0. Event type=STATUS_CHANGED; time=seconds: 1712173989, nanos: 937816549; taskState=STATE_UNSPECIFIED; description=Job state is set from RUNNING to SUCCEEDED for job projects/392615380452/locations/us-south1/jobs/job-ba81bad8-82e9-4d95-8fc0-04dfbbd746da.; taskExecution.exitCode=0; ```. What we define as execution events:. ```; ExecutionEvent(Job state is set from QUEUED to SCHEDULED for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:10:01.704137839Z,None); ExecutionEvent(Job state is set from SCHEDULED",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412:4004,SCHEDUL,SCHEDULED,4004,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412,1,['SCHEDUL'],['SCHEDULED']
Energy Efficiency,"```; Â» java -jar womtool-80.jar validate sc_rna/rna_pipeline_test.wdl; Success!; Â» java -jar womtool-80.jar parse sc_rna/rna_pipeline_test.wdl; Exception in thread ""main"" wdl.draft2.parser.WdlParser$SyntaxError: Unrecognized token on line 28, column 46:. cellranger count --id=~{id} --sample=~{sample} --transcriptome=~{rna_refdir} --fastqs=~{fastq_dir}; ^; 	at wdl.draft2.parser.WdlParser.unrecognized_token(WdlParser.java:6975); 	at wdl.draft2.parser.WdlParser.lex(WdlParser.java:7048); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:263); 	at wdl.draft2.model.AstTools$.getAst(AstTools.scala:276); 	at womtool.WomtoolMain$.parse(WomtoolMain.scala:82); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:55); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:161); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:166); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:27); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1(App.scala:76); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563); 	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:926); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:27); 	at womtool.WomtoolMain.main(WomtoolMain.scala); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6792:1204,adapt,adapted,1204,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6792,1,['adapt'],['adapted']
Energy Efficiency,"a.util.Success.map(Try.scala:237) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.pollAndExecAll(ForkJoinPool.java:1253) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1346) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 2016-06-01 16:10:15,093 cromwell-system-akka.actor.default-dispatcher-15 ERROR - WorkflowActor [UUID(88b21d2d)]: Call failed to initialize: failed to create call actor for PairedEndSingleSampleWorkflow.$final_call$copy_workflow_log: None.get; 2016-06-01 16:10:15,093 cromwell-system-akka.actor.default-dispatcher-15 INFO - WorkflowActor [UUID(88b21d2d)]: persisting status of PairedEndSingleSampleWorkflow",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/927:3685,Adapt,AdaptedForkJoinTask,3685,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/927,1,['Adapt'],['AdaptedForkJoinTask']
Energy Efficiency,a.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71). 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:15474,allocate,allocateResultLob,15474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['allocate'],['allocateResultLob']
Energy Efficiency,a.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:6325,allocate,allocateResultLob,6325,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['allocate'],['allocateResultLob']
Energy Efficiency,a18-4241-8f6b-0b72e181f59a failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:341); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:341); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:341); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:99); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:269); cats.effect.IO.unsafeToFuture(IO.scala:341); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:152); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatc,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4051:2036,adapt,adapted,2036,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4051,1,['adapt'],['adapted']
Energy Efficiency,a:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	a,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:5711,allocate,allocateLobForResult,5711,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,2,['allocate'],['allocateLobForResult']
Energy Efficiency,adding coursier plugin to reduce resolution times,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3015:26,reduce,reduce,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3015,1,['reduce'],['reduce']
Energy Efficiency,"ader.SdkHttpServiceProviderChain.loadService(SdkHttpServiceProviderChain.java:44); 	at software.amazon.awssdk.core.internal.http.loader.CachingSdkHttpServiceProvider.loadService(CachingSdkHttpServiceProvider.java:46); 	at software.amazon.awssdk.core.internal.http.loader.DefaultSdkHttpClientBuilder.buildWithDefaults(DefaultSdkHttpClientBuilder.java:40); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.lambda$resolveSyncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 more. 2019-07-02 19:16:37,991 cromwell-system-akka.dispatchers.engine-dispatcher-31 INFO - WorkflowManagerActor WorkflowActor-10f172e8-b7ba-416f-964e-22ab8c7b38e3 is in a terminal state: WorkflowFailedState",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:16256,adapt,adapted,16256,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['adapt'],['adapted']
Energy Efficiency,"all-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:3:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> CAST/EiJ;genotype -> Wild-type;treatment -> Clean-air;tissue -> kidney), WomString(series) -> WomString(GSE108990), WomString(organism) -> WomString(Mus musculus), WomString(run) -> WomString(SRR6456754), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra57/SRR/006305/SRR6456754), WomString(name) -> WomString(GSM2927750), WomString(gsm) -> WomString(GSM2927750), WomString(title) -> WomString(RNA_105_kidney_Control))), WomString(run) -> WomString(SRR6456754), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae14",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555:7418,monitor,monitoring,7418,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555,1,['monitor'],['monitoring']
Energy Efficiency,aphPrint.handleConditional(GraphPrint.scala:87); 	at wom.views.GraphPrint.$anonfun$listAllGraphNodes$2(GraphPrint.scala:36); 	at scala.collection.Iterator$$anon$10.next(Iterator.scala:459); 	at scala.collection.Iterator.foreach(Iterator.scala:941); 	at scala.collection.Iterator.foreach$(Iterator.scala:941); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1429); 	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:160); 	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:158); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429); 	at cats.kernel.Monoid.combineAll(Monoid.scala:82); 	at cats.kernel.Monoid.combineAll$(Monoid.scala:81); 	at cats.derived.MkMonoidDerivation$$anon$1.combineAll(monoid.scala:45); 	at cats.instances.ListInstances$$anon$1.foldMap(list.scala:70); 	at cats.instances.ListInstances$$anon$1.foldMap(list.scala:12); 	at cats.Foldable$Ops.foldMap(Foldable.scala:31); 	at cats.Foldable$Ops.foldMap$(Foldable.scala:31); 	at cats.Foldable$ToFoldableOps$$anon$5.foldMap(Foldable.scala:31); 	at wom.views.GraphPrint.listAllGraphNodes(GraphPrint.scala:32); 	at wom.views.GraphPrint.dotString(GraphPrint.scala:16); 	at womtool.WomtoolMain$.graph(WomtoolMain.scala:132); 	at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:55); 	at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:157); 	at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:162); 	at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:25); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1$adapted(App.scala:80); 	at scala.collection.immutable.List.foreach(List.scala:392); 	at scala.App.main(App.scala:80); 	at scala.App.main$(App.scala:78); 	at womtool.WomtoolMain$.main(WomtoolMain.scala:25); 	at womtool.WomtoolMain.main(WomtoolMain.scala),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6744:6760,adapt,adapted,6760,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6744,1,['adapt'],['adapted']
Energy Efficiency,"as possible._**; No. The job is killed because of memory limitations _e.g_ `/bin/bash: line 1: 172 Killed`. **_Which backend are you running?_**; I'm running `Cromwell` on a local machine, I should have enough memory to run the process but somehow to amount visible/accessible by `Cromwell` is limited. **_Paste/Attach your workflow if possible:_**; ```; version 1.0. workflow step2 {; input {; String PANGENIE_CONTAINER = ""overcraft90/eblerjana_pangenie:2.1.2""; ; File FORWARD_FASTQ # compressed R1; File REVERSE_FASTQ # compressed R2; String NAME = ""sample"" # how to loop over samples' name in numerical order (maybe grub names' prefix)!?. File PANGENOME_VCF # input vcf with variants to be genotyped; File REF_GENOME # reference for variant calling; String VCF_PREFIX = ""genotype"" # string to attach to a sample's genotype; String EXE_PATH = ""/app/pangenie/build/src/PanGenie"" # path to PanGenie executable in Docker. Int CORES = 24 # number of cores to allocate for PanGenie execution; Int DISK = 300 # storage memory for output files; Int MEM = 100 # RAM memory allocated; }. call reads_extraction_and_merging {; input:; in_container_pangenie=PANGENIE_CONTAINER,; in_forward_fastq=FORWARD_FASTQ,; in_reverse_fastq=REVERSE_FASTQ,; in_label=NAME, #later can be plural; in_cores=CORES,; in_disk=DISK,; in_mem=MEM; }. call genome_inference {; input:; in_container_pangenie=PANGENIE_CONTAINER, # not sure whether Docker needs to be re-run; in_pangenome_vcf=PANGENOME_VCF,; in_reference_genome=REF_GENOME,; in_executable=EXE_PATH,; in_fastq_file=reads_extraction_and_merging.fastq_file, # how to feed a task output to another one!!!; prefix_vcf=VCF_PREFIX,; in_cores=CORES,; in_disk=DISK,; in_mem=MEM; }. output {; File sample = reads_extraction_and_merging.fastq_file; File genotype = genome_inference.vcf_file; }; }. task reads_extraction_and_merging {; input {; String in_container_pangenie; File in_forward_fastq; File in_reverse_fastq; String in_label; Int in_cores; Int in_disk; Int in_mem; }; c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6966:1408,allocate,allocate,1408,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6966,2,['allocate'],"['allocate', 'allocated']"
Energy Efficiency,assLoader.java:763); 	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142); 	at java.net.URLClassLoader.defineClass(URLClassLoader.java:467); 	at java.net.URLClassLoader.access$100(URLClassLoader.java:73); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:368); 	at java.net.URLClassLoader$1.run(URLClassLoader.java:362); 	at java.security.AccessController.doPrivileged(Native Method); 	at java.net.URLClassLoader.findClass(URLClassLoader.java:361); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:424); 	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349); 	at java.lang.ClassLoader.loadClass(ClassLoader.java:357); 	at com.zaxxer.hikari.HikariConfig.attemptFromContextLoader(HikariConfig.java:970); 	at com.zaxxer.hikari.HikariConfig.setDriverClassName(HikariConfig.java:480); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.$anonfun$forConfig$3(HikariCPJdbcDataSource.scala:33); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.$anonfun$forConfig$3$adapted(HikariCPJdbcDataSource.scala:33); 	at scala.Option.foreach(Option.scala:437); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:33); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource$.forConfig(HikariCPJdbcDataSource.scala:21); 	at slick.jdbc.JdbcDataSource$.forConfig(JdbcDataSource.scala:47); 	at slick.jdbc.JdbcBackend$DatabaseFactoryDef.forConfig(JdbcBackend.scala:341); 	at slick.jdbc.JdbcBackend$DatabaseFactoryDef.forConfig$(JdbcBackend.scala:337); 	at slick.jdbc.JdbcBackend$$anon$1.forConfig(JdbcBackend.scala:32); 	at slick.jdbc.JdbcBackend.createDatabase(JdbcBackend.scala:35); 	at slick.jdbc.JdbcBackend.createDatabase$(JdbcBackend.scala:35); 	at slick.jdbc.JdbcBackend$.createDatabase(JdbcBackend.scala:574); 	at slick.jdbc.JdbcBackend$.createDatabase(JdbcBackend.scala:574); 	at slick.basic.DatabaseConfig$$anon$1.db$lzycompute(DatabaseConfig.scala:103); 	at slick.basic.DatabaseConfig$$anon$1.db(DatabaseConfig.scala:102); 	at cromwell.database.sli,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6830:1475,adapt,adapted,1475,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6830,1,['adapt'],['adapted']
Energy Efficiency,"at could use optimization, without the need for any configuration or code (or any changes to the workflow). It's also much easier than the current state-of-the-art, i.e. parsing task-level monitoring logs. 2) Scripts can easily get aggregate statistics on resource utilization and could produce suggestions based on those. This could provide a path towards automatic runtime configuration based on the models trained with historical data. One could also detect situations like out-of-memory calls and automatically adjust resources according to those. It would also be pretty easy to add logic for estimation of task call-level cost based on the pricing of associated resources. This could provide a long-sought feature of real-time cost monitoring/control (thanks to @TimothyTickle for the suggestion). Monitoring is done using the new ""monitoring action"" for PAPIv2, which currently uses the hard-coded [quay.io/broadinstitute/cromwell-monitor](https://quay.io/repository/broadinstitute/cromwell-monitor) image, built from https://github.com/broadinstitute/cromwell-monitor (I wasn't sure if that code belonged here or in a separate repo). This is advantageous to just using it as a _monitoring_script_, because it removes all assumptions on the ""user"" Docker image (for the task itself). For example, we don't have to assume a particular distribution or presence of Python and its libraries. So it should work exactly the same for any task. Per @geoffjentry's suggestion, we've [consulted](https://groups.google.com/forum/#!topic/google-genomics-discuss/caYM7oHbfx0) with the Google Genomics team, and they don't see any apparent issues with the concept. We could expose this as a workflow option like `monitoring_image`, and allow configuring it at the Cromwell level, so e.g. any user of Terra (or any other hosted Cromwell with PAPIv2 backend) could get usage reports without having to configure anything. The metrics are reported in their GCP project, so a user gets automatic access to them a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510:1544,monitor,monitor,1544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510,1,['monitor'],['monitor']
Energy Efficiency,"ave two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032:731,adapt,adapters,731,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032,3,['adapt'],"['adapter', 'adapters']"
Energy Efficiency,bb6-4d9b-89ea-7da193994220 failed (during MaterializingWorkflowDescriptorState): cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anon$1: Workflow input processing failed:; Boxed Error; scala.concurrent.impl.Promise$.resolver(Promise.scala:83); scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolveTry(Promise.scala:75); scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:280); scala.concurrent.Promise.complete(Promise.scala:49); scala.concurrent.Promise.complete$(Promise.scala:48); scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:183); scala.concurrent.Promise.failure(Promise.scala:100); scala.concurrent.Promise.failure$(Promise.scala:100); scala.concurrent.impl.Promise$DefaultPromise.failure(Promise.scala:183); cats.effect.IO.$anonfun$unsafeToFuture$2(IO.scala:328); scala.util.Either.fold(Either.scala:189); cats.effect.IO.$anonfun$unsafeToFuture$1(IO.scala:328); cats.effect.IO.$anonfun$unsafeToFuture$1$adapted(IO.scala:328); cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:98); cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); cats.effect.IO.unsafeRunAsync(IO.scala:257); cats.effect.IO.unsafeToFuture(IO.scala:328); cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:146); scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:303); scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37); scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); akka.dispatch.BatchingExecutor$BlockableBatc,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457:99771,adapt,adapted,99771,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3736#issuecomment-395480457,1,['adapt'],['adapted']
Energy Efficiency,"by task basis. It would be very helpful if this could be generalized, so that I could tell Cromwell to retry all tasks that fail -- for any reason, not just preemption. I imagine this being configured via a workflow option like ""failed_task_retries: 3"", which would tell Cromwell to run each task in the workflow up to 3 times if any type of failure is encountered. # Why it would be valuable. For people running many instances of a well-tested workflow, such as Green Team and Mint Team production at Broad, the vast majority of failures are due to transient problems in the cloud, and it is very time consuming to deal with them. Having this auto-retry capability in Cromwell would be a huge help in making these workflows more robust and would greatly reduce the amount of manual work required to relaunch failed workflows (or save people from having to write their own bespoke scripts to auto-retry failed workflows). Having retries at the task level (rather than having to resubmit the whole workflow) would also be more efficient, especially when call caching is not in use. # Difference from existing issue. I believe this feature would satisfy the use cases of many (but not all) of the commenters on #1991, but in a simpler way. In contrast to that issue, no error messages need to be parsed here and there is no added functionality around auto increasing memory or disk. (For Mint Team produciton, we're interested in something like #1991, too, especially the stderr pattern matching, but I am guessing it would take longer to make happen given the wdl changes required, etc. The issue I'm filing here is the low hanging fruit for us.). # Combining with preemptibles. There is a question to resolve about what to do for a preemptible task in a workflow where failed_task_retries has also been set. My preference would be to make them additive. If the task says ""preemptible: 5"" and the workflow says ""failed_task_retries: 3"", then Cromwell will retry that task up to 8 times. The first 3 ret",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3161:1169,efficient,efficient,1169,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3161,1,['efficient'],['efficient']
Energy Efficiency,"call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-3/ScatterAt40_16/fae142d3-7b38-418e-82cb-a1a437458c72/call-salmon/shard-0/execution/quant_SRR6456754/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:2:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> CAST/EiJ;genotype -> Wild-type;treatment -> Clean-air;tissue -> liver), WomString(series) -> WomString(GSE108990), WomString(organism) -> WomString(Mus musculus), WomString(run) -> WomString(SRR6456687), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra57/SRR/006305/SRR6456687), WomString(name) -> WomString(GSM2927683), WomString(gsm) -> WomString(GSM2927683), WomString(title) -> WomString(RNA_105_liver_Control))), WomString(run) -> WomString(SRR6456687), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-2/ScatterAt40_16/dc76f95e-2040-4e38-a0d7-0b82c48bbca6/call-salmon/shard-0/execution/quant_SRR6456687), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-2/ScatterAt40_16/dc76f95",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555:9132,monitor,monitoring,9132,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555,1,['monitor'],['monitoring']
Energy Efficiency,"cess by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to method java.nio.Bits.unaligned(); WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.selectedKeys; WARNING: Illegal reflective access by org.python.netty.util.internal.ReflectionUtil (file:/home/jeremiah/code/fresh/really/cromwell/server/target/scala-2.12/cromwell-40-aa86539-SNAP.jar) to field sun.nio.ch.SelectorImpl.publicSelectedKeys; [2019-04-18 17:19:50,24] [info] Pre Processing Inputs...; Exception in thread ""MainThread"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Cannot find a tool or workflow with ID 'None' in file file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl's set: [file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl#main, file:///home/jeremiah/fail_cromwell/test_wf_pack.cwl#touch.cwl]; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:255); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:255); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:251); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:62); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416:4311,adapt,adapted,4311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-484714416,1,['adapt'],['adapted']
Energy Efficiency,ckends.<init>(CromwellBackends.scala:14); 	at cromwell.engine.backend.CromwellBackends$.initBackends(CromwellBackends.scala:42); 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:62); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:96); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:96); 	at cromwell.CromwellEntryPoint$.runServer(CromwellEntryPoint.scala:50); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:15); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); 	Suppressed: java.lang.reflect.InvocationTargetException: null; 		at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method); 		at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62); 		at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45); 		at java.lang.reflect.Constructor.newInstance(Constructor.java:423); 		at cromwell.engine.backend.BackendConfigurationEntry.$anonfun$asBackendLifecycleActorFactory$1(BackendConfiguration.scala:13); 		at scala.util.Try$.apply(Try.scala:209); 		at cromwell.engine.backend.BackendConfigurationEntry.asBackendLifecycleActorFactory(BackendConfiguration.scala:14); 		at cromwell.engine.backend.Cromwel,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4553:1745,adapt,adapted,1745,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4553,1,['adapt'],['adapted']
Energy Efficiency,"concurrent.Await$.ready(package.scala:169); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala:29); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:433); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala); at scala.concurrent.impl.CallbackRunnable.run(Redefined); at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(Redefined); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java). ""cromwell-system-scheduler-1"" #14 prio=5 os_prio=31 tid=0x00007fb76aa14800 nid=0x6103 runnable [0x00000001295b3000]; java.lang.Thread.State: RUNNABLE; at com.jprofiler.agent.InstrumentationCallee.exitFilteredMethod(Native Method); at com.jprofiler.agent.InstrumentationCallee.__ejt_filter_exitMethod(ejt:86); at akka.actor.LightArrayRevolverScheduler.clock(Scheduler.scala:213); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Redefined); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Redefined); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.actor.default-dispatcher-4"" #13 prio=5 os_prio=31 tid=0x00007fb76b38c000 nid=0x5f03 waiting on condition [0x000000012ac3d000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c002f9e0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool); at scala.concurrent.forkjoin.ForkJoinPool.scan(Redefined); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(Redefined); at scala.concurrent.forkjoin.ForkJoinWorkerThr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:46610,schedul,scheduler-,46610,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['schedul'],['scheduler-']
Energy Efficiency,cromwell is limiting job scheduling,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5218:25,schedul,scheduling,25,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5218,1,['schedul'],['scheduling']
Energy Efficiency,"cs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.9.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Pair/cnv_somatic_pair_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/CNV-Panel/cnv_somatic_panel_workflow_do_gc_wes.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.aws.inputs; centaur/src/main/resources/integrationTestCases/Somatic/Mutect2/Mutect2.inputs; centaur/src/main/resources/integrationTestCases/germline/haplotype-caller-workflow/HaplotypeCallerWF.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-production-workflow/PairedEndSingleSampleWf.options.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; centaur/src/main/resources/integrationTestCases/green/arrays/arrays.wdl; womtool/src/test/resources/validate/wdl_draft3/valid/HaplotypeCallerWF/HaplotypeCallerWF.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/cnv_somatic_pair_workflow/cnv_somatic_pair_workflow.inputs.json; womtool/src/test/resources/validate/wdl_draft3/valid/joint-discovery-gatk/joint-discovery-gatk.inputs.json; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.github.jbwheatley"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.github.jbwheatley"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7294:1922,green,green,1922,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7294,1,['green'],['green']
Energy Efficiency,"ctorSpec.localizationSpec(SharedFileSystemJobExecutionActorSpec.scala:119); 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.$anonfun$new$4(SharedFileSystemJobExecutionActorSpec.scala:156); 14:08:29 cromwell-test_1 | [info] at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); 14:08:29 cromwell-test_1 | [info] at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); 14:08:29 cromwell-test_1 | [info] at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); 14:08:29 cromwell-test_1 | [info] at org.scalatest.Transformer.apply(Transformer.scala:22); 14:08:29 cromwell-test_1 | [info] at org.scalatest.Transformer.apply(Transformer.scala:20); 14:08:29 cromwell-test_1 | [info] ...; 14:08:29 cromwell-test_1 | [info] Cause: org.scalatest.concurrent.Futures$FutureConcept$$anon$1: A timeout occurred waiting for a future to complete. Queried 21 times, sleeping 500 milliseconds between each query.; 14:08:29 cromwell-test_1 | [info] ...; 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.$anonfun$localizationSpec$1(SharedFileSystemJobExecutionActorSpec.scala:137); 14:08:29 cromwell-test_1 | [info] at cromwell.backend.sfs.SharedFileSystemJobExecutionActorSpec.$anonfun$localizationSpec$1$adapted(SharedFileSystemJobExecutionActorSpec.scala:119); 14:08:30 cromwell-test_1 | [info] at org.scalatest.enablers.UnitTableAsserting$TableAssertingImpl.$anonfun$forAll$7(TableAsserting.scala:505); 14:08:30 cromwell-test_1 | [info] at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:789); 14:08:30 cromwell-test_1 | [info] at scala.collection.immutable.List.foreach(List.scala:389); 14:08:30 cromwell-test_1 | [info] at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 14:08:30 cromwell-test_1 | [info] at org.scalatest.enablers.UnitTableAsserting$TableAssertingImpl.forAll(TableAsserting.scala:503); 14:08:30 cromwell-test_1 | [info] ...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4319:2665,adapt,adapted,2665,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4319,1,['adapt'],['adapted']
Energy Efficiency,"d outputs forever. However we have a large volume of intermediate files which end up in our cromwell-executions bucket. We love caching. It works great. A fully cached workflow runs in about 5 minutes at next to no cost. Fresh workflows (no cache hits) cost on the order of $0.50 for typical examples, and run for a few hours. Object storage has been eating us up, though. We've worked out that for a single one of these workflows the break even point at which it's cheaper to rerun it than to save it and cache it is about a week. If you take into account that we re-run workflows only a small part of the time, it probably doesn't even pay to keep the execution folders at all (except in the intangible wall clock time). [And nearline / coldline makes no sense at all. Each cached file is accessed multiple times which makes cached runs way way more expensive than fresh runs.]. Weâ€™ve examined the pipeline, and we see that we could reduce the size of intermediate outputs, from 126G to 40G by combining separate tasks, which obviates the need to make the large file an output of the first task and input to the second. This leads me to a question for the deep thinkers in Cromwell caching. I want to ask if something makes sense in theory, for the purpose of making caching more feasible for us. Suppose I took the two tasks I spoke of, one of which â€œpassesâ€ a large file to the second, and made them into a sub-workflow. And I mark the large files as â€œtoo big to keepâ€ so they Cromwell would strip them out of the execution folder after the run completed. If caching were to work by looking at the inputs and outputs of the sub-workflow, and not at each task one by one, then it would be possible to cache the entire sub-workflow. Right?. Letâ€™s say this sounds theoretically possible. Wouldnâ€™t it be possible then, to skip making an actual sub-workflow at all to bracket the trashed intermediates, but just have clever Cromwell analyse the execution tree and the presence of â€œtoo big to keepâ€ vaca",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4064:2349,reduce,reduce,2349,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4064,1,['reduce'],['reduce']
Energy Efficiency,"dArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fd",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5291,Schedul,ScheduledThreadPoolExecutor,5291,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Schedul'],['ScheduledThreadPoolExecutor']
Energy Efficiency,"dException: Migration failed for change set changesets/replace_empty_custom_labels.xml::replace_empty_custom_labels::rmunshi:; Reason: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '' in 'where clause' [Failed SQL: UPDATE WORKFLOW_STORE_ENTRY; SET CUSTOM_LABELS = ""{}""; WHERE CUSTOM_LABELS = """"]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55);",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606:2156,adapt,adapted,2156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606,1,['adapt'],['adapted']
Energy Efficiency,"e directory is correctly delocalized:; ```; $ gsutil ls -l gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/d; 0 2022-02-13T00:00:00Z gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/d/x; 0 2022-02-13T00:00:00Z gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/d/y; TOTAL: 2 objects, 0 bytes (0 B); ```. The delocalization script is aware that `d` is directory:; ```; $ gsutil cat gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/gcs_delocalization.sh; source '/cromwell_root/gcs_transfer.sh'. timestamped_message 'Delocalization script execution started...'. # xxx; delocalize_6c578056c74a8d9a80724855ddac131c=(; ""mccarroll-mocha"" # project; ""3"" # max attempts; ""150M"" # parallel composite upload threshold, will not be used for directory types; ""file""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/memory_retry_rc""; ""/cromwell_root/memory_retry_rc""; ""optional""; ""text/plain; charset=UTF-8""; ""file""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/rc""; ""/cromwell_root/rc""; ""required""; ""text/plain; charset=UTF-8""; ""file""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/monitoring.log""; ""/cromwell_root/monitoring.log""; ""required""; ""text/plain; charset=UTF-8""; ""file""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/stdout""; ""/cromwell_root/stdout""; ""required""; ""text/plain; charset=UTF-8""; ""file""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/stderr""; ""/cromwell_root/stderr""; ""required""; ""text/plain; charset=UTF-8""; ""directory""; ""gs://xxx/cromwell-executions/main/01234567-89ab-cdef-0123-456789abcdef/call-main/d""; ""/cromwell_root/d""; ""required""; """"; ). delocalize ""${delocalize_6c578056c74a8d9a80724855ddac131c[@]}""; ; timestamped_message 'Delocalization script execution complete.'; ```. Bu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6677:2167,monitor,monitoring,2167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6677,1,['monitor'],['monitoring']
Energy Efficiency,"e docker container. It seems like only the first argument is actually being used. This isn't an issue with my python script, because I can run it directly and everything works fine. Cromwell showing the command line:; ```; cromwell_1 | 2018-11-12 06:57:56,451 cromwell-system-akka.dispatchers.backend-dispatcher-40 INFO - PipelinesApiAsyncBackendJobExecutionActor [UUID(5d4c4459)germline_variant_calling.fastqc:0:1]: `/app/fastqc_docker.py --output-dir . --read ""/cromwell_root/genovic-test-data/cardiom/NA12878_CARDIACM_MUTATED_L001_R1.fastq.gz"" --format fastq`; ```. Cromwell failing with an error because the `--read` argument is missing (even though you can see it's not, in the above log):; ```; cromwell_1 | java.lang.Exception: Task germline_variant_calling.fastqc:0:1 failed. The job was stopped before the command finished. PAPI error code 10. 11: Docker run failed: command failed: usage: fastqc_docker.py [-h] -r READ -o OUTPUT_DIR [-c CONTAMINANTS]; cromwell_1 | [-a ADAPTERS] [-l LIMITS] [-f FORMAT] [-n NO_GROUP]; cromwell_1 | [-e EXTRA_OPTIONS]; cromwell_1 | fastqc_docker.py: error: argument -r/--read is required; cromwell_1 | . See logs at gs://genovic-cromwell/cromwell-execution/trio/f5454139-c51d-4d04-ae0a-9b9d4ce650aa/call-germline_variant_calling/shard-0/germline_variant_calling/5d4c4459-a91c-4d3b-8ca4-b98457134750/call-fastqc/shard-0/; cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor$.StandardException(PipelinesApiAsyncBackendJobExecutionActor.scala:79); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleFailedRunStatus$1(PipelinesApiAsyncBackendJobExecutionActor.scala:585); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecutionFailure(PipelinesApiAsyncBackendJobExecutionActor.scala:592); cromwell_1 | at cromwell.backend.google.pipelines.common.PipelinesApiAsyncBackendJobExecutionActor.handleExecution",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4381:1268,ADAPT,ADAPTERS,1268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4381,1,['ADAPT'],['ADAPTERS']
Energy Efficiency,"e#diff-Y3dsL3NyYy9tYWluL3NjYWxhL2N3bC9FeHByZXNzaW9uSW50ZXJwb2xhdG9yLnNjYWxh) | `86.2% <0%> (+1.14%)` | :arrow_up: |; | [...ll/engine/workflow/WorkflowDockerLookupActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS93b3JrZmxvdy9Xb3JrZmxvd0RvY2tlckxvb2t1cEFjdG9yLnNjYWxh) | `95.34% <0%> (+1.16%)` | :arrow_up: |; | [...cle/execution/callcaching/CallCacheDiffActor.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-ZW5naW5lL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2VuZ2luZS93b3JrZmxvdy9saWZlY3ljbGUvZXhlY3V0aW9uL2NhbGxjYWNoaW5nL0NhbGxDYWNoZURpZmZBY3Rvci5zY2FsYQ==) | `96.38% <0%> (+1.2%)` | :arrow_up: |; | [...scala/cromwell/languages/util/ImportResolver.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-bGFuZ3VhZ2VGYWN0b3JpZXMvbGFuZ3VhZ2UtZmFjdG9yeS1jb3JlL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2xhbmd1YWdlcy91dGlsL0ltcG9ydFJlc29sdmVyLnNjYWxh) | `98.7% <0%> (+1.29%)` | :arrow_up: |; | [...src/main/scala/wdl/draft2/model/WdlNamespace.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL1dkbE5hbWVzcGFjZS5zY2FsYQ==) | `92.37% <0%> (+1.34%)` | :arrow_up: |; | ... and [402 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5076/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=footer). Last update [8055dad...803ebd8](https://codecov.io/gh/broadinstitute/cromwell/pull/5076?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251:4436,Power,Powered,4436,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5076#issuecomment-516064251,1,['Power'],['Powered']
Energy Efficiency,"e;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:10,85] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2018-08-30 17:36:10,87] [info] [RenameWorkflowOptionsInMetadata] 100%; [2018-08-30 17:36:11,02] [info] Running with database db.url = jdbc:hsqldb:mem:5893545c-e081-4c3d-827d-000af3765fc4;shutdown=false;hsqldb.tx=mvcc; [2018-08-30 17:36:11,72] [info] Slf4jLogger started; Exception in thread ""main"" cromwell.CromwellEntryPoint$$anon$1: ERROR: Unable to submit workflow to Cromwell::; Workflow source does not exist: does-not-exist.wdl; 	at cromwell.CromwellEntryPoint$.$anonfun$validOrFailSubmission$1(CromwellEntryPoint.scala:219); 	at cats.data.Validated.valueOr(Validated.scala:48); 	at cromwell.CromwellEntryPoint$.validOrFailSubmission(CromwellEntryPoint.scala:219); 	at cromwell.CromwellEntryPoint$.validateRunArguments(CromwellEntryPoint.scala:215); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:56); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ```; Command-line tools are subject to usability standards identical to those of our other user interfaces. Unless the intended audience of this tool is Cromwell engineers, the stacktrace information is likely overwhelmingâ€”that is, the signal-to-noise ratio of this output can likely be improved.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4060:1657,adapt,adapted,1657,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4060,1,['adapt'],['adapted']
Energy Efficiency,eaccount.com does not have serviceusage.services.use access to the Google Cloud project.; 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); 	at com.google.cloud.storage.spi.v1.HttpStorageRpc.create(HttpStorageRpc.java:308); 	at com.google.cloud.storage.StorageImpl$3.call(StorageImpl.java:213); 	at com.google.cloud.storage.StorageImpl$3.call(StorageImpl.java:210); 	at com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); 	at com.google.cloud.RetryHelper.run(RetryHelper.java:76); 	at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); 	at com.google.cloud.storage.StorageImpl.internalCreate(StorageImpl.java:209); 	at com.google.cloud.storage.StorageImpl.create(StorageImpl.java:171); 	at cromwell.filesystems.gcs.GcsPath.request$1(GcsPathBuilder.scala:196); 	at cromwell.filesystems.gcs.GcsPath.$anonfun$writeContent$2(GcsPathBuilder.scala:203); 	at cromwell.filesystems.gcs.GcsPath.$anonfun$writeContent$2$adapted(GcsPathBuilder.scala:203); 	at cromwell.filesystems.gcs.GcsEnhancedRequest$.$anonfun$recoverFromProjectNotProvided$3(GcsEnhancedRequest.scala:18); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:87); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:355); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:376); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:316); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorker,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594:2090,adapt,adapted,2090,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594,2,['adapt'],['adapted']
Energy Efficiency,"ediaHttpUploader.java:562) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.media.MediaHttpUploader.resumableUpload(MediaHttpUploader.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.media.MediaHttpUploader.upload(MediaHttpUploader.java:336) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:427) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357) ~[cromwell.jar:0.19]; at java.util.concurrent.FutureTask.run(FutureTask.java:266) [na:1.8.0_72]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; 2016-06-01T20:19:40.362-0400: 104296.863: [GC (Allocation Failure) [PSYoungGen: 1130870K->235812K(1864192K)] 3755109K->2861554K(7456768K), 0.0464226 secs] [Times: user=0.25 sys=0.00, real=0.04 secs] ; 2016-06-01T20:19:42.554-0400: 104299.055: [GC (Allocation Failure) [PSYoungGen: 1052454K->177588K(1864192K)] 3678197K->2815074K(7456768K), 0.0805924 secs] [Times: user=0.59 sys=0.00, real=0.08 secs] ; 2016-06-01T20:20:06.449-0400: 104322.950: [GC (Allocation Failure) [PSYoungGen: 1109940K->381765K(1864192K)] 3747426K->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/932:5059,Adapt,AdaptedForkJoinTask,5059,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/932,1,['Adapt'],['AdaptedForkJoinTask']
Energy Efficiency,"ely polls the filesystem, looking for the `rc` file within the execution directory (potentially `stdout` too if its looking for the job id). This is also logically verified by looking at the `script` file that Cromwell generates, the way it collects the return code and places it in the expected directory. - Based on my initial incorrect understanding, I then believed if I couldn't get the containerId, then how would Cromwell know that the job has actually finished. ### My additional errors. These tended to redirect me away from the actual misunderstanding. . 1. In the udocker config I [posted before](https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364) (corrected with comment), I was accidentally providing the `--entry-point` param, which was causing an interactive shell to open, stopping the rest of the script from executing. 2. I failed to notice that `${out}` and `${err}` change between `submit` and `submit-docker`. When I would check the job that Cromwell schedules through SLURM, it would always fail. But I'm fairly sure that the job was failing to start because it was trying to write stdout to `/cromwell-executions/.../execution/stdout`, this is what led me to #1499. 3. An easy fix, but if your backend doesn't export a job-id, you need to set `run-in-background = true` in that backend's config. ### The results. The following execution strings can be inserted into the two container configs:; - Singularity: `singularity exec --bind ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}`; - udocker: `udocker run ${""--user "" + docker_user} --rm -v ${cwd}:${docker_cwd} ${docker} ${script}`. My _container_ config template for no workflow manager:; ```HOCON; include required(classpath(""application"")). # uncomment if using udocker; # docker.hash-lookup.enabled = false. backend {; default: singularity; providers: {; singularity {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycle",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840:1948,schedul,schedules,1948,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-461275840,1,['schedul'],['schedules']
Energy Efficiency,"emory_mb_per_core = 8000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ; }; }; }; ```. When I ran `java -Dconfig.file=$(realpath spartan.conf) -jar cromwell-36.jar`, the error it printed was:. ```; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); Caused by: com.typesafe.config.ConfigException$Parse: /data/cephfs/punim0751/spartan.conf: 27: expecting a close parentheses ')' here, not: end of file; 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:201); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseError(ConfigDocumentParser.java:197); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseKey(ConfigDocumentParser.java:279); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseObject(ConfigDocumentParser.java:450); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseContext.parseValue(ConfigDocumentParser.java:247); 	at com.typesafe.config.impl.ConfigDocumentParser$ParseConte",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549:1305,adapt,adapted,1305,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549,1,['adapt'],['adapted']
Energy Efficiency,ent.netty.request.NettyRequestSender.resolveAddresses(NettyRequestSender.java:355); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithNewChannel(NettyRequestSender.java:298); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequestWithCertainForceConnect(NettyRequestSender.java:140); at org.asynchttpclient.netty.request.NettyRequestSender.sendRequest(NettyRequestSender.java:111); at org.asynchttpclient.DefaultAsyncHttpClient.execute(DefaultAsyncHttpClient.java:240); at org.asynchttpclient.DefaultAsyncHttpClient.executeRequest(DefaultAsyncHttpClient.java:209); at org.asynchttpclient.BoundRequestBuilder.execute(BoundRequestBuilder.java:35); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1(AsyncHttpClientBackend.scala:53); at com.softwaremill.sttp.asynchttpclient.AsyncHttpClientBackend.$anonfun$send$1$adapted(AsyncHttpClientBackend.scala:42); at cats.effect.IO$.$anonfun$async$1(IO.scala:1042); at cats.effect.IO$.$anonfun$async$1$adapted(IO.scala:1040); at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:329); at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:118); at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:35); at cats.effect.IO.unsafeRunAsync(IO.scala:269); at cats.effect.IO.unsafeToFuture(IO.scala:341); at cromwell.languages.util.ImportResolver$.$anonfun$httpResolverWithHeaders$1(ImportResolver.scala:92); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$2(package.scala:25); at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:122); at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:118); at scala.collection.immutable.List.foldLeft(List.scala:86); at common.transforms.package$CheckedAtoB$.$anonfun$firstSuccess$1(package.scala:22); at cats.data.Kleisli.$anonfun$andThen$1(Kleisli.scala:37); at wdl.draft3.transforms.wdlom2wom.FileElementToWomBundle$.wdl$draft3$transforms$wdlom2,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3977:2374,adapt,adapted,2374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977,1,['adapt'],['adapted']
Energy Efficiency,"erAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:4:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> indigenous;location -> Chengdu, Sichuan province, China;tissue -> liver;age -> ~4 years old), WomString(series) -> WomString(GSE77020), WomString(organism) -> WomString(Bos taurus), WomString(run) -> WomString(SRR3109705), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra37/SRR/003036/SRR3109705), WomString(name) -> WomString(GSM2042593), WomString(gsm) -> WomString(GSM2042593), WomString(title) -> WomString(cattle_liver_1))), WomString(run) -> WomString(SRR3109705), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555:3968,monitor,monitoring,3968,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555,1,['monitor'],['monitoring']
Energy Efficiency,"erAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-4/ScatterAt40_16/c24f9f18-1429-473b-a2a6-bd92e5975d30/call-salmon/shard-0/execution/quant_SRR3109705/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:5:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2500), WomString(characteristics) -> WomString(strain -> indigenous;location -> Chengdu, Sichuan province, China;tissue -> kidney;age -> ~4 years old), WomString(series) -> WomString(GSE77020), WomString(organism) -> WomString(Bos taurus), WomString(run) -> WomString(SRR3109708), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra38/SRR/003036/SRR3109708), WomString(name) -> WomString(GSM2042596), WomString(gsm) -> WomString(GSM2042596), WomString(title) -> WomString(cattle_kidney_1))), WomString(run) -> WomString(SRR3109708), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897d0635-6fdf-4b22-b98f-36d49683ce08/call-salmon/shard-0/execution/quant_SRR3109708), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-5/ScatterAt40_16/897",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555:5692,monitor,monitoring,5692,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555,1,['monitor'],['monitoring']
Energy Efficiency,"ers encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,180] [info] Message [akka.actor.Status$Failure] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,182] [error] WorkflowManagerActor: Workflow failed submission: cannot create children while terminating or terminated; java.lang.IllegalStateException: cannot create children while terminating or terminated; at akka.actor.dungeon.Children$class.makeChild(Children.scala:199); at akka.actor.dungeon.Children$class.actorOf(Children.scala:37); at akka.actor.ActorCell.actorOf(ActorCell.scala:369); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$11.apply(WorkflowManagerActor.scala:246); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$11.apply(WorkflowManagerActor.scala:245); at scala.util.Success$$anonfun$map$1.apply(Try.scala:237); at scala.util.Try$.apply(Try.scala:192); at scala.util.Success.map(Try.scala:237); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:235); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); cromwell run hello.wdl hello.json 9.53s user 0.80s system 108% cpu 9.542 total; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/334:3872,Adapt,AdaptedForkJoinTask,3872,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/334,1,['Adapt'],['AdaptedForkJoinTask']
Energy Efficiency,"etails</summary>. What GCP provides:. ```; Event type=STATUS_CHANGED; time=seconds: 1712173852,nanos: 952604950; taskState=STATE_UNSPECIFIED,; description=Job state is set from QUEUED to SCHEDULED for job projects/392615380452/locations/us-south1/jobs/job-ba81bad8-82e9-4d95-8fc0-04dfbbd746da.; taskExecution.exitCode=0. Event type=STATUS_CHANGED,; time=seconds: 1712173947, nanos: 568998105; taskState=STATE_UNSPECIFIED; description=Job state is set from SCHEDULED to RUNNING for job projects/392615380452/locations/us-south1/jobs/job-ba81bad8-82e9-4d95-8fc0-04dfbbd746da.; taskExecution.exitCode=0. Event type=STATUS_CHANGED; time=seconds: 1712173989, nanos: 937816549; taskState=STATE_UNSPECIFIED; description=Job state is set from RUNNING to SUCCEEDED for job projects/392615380452/locations/us-south1/jobs/job-ba81bad8-82e9-4d95-8fc0-04dfbbd746da.; taskExecution.exitCode=0; ```. What we define as execution events:. ```; ExecutionEvent(Job state is set from QUEUED to SCHEDULED for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:10:01.704137839Z,None); ExecutionEvent(Job state is set from SCHEDULED to RUNNING for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:11:30.631264449Z,None); ExecutionEvent(Job state is set from RUNNING to SUCCEEDED for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:12:16.898798407Z,None); ```. </details>; </details>. ## Load test results. We have executed many load tests, this is the latest one involving 14k jobs. Data / Backend | Batch with Mysql | PAPIv2 with Mysql; ------------- | -------------|---------; Jobs | 14400 | 14400; Execution time | 20936 seconds | 24451 seconds. Overall, all our tests indicate that Batch finishes executing the jobs faster than PAPIv2. <details>; <summary>Load tests settings</summary>. We have ran Cromwell in server mode with the following setti",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412:4791,SCHEDUL,SCHEDULED,4791,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412,1,['SCHEDUL'],['SCHEDULED']
Energy Efficiency,"etect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_1dec3320bf1ad48ec05404d0a505d12b.tmp \; --adapters gs://atac-seq-pipeline-workflows/ENCSR889WQX/atac/17f0658f-a4ac-4af8-a8c6-c8910c7f303c/call-trim_adapter/shard-1/write_tsv_d3da014369f27e577cdffc1919be7d8e.tmp \; \; --auto-detect-adapter \; \; \; --nth 2; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032:2142,adapt,adapters,2142,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032,2,['adapt'],"['adapter', 'adapters']"
Energy Efficiency,"eue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""Hikari Housekeeping Timer (pool db)"" #24 daemon prio=5 os_prio=31 tid=0x00007fb76d88f000 nid=0x7503 waiting on condition [0x000000012cebb000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0623fc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.io.pinned-dispatcher-7"" #23 prio=5 os_prio=31 tid=0x00007fb76b450800 nid=0x7303 runnable [0x000000012cdb8000]; java.lang.Thread.State: RUNNABLE; at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method); at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198); at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103); at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86); - locked <0x00000006c0612e70> (a sun.nio.ch.Util$2); - locked <0x00000006c0612e80> (a java.util.Collections$UnmodifiableSet); - locked <0x00000006c0612e20> (a sun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:41942,Schedul,ScheduledThreadPoolExecutor,41942,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Schedul'],['ScheduledThreadPoolExecutor']
Energy Efficiency,"false;hsqldb.tx=mvcc; [2015-12-18 08:43:17,516] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = e67af113-c3a7-41f4-9178-6640c1c652e9; [2015-12-18 08:43:17,592] [info] WorkflowManagerActor Found no workflows to restart.; [2015-12-18 08:43:18,816] [error] SingleWorkflowRunnerActor: Ask timed out on [Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312]] after [5000 ms]; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312]] after [5000 ms]; at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:334); at akka.actor.Scheduler$$anon$7.run(Scheduler.scala:117); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:599); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:597); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467); at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375); at java.lang.Thread.run(Thread.java:745); [2015-12-18 08:43:19,174] [info] Message [cromwell.engine.workflow.WorkflowManagerActor$RestartWorkflows] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,180] [info] Message [akka.actor.Status$Failure] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/334:1743,Schedul,Scheduler,1743,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/334,1,['Schedul'],['Scheduler']
Energy Efficiency,"flow input id = None, effective id = e67af113-c3a7-41f4-9178-6640c1c652e9; [2015-12-18 08:43:17,592] [info] WorkflowManagerActor Found no workflows to restart.; [2015-12-18 08:43:18,816] [error] SingleWorkflowRunnerActor: Ask timed out on [Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312]] after [5000 ms]; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312]] after [5000 ms]; at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:334); at akka.actor.Scheduler$$anon$7.run(Scheduler.scala:117); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:599); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:597); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467); at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375); at java.lang.Thread.run(Thread.java:745); [2015-12-18 08:43:19,174] [info] Message [cromwell.engine.workflow.WorkflowManagerActor$RestartWorkflows] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,180] [info] Message [akka.actor.Status$Failure] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and '",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/334:1831,Schedul,Scheduler,1831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/334,1,['Schedul'],['Scheduler']
Energy Efficiency,"from Ruchi. > Currently we have a lot of information that gets logged by Cromwell and some of it seems like it may not be useful to our customers. For example, every actor state transition gets logged, but it's unclear who is using that information.; > ; > We tech talked today an the suggestion was to meet with some key customer's, figure out if there are aspects of our logging they don't need to help reduce, and instead just redirect the unwanted logging to debug level, so that it can still be used for debugging purposes.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/778#issuecomment-232424253:405,reduce,reduce,405,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/778#issuecomment-232424253,1,['reduce'],['reduce']
Energy Efficiency,fwiw - green team currently run their own cromwell instances - so they are not currently impacted by anything we do on WB prod for their pipelines. Plus they generally are not as aggressive at taking newer cromwell versions - so even if you disable (remove) that endpoint they would likely not see the results for quite a bit.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-395762351:7,green,green,7,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3713#issuecomment-395762351,1,['green'],['green']
Energy Efficiency,"g_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; el",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:4908,monitor,monitoring,4908,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,2,['monitor'],['monitoring']
Energy Efficiency,"gotcha on the need for an `optional` type. I see your points here. At a high level, my concern is making the response from this API easily parseable - and the large variety of key names (optionalType, arrayType, valueType, etc.) adds some complexity. I like @aednichols ' suggestion of using e.g. `innerType` where possible to reduce the set of distinct key names.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4432#issuecomment-444197460:327,reduce,reduce,327,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4432#issuecomment-444197460,1,['reduce'],['reduce']
Energy Efficiency,"hat I have. A word of warning that I found it in an email thread where a user was saying it didn't work for them, but it came from someone for whom it **did** work so YMMV. I'm going to try to try this out myself later although it'll take me a while before I get time to install `udocker` and such. ```; backend {. # Override the default backend. #default = ""LocalExample"". . # The list of providers. providers {. . # The local provider is included by default in the reference.conf. This is an example. . # Define a new backend provider. Local {. # The actor that runs the backend. In this case, it's the Shared File System (SFS) ConfigBackend. actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". . # The backend custom configuration. config {. . # Optional limits on the number of concurrent jobs. #concurrent-job-limit = 5. . # If true submits scripts to the bash background using ""&"". Only usefull for dispatchers that do NOT submit. # the job and then immediately return a scheduled job id. run-in-background = true. . # `temporary-directory` creates the temporary directory for commands. #. # If this value is not set explicitly, the default value creates a unique temporary directory, equivalent to:. # temporary-directory = ""$(mktemp -d \""$PWD\""/tmp.XXXXXX)"". #. # The expression is run from the execution directory for the script. The expression must create the directory. # if it does not exist, and then return the full path to the directory. #. # To create and return a non-random temporary directory, use something like:. # temporary-directory = ""$(mkdir -p /tmp/mydir && echo /tmp/mydir)"". . # `script-epilogue` configures a shell command to run after the execution of every command block. #. # If this value is not set explicitly, the default value is `sync`, equivalent to:. # script-epilogue = ""sync"". #. # To turn off the default `sync` behavior set this value to an empty string:. # script-epilogue = """". . # The list of possible runtime custom at",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595:1032,schedul,scheduled,1032,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412883595,1,['schedul'],['scheduled']
Energy Efficiency,"hello.wdl -h http://localhost:8000; [2021-05-14 14:28:43,33] [info] Slf4jLogger started; [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; java.lang.IllegalStateException: Pool shutdown unexpectedly; 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); 	at akka.dispatch.Mailbox.exec(Mailbox.scala:241); 	at akka.dispatch.forkjo",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550:1103,adapt,adapted,1103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-841048550,1,['adapt'],['adapted']
Energy Efficiency,"hingExecutor.scala:55); 	at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); 	at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2017-12-06 04:38:38,467 cromwell-system-akka.dispatchers.engine-dispatcher-7 ERROR - WorkflowManagerActor Workflow 20f2c75f-5250-4525-8e30-2330f25dbbec failed (during ExecutingWorkflowState): Unexpected failure or termination of the actor monitoring ps:NA:1; java.lang.RuntimeException: Unexpected failure or termination of the actor monitoring ps:NA:1; 	at cromwell.engine.workflow.lifecycle.execution.WorkflowExecutionActor.onFailure(WorkflowExecutionActor.scala:242); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:13); 	at cromwell.util.StopAndLogSupervisor$$anonfun$stoppingDecider$1$1.applyOrElse(StopAndLogSupervisor.scala:11); 	at akka.actor.SupervisorStrategy.handleFailure(FaultHandling.scala:296); 	at akka.actor.dungeon.FaultHandling.handleFailure(FaultHandling.scala:263); 	at akka.actor.dungeon.FaultHandling.handleFailure$(FaultHandling.scala:254); 	at akka.actor.ActorCell.handleFailure(ActorCell.scala:370); 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:460); 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:484); 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012:5860,monitor,monitoring,5860,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012,1,['monitor'],['monitoring']
Energy Efficiency,"https://gatkforums.broadinstitute.org/firecloud/discussion/11853/error-message-the-job-was-aborted-from-outside-cromwell. AC: The message ""Job was aborted from outside Cromwell"" itself doesn't have enough information to understand what happened, and what to do next. It seems like there are two known failures that can lead to that error message:. 1. As described above, when an operation self-cancels due to a timeout, Cromwell could supplement the existing message with:; ```Jobs that run for longer than a week are not yet supported, and thus this job was cancelled because it exceeded that upper-limit. Please try to reduce the duration of your job. To get more details about your jobs, here are links to the stdout/stderr files...```. 2. As described in the forum post above, when Cromwell restarts a workflow and a job had been started by the engine/backend but didn't have an op id -- it gets marked as a failed job with the same error. Instead, if Cromwell knows that the reason it couldn't find an op id was because it is restarting -- then it should not report the message it does today at all and simply say:; ```When Cromwell restarted, it realized the job was not yet started, and so this is a benign failure. Cromwell will try attempting to execute this job again.```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266:621,reduce,reduce,621,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2496#issuecomment-424966266,1,['reduce'],['reduce']
Energy Efficiency,https://github.com/broadinstitute/cromwell/issues/4877. I also have this (only barely tested) script that may be useful to other HPC users:. https://gist.github.com/EvanTheB/8f9e07746af0c84831fc17f94ac4672d. It reduces the load on qstat of having thousands of jobs running. Let me know if it can be improved or incorporated here somewhere,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4905:211,reduce,reduces,211,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4905,1,['reduce'],['reduces']
Energy Efficiency,"if a job is aborted before it is started, cancel it efficiently",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2966:52,efficient,efficiently,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2966,1,['efficient'],['efficiently']
Energy Efficiency,importResolvers$lzycompute$1(WomGraphMaker.scala:28); at womtool.input.WomGraphMaker$.importResolvers$1(WomGraphMaker.scala:27); at womtool.input.WomGraphMaker$.$anonfun$getBundleAndFactory$1(WomGraphMaker.scala:39); at scala.util.Either.flatMap(Either.scala:352); at womtool.input.WomGraphMaker$.getBundleAndFactory(WomGraphMaker.scala:30); at womtool.input.WomGraphMaker$.fromFiles(WomGraphMaker.scala:46); at womtool.validate.Validate$.validate(Validate.scala:26); at womtool.WomtoolMain$.dispatchCommand(WomtoolMain.scala:54); at womtool.WomtoolMain$.runWomtool(WomtoolMain.scala:161); at womtool.WomtoolMain$.delayedEndpoint$womtool$WomtoolMain$1(WomtoolMain.scala:166); at womtool.WomtoolMain$delayedInit$body.apply(WomtoolMain.scala:27); at scala.Function0.apply$mcV$sp(Function0.scala:42); at scala.Function0.apply$mcV$sp$(Function0.scala:42); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); at scala.App.$anonfun$main$1(App.scala:98); at scala.App.$anonfun$main$1$adapted(App.scala:98); at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:575); at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:573); at scala.collection.AbstractIterable.foreach(Iterable.scala:933); at scala.App.main(App.scala:98); at scala.App.main$(App.scala:96); at womtool.WomtoolMain$.main(WomtoolMain.scala:27); at womtool.WomtoolMain.main(WomtoolMain.scala); Caused by: com.typesafe.config.ConfigException$IO: application.conf: java.io.IOException: resource not found on classpath: application.conf; at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:190); at com.typesafe.config.impl.Parseable.parseValue(Parseable.java:174); at com.typesafe.config.impl.Parseable.parse(Parseable.java:152); at com.typesafe.config.impl.SimpleIncluder.fromBasename(SimpleIncluder.java:185); ... 48 more; Caused by: java.io.IOException: resource not found on classpath: application.conf; at com.typesafe.config.impl.Parseable$ParseableResources.rawParseValue(Parseabl,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7255:4193,adapt,adapted,4193,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7255,1,['adapt'],['adapted']
Energy Efficiency,"ion/glob-4f26c666d13d1cb48973da7f646a7de2 2> /dev/null ) || ( ln merge_fastqs_R?_*.fastq.gz /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 ). # list all the files that match the glob into a file called glob-[md5 of glob].list; ls -1 /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if pair",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:2050,adapt,adapters,2050,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,2,['adapt'],['adapters']
Energy Efficiency,is this related to long overheads I'm seeing in a 900 way scatter in firecloud (as I see that firecloud is on 24 now...). https://portal.firecloud.org/#workspaces/broad-ccdg-dev%3AFunctionallyEquivalent-CCDG/Monitor/97002137-7006-47a6-90c8-faf471f0b2d1,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277106755:208,Monitor,Monitor,208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1938#issuecomment-277106755,1,['Monitor'],['Monitor']
Energy Efficiency,"it end; [2023-02-08 16:32:21,82] [info] checkpointClose end; [2023-02-08 16:32:21,82] [info] Checkpoint end - txts: 5348; [2023-02-08 16:32:21,89] [error] Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.LockException: Could not acquire change log lock. Currently locked by fdb0:cafe:d0d0:ceb4:ba59:9fff:fec3:33de%p1p1 (fdb0:cafe:d0d0:ceb4:ba59:9fff:fec3:33de%p1p1) since 2/8/23, 4:23 PM; 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:270); 	at liquibase.Liquibase.lambda$update$1(Liquibase.java:214); 	at liquibase.Scope.lambda$child$0(Scope.java:180); 	at liquibase.Scope.child(Scope.java:189); 	at liquibase.Scope.child(Scope.java:179); 	at liquibase.Scope.child(Scope.java:158); 	at liquibase.Liquibase.runInScope(Liquibase.java:2405); 	at liquibase.Liquibase.update(Liquibase.java:211); 	at liquibase.Liquibase.update(Liquibase.java:197); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:74); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:46); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$3.liftedTree1$1(BasicBackend.scala:276); 	at slick.basic.BasicBackend$DatabaseDef$$anon$3.run(BasicBackend.scala:276); 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144); 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642); 	at java.base/java.lang.Thread.run(Thread.java:1589); ```. What Am I doing wrong ?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7009:4453,adapt,adapted,4453,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7009,1,['adapt'],['adapted']
Energy Efficiency,"it for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d1000 nid=0x9f8 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread10"" #15 daemon prio=9 os_prio=0 tid=0x00007fdbcc2cf000 nid=0x9f7 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread9"" #14 daemon prio=9 os_prio=0 tid=0x00007fdbcc2cd000 nid=0x9f6 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE; ""C1 CompilerThread8"" #13 daemon prio=9 os_prio=0 tid=0x00007fdbcc2cb000 nid=0x9f5 waitin",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5973,monitor,monitor,5973,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['monitor'],['monitor']
Energy Efficiency,job scheduling on Mesos,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3461:4,schedul,scheduling,4,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3461,1,['schedul'],['scheduling']
Energy Efficiency,"k.basic.DatabaseConfig$$anon$1.db(DatabaseConfig.scala:102); 	at cromwell.database.slick.SlickDatabase.<init>(SlickDatabase.scala:73); 	at cromwell.database.slick.EngineSlickDatabase.<init>(EngineSlickDatabase.scala:15); 	at cromwell.database.slick.EngineSlickDatabase$.fromParentConfig(EngineSlickDatabase.scala:10); 	at cromwell.services.EngineServicesStore$.engineDatabaseInterface$lzycompute(EngineServicesStore.scala:13); 	at cromwell.services.EngineServicesStore$.engineDatabaseInterface(EngineServicesStore.scala:12); 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:27); 	at cromwell.CromwellEntryPoint$$anon$1.<init>(CromwellEntryPoint.scala:122); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:122); 	at scala.util.Try$.apply(Try.scala:210); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:122); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:65); 	at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); 	at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); 	at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); 	at scala.Function0.apply$mcV$sp(Function0.scala:39); 	at scala.Function0.apply$mcV$sp$(Function0.scala:39); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); 	at scala.App.$anonfun$main$1(App.scala:76); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563); 	at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:926); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CromwellApp$.main(CromwellApp.scala:3); 	at cromwell.CromwellApp.main(CromwellApp.scala); ```; when trying to run Cromwell 83. I see that the Java requirement has been updated to 1.11, but it's still listed as Java 1.8 in this documentation.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6830:3840,adapt,adapted,3840,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6830,1,['adapt'],['adapted']
Energy Efficiency,"k.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5341,Schedul,ScheduledThreadPoolExecutor,5341,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Schedul'],['ScheduledThreadPoolExecutor']
Energy Efficiency,"kedInterruptibly(ManagedArrayBlockingQueue.scala:265); at slick.util.ManagedArrayBlockingQueue.take(ManagedArrayBlockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""Hikari Housekeeping Timer (pool db)"" #24 daemon prio=5 os_prio=31 tid=0x00007fb76d88f000 nid=0x7503 waiting on condition [0x000000012cebb000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0623fc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.io.pinned-dispatcher-7"" #23 prio=5 os_prio=31 tid=0x00007fb76b450800 nid=0x7303 runnable [0x000000012cdb8000]; java.lang.Thread.State: RUNNABLE; at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method); at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198); at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103); at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86); - locked <0x00000006c0612e70> (a sun.nio.ch.Util$2); - locked <0x00000006c0612e80> (a java.util.Collections$UnmodifiableSet",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:41892,Schedul,ScheduledThreadPoolExecutor,41892,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Schedul'],['ScheduledThreadPoolExecutor']
Energy Efficiency,"l.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.execute(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$executeAsync$1(StandardAsyncExecutionActor.scala:639); at scala.util.Try$.apply(Try.scala:209); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync(StandardAsyncExecutionActor.scala:639); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:639); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:954); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:946); at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more; Caused by: common.exception.AggregatedMessageException: Error(s):; Cannot interpolate Array[Nothing] into a command string with attribute set [PlaceholderAttributeSet(None,None,None,None)]; at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); at cromwell.backend.standard.StandardAsyncExecutionActor.instantiatedCommand(StandardAsyncExecutionActor.scala:574); ... 35 more; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5092:5850,Schedul,Scheduler,5850,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5092,2,['Schedul'],['Scheduler']
Energy Efficiency,"ll was accidentally terminated, while Cromwell was terminated the job finished (and an RC file with status 0 was created). When I restart Cromwell, it checks all the jobs successfully and then for the task that was running, Cromwell does the following:. 1. `Restarting alignsortedbam.samtools`; 2. `Assigned new job execution tokens to the following groups: cd9b05d1: 1`; 3. `executing: squeue -u $(whoami)`; 4. `job id: 3342271`; 5. `Cromwell will watch for an rc file but will *not* double-check whether this job is actually alive (unless Cromwell restarts)`; 6. `Status change from - to Running`; 7. `Status change from Running to Done`; 8. ~~_Nothing_ - the next job is NOT started.~~ (_See my edit below_). I was under the impression through the comment from @kshakir [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328880929):. > - Currently the mechanism for ""checking if a job is done""-- in tests and main code-- is to look for rc files; > - On restart if the rc file is missing, there's a single extra check to the scheduler to see if the job is alive, by running a external command line process per job. However, when I restart a Cromwell-39 server, it calls the `check-alive` block before it checks for the RC file. It is calling the correct `squeue -j ${jobid}` (as discussed in the [doc: Slurm config](https://cromwell.readthedocs.io/en/stable/backends/SLURM/). For reference this returns:. ```; slurm_load_jobs error: Invalid job id specified; ```; I tried swapping it out for `squeue -u ${user}` (and also `-u $(whoami)`) option that @MatthewMah mentioned [here](https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-328984482) (just to cover my bases) which returns:. ```; JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON); ```. Cromwell doesn't seem to store the completed results, even though it successfully finds the RC file and marks the (samtools) task as Done, ~~as when I restarted the Cromwell server (after 20 minutes), it per",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736:1110,schedul,scheduler,1110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1499#issuecomment-487771736,1,['schedul'],['scheduler']
Energy Efficiency,llBackends.$anonfun$backendLifecycleActorFactories$1(CromwellBackends.scala:14); at scala.collection.immutable.List.map(List.scala:246); at cromwell.engine.backend.CromwellBackends.<init>(CromwellBackends.scala:14); at cromwell.engine.backend.CromwellBackends$.initBackends(CromwellBackends.scala:42); at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:68); at cromwell.CromwellEntryPoint$$anon$1.<init>(CromwellEntryPoint.scala:123); at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:123); at scala.util.Try$.apply(Try.scala:210); at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:123); at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:66); at cromwell.CromwellApp$.runCromwell(CromwellApp.scala:14); at cromwell.CromwellApp$.delayedEndpoint$cromwell$CromwellApp$1(CromwellApp.scala:25); at cromwell.CromwellApp$delayedInit$body.apply(CromwellApp.scala:3); at scala.Function0.apply$mcV$sp(Function0.scala:39); at scala.Function0.apply$mcV$sp$(Function0.scala:39); at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17); at scala.App.$anonfun$main$1(App.scala:76); at scala.App.$anonfun$main$1$adapted(App.scala:76); at scala.collection.IterableOnceOps.foreach(IterableOnce.scala:563); at scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:561); at scala.collection.AbstractIterable.foreach(Iterable.scala:926); at scala.App.main(App.scala:76); at scala.App.main$(App.scala:74); at cromwell.CromwellApp$.main(CromwellApp.scala:3); at cromwell.CromwellApp.main(CromwellApp.scala); ```; which to me implies I got the format incorrect. The format is a lot different to `reference-disk-localization-manifests` so just checking. . Are there any examples that can be shared as `gs://gcp-public-data--broad-references/refdisk_manifest.json` and the script at https://github.com/broadinstitute/cromwell/blob/develop/scripts/reference_disks/create_images.sh was very helpful!. Thanks!,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6953:3887,adapt,adapted,3887,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6953,1,['adapt'],['adapted']
Energy Efficiency,local monitoring,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2642:6,monitor,monitoring,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2642,1,['monitor'],['monitoring']
Energy Efficiency,"localhost:8000; > [2021-05-14 14:28:43,33] [info] Slf4jLogger started; > [2021-05-14 14:28:44,23] [info] Workflow 51376acd-e9c5-485a-856f-6aa501f25808 submitted to http://localhost:8000; > [ERROR] [05/14/2021 14:28:44.259] [SubmitSystem-akka.actor.default-dispatcher-16] [akka://SubmitSystem/system/pool-master] connection pool for Pool(shared->http://localhost:8000) has shut down unexpectedly; > java.lang.IllegalStateException: Pool shutdown unexpectedly; > 	at akka.http.impl.engine.client.PoolInterface$Logic.postStop(PoolInterface.scala:214); > 	at akka.stream.impl.fusing.GraphInterpreter.finalizeStage(GraphInterpreter.scala:579); > 	at akka.stream.impl.fusing.GraphInterpreter.finish(GraphInterpreter.scala:310); > 	at akka.stream.impl.fusing.GraphInterpreterShell.tryAbort(ActorGraphInterpreter.scala:644); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1(ActorGraphInterpreter.scala:780); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.$anonfun$postStop$1$adapted(ActorGraphInterpreter.scala:780); > 	at scala.collection.immutable.Set$Set2.foreach(Set.scala:181); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.postStop(ActorGraphInterpreter.scala:780); > 	at akka.actor.Actor.aroundPostStop(Actor.scala:558); > 	at akka.actor.Actor.aroundPostStop$(Actor.scala:558); > 	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundPostStop(ActorGraphInterpreter.scala:671); > 	at akka.actor.dungeon.FaultHandling.finishTerminate(FaultHandling.scala:215); > 	at akka.actor.dungeon.FaultHandling.terminate(FaultHandling.scala:173); > 	at akka.actor.dungeon.FaultHandling.terminate$(FaultHandling.scala:143); > 	at akka.actor.ActorCell.terminate(ActorCell.scala:447); > 	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:555); > 	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:571); > 	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:293); > 	at akka.dispatch.Mailbox.run(Mailbox.scala:228); > 	at akka.dispatch.Mailbox.exec(Mailbox.scala:24",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053:1133,adapt,adapted,1133,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6352#issuecomment-1264217053,1,['adapt'],['adapted']
Energy Efficiency,"ls(DefaultCredentialsProvider.java:92); cromwell_1 | 	at software.amazon.awssdk.awscore.client.handler.AwsClientHandlerUtils.createExecutionContext(AwsClientHandlerUtils.java:70); cromwell_1 | 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.createExecutionContext(AwsSyncClientHandler.java:68); cromwell_1 | 	at software.amazon.awssdk.core.client.handler.BaseSyncClientHandler.execute(BaseSyncClientHandler.java:68); cromwell_1 | 	at software.amazon.awssdk.core.client.handler.SdkSyncClientHandler.execute(SdkSyncClientHandler.java:44); cromwell_1 | 	at software.amazon.awssdk.awscore.client.handler.AwsSyncClientHandler.execute(AwsSyncClientHandler.java:55); cromwell_1 | 	at software.amazon.awssdk.services.sts.DefaultStsClient.getCallerIdentity(DefaultStsClient.java:673); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:86); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:76); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:91); cromwell_1 | 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); cromwell_1 | 	at scala.util.Try$.apply(Try.scala:213); cromwell_1 | 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:91); cromwell_1 | 	... 46 more; cromwell_1 | ; cromwell_1 | 2020-03-15 16:09:58,022 cromwell-system-akka.dispatchers.engine-dispatcher-59 INFO - WorkflowManagerActor WorkflowActor-c4ee3308-f9bf-41d2-acdb-70c02b6cc4b3 is in a terminal state: WorkflowFailedState`. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5452:4240,adapt,adapted,4240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5452,1,['adapt'],['adapted']
Energy Efficiency,"lure_to_message::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures[%]%:failure' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = CONCAT(TRIM(TRAILING ':failure' FROM METADATA_KEY), "":message""); WHERE METADATA_KEY LIKE ""%failures[%]%:failure""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatemen",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103:2039,adapt,adapted,2039,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459580103,1,['adapt'],['adapted']
Energy Efficiency,"lways appears to be `true`, and thus not needed.; - `!isAliveCache.contains` followed by `isAliveCache.get` should be `isAliveCache.getOrElseUpdate(job, IsAliveCache(Instant.now))`.; - There should be only one `SharedFileSystemJob` per `SharedFileSystemAsyncJobExecutionActor`. The reason the values are passed around is because the actor is also partially stateless, using `context.become` to track the `SharedFileSystemJob`. This PR adds state more to the actor, outside of the context, but that shouldn't be needed if the `isAlive` check is switched to running due to multiple `scheduleOnce` calls. The tests are likely timing out because the of the extra checks to `isAlive`. The tests are meant to run as quickly as possible. In general, the order of the job completion checking should always be 1) multiple rc file polls, 2) 30 seconds later `isAlive` checks as necessary. This individual polling per job may overwhelm the SGE scheduler if hundreds or thousands of scatter jobs are running, so more work may need to be done in the future to simplify the qstat process to check jobs in batches. Notes on configuration:. The initial scheduling should also be configurable. It should be off by default. Also, the error code that is returned may want to be configurable and/or we would want the actor to handle this special case differently and reattempt instead of fail. Cromwell's SFS implementation assumes jobs will always be writing their rc files. If something else out there is truly killing the jobs, we will have to wire in a way for `poll` to return a `FailedRetryableExecutionHandle`. I'm not sure that writing a value into the rc file is the best way to do this, and not yet sure what a suitable alternative is also. That's all I've got for now. Thanks again for all your work so far! I'm also game if we move this discussion over to a github issue instead of a PR, as I suspect the final version will look a bit different, and we can discuss and capture any other design there instead.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238:2474,schedul,scheduling,2474,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1346#issuecomment-243562238,1,['schedul'],['scheduling']
Energy Efficiency,"m using FireCloud (workspace: broad-firecloud-dsde/dsde-methods-sv-dev); <!-- Paste/Attach your workflow if possible: -->; The WDL can be found in GATK's repo: [cnv_germline_cohort_workflow.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/germline/cnv_germline_cohort_workflow.wdl) that imports [cnv_common_tasks.wdl](https://github.com/broadinstitute/gatk/blob/master/scripts/cnv_wdl/cnv_common_tasks.wdl). This is the graph that ```wdltools``` output for that WDL; [graph.pdf](https://github.com/broadinstitute/cromwell/files/2406647/graph.pdf); ![graph](https://user-images.githubusercontent.com/791104/45901323-88187c80-bdb0-11e8-91df-c9a61a12a96a.png). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. As you can see in the monitor's ""Failure"" [report ](https://portal.firecloud.org/#workspaces/broad-firecloud-dsde/dsde-methods-sv-dev/monitor/88f444ae-0898-4b5e-af0c-ede98216641d/6d980272-4aa7-4d32-ab90-84880a0723b2)```GermlineCNVCallerCohortMode``` scatter task never get calls before the dependent ```PostprocessGermineCNVCalls```.; <img width=""788"" alt=""screen shot 2018-09-21 at 3 21 43 pm"" src=""https://user-images.githubusercontent.com/791104/45901815-47b9fe00-bdb2-11e8-9043-9f771ee8bd9e.png"">. The log confirms this if one searches for ""Starting"":; ```; 2018-09-20 22:45:12,561 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.PreprocessIntervals; 2018-09-20 23:03:42,454 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.CollectCounts (95 shards), CNVGermlineCohortWorkflow.ScatterIntervals; 2018-09-21 02:12:52,275 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32-ab90-84880a0723b2 [UUID(6d980272)]: ; Starting CNVGermlineCohortWorkflow.DetermineGermlineContigPloidyCohortMode; 2018-09-21 02:31:18,476 INFO - WorkflowExecutionActor-6d980272-4aa7-4d32",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4136:1573,monitor,monitor,1573,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4136,1,['monitor'],['monitor']
Energy Efficiency,metering cromwell server jobs?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5503:0,meter,metering,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5503,1,['meter'],['metering']
Energy Efficiency,monitoring log does not exist,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4330:0,monitor,monitoring,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4330,1,['monitor'],['monitoring']
Energy Efficiency,"mwell?useSSL=false; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at cromwell.server.CromwellSystem.$init$(CromwellSystem.scala:24); 	at cromwell.CromwellEntryPoint$$anon$2.<init>(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.$anonfun$buildCromwellSystem$1(CromwellEntryPoint.scala:63); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.CromwellEntryPoint$.buildCromwellSystem(CromwellEntryPoint.scala:63); 	at cromwell.CromwellEntryPoint$.runSingle(CromwellEntryPoint.scala:47); 	at cromwell.CommandLineParser$.runCromwell(CommandLineParser.scala:95); 	at cromwell.CommandLineParser$.delayedEndpoint$cromwell$CommandLineParser$1(CommandLineParser.scala:105); 	at cromwell.CommandLineParser$delayedInit$body.apply(CommandLineParser.scala:8); 	at scala.Function0.apply$mcV$sp(Function0.scala:34); 	at scala.Function0.apply$mcV$sp$(Function0.scala:34); 	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12); 	at scala.App.$anonfun$main$1$adapted(App.scala:76); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at scala.App.main(App.scala:76); 	at scala.App.main$(App.scala:74); 	at cromwell.CommandLineParser$.main(CommandLineParser.scala:8); 	at cromwell.CommandLineParser.main(CommandLineParser.scala); Caused by: java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 5004ms.; 	at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:548); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:186); 	at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:145); 	at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:83); 	at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdbcDataSource.scala:18); 	at slick.jdbc.JdbcBackend$BaseSession.<init>(JdbcBackend.scala:439); 	at slick.jdbc.JdbcBackend$DatabaseDef.createSession(JdbcBackend.scala:47); 	at slick.jdbc.JdbcBackend$DatabaseDef.crea",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453:2257,adapt,adapted,2257,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387#issuecomment-372264453,1,['adapt'],['adapted']
Energy Efficiency,"n trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # this prefix will be later stripped in bowtie2 task; Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); }; runtime {; cpu : select_first([cpu,2]); memory : ""${select_first([mem_mb,'12000'])} MB""; time : select_first([time_hr,24]); disks : select_first([disks,""local-disk 100 HDD""]); }; }; ```. My backend.conf :; ```; include required(classpath(""application"")). backend {; default=""SGE""; providers {; SGE {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 10000; runtime-a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:3114,adapt,adapters,3114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,3,['adapt'],"['adapter', 'adapters']"
Energy Efficiency,ncExecutionActor.scala:644); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:959); 	at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:951); 	at cromwell.backend.impl.sfs.config.BackgroundConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:200); 	at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); 	at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); 	at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); 	at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); 	at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); 	at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: common.exception.AggregatedMessageException: Error(s):; Error evaluating ad hoc files:; <path_prefix>/cromwell/cromwell-executions/main/c9194073-c6ed-4c2a-97d6-fbc6a2314883/call-main/execution/centaur/src/main/resources/standardTestCases/cwl_dynamic_initial_workdir/testdir; 	at common.validation.Validation$ValidationTry$.toTry$extension1(Validation.scala:77); 	at common.validation.Validation$ValidationTry$.toTry$extension0(Validation.scala:73); 	at cromwell.bac,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211:3777,Schedul,Scheduler,3777,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4725#issuecomment-472514211,1,['Schedul'],['Scheduler']
Energy Efficiency,"ncHttpClient$4(SdkDefaultClientBuilder.java:245); 	at java.util.Optional.orElseGet(Optional.java:267); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.resolveSyncHttpClient(SdkDefaultClientBuilder.java:245); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.finalizeSyncConfiguration(SdkDefaultClientBuilder.java:210); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.syncClientConfiguration(SdkDefaultClientBuilder.java:148); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:27); 	at software.amazon.awssdk.services.sts.DefaultStsClientBuilder.buildClient(DefaultStsClientBuilder.java:22); 	at software.amazon.awssdk.core.client.builder.SdkDefaultClientBuilder.build(SdkDefaultClientBuilder.java:119); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1(AwsAuthMode.scala:77); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$credentialValidation$1$adapted(AwsAuthMode.scala:69); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.$anonfun$validateCredential$1(AwsAuthMode.scala:84); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); 	at scala.util.Try$.apply(Try.scala:209); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:84); 	... 48 common frames omitted; 2019-07-02 19:16:37,967 cromwell-system-akka.dispatchers.engine-dispatcher-30 ERROR - WorkflowManagerActor Workflow 10f172e8-b7ba-416f-964e-22ab8c7b38e3 failed (during MaterializingWorkflowDescriptorState): java.lang.RuntimeException: Credentials are invalid: software.amazon.awssdk.http.SdkHttpService: Provider software.amazon.awssdk.http.apache.ApacheSdkHttpService not found; 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential(AwsAuthMode.scala:85); 	at cromwell.cloudsupport.aws.auth.AwsAuthMode.validateCredential$(AwsAuthMode.scala:83); 	at cromwell.cloudsupport.aws.auth.DefaultMode.validateCredential(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273:8545,adapt,adapted,8545,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5056#issuecomment-507840273,1,['adapt'],['adapted']
Energy Efficiency,"ncurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""Hikari Housekeeping Timer (pool db)"" #24 daemon prio=5 os_prio=31 tid=0x00007fb76d88f000 nid=0x7503 waiting on condition [0x000000012cebb000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0623fc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.io.pinned-dispatcher-7"" #23 prio=5 os_prio=31 tid=0x00007fb76b450800 nid=0x7303 runnable [0x000000012cdb8000]; java.lang.Thread.State: RUNNABLE; at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method); at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198); at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103); at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86); - locked <0x00000006c0612e70> (a sun.nio.ch.Util$2); - locked <0x00000006c0612e80> (a java.util.Collections$UnmodifiableSet); - locked <0x00000006c0612e20> (a sun.nio.ch.KQueueSelectorImpl); at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97); at sun.nio.ch.SelectorImpl.s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:42056,Schedul,ScheduledThreadPoolExecutor,42056,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Schedul'],['ScheduledThreadPoolExecutor']
Energy Efficiency,nd$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71). 	at scala.collection.IterableLike.foreach$(I,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:15413,allocate,allocateLobForResult,15413,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['allocate'],['allocateLobForResult']
Energy Efficiency,nd$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(I,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:6264,allocate,allocateLobForResult,6264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['allocate'],['allocateLobForResult']
Energy Efficiency,"nd_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi; cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.${barcode}.R1.trimmed.gz -p ${sampleName}.${barcode}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; cpu : 2; memory : '${memory_task2} MB'; time : 24; }. output {; File fastq_trimmed_R1 = ""${sampleName}.${barcode}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.${barcode}.R2.trimmed.gz""; }; }. ### Below my json file:; {; ""scMethTask3.monitoring_script"": ""monitoring.sh"",; ""scMethTask3.command"": ""moveBarcodeToID.pl"",; ""scMethTask3.meta_data"": ""test_no_barcode.txt"",; ""scMethTask3.bases"": 6,. ""scMethTask3.memory_task1"":45,. ""scMethTask3.TAG"": ""'length='"",; ""scMethTask3.low_quality_cutoff"": 21,; ""scMethTask3.read_length_cutoff"": 62,; ""scMethTask3.trim_start_R1"": 11,; ""scMethTask3.trim_end_R1"": -16,; ""scMethTask3.trim_start_R2"": 25,; ""scMethTask3.trim_end_R2"": -2,; ""scMethTask3.adapters_1"": ""AGATCGGAAGAGCACACGTCTGAAC"",; ""scMethTask3.adapters_2"": ""AGATCGGAAGAGCGTCGTGTAGGGA"",. ""scMethTask3.memory_task2"": 20; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:6579,monitor,monitoring,6579,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,1,['monitor'],['monitoring']
Energy Efficiency,"nfiguring it at the Cromwell level, so e.g. any user of Terra (or any other hosted Cromwell with PAPIv2 backend) could get usage reports without having to configure anything. The metrics are reported in their GCP project, so a user gets automatic access to them as long as they're a viewer. We could also easily expose a link to workflow- and task-level reports in Job Manager UI, so they will be literally point-and-click away. Each timepoint is designed to be self-sufficient, as it is labeled with:; - Cromwell-specific values, such as workflow ID, task call name, index and attempt.; - GCP instance values such as instance name, zone, number of CPU cores, total memory and disk size. Here's an example graph of cpu/memory/disk utilization for one of our production workflows, as it is running right now - one can already see we could probably save ~40% of the cost:; <img width=""1869"" alt=""screen shot 2019-01-02 at 4 43 20 pm"" src=""https://user-images.githubusercontent.com/137337/50614108-c0e6e800-0ead-11e9-9ef4-02029725a44c.png"">. Reporting itself costs very little if anything at all, because Stackdriver provides a generous free tier worth ~65K instance-hours each month, and ~$0.0006 per instance-hour after that (at the current rate of 5 metric points reported each minute). @kshakir suggested using a ""vendor-neutral"" reporting library such as [Micrometer.io](http://micrometer.io/), although I have reservations around that - mostly because that may require additional setup and we want this to ""just work""; but also because the implementation is currently PAPIv2-specific anyway, so it is already non-vendor agnostic. Likewise, one could export metrics from Stackdriver monitoring if they wanted to. But we're open to the idea. Finally, I haven't added any tests yet, as it's unclear in which shape or form (if at all) you'd like to integrate this code. Thanks in advance for any feedback!; ~[@broadinstitute/wintergreen](https://github.com/orgs/broadinstitute/teams/wintergreen) team.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510:3968,monitor,monitoring,3968,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510,1,['monitor'],['monitoring']
Energy Efficiency,"ng(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240), WomString(lib) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/lib_format_counts.json), WomString(quant) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-1/ScatterAt40_16/abdbed6b-1162-44d6-ad7c-8a39fa8720c4/call-salmon/shard-0/execution/quant_SRR2014240/quant.sf)),List())))WorkflowFailure(Unexpected failure or termination of the actor monitoring SubWorkflow-ScatterAt40_16:0:1,List(WorkflowFailure(Cannot construct WomMapType(WomStringType,WomAnyType) with mixed types: Map(WomString(metadata) -> WomMap(WomMapType(WomStringType,WomStringType),Map(WomString(layout) -> WomString(PAIRED), WomString(model) -> WomString(Illumina HiSeq 2000), WomString(characteristics) -> WomString(number of donors -> 1;age -> 64 years old;tissue -> Liver;vendor -> Biochain;isolate -> Lot no.: B510092;gender -> Male), WomString(series) -> WomString(GSE69360), WomString(organism) -> WomString(Homo sapiens), WomString(run) -> WomString(SRR2014238), WomString(strategy) -> WomString(RNA-Seq), WomString(path) -> WomString(https://sra-download.ncbi.nlm.nih.gov/traces/sra29/SRR/001967/SRR2014238), WomString(name) -> WomString(Biochain_Adult_Liver), WomString(gsm) -> WomString(GSM1698568), WomString(title) -> WomString(Biochain_Adult_Liver))), WomString(run) -> WomString(SRR2014238), WomString(folder) -> WomSingleFile(/data/cromwell-executions/quantification/cf14203a-6554-4c12-9908-6de88a20f083/call-ScatterAt40_16/shard-0/ScatterAt40_16/8c83d187-db36-4dc0-a6c4-f7e91b3d80f3/call-salmon/shard-0/execution/quant_SRR2014238), WomStr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4555:2208,monitor,monitoring,2208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4555,1,['monitor'],['monitoring']
Energy Efficiency,"ngth_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; }; call trimAdapters {; input:; input_r1=trimCellBarcode.fastqDebarcodedR1,; input_r2=trimCellBarcode.fastqDebarcodedR2,; sampleName=sampleName,; barcode=barcode,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; } ; }. task trimCellBarcode {; File f1; File f2; String sampleName; String? barcode; File command; Int bases; File? monitoring_script; Int memory_task1. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitor",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:3807,monitor,monitoring,3807,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,2,['monitor'],['monitoring']
Energy Efficiency,"nitoring API. This serves 2 important goals:. 1) Users can easily plot real-time resource usage statistics across all tasks in a workflow, or for a single task call across many workflow runs, etc. This can be very powerful to quickly determine outlier tasks that could use optimization, without the need for any configuration or code (or any changes to the workflow). It's also much easier than the current state-of-the-art, i.e. parsing task-level monitoring logs. 2) Scripts can easily get aggregate statistics on resource utilization and could produce suggestions based on those. This could provide a path towards automatic runtime configuration based on the models trained with historical data. One could also detect situations like out-of-memory calls and automatically adjust resources according to those. It would also be pretty easy to add logic for estimation of task call-level cost based on the pricing of associated resources. This could provide a long-sought feature of real-time cost monitoring/control (thanks to @TimothyTickle for the suggestion). Monitoring is done using the new ""monitoring action"" for PAPIv2, which currently uses the hard-coded [quay.io/broadinstitute/cromwell-monitor](https://quay.io/repository/broadinstitute/cromwell-monitor) image, built from https://github.com/broadinstitute/cromwell-monitor (I wasn't sure if that code belonged here or in a separate repo). This is advantageous to just using it as a _monitoring_script_, because it removes all assumptions on the ""user"" Docker image (for the task itself). For example, we don't have to assume a particular distribution or presence of Python and its libraries. So it should work exactly the same for any task. Per @geoffjentry's suggestion, we've [consulted](https://groups.google.com/forum/#!topic/google-genomics-discuss/caYM7oHbfx0) with the Google Genomics team, and they don't see any apparent issues with the concept. We could expose this as a workflow option like `monitoring_image`, and allow config",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510:1284,monitor,monitoring,1284,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510,1,['monitor'],['monitoring']
Energy Efficiency,"nitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi; cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.${barcode}.R1.trimmed.gz -p ${sampleName}.${barcode}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; cpu : 2; memory : '${memory_task2} MB'; time : 24; }. output {; File fastq_trimmed_R1 = ""${sampleName}.${barcode}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.${barcode}.R2.trimmed.gz""; }; }. ### Below my json file:; {; ""scMethTask3.monitoring_script"": ""monitoring.sh"",; ""scMethTask3.command"": ""moveBarcodeToID.pl"",; ""scMethTask3.meta_data"": ""test_no_barcode.txt"",; ""scMethTask3.bases"": 6,. ""scMethTask3.memory_task1"":45,. ""scMethTask3.TAG"": ""'length='"",; ""scMethTask3.low_quality_cutoff"": 21,; ""scMethTask3.read_length_cutoff"": 62,; ""scMethTask3.trim_start_R1"": 11,; ""scMethTask3.trim_end_R1"": -16,; ""scMethTask3.trim_sta",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:5934,monitor,monitoring,5934,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,2,['monitor'],['monitoring']
Energy Efficiency,o Provide health monitor infrastructure to know status of underlying systems; o Provide status endpoint which will query current contents of health monitor; o Moved some google code to a new cloudSupport project; o Moved some general docker code to core from the dockerHashing project,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2653:17,monitor,monitor,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2653,2,['monitor'],['monitor']
Energy Efficiency,o Provide health monitor infrastructure to know status of underlying systems; o Provide status endpoint which will query current contents of health monitor; o Provide health monitor implementations for typical and workbench usages; o Moved some google code to a new cloudSupport project,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2654:17,monitor,monitor,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2654,3,['monitor'],['monitor']
Energy Efficiency,"o SUCCEEDED for job projects/392615380452/locations/us-south1/jobs/job-ba81bad8-82e9-4d95-8fc0-04dfbbd746da.; taskExecution.exitCode=0; ```. What we define as execution events:. ```; ExecutionEvent(Job state is set from QUEUED to SCHEDULED for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:10:01.704137839Z,None); ExecutionEvent(Job state is set from SCHEDULED to RUNNING for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:11:30.631264449Z,None); ExecutionEvent(Job state is set from RUNNING to SUCCEEDED for job projects/392615380452/locations/us-south1/jobs/job-321db1bc-9a68-4171-aa2a-46885d781656.,2024-04-03T20:12:16.898798407Z,None); ```. </details>; </details>. ## Load test results. We have executed many load tests, this is the latest one involving 14k jobs. Data / Backend | Batch with Mysql | PAPIv2 with Mysql; ------------- | -------------|---------; Jobs | 14400 | 14400; Execution time | 20936 seconds | 24451 seconds. Overall, all our tests indicate that Batch finishes executing the jobs faster than PAPIv2. <details>; <summary>Load tests settings</summary>. We have ran Cromwell in server mode with the following settings:. - request-timeout: 10m; - idle-timeout: 10m; - job-rate-control: jobs = 20, per = 10 seconds; - max-workflow-launch-count: 50; - new-workflow-poll-rate: 1; - database: MySQL; - virtual-private-cloud setup; - maximum-polling-interval: 600s; - localization-attempts: 3; - google.auth: service account; - request-workers: 3; - concurrent-job-limit: 14400. JVM Options:; - `-Xms512m -Xmx64g`. **NOTE**: Initially we found a bottleneck on Batch but Google enabled an experimental settings to schedule many jobs concurrently which reduced the total execution time. Server capacity (from Google Cloud):; - VM Machine Type: n2-standard-16; - Virtual CPUs: 16; - Memory: 64G; - Architecture: x86/64; - CPU Platform: Intel Cascade Lake. </details>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7412:6311,schedul,schedule,6311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7412,2,"['reduce', 'schedul']","['reduced', 'schedule']"
Energy Efficiency,"ockingQueue.scala:104); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""Hikari Housekeeping Timer (pool db)"" #24 daemon prio=5 os_prio=31 tid=0x00007fb76d88f000 nid=0x7503 waiting on condition [0x000000012cebb000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0623fc0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.io.pinned-dispatcher-7"" #23 prio=5 os_prio=31 tid=0x00007fb76b450800 nid=0x7303 runnable [0x000000012cdb8000]; java.lang.Thread.State: RUNNABLE; at sun.nio.ch.KQueueArrayWrapper.kevent0(Native Method); at sun.nio.ch.KQueueArrayWrapper.poll(KQueueArrayWrapper.java:198); at sun.nio.ch.KQueueSelectorImpl.doSelect(KQueueSelectorImpl.java:103); at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86); - locked <0x00000006c0612e70> (a sun.nio.ch.Util$2); - locked <0x00000006c0612e80> (a java.util.Collections$UnmodifiableSet); - locked <0x00000006c0612e20> (a sun.nio.ch.KQueueSelectorImpl); at sun.nio.ch.SelectorImpl.select(SelectorImpl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:42006,Schedul,ScheduledThreadPoolExecutor,42006,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Schedul'],['ScheduledThreadPoolExecutor']
Energy Efficiency,"oh nvm, that one was exceeding the maximum log length. in any case there are some systemic test issues that we are working on so this PR should go green â€“ or show evidence of a PR-specific problem â€“ in due time",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557684483:147,green,green,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5297#issuecomment-557684483,1,['green'],['green']
Energy Efficiency,"omwell_execution/travis/centaur_workflow/0310fa51-e985-4c54-8cdb-5058155f452e/call-centaur/cromwell_root/logs/). ```java; 2017-08-25 05:43:25,399 cromwell-system-akka.dispatchers.engine-dispatcher-51 ERROR - WorkflowManagerActor Workflow dabddbe7-a385-4df4-be97-c1ef7b884823 failed (during ExecutingWorkflowState): Could not evaluate composeEngineFunctions.y = read_int(stderr()) + x + read_string(blah); java.lang.RuntimeException: Could not evaluate composeEngineFunctions.y = read_int(stderr()) + x + read_string(blah); 	at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:190); 	at wdl4s.wdl.WdlTask$$anonfun$4.applyOrElse(WdlTask.scala:189); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at scala.util.Failure.recoverWith(Try.scala:232); 	at wdl4s.wdl.WdlTask.$anonfun$evaluateOutputs$2(WdlTask.scala:189); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:157); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:155); 	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104); 	at wdl4s.wdl.WdlTask.evaluateOutputs(WdlTask.scala:182); 	at cromwell.backend.wdl.OutputEvaluator$.evaluateOutputs(OutputEvaluator.scala:15); 	at cromwell.backend.standard.StandardAsyncExecutionActor.evaluateOutputs(StandardAsyncExecutionActor.scala:406); 	at cromwell.backend.standard.StandardAsyncExecutionActor.evaluateOutputs$(StandardAsyncExecutionAct",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2576:1188,adapt,adapted,1188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2576,1,['adapt'],['adapted']
Energy Efficiency,"onfun$ready$1.apply(package.scala); at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2$$anon$4.block(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinPool.managedBlock(Redefined); at scala.concurrent.impl.ExecutionContextImpl$DefaultThreadFactory$$anon$2.blockOn(ExecutionContextImpl.scala:44); at scala.concurrent.Await$.ready(package.scala:169); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala:29); at cromwell.server.CromwellServer$$anonfun$run$1.applyOrElse(CromwellServer.scala); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala:433); at scala.concurrent.Future$$anonfun$andThen$1.apply(Future.scala); at scala.concurrent.impl.CallbackRunnable.run(Redefined); at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(Redefined); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java). ""cromwell-system-scheduler-1"" #14 prio=5 os_prio=31 tid=0x00007fb76aa14800 nid=0x6103 runnable [0x00000001295b3000]; java.lang.Thread.State: RUNNABLE; at com.jprofiler.agent.InstrumentationCallee.exitFilteredMethod(Native Method); at com.jprofiler.agent.InstrumentationCallee.__ejt_filter_exitMethod(ejt:86); at akka.actor.LightArrayRevolverScheduler.clock(Scheduler.scala:213); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Redefined); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Redefined); at java.lang.Thread.run(Redefined). ""cromwell-system-akka.actor.default-dispatcher-4"" #13 prio=5 os_prio=31 tid=0x00007fb76b38c000 nid=",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:46244,Adapt,AdaptedForkJoinTask,46244,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['Adapt'],['AdaptedForkJoinTask']
Energy Efficiency,ontext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:337); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:258); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:372); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:312); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; </details>. A workaround is setting up a registry to host the images (so we can,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5178:3916,adapt,adapted,3916,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178,1,['adapt'],['adapted']
Energy Efficiency,"ontext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:341); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:257); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:355); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:376); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:316); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2020-10-08 16:08:57,571 cromwell-system-akka.dispatchers.engine-dispatcher-33 WARN -",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:3232,adapt,adapted,3232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,1,['adapt'],['adapted']
Energy Efficiency,"other benefit is that we'd reduce our dependency on dockerhub. Green team is seeing issues that look like they're throttling us, namely a bunch of these:. ```; Execution failed: pulling image: docker pull: generic::unknown: retry budget exhausted (10 attempts): ; running [""docker"" ""pull"" ""google/cloud-sdk:slim""]: exit status 1 (standard error: ""Error response from ; daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection ; (Client.Timeout exceeded while awaiting headers)\n"") at ; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4640#issuecomment-463034541:27,reduce,reduce,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4640#issuecomment-463034541,2,"['Green', 'reduce']","['Green', 'reduce']"
Energy Efficiency,papi v2 monitoring script,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4551:8,monitor,monitoring,8,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4551,1,['monitor'],['monitoring']
Energy Efficiency,pl$MultiInsertAction.run(JdbcActionComponent.scala:526); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:30); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:27); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:239); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:239); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:237); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:533); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:533); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:570); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); ```; The Cromwell configuration is:; ```; ,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3607:4933,adapt,adapted,4933,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3607,1,['adapt'],['adapted']
Energy Efficiency,pl$MultiInsertAction.run(JdbcActionComponent.scala:527); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:31); 	at slick.jdbc.JdbcActionComponent$SimpleJdbcProfileAction.run(JdbcActionComponent.scala:28); 	at slick.dbio.DBIOAction$$anon$4.$anonfun$run$3(DBIOAction.scala:240); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:240); 	at slick.dbio.DBIOAction$$anon$4.run(DBIOAction.scala:238); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.$anonfun$run$4$adapted(DBIOAction.scala:534); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:4592,adapt,adapted,4592,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,2,['adapt'],['adapted']
Energy Efficiency,"quencing library is made the user wants to trim the cell barcodes or not. So in case the user is providing the metadata with the name of the barcodes in the 4th column the task of trimming the barcode should be ""on"". In contrary, it should be off. This, of course, depends on whether the barcodes are provided in the metadata or not. What I am trying to do is to make the string that is in the barcode with the condition ""?"" (please see the workflow below in the scatter). When the scatter is reading the metadata, in the case in which there is no barcode the WDL is interrupted with this error: . ```; ""message"": ""Failed to evaluate 'scMethTask3.barcode' (reason 1 of 1): Evaluating files_and_metadata_row[3] failed: Failed to find index Success(WomInteger(3)); on array:\n\nSuccess([\""SRR5395068\"", \""SRR5395068_1.fastq.gz\"", \""SRR5395068_2.fastq.gz\""])\n\n3"",; ""causedBy"": []; ```; How can i avoid this? Or is there a way to accomplish what I am trying to do?. ### Which backend are you running? ; Unix terminal within slurm scheduler. ### Example meta_data files:; 1) without barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz	; ```; 2) with barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz ATCGCT	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz ATCGGA; ```; ### Below my workflow:. workflow scMethTask3 {. #information about the monitoring scrip and the number of samples; File? monitoring_script; File meta_data. #information for trimming the cell barcode; File command; Int bases; ; #information for trimming the adapters and low quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memory_task1; Int memory_task2. #Start the call. Array[Array[String]] files_and_metadata = read_tsv(meta_data). scatter(files_and_metadata",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:1161,schedul,scheduler,1161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,1,['schedul'],['scheduler']
Energy Efficiency,rThread.java:107); 11:09:46 cromwell-test_1 | Caused by: liquibase.exception.LockException: java.lang.NullPointerException; 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.acquireLock(StandardLockService.java:242); 11:09:46 cromwell-test_1 | 	at liquibase.lockservice.StandardLockService.waitForLock(StandardLockService.java:170); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:196); 11:09:46 cromwell-test_1 | 	at liquibase.Liquibase.update(Liquibase.java:192); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:58); 11:09:46 cromwell-test_1 | 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:31); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 11:09:46 cromwell-test_1 | 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:96); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 11:09:46 cromwell-test_1 | 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 11:09:46 cromwell-test_1 | 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 11:09:46 cromwell-test_1 | 	at java.lang.Thread.run(Thread.java:748); 11:09:46 cromwell-test_1 | Caused by: java.lang.NullPointerException: null; 11:09:46 cromwell-test_1 | 	at liquibase.sqlgenerator.SqlGenera,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766:4339,adapt,adapted,4339,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4328#issuecomment-434037766,1,['adapt'],['adapted']
Energy Efficiency,"re_metadata.xml::causedByLists::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column '%failures%causedBy:%' in 'where clause' [Failed SQL: UPDATE METADATA_ENTRY; SET METADATA_KEY = REPLACE(METADATA_KEY, ""causedBy:"", ""causedBy[0]:""); WHERE METADATA_KEY LIKE ""%failures%causedBy:%""]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809:1963,adapt,adapted,1963,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459583809,1,['adapt'],['adapted']
Energy Efficiency,reduce logs closes #1002,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1062:0,reduce,reduce,0,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1062,1,['reduce'],['reduce']
Energy Efficiency,"related to #2399 and #2830 . As a **Cromwell dev**, I want to **automatically release Cromwell once Travis is green**, so that **Travis doesn't release when it's failing**.; - effort: small; - risk: small; - business value: small (to medium); - have there been any regressions that Travis would have caught but it was red when we released?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964:110,green,green,110,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2401#issuecomment-344715964,1,['green'],['green']
Energy Efficiency,reparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71). 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1954/1995472759.apply(Unknown Source); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSessi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:15975,adapt,adapted,15975,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['adapt'],['adapted']
Energy Efficiency,reparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15$adapted(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1955/223664727.apply(Unknown Source); 	at scala.collection.Iterator.foreach(Iterator.scala:929); 	at scala.collection.Iterator.foreach$(Iterator.scala:929); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$14(JdbcActionComponent.scala:529); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction$$Lambda$1954/1995472759.apply(Unknown Source); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement(JdbcBackend.scala:372); 	at slick.jdbc.JdbcBackend$SessionDef.withPreparedStatement$(JdbcBackend.scala:367); 	at slick.jdbc.JdbcBackend$BaseSessi,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:6826,adapt,adapted,6826,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,1,['adapt'],['adapted']
Energy Efficiency,"s a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. perl ${command} paired ${f1} ${f2} ${bases} ${sampleName}.${barcode}.R1.debarcoded.fq.gz ${sampleName}.${barcode}.R2.debarcoded.fq.gz; >>>. runtime {; cpu : 1; memory : '${memory_task1} MB'; time : 24; }. output {; File fastqDebarcodedR1 = ""${sampleName}.${barcode}.R1.debarcoded.fq.gz""; File fastqDebarcodedR2 = ""${sampleName}.${barcode}.R2.debarcoded.fq.gz""; }; }; ; task trimAdaptersWithoutBarcodes {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; if [ ! -z ""${monitoring_script}"" ]; then; chmod a+x ${monitoring_script}; ${monitoring_script} > monitoring.log &; else; echo ""No monitoring script given as input"" > monitoring.log &; fi. cutadapt -f fastq -q ${low_quality_cutoff} -m ${read_length_cutoff} -a ${adapters_1} -A ${adapters_2} -u ${trim_start_R1} -u ${trim_end_R1} -U ${trim_start_R2} -U ${trim_end_R2} --length-tag=${TAG} -o ${sampleName}.R1.trimmed.gz -p ${sampleName}.R2.trimmed.gz ${input_r1} ${input_r2}; >>>. runtime {; docker_user: ""ngs""; }; output {; File fastq_trimmed_R1 = ""${sampleName}.R1.trimmed.gz""; File fastq_trimmed_R2 = ""${sampleName}.R2.trimmed.gz""; }; }. task trimAdapters {; File input_r1; File input_r2; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; String sampleName; String? barcode; Int memory_task2; File? monitoring_script. command <<<; set -euo pipefail. #if the WDL/task contains a monitoring script as input; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:4750,monitor,monitoring,4750,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,2,['monitor'],['monitoring']
Energy Efficiency,scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]; ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618:5983,Adapt,AdaptedForkJoinTask,5983,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-215187618,1,['Adapt'],['AdaptedForkJoinTask']
Energy Efficiency,"t changesets/resync_engine_schema.xml::restore_auto_increment_call_caching_hash_entry_id_postgresql::kshakir:; Reason: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:637); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:53); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.Liquibase.update(Liquibase.java:202); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:67); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:39); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:156); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: ERROR: syntax error at or near ""as""; Position: 73 [Failed SQL: alter sequence ""CALL_CACHING_HASH_ENTRY_CALL_CACHING_HASH_ENTRY_ID_seq"" as bigint]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:356); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcEx",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5083:36030,adapt,adapted,36030,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5083,1,['adapt'],['adapted']
Energy Efficiency,"t; 	}; 	output {; 		File out = glob('*.txt')[0]; 	}; }; ```; This code does not work.; ```; $ java -jar ../cromwell-30.jar run test_conditionals_in_cromwell-30.wdl; Picked up _JAVA_OPTIONS: -Xms256M -Xmx1024M -XX:ParallelGCThreads=1; [2017-12-05 09:40:22,36] [info] Running with database db.url = jdbc:hsqldb:mem:ee347d5b-2cdf-4b76-b68a-dc5d09a93aeb;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 09:40:28,42] [info] Running migration RenameWorkflowOptionsInMetadata with a read batch size of 100000 and a write batch size of 100000; [2017-12-05 09:40:28,44] [info] [RenameWorkflowOptionsInMetadata] 100%; [2017-12-05 09:40:28,54] [info] Running with database db.url = jdbc:hsqldb:mem:68a1b424-aa08-4f22-bc04-952c5eb83e7e;shutdown=false;hsqldb.tx=mvcc; [2017-12-05 09:40:29,02] [info] Slf4jLogger started; [2017-12-05 09:40:29,28] [info] Metadata summary refreshing every 2 seconds.; [2017-12-05 09:40:29,29] [info] Starting health monitor with the following checks: DockerHub, Engine Database; [2017-12-05 09:40:29,30] [info] WriteMetadataActor configured to write to the database with batch size 200 and flush rate 5 seconds.; [2017-12-05 09:40:29,35] [info] CallCacheWriteActor configured to write to the database with batch size 100 and flush rate 3 seconds.; [2017-12-05 09:40:30,63] [info] SingleWorkflowRunnerActor: Submitting workflow; [2017-12-05 09:40:30,68] [info] Workflow 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5 submitted.; [2017-12-05 09:40:30,68] [info] SingleWorkflowRunnerActor: Workflow submitted 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5; [2017-12-05 09:40:30,69] [info] 1 new workflows fetched; [2017-12-05 09:40:30,69] [info] WorkflowManagerActor Starting workflow 6a6ee0eb-5576-43af-a64c-8ed7d288bbc5; [2017-12-05 09:40:30,70] [info] WorkflowManagerActor Successfully started WorkflowActor-6a6ee0eb-5576-43af-a64c-8ed7d288bbc5; [2017-12-05 09:40:30,70] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2017-12-05 09:40:31,66] [error] WorkflowManagerActor Workflow 6a6ee0eb-5576-43a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2992:1358,monitor,monitor,1358,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2992,1,['monitor'],['monitor']
Energy Efficiency,"that's a good question. I'd say to wire it in the same way as the current monitoring script option (don't have that answer easily available to me atm). . re why default off, i've learned to be conservative w/ these sorts of things",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451516509:74,monitor,monitoring,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510#issuecomment-451516509,1,['monitor'],['monitoring']
Energy Efficiency,"tion [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread3"" #8 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b8800 nid=0x9f0 waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread2"" #7 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b6800 nid=0x9ef waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread1"" #6 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b4800 nid=0x9ee waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread0"" #5 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b1800 nid=0x9ec waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""Signal Dispatcher"" #4 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b0000 nid=0x9ea waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007fdbcc27d800 nid=0x9e8 in Object.wait() [0x00007fdb8d2d9000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b4175f0> (a java.lang.ref.ReferenceQueue$Lock); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164); at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209). ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007fdbcc279000 nid=0x9e7 in Object.wait() [0x00007fdb8d3da000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at java.lang.ref.Reference.tryHandlePending(Reference.java:191); - locked <0x000000015b4177a8> (a java.lang.ref.Reference$Lock); at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153). ""main"" #1 prio=5 os_prio=0 tid=0x00007fdbcc00a000 nid=0x9d7 in Object.wait() [0x00007fdbd452c000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Thread.join(Threa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:8615,monitor,monitor,8615,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['monitor'],['monitor']
Energy Efficiency,tion(DataTypeFactory.java:251); 	at liquibase.change.core.CreateTableChange.generateStatements(CreateTableChange.java:70); 	at liquibase.change.AbstractChange.generateStatementsVolatile(AbstractChange.java:287); 	at liquibase.change.AbstractChange.warn(AbstractChange.java:358); 	at liquibase.changelog.visitor.ValidatingVisitor.visit(ValidatingVisitor.java:109); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:83); 	at liquibase.changelog.DatabaseChangeLog.validate(DatabaseChangeLog.java:269); 	at liquibase.Liquibase.update(Liquibase.java:198); 	at liquibase.Liquibase.update(Liquibase.java:179); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:62); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(LiquibaseUtils.scala:34); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1(ServicesStore.scala:11); 	at cromwell.services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```; with the linked change; https://github.com/jeremiahsavage/cromwell/commit/88f82a0d699184358149a17a5c1d957704cdced3; the database table creation proceeds; as it undoes one of the changes in this commit; https://github.com/broadinstitute/cromwell/commit/775d2cb414734080978cc9c2533cab41b4acfad5; ```; [jeremiah@localhost cromwell]$ java -Dconfig.file=,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4605:3378,adapt,adapted,3378,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4605,1,['adapt'],['adapted']
Energy Efficiency,tion.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 	at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:253); 	at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:59); 	at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:50); 	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:186); 	at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:44); 	at scala.collection.TraversableLike.to(TraversableLike.scala:590); 	at scala.collection.TraversableLike.to$(TraversableLike.scala:587); 	at scala.collection.AbstractTraversable.to(Traversable.scala:104); 	at scala.collection.TraversableOnce.toList(TraversableOnce.scala:294); 	at scala.collection.TraversableOnce.toList$(TraversableOnce.scala:294); 	at scala.collection.AbstractTraversable.toList(Traversable.scala:104); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4(EcmaScriptUtil.scala:111); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$4$adapted(EcmaScriptUtil.scala:107); 	at scala.collection.MapLike$MappedValues.$anonfun$foreach$3(MapLike.scala:253); 	at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:789); 	at scala.collection.immutable.Map$Map2.foreach(Map.scala:146); 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:788); 	at scala.collection.MapLike$MappedValues.foreach(MapLike.scala:253); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1(EcmaScriptUtil.scala:107); 	at cwl.internal.EcmaScriptUtil$.$anonfun$evalStructish$1$adapted(EcmaScriptUtil.scala:97); 	at cwl.internal.EnhancedRhinoSandbox.eval(EnhancedRhinoSandbox.scala:61); 	at cwl.internal.EcmaScriptUtil$.evalRaw(EcmaScriptUtil.scala:69); 	at cwl.internal.EcmaScriptUtil$.evalStructish(EcmaScriptUtil.scala:97); 	at cwl.ExpressionEvaluator$.eval(ExpressionEvaluator.scala:76); 	at cwl.ExpressionEvaluator$.evaluator$1(ExpressionEvaluator.scala:40); 	at cwl.ExpressionEvaluator$.$anonfun$eva,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787:2077,adapt,adapted,2077,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3012#issuecomment-377570787,1,['adapt'],['adapted']
Energy Efficiency,"titute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/async/AsyncBackendJobExecutionActor.scala) file, and some of the important stores would be the [ExecutionStore](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/core/src/main/scala/cromwell/core/ExecutionStore.scala), the [BackendJobDescriptor and BackendJobDescriptorKey](https://github.com/broadinstitute/cromwell/blob/832387f34f57062abd2ce6cfa9e206407170ba72/backend/src/main/scala/cromwell/backend/package.scala#L17-31), which contain the [Call containing the AST](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Call.scala#L10-61) and sequence of [Tasks](https://github.com/broadinstitute/wdl4s/blob/d7e19c9f4dfbc5ad912cf641af9c640eb8a9a9c7/src/main/scala/wdl4s/Task.scala). Since the WorkflowManagerActor (WMA) is just an asynchronous queue selecting the workflow based on the root and its dependencies, then it sounds to be just a scheduling pool service submitting to the EJEA, which prepares it for the specific backend. The recovery for the EJEA is assumed to be an uniform designed protocol, which prepares the execution for the specific backend. . Regarding the backend recovery, since at the core the implementations is really Java (even though everything is in Scala), one can save the running state periodically through serialized snapshots, using something like [Apache JavaFlow](http://commons.apache.org/sandbox/commons-javaflow/) or another similar approach. If this becomes too cumbersome and the cost of resubmitting a job to a specific Backend is on the average time-span not excessive, then resubmitting the whole job might be Occam's razor. There are other approaches, depending on the preferability of flexibility, and I am sure I might have miswrote/misinterpreted something here based on my periodic analysis of the source code - so feel free to correct me :). Thanks,; ~p",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371:1239,schedul,scheduling,1239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1117#issuecomment-230645371,1,['schedul'],['scheduling']
Energy Efficiency,"titute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL21vZGVsL2RyYWZ0Mi9zcmMvbWFpbi9zY2FsYS93ZGwvZHJhZnQyL21vZGVsL3BhY2thZ2Uuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...ool/src/main/scala/womtool/validate/Validate.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d29tdG9vbC9zcmMvbWFpbi9zY2FsYS93b210b29sL3ZhbGlkYXRlL1ZhbGlkYXRlLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...king/expression/files/BiscayneFileEvaluators.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmlzY2F5bmUvbGlua2luZy9leHByZXNzaW9uL2ZpbGVzL0Jpc2NheW5lRmlsZUV2YWx1YXRvcnMuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | [...in/scala/cromwell/backend/impl/bcs/BcsDocker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c3VwcG9ydGVkQmFja2VuZHMvYmNzL3NyYy9tYWluL3NjYWxhL2Nyb213ZWxsL2JhY2tlbmQvaW1wbC9iY3MvQmNzRG9ja2VyLnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | [...in/scala/cromwell/services/metadata/metadata.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree#diff-c2VydmljZXMvc3JjL21haW4vc2NhbGEvY3JvbXdlbGwvc2VydmljZXMvbWV0YWRhdGEvbWV0YWRhdGEuc2NhbGE=) | `0% <0%> (-100%)` | :arrow_down: |; | ... and [645 more](https://codecov.io/gh/broadinstitute/cromwell/pull/4947/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=footer). Last update [26085f5...01c37f1](https://codecov.io/gh/broadinstitute/cromwell/pull/4947?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620:4433,Power,Powered,4433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4947#issuecomment-491028620,1,['Power'],['Powered']
Energy Efficiency,"to the workflow). It's also much easier than the current state-of-the-art, i.e. parsing task-level monitoring logs. 2) Scripts can easily get aggregate statistics on resource utilization and could produce suggestions based on those. This could provide a path towards automatic runtime configuration based on the models trained with historical data. One could also detect situations like out-of-memory calls and automatically adjust resources according to those. It would also be pretty easy to add logic for estimation of task call-level cost based on the pricing of associated resources. This could provide a long-sought feature of real-time cost monitoring/control (thanks to @TimothyTickle for the suggestion). Monitoring is done using the new ""monitoring action"" for PAPIv2, which currently uses the hard-coded [quay.io/broadinstitute/cromwell-monitor](https://quay.io/repository/broadinstitute/cromwell-monitor) image, built from https://github.com/broadinstitute/cromwell-monitor (I wasn't sure if that code belonged here or in a separate repo). This is advantageous to just using it as a _monitoring_script_, because it removes all assumptions on the ""user"" Docker image (for the task itself). For example, we don't have to assume a particular distribution or presence of Python and its libraries. So it should work exactly the same for any task. Per @geoffjentry's suggestion, we've [consulted](https://groups.google.com/forum/#!topic/google-genomics-discuss/caYM7oHbfx0) with the Google Genomics team, and they don't see any apparent issues with the concept. We could expose this as a workflow option like `monitoring_image`, and allow configuring it at the Cromwell level, so e.g. any user of Terra (or any other hosted Cromwell with PAPIv2 backend) could get usage reports without having to configure anything. The metrics are reported in their GCP project, so a user gets automatic access to them as long as they're a viewer. We could also easily expose a link to workflow- and task-level ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4510:1614,monitor,monitor,1614,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4510,1,['monitor'],['monitor']
Energy Efficiency,tor.scala:1417); 	at scala.collection.IterableLike.foreach(IterableLike.scala:71); 	at scala.collection.IterableLike.foreach$(IterableLike.scala:70); 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54); 	at slick.dbio.SynchronousDatabaseAction$FusedAndThenAction.run(DBIOAction.scala:534); 	at slick.dbio.SynchronousDatabaseAction$$anon$11.run(DBIOAction.scala:571); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:240); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:240); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.hsqldb.HsqlException: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.error.Error.error(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	... 42 common frames omitted; Caused by: java.lang.OutOfMemoryError: Java heap space; 	at org.hsqldb.persist.LobStoreMem.setBlockBytes(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesISNormal(Unknown Source); 	at org.hsqldb.persist.LobManager.setBytesIS(Unknown Source); 	at org.hsqldb.persist.LobManager.setCharsForNewClob(Unknown Source); 	at org.hsqldb.SessionData.allocateLobForResult(Unknown Source); 	at org.hsqldb.Session.allocateResultLob(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.performPreExecute(Unknown Source); 	at org.hsqldb.jdbc.JDBCPreparedStatement.addBatch(Unknown Source); 	at com.zaxxer.hikari.pool.HikariProxyPreparedStatement.addBatch(HikariProxyPreparedStatement.java); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$MultiInsertAction.$anonfun$run$15(JdbcActionComponent.scala:531); 	at slick.jdbc.JdbcActionComponent$InsertActionComposerImpl$Mu,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3387:5772,allocate,allocateResultLob,5772,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3387,2,['allocate'],['allocateResultLob']
Energy Efficiency,"tter). When the scatter is reading the metadata, in the case in which there is no barcode the WDL is interrupted with this error: . ```; ""message"": ""Failed to evaluate 'scMethTask3.barcode' (reason 1 of 1): Evaluating files_and_metadata_row[3] failed: Failed to find index Success(WomInteger(3)); on array:\n\nSuccess([\""SRR5395068\"", \""SRR5395068_1.fastq.gz\"", \""SRR5395068_2.fastq.gz\""])\n\n3"",; ""causedBy"": []; ```; How can i avoid this? Or is there a way to accomplish what I am trying to do?. ### Which backend are you running? ; Unix terminal within slurm scheduler. ### Example meta_data files:; 1) without barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz	; ```; 2) with barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz ATCGCT	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz ATCGGA; ```; ### Below my workflow:. workflow scMethTask3 {. #information about the monitoring scrip and the number of samples; File? monitoring_script; File meta_data. #information for trimming the cell barcode; File command; Int bases; ; #information for trimming the adapters and low quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memory_task1; Int memory_task2. #Start the call. Array[Array[String]] files_and_metadata = read_tsv(meta_data). scatter(files_and_metadata_row in files_and_metadata) {; String sampleName = files_and_metadata_row[0]; File f1 = files_and_metadata_row[1]; File f2 = files_and_metadata_row[2]; String? barcode = files_and_metadata_row[3]; #if the barcode is passed, proceed with it.; if (defined(barcode)) {; call trimCellBarcode {; input:; f1=f1,; f2=f2,; sampleName=sampleName,; barcode=barcode,; monitoring_script=monitoring_script,; command=command,; memory_task1=memory_task1,; bases=bases; }; }; #if t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:1569,monitor,monitoring,1569,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,1,['monitor'],['monitoring']
Energy Efficiency,"two thumbs, green tests, and it's holding up workbench... so merging!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1714#issuecomment-263888360:12,green,green,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1714#issuecomment-263888360,1,['green'],['green']
Energy Efficiency,"ubmit$2(Execute.scala:269) at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16) at sbt.Execute.work(Execute.scala:278) at sbt.Execute.$anonfun$submit$1(Execute.scala:269) at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178) at sbt.CompletionService$$anon$2.call(CompletionService.scala:37) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Cause: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://test-system-6/user/$l#-102797778]] after [30000 ms]. Sender[Actor[akka://test-system-6/system/testActor-24#-1294021439]] sent message of type ""cromwell.engine.workflow.SingleWorkflowRunnerActor$RunWorkflow$"". at akka.pattern.PromiseActorRef$.$anonfun$defaultOnTimeout$1(AskSupport.scala:596) at akka.pattern.PromiseActorRef$.$anonfun$apply$1(AskSupport.scala:606) at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205) at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:870) at scala.concurrent.BatchingExecutor.execute(BatchingExecutor.scala:109) at scala.concurrent.BatchingExecutor.execute$(BatchingExecutor.scala:103) at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:868) at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(LightArrayRevolverScheduler.scala:328) at akka.actor.LightArrayRevolverScheduler$$anon$4.executeBucket$1(LightArrayRevolverScheduler.scala:279) at akka.actor.LightArrayRevolverScheduler$$anon$4.nextTick(LightArrayRevolverScheduler.scala:283) at akka.actor.LightArrayRevolverScheduler$$anon$4.run(LightArrayRevolverScheduler.scala:235) at java.lang.Thread.run(Thread.java:748)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4350:6698,Schedul,Scheduler,6698,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4350,2,['Schedul'],['Scheduler']
Energy Efficiency,un$46.apply(SlickDataAccess.scala:569) ~[cromwell.jar:0.19]; 905198- at cromwell.engine.db.slick.SlickDataAccess$$anonfun$46.apply(SlickDataAccess.scala:568) ~[cromwell.jar:0.19]; 905199- at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; 905200- at slick.backend.DatabaseComponent$DatabaseDef$$anonfun$runInContext$1.apply(DatabaseComponent.scala:146) ~[cromwell.jar:0.19]; 905201- at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) ~[cromwell.jar:0.19]; 905202- at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:249) ~[cromwell.jar:0.19]; 905203- at scala.concurrent.impl.CallbackRunnable.run_aroundBody0(Promise.scala:32) ~[cromwell.jar:0.19]; 905204- at scala.concurrent.impl.CallbackRunnable$AjcClosure1.run(Promise.scala:1) ~[cromwell.jar:0.19]; 905205- at org.aspectj.runtime.reflect.JoinPointImpl.proceed(JoinPointImpl.java:149) ~[cromwell.jar:0.19]; 905206- at kamon.scala.instrumentation.FutureInstrumentation$$anonfun$aroundExecution$1.apply(FutureInstrumentation.scala:44) ~[cromwell.jar:0.19]; 905207- at kamon.trace.Tracer$.withContext(TracerModule.scala:53) ~[cromwell.jar:0.19]; 905208- at kamon.scala.instrumentation.FutureInstrumentation.aroundExecution(FutureInstrumentation.scala:43) ~[cromwell.jar:0.19]; 905209- at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:31) ~[cromwell.jar:0.19]; 905210- at scala.concurrent.impl.ExecutionContextImpl$AdaptedForkJoinTask.exec(ExecutionContextImpl.scala:121) ~[cromwell.jar:0.19]; 905211- at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [cromwell.jar:0.19]; 905212- at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [cromwell.jar:0.19]; 905213- at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [cromwell.jar:0.19]; 905214- at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [cromwell.jar:0.19]. ```,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102:7168,Adapt,AdaptedForkJoinTask,7168,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102,1,['Adapt'],['AdaptedForkJoinTask']
Energy Efficiency,"un.nio.ch.SelectorImpl.select(SelectorImpl.java:97); at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101); at akka.io.SelectionHandler$ChannelRegistryImpl$$anon$3.tryRun(SelectionHandler.scala:114); at akka.io.SelectionHandler$ChannelRegistryImpl$Task.run(SelectionHandler.scala:215); at akka.io.SelectionHandler$ChannelRegistryImpl$$anon$3.run(SelectionHandler.scala:147); at akka.util.SerializedSuspendableExecutionContext.run$1(SerializedSuspendableExecutionContext.scala:64); at akka.util.SerializedSuspendableExecutionContext.run(SerializedSuspendableExecutionContext.scala); at akka.dispatch.TaskInvocation.run(Redefined); at java.util.concurrent.ThreadPoolExecutor.runWorker(Redefined); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Redefined). ""Abandoned connection cleanup thread"" #22 daemon prio=5 os_prio=31 tid=0x00007fb76a4f5800 nid=0x7103 in Object.wait() [0x000000012ccb5000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x00000006c0624180> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""ForkJoinPool-3-worker-15"" #19 daemon prio=5 os_prio=31 tid=0x00007fb76abaa800 nid=0x6b03 waiting on condition [0x000000012a1e3000]; java.lang.Thread.State: WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x00000006c0060ab0> (a java.util.concurrent.CountDownLatch$Sync); at java.util.concurrent.locks.LockSupport.park(Redefined); at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836); at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997); at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchro",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/844:43994,monitor,monitor,43994,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/844,1,['monitor'],['monitor']
Energy Efficiency,unLoop.scala:362); 	at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); 	at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:337); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:258); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:351); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:372); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:312); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5178:3403,adapt,adapted,3403,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5178,1,['adapt'],['adapted']
Energy Efficiency,unLoop.scala:366); 	at cats.effect.internals.Trampoline.cats$effect$internals$Trampoline$$immediateLoop(Trampoline.scala:70); 	at cats.effect.internals.Trampoline.startLoop(Trampoline.scala:36); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.super$startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.$anonfun$startLoop$1(TrampolineEC.scala:93); 	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23); 	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:85); 	at cats.effect.internals.TrampolineEC$JVMTrampoline.startLoop(TrampolineEC.scala:93); 	at cats.effect.internals.Trampoline.execute(Trampoline.scala:43); 	at cats.effect.internals.TrampolineEC.execute(TrampolineEC.scala:44); 	at cats.effect.internals.ForwardCancelable.loop$1(ForwardCancelable.scala:46); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1(ForwardCancelable.scala:52); 	at cats.effect.internals.ForwardCancelable.$anonfun$cancel$1$adapted(ForwardCancelable.scala:52); 	at cats.effect.internals.IORunLoop$RestartCallback.start(IORunLoop.scala:341); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:119); 	at cats.effect.internals.IORunLoop$.start(IORunLoop.scala:34); 	at cats.effect.IO.unsafeRunAsync(IO.scala:257); 	at cats.effect.internals.IORace$.onSuccess$1(IORace.scala:40); 	at cats.effect.internals.IORace$.$anonfun$simple$4(IORace.scala:79); 	at cats.effect.internals.IORace$.$anonfun$simple$4$adapted(IORace.scala:77); 	at cats.effect.internals.IORunLoop$.cats$effect$internals$IORunLoop$$loop(IORunLoop.scala:136); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:355); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:376); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:316); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5925:2719,adapt,adapted,2719,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5925,1,['adapt'],['adapted']
Energy Efficiency,"uns bespoke configured PBSPro. I have successfully managed to run ""hello world"" example workflow using the following configuration for the backend. However, I am unable to modify certain parameters as errors are thrown. . My current configuration is as follows:. ```; runtime-attributes = """"""; Int cpu = 1; Int memory = 1; String raijin_queue = ""express""; String walltime = ""01:00:00""; String jobfs = ""1GB""; String raijin_project_id = ""myproject""; """"""; #Submit string when there is no ""docker"" runtime attribute.; submit = """"""; qsub \; -V \; -N ${job_name} \; -o ${out}.qsub \; -e ${err}.qsub \; -l ncpus=${cpu} \; -l mem=${memory}""GB"" \; -l walltime=${walltime} \; -l jobfs=${jobfs} \; ${""-q "" + raijin_queue} \; -P ${raijin_project_id} \; ${script}; """"""; ```. My specific questions:. 1. I have tried `Float memory_gb = 1.0` as the runtime attribute and `${""-l mem="" + memory_gb + ""GB""}` as the submit string but this fails with `qsub: Illegal attribute or resource value Resource_List.mem` error. Could you please help me with the correct formatting of this attribute? I have copied structure of this from [SGE.conf](https://github.com/broadinstitute/cromwell/blob/787943c0eda793fcc407a3e748b56805f4a2795b/cromwell.example.backends/SGE.conf).; 2. I would like to use `$PROJECT` environment variable as the default value for `raijin_project_id` runtime attribute so that each user can run the same workflow without modification within their allocated project. Is there a way to use environment variable in the config file? I tried ${?PROJECT} and ${PROJECT} as per the recommendations for HOCON but to no avail. I am yet to understand the syntax of HOCON completely to solve this but your help at this time would be much appreciated.; 3. `jobfs` is a parameter used to control scratch space local to the execution node. Currently it is being passed as a string. Is there a way to convert that into GB same as memory but without the use of keyword memory?; Thank you so much for your efforts.; Hardip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4967:1650,allocate,allocated,1650,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4967,1,['allocate'],['allocated']
Energy Efficiency,"unts:; - disk: local-disk; path: /cromwell_root; - commands:; - -c; - printf '%s %s\n' ""$(date -u '+%Y/%m/%d %H:%M:%S')"" Done\ delocalization.; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; logging: Delocalization; timeout: 300s; - alwaysRun: true; commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/1xxxxxx.sh && chmod u+x /tmp/1xxxxxx.sh; && sh /tmp/1xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; - alwaysRun: true; commands:; - -c; - python -c 'import base64; print(base64.b64decode(""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh; entrypoint: /bin/sh; imageUri: gcr.io/google.com/cloudsdktool/cloud-sdk:276.0.0-slim; labels:; tag: Delocalization; environment:; MEM_SIZE: '2.0'; MEM_UNIT: GB; resources:; virtualMachine:; bootDiskSizeGb: 12; bootImage: projects/cos-cloud/global/images/family/cos-stable; disks:; - name: local-disk; sizeGb: 10; type: pd-ssd; labels:; cromwell-workflow-id: xxxxxx; goog-pipelines-worker: 'true'; wdl-task-name: hello; machineType: custom-1-2048; network: {}; nvidiaDriverVersion: 450.51.06; serviceAccount:; email: default; scopes:; - https://www.googleapis.com/auth/compute; - https://www.googleapis.com/auth/devstorage.full_control; - https://www.googleapis.com/auth/cloudkms; - https://www.googleapis.com/auth/userinfo.email; - https://www.googleapis.com/auth/userinfo.profile; - https://www.googleapis.com/auth/monitoring.write; - https://www.googleapis.com/auth/bigquery; - https://www.googleapis.com/auth/cloud-platform; volumes:; - persistentDisk:; sizeGb: 10; type: pd-ssd; volume: local-disk; zones:; - us-central1-a; - us-central1-b; timeout: 604800s; startTime: '2021-08-03T15:22:07.789742627Z'; name: projects/xxxxxx/locations/us-central1/operations/xxxxxx; response:; '@type': type.googleapis.com/cloud.lifesciences.pipelines.RunPipelineResponse. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:21640,monitor,monitoring,21640,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['monitor'],['monitoring']
Energy Efficiency,"uteAsync(StandardAsyncExecutionActor.scala:637); at cromwell.backend.standard.StandardAsyncExecutionActor.executeAsync$(StandardAsyncExecutionActor.scala:637); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeAsync(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover(StandardAsyncExecutionActor.scala:952); at cromwell.backend.standard.StandardAsyncExecutionActor.executeOrRecover$(StandardAsyncExecutionActor.scala:944); at cromwell.backend.impl.sfs.config.DispatchedConfigAsyncJobExecutionActor.executeOrRecover(ConfigAsyncJobExecutionActor.scala:211); at cromwell.backend.async.AsyncBackendJobExecutionActor.$anonfun$robustExecuteOrRecover$1(AsyncBackendJobExecutionActor.scala:65); at cromwell.core.retry.Retry$.withRetry(Retry.scala:38); at cromwell.core.retry.Retry$$anonfun$withRetry$1.$anonfun$applyOrElse$3(Retry.scala:45); at akka.pattern.FutureTimeoutSupport.liftedTree1$1(FutureTimeoutSupport.scala:26); at akka.pattern.FutureTimeoutSupport.$anonfun$after$1(FutureTimeoutSupport.scala:26); at akka.actor.Scheduler$$anon$4.run(Scheduler.scala:205); ... 6 more. </code></pre>; </details>. This is a minimal example of a config which gets such an error:; `Could not evaluate expression: ""echo "" + memory: Cannot perform operation: echo + WomLong(4)`; ```; include required(classpath(""application"")); webservice {; port = 8000; }; backend {; default=""Local""; providers {; Local {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int? memory; String? docker; String? docker_user; """"""; submit = """"""; bash ${script}; ${""echo "" + memory}; """"""; }; }; }; } ; ```. This means that the launch command given in the cromwell docs [here](https://cromwell.readthedocs.io/en/stable/backends/SGE/) will not work. A current workaround would be to use an expression like this instead:; `${true=""echo"" false="""" defined(memory)} ${memory}`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4659:5161,Schedul,Scheduler,5161,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4659,2,['Schedul'],['Scheduler']
Energy Efficiency,"utor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Hikari Housekeeping Timer (pool db)"" #35 daemon prio=5 os_prio=0 tid=0x00007fdaf8212800 nid=0xa0b waiting on condition [0x00007fdb80cd2000]; java.lang.Thread.State: TIMED_WAITING (parking); at sun.misc.Unsafe.park(Native Method); - parking to wait for <0x000000015b74b1f0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject); at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215); at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:1093); at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:809); at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). ""Abandoned connection cleanup thread"" #34 daemon prio=5 os_prio=0 tid=0x00007fdaf81fc000 nid=0xa0a in Object.wait() [0x00007fdb80fd3000]; java.lang.Thread.State: TIMED_WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b76b538> (a java.lang.ref.ReferenceQueue$Lock); at com.mysql.jdbc.AbandonedConnectionCleanupThread.run(AbandonedConnectionCleanupThread.java:43). ""Service Thread"" #17 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d4000 nid=0x9f9 runnable [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C1 CompilerThread11"" #16 daemon prio=9 os_prio=0 tid=0x00007fdbcc2d",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:5455,Schedul,ScheduledThreadPoolExecutor,5455,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['Schedul'],['ScheduledThreadPoolExecutor']
Energy Efficiency,val$Defer.value(Eval.scala:257); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:73); 	at cats.instances.ListInstances$$anon$1.traverse(list.scala:12); 	at cats.Traverse$Ops.traverse(Traverse.scala:19); 	at cats.Traverse$Ops.traverse$(Traverse.scala:19); 	at cats.Traverse$ToTraverseOps$$anon$3.traverse(Traverse.scala:19); 	at cromwell.core.path.PathBuilderFactory$.instantiatePathBuilders(PathBuilderFactory.scala:23); 	at cromwell.engine.EngineFilesystems$.pathBuildersForWorkflow(EngineFilesystems.scala:27); 	at cromwell.engine.workflow.WorkflowActor$$anonfun$8.bruteForcePathBuilders$1(WorkflowActor.scala:432); 	at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:436); 	at cromwell.engine.workflow.WorkflowActor$$anonfun$8.applyOrElse(WorkflowActor.scala:407); 	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:34); 	at akka.actor.FSM.$anonfun$handleTransition$1(FSM.scala:627); 	at akka.actor.FSM.$anonfun$handleTransition$1$adapted(FSM.scala:627); 	at scala.collection.immutable.List.foreach(List.scala:389); 	at akka.actor.FSM.handleTransition(FSM.scala:627); 	at akka.actor.FSM.makeTransition(FSM.scala:709); 	at akka.actor.FSM.makeTransition$(FSM.scala:702); 	at cromwell.engine.workflow.WorkflowActor.makeTransition(WorkflowActor.scala:186); 	at akka.actor.FSM.applyState(FSM.scala:694); 	at akka.actor.FSM.applyState$(FSM.scala:692); 	at cromwell.engine.workflow.WorkflowActor.applyState(WorkflowActor.scala:186); 	at akka.actor.FSM.processEvent(FSM.scala:689); 	at akka.actor.FSM.processEvent$(FSM.scala:681); 	at cromwell.engine.workflow.WorkflowActor.akka$actor$LoggingFSM$$super$processEvent(WorkflowActor.scala:186); 	at akka.actor.LoggingFSM.processEvent(FSM.scala:820); 	at akka.actor.LoggingFSM.processEvent$(FSM.scala:802); 	at cromwell.engine.workflow.WorkflowActor.processEvent(WorkflowActor.scala:186); 	at akka.actor.FSM.akka$actor$FSM$$processMsg(FSM.scala:678); 	at akka.actor.FSM$$anonfun$receiv,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4916:2996,adapt,adapted,2996,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4916,1,['adapt'],['adapted']
Energy Efficiency,"w/ Chris when he gets back next week as we introduced the reference equality in the first place and I'm not aware of his motivation to do so. ---. @curoli commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334577609). The performance issues aren't down the road. When I try to build a WOM; graph right now, it slows down after the first 100 nodes and never finishes. On Thu, Oct 5, 2017 at 4:01 PM, Dan Billings <notifications@github.com>; wrote:. > I suggest we leave this as-is with the understanding that it could be a; > performance issue down the road.; >; > rework the whole thing later; > This is a specific anti-goal.; >; > As I suggested, I would like to discuss w/ Chris when he gets back next; > week as we introduced the reference equality in the first place and I'm not; > aware of his motivation to do so.; >; > â€”; > You are receiving this because you were assigned.; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334575875>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AG_4aJnzYP8ru5JvHrjbR5jwKwO9Brncks5spTV8gaJpZM4PttJd>; > .; >. -- ; Oliver Ruebenacker; Senior Software Engineer, Diabetes Portal; <http://www.type2diabetesgenetics.org/>, Broad Institute; <http://www.broadinstitute.org/>. ---. @geoffjentry commented on [Thu Oct 05 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-334577969). +1 on waiting for chris. @curoli- at the moment our priority is getting our own development of this as efficient as possible, not supporting users trying to make use of it. ---. @cjllanwarne commented on [Wed Oct 11 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-335851314). Resolved in person but will be done in the new composite cromwell repo. ---. @katevoss commented on [Wed Oct 11 2017](https://github.com/broadinstitute/wdl4s/issues/248#issuecomment-335852862). This PR will be in Cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2694:4247,efficient,efficient,4247,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2694,2,['efficient'],['efficient']
Energy Efficiency,"wdl; [2015-12-18 08:43:13,439] [info] Inputs: hello.json; [2015-12-18 08:43:13,560] [info] input: test.hello.name => ""world""; [2015-12-18 08:43:13,776] [info] SingleWorkflowRunnerActor: launching workflow; [2015-12-18 08:43:15,936] [info] Running with database db.url = jdbc:hsqldb:mem:86473284-494c-43d2-94fd-d00107a2a787;shutdown=false;hsqldb.tx=mvcc; [2015-12-18 08:43:17,516] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = e67af113-c3a7-41f4-9178-6640c1c652e9; [2015-12-18 08:43:17,592] [info] WorkflowManagerActor Found no workflows to restart.; [2015-12-18 08:43:18,816] [error] SingleWorkflowRunnerActor: Ask timed out on [Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312]] after [5000 ms]; akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312]] after [5000 ms]; at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:334); at akka.actor.Scheduler$$anon$7.run(Scheduler.scala:117); at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:599); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:597); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467); at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375); at java.lang.Thread.run(Thread.java:745); [2015-12-18 08:43:19,174] [info] Message [cromwell.engine.workflow.WorkflowManagerActor$RestartWorkflows] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configurati",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/334:1407,Schedul,Scheduler,1407,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/334,1,['Schedul'],['Scheduler']
Energy Efficiency,"we had emailed Dion about this a few weeks back, he said, . > ""Yep, we do run more tasks in prod, but even with those tasks we can not guarantee 100% of RPCs succeeding. Internally we do retry any backend dependencies silently (may manifest in slightly higher response times), but it's not unexpected to have a few sneak through. For these situations it's advisable to have a backoff / retry for 5xx level errors that are clearly a problem on our end.; > I've checked back on the time range on those two operations, there doesn't seem to be any wide spread issues during that time on our end. We do have monitoring on the unexpected error rates, would you say your error rates are higher than 0.1 or 0.01% ?â€‚â€‚(per RPC call vs per operation, as I think you poll each operation a significant number of times?)."". In conversation, Miguel said:. > ""We have a retry on this call, but it does not back off very aggressively. I'll make a note of it with the Cromwell devs."". Almost all of these failures happened on 5/25. It seems like JES is mostly available, but when unavailable this error causes almost everything running to fail.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850:604,monitor,monitoring,604,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/903#issuecomment-222799850,1,['monitor'],['monitoring']
Energy Efficiency,"when the status does not have text, I see this in the console log:; `Unknown workflow status: null` `compiled.js:5187`. ```; broadfcui.page.workspace.monitor.common.icon_for_wf_status=function(a){if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_success_statuses,a))return broadfcui.page.workspace.monitor.common.success_icon;if(cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_running_statuses,a))return broadfcui.page.workspace.monitor.common.running_icon;cljs.core.contains_QMARK_.call(null,broadfcui.page.workspace.monitor.common.wf_failure_statuses,a)||console.log(cljs.core.identity.call(null,; ""Unknown workflow status: ""),cljs.core.identity.call(null,a));return broadfcui.page.workspace.monitor.common.failure_icon};broadfcui.page.workspace.monitor.common.icon_for_sub_status=function(a){if(cljs.core.contains_QMARK_.call(null,a,""Failed""))return broadfcui.page.workspace.monitor.common.failure_icon;if(cljs.core.contains_QMARK_.call(null,a,""Succeeded""))return broadfcui.page.workspace.monitor.common.success_icon;console.log(cljs.core.identity.call(null,""Unknown submission status""));return null};; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010:150,monitor,monitor,150,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1978#issuecomment-279501010,10,['monitor'],['monitor']
Energy Efficiency,"wth is going on with Coveralls, we get green ticks for -0.2% now?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210132769:39,green,green,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210132769,1,['green'],['green']
Energy Efficiency,"x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""C2 CompilerThread0"" #5 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b1800 nid=0x9ec waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""Signal Dispatcher"" #4 daemon prio=9 os_prio=0 tid=0x00007fdbcc2b0000 nid=0x9ea waiting on condition [0x0000000000000000]; java.lang.Thread.State: RUNNABLE. ""Finalizer"" #3 daemon prio=8 os_prio=0 tid=0x00007fdbcc27d800 nid=0x9e8 in Object.wait() [0x00007fdb8d2d9000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:143); - locked <0x000000015b4175f0> (a java.lang.ref.ReferenceQueue$Lock); at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:164); at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209). ""Reference Handler"" #2 daemon prio=10 os_prio=0 tid=0x00007fdbcc279000 nid=0x9e7 in Object.wait() [0x00007fdb8d3da000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Object.wait(Object.java:502); at java.lang.ref.Reference.tryHandlePending(Reference.java:191); - locked <0x000000015b4177a8> (a java.lang.ref.Reference$Lock); at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:153). ""main"" #1 prio=5 os_prio=0 tid=0x00007fdbcc00a000 nid=0x9d7 in Object.wait() [0x00007fdbd452c000]; java.lang.Thread.State: WAITING (on object monitor); at java.lang.Object.wait(Native Method); at java.lang.Thread.join(Thread.java:1245); - locked <0x000000015d89d070> (a scala.sys.ShutdownHookThread$$anon$1); at java.lang.Thread.join(Thread.java:1319); at java.lang.ApplicationShutdownHooks.runHooks(ApplicationShutdownHooks.java:106); at java.lang.ApplicationShutdownHooks$1.run(ApplicationShutdownHooks.java:46); at java.lang.Shutdown.runHooks(Shutdown.java:123); at java.lang.Shutdown.sequence(Shutdown.java:167); at java.lang.Shutdown.exit(Shutdown.java:212); - locked <0x000000015b6815a8> (a java.la",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914:9096,monitor,monitor,9096,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258913914,1,['monitor'],['monitor']
Energy Efficiency,xecution.callcaching.FetchCachedResultsActor.$anonfun$new$2(FetchCachedResultsActor.scala:32); at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:234); at scala.collection.Iterator.foreach(Iterator.scala:944); at scala.collection.Iterator.foreach$(Iterator.scala:944); at scala.collection.AbstractIterator.foreach(Iterator.scala:1432); at scala.collection.IterableLike.foreach(IterableLike.scala:71); at scala.collection.IterableLike.foreach$(IterableLike.scala:70); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike.map(TraversableLike.scala:234); at scala.collection.TraversableLike.map$(TraversableLike.scala:227); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1(FetchCachedResultsActor.scala:32); at cromwell.engine.workflow.lifecycle.execution.callcaching.FetchCachedResultsActor.$anonfun$new$1$adapted(FetchCachedResultsActor.scala:30); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91); at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:44); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ```; I,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4023:2117,adapt,adapted,2117,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4023,1,['adapt'],['adapted']
Energy Efficiency,"xt task. The next task takes in a file of files. ; The python inside the wdl is very counter-intuitive, prone to error, and unnecessary in other execution managers. See my real example below... ``` wdl; workflow crsp_validation_workflow {. ....snip....; Array[Array[File]] triplet_file_array = read_tsv(input_triplet_file_list); Float ploidy=""2"". scatter (triplet in triplet_file_array) {; ....snip.... call run_sensitivity_precision {; input:; entity_id=triplet[0],; oncotated_target_seg_gt_file = oncotate.oncotated_target_seg_gt_file,; ploidy=ploidy; }; }. call run_plot_purity_series {; input:; output_dir=""plots/"",; amp_sens_prec=run_sensitivity_precision.amp_sens_prec_file,; del_sens_prec=run_sensitivity_precision.del_sens_prec_file,; small_sens=run_sensitivity_precision.small_sens_file; }; }; ....snip....; task run_sensitivity_precision {; File oncotated_target_seg_gt_file; Float ploidy; String entity_id. command {; # Ignore chromosome 2, since the normal has this event and HCC1143T does not, so ground truth may be off, since; # detection of deletions could be reduced. Chromosome 6 may have a similar issue.; run_sensitivity_precision -i ""[2]"" ${oncotated_target_seg_gt_file} ${ploidy} ${entity_id}.sens_prec; }. output {; File amp_sens_prec_file = ""${entity_id}.sens_prec.amp.tsv""; File del_sens_prec_file = ""${entity_id}.sens_prec.del.tsv""; File small_sens_file = ""${entity_id}.sens_prec.small_segs.tsv""; File gene_segs_sens_prec_file = ""${entity_id}.sens_prec.gene_seg""; }. runtime {; docker: ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; memory: ""2GB""; }; }. task run_plot_purity_series {; String output_dir; Array[File] amp_sens_prec; Array[File] del_sens_prec; Array[File] small_sens. command {; ################# HERE; python <<CODE; files = ""${sep="","" amp_sens_prec}"".split("",""); files.extend(""${sep="","" del_sens_prec}"".split("","")); with open(""sens_prec_aggregate.txt"", ""w"") as fp:; fp.write('\n'.join(files)); CODE; wc -l sens_prec_aggregate.txt. python <<CODE;",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1263:1212,reduce,reduced,1212,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1263,1,['reduce'],['reduced']
Energy Efficiency,"yYy9tYWluL3NjYWxhL2N3bC9Xb3JrZmxvdy5zY2FsYQ==) | `95.52% <100%> (+7.46%)` | :arrow_up: |; | [...ain/scala/wdl/transforms/base/wdlom2wom/Util.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvbmV3LWJhc2Uvc3JjL21haW4vc2NhbGEvd2RsL3RyYW5zZm9ybXMvYmFzZS93ZGxvbTJ3b20vVXRpbC5zY2FsYQ==) | `100% <100%> (Ã¸)` | :arrow_up: |; | [...dlom2wom/WdlDraft2WomWorkflowDefinitionMaker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvZHJhZnQyL3NyYy9tYWluL3NjYWxhL3dkbC90cmFuc2Zvcm1zL2RyYWZ0Mi93ZGxvbTJ3b20vV2RsRHJhZnQyV29tV29ya2Zsb3dEZWZpbml0aW9uTWFrZXIuc2NhbGE=) | `87.5% <75%> (-12.5%)` | :arrow_down: |; | [...m2wom/WdlDraft2WomCommandTaskDefinitionMaker.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-d2RsL3RyYW5zZm9ybXMvZHJhZnQyL3NyYy9tYWluL3NjYWxhL3dkbC90cmFuc2Zvcm1zL2RyYWZ0Mi93ZGxvbTJ3b20vV2RsRHJhZnQyV29tQ29tbWFuZFRhc2tEZWZpbml0aW9uTWFrZXIuc2NhbGE=) | `95.23% <75%> (-4.77%)` | :arrow_down: |; | [.../scala/cromiam/webservice/EngineRouteSupport.scala](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree#diff-Q3JvbUlBTS9zcmMvbWFpbi9zY2FsYS9jcm9taWFtL3dlYnNlcnZpY2UvRW5naW5lUm91dGVTdXBwb3J0LnNjYWxh) | `0% <0%> (-100%)` | :arrow_down: |; | ... and [626 more](https://codecov.io/gh/broadinstitute/cromwell/pull/5053/diff?src=pr&el=tree-more) | |. ------. [Continue to review full report at Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=continue).; > **Legend** - [Click here to learn more](https://docs.codecov.io/docs/codecov-delta); > `Î” = absolute <relative> (impact)`, `Ã¸ = not affected`, `? = missing data`; > Powered by [Codecov](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=footer). Last update [da601c8...ae566b9](https://codecov.io/gh/broadinstitute/cromwell/pull/5053?src=pr&el=lastupdated). Read the [comment docs](https://docs.codecov.io/docs/pull-request-comments).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509805758:4245,Power,Powered,4245,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5053#issuecomment-509805758,1,['Power'],['Powered']
Energy Efficiency,"~Opening draft PR for early access viewing. Also to get assistance from Travis to run the full test suite.~; Un-drafting this PR. I'd still like to build in some monitoring of checksum failures, but right now I'm more interested in getting the current changes reviewed. Still TODO:; * ~enumerate hash types~; * ~implement remaining hash calculations~; * ~add additional tests for successful checksum and retry~; * figure out how to monitor checksum failures",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6683:162,monitor,monitoring,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6683,2,['monitor'],"['monitor', 'monitoring']"
Energy Efficiency,ðŸ‘ LGTM pending Travis greenness!. [![Approved with PullApprove](https://img.shields.io/badge/two_reviewers-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/4523/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4523#issuecomment-452067058:22,green,greenness,22,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4523#issuecomment-452067058,1,['green'],['greenness']
Energy Efficiency,"ðŸ‘ but it'd be nice to get travis to go green before merging, even if the current failure is unrelated to this change. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2121/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2121#issuecomment-294180291:39,green,green,39,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2121#issuecomment-294180291,1,['green'],['green']
Energy Efficiency,ðŸ–– ðŸ‘ . I thought there were more than these but I guess not. Didn't have the energy to go back and do them again once I abandoned my first branch :). [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/2433/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2433#issuecomment-314234253:76,energy,energy,76,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2433#issuecomment-314234253,1,['energy'],['energy']
Integrability,"	! ACT		Invert ACT's success/failure; 	ACT1 [-a] ACT2	If ACT1 fails, stop, else do ACT2; 	ACT1 -o ACT2	If ACT1 succeeds, stop, else do ACT2; 			Note: -a has higher priority than -o; 	-name PATTERN	Match file name (w/o directory name) to PATTERN; 	-iname PATTERN	Case insensitive -name; 	-path PATTERN	Match path to PATTERN; 	-ipath PATTERN	Case insensitive -path; 	-regex PATTERN	Match path to regex PATTERN; 	-type X		File type is X (one of: f,d,l,b,c,...); 	-perm MASK	At least one mask bit (+MASK), all bits (-MASK),; 			or exactly MASK bits are set in file's mode; 	-mtime DAYS	mtime is greater than (+N), less than (-N),; 			or exactly N days in the past; 	-mmin MINS	mtime is greater than (+N), less than (-N),; 			or exactly N minutes in the past; 	-newer FILE	mtime is more recent than FILE's; 	-user NAME/ID	File is owned by given user; 	-group NAME/ID	File is owned by given group; 	-size N[bck]	File size is N (c:bytes,k:kbytes,b:512 bytes(def.)); 			+/-N: file size is bigger/smaller than N; 	-prune		If current file is directory, don't descend into it; If none of the following actions is specified, -print is assumed; 	-print		Print file name; 	-print0		Print file name, NUL terminated; 	-exec CMD ARG ;	Run CMD with all instances of {} replaced by; 			file name. Fails if CMD exits with nonzero. xargs: invalid option -- 'I'; BusyBox v1.22.1 (2014-05-23 01:24:27 UTC) multi-call binary. Usage: xargs [OPTIONS] [PROG ARGS]. Run PROG on every item given by stdin. 	-r	Don't run command if input is empty; 	-0	Input is separated by NUL characters; 	-t	Print the command on stderr before execution; 	-e[STR]	STR stops input processing; 	-n N	Pass no more than N args to PROG; 	-s N	Pass command line of no more than N bytes; 	-x	Exit if size is exceeded; ```. Notice that Nextflow had a [similar issue](https://github.com/nextflow-io/nextflow/issues/321) that I reported a few months ago, and is now fixed, allowing seamless integration of Biocontainers with Nextflow pipelines. Thank you!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4607:3346,integrat,integration,3346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4607,1,['integrat'],['integration']
Integrability," "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. ```. -------------. If the workflow has multiple tasks, and downstream tasks depends on (i.e. File input) upstream task that should have produced the file as output, previously the workflow would fail, now the workflow just hangs there. Example (ID: 55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8, location: `gs://broad-dsde-methods/cromwell-execution-34/TestMultiStage/55f8ac4e-a6e1-4b1f-9086-f6d04fec5bb8`). some json input content, WDL below:. ```wdl; workflow TestMultiStage {. Array[String] dummy_array. scatter (ele in dummy_array) {; call PrintsToFile as UpstreamPrintToFile {; input:; out_prefix = ele,; to_print = ele; }. output {; UpstreamPrintToFile.out_txt; UpstreamPrintToFile.out_md; }; }. call DownstreamConsumer {; input:; txt_array = UpstreamPrintToFile.out_txt,; md_array = UpstreamPrintToFile.out_md; }. output {; File merged_txt = DownstreamConsumer.cat_txt; File merged_md = DownstreamConsumer.cat_md; }; }. # upstream task that supposed to be producing 2 out files; task PrintsToFile {. String out_prefix; String to_print. command {; touch ${out_prefix}.txt; echo ""${to_print}"" > ${out_prefix}.txt; # delibrately forgetting to generate a file, so cromwell should capture that and report failure; # touch ${out_prefix}.md; # echo ""${to_print}"" > ${out_prefix}.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""10"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File out_txt = ""${out_prefix}.txt""; File out_md = ""${out_prefix}.md""; }; }. # downstream task that depends on upstream task outputing all files; task DownstreamConsumer {; Array[File] txt_array; Array[File] md_array. command {; cat ${sep="" ""} txt_array > merged.txt; cat ${sep="" ""} md_array > merged.md; }. runtime {; docker: ""ubuntu:trusty""; disks: ""local-disk "" + ""50"" + "" HDD""; cpu: ""1""; preemptible: 1; }. output {; File cat_txt = ""merged.txt""; File cat_md = ""merged.md""; }; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4147:3073,depend,depends,3073,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4147,1,['depend'],['depends']
Integrability," 14:45 foo/bar3.wdl; 99 02-07-2017 14:45 foo/bar5.wdl; 99 02-07-2017 14:45 foo/bar4.wdl; 99 02-07-2017 14:45 foo/bar6.wdl; --------- -------; 1089 12 files; ```. The content of all the task dependencies is just a variation on:; ```; [conradL@qimr13054 ~]$ cat foo/bar.wdl ; task doIt {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; }; ```. Submit to the server:; ```; curl http://localhost:8000/api/workflows/V1 -FwdlSource=@goodImport.wdl -FwdlDependencies=@foo.zip; ```. Now tailing the server logs, the first time this is submitted, the workflow succeeds and the log shows nothing out of the ordinary. But ""sometimes"" (meaning, I can submit it 5 times and not see it, or twice and see it both times) I see this:; ```; 2017-02-07 15:01:10,781 cromwell-system-akka.dispatchers.service-dispatcher-30 ERROR - Sending Actor[akka://cromwell-system/user/cromwell-service/WorkflowManagerActor/WorkflowActor-84a51727-cfda-41e7-a03c-9e3af35eb0dc/MaterializeWorkflowDescriptorActor#972983209] failure message MetadataPutFailed(PutMetadataAction(Stream(MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar6.wdl),Some(MetadataValue(task doIt6 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.772+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar4.wdl),Some(MetadataValue(task doIt4 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.774+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar5.wdl),Some(MetadataValue(task doIt5 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.775+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar3.wdl),S",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:1762,message,message,1762,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['message'],['message']
Integrability," 17, 2018 03:08PM ; > So, gdk - will Message 13 - only happen with pre-emptibles? Will a non-preemptible vm that is somehow shutdown also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; >",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:11198,Message,Message,11198,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['Message'],['Message']
Integrability," 4.13.1 release notes</li>; <li><a href=""https://github.com/junit-team/junit4/commit/c29dd8239d6b353e699397eb090a1fd27411fa24""><code>c29dd82</code></a> Change version to 4.13.1-SNAPSHOT</li>; <li><a href=""https://github.com/junit-team/junit4/commit/1d174861f0b64f97ab0722bb324a760bfb02f567""><code>1d17486</code></a> Add a link to assertThrows in exception testing</li>; <li><a href=""https://github.com/junit-team/junit4/commit/543905df72ff10364b94dda27552efebf3dd04e9""><code>543905d</code></a> Use separate line for annotation in Javadoc</li>; <li><a href=""https://github.com/junit-team/junit4/commit/510e906b391e7e46a346e1c852416dc7be934944""><code>510e906</code></a> Add sub headlines to class Javadoc</li>; <li><a href=""https://github.com/junit-team/junit4/commit/610155b8c22138329f0723eec22521627dbc52ae""><code>610155b</code></a> Merge pull request from GHSA-269g-pwp5-87pp</li>; <li><a href=""https://github.com/junit-team/junit4/commit/b6cfd1e3d736cc2106242a8be799615b472c7fec""><code>b6cfd1e</code></a> Explicitly wrap float parameter for consistency (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1671"">#1671</a>)</li>; <li><a href=""https://github.com/junit-team/junit4/commit/a5d205c7956dbed302b3bb5ecde5ba4299f0b646""><code>a5d205c</code></a> Fix GitHub link in FAQ (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1672"">#1672</a>)</li>; <li><a href=""https://github.com/junit-team/junit4/commit/3a5c6b4d08f408c8ca6a8e0bae71a9bc5a8f97e8""><code>3a5c6b4</code></a> Deprecated since jdk9 replacing constructor instance of Double and Float (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1660"">#1660</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/junit-team/junit4/compare/r4.13...r4.13.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=junit:junit&package-manager=maven&pr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5941:3011,wrap,wrap,3011,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5941,1,['wrap'],['wrap']
Integrability," > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fzAufMBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EPOYsKiLLBib6JnQtvmKzPoBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EL-QlNKLLBjeuPH9gd3Ck24gven-ztEBKg9wcm9kdWN0aW9uUXVldWU; > operations/EK6y-aWLLBjV36D2ueHGsKYBIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMPd46GLLBj1iYrpkrCipPsBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl; > operations/ENTd46GLLBiN8JPluoXAzFUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EMPehaqLLBiS7p7OzdzYu5wBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl. > ------------------------------- ; > kcibul@broadinstitute.org <kcibul@broadinstitute.org> #2 Jan 8, 2018 03:52PM ; > This is important to understand so Cromwell can do the right thing. It ; > hasn't been clear in the past why we sometimes get 13s on these preemptible ; > jobs ; > ; > Kristian Cibulski",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:2059,MESSAGE,MESSAGE,2059,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['MESSAGE'],['MESSAGE']
Integrability," CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 2019-01-31 20:30:56,617 INFO - changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne: Successfully released change log lock; 2019-01-31 20:30:56,631 ERROR - Failed to instantiate Cromwell System. Shutting down Cromwell.; liquibase.exception.MigrationFailedException: Migration failed for change set changesets/failure_metadata.xml::guaranteed_caused_bys::cjllanwarne:; Reason: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.changelog.ChangeSet.execute(ChangeSet.java:619); 	at liquibase.changelog.visitor.UpdateVisitor.visit(UpdateVisitor.java:51); 	at liquibase.changelog.ChangeLogIterator.run(ChangeLogIterator.java:79); 	at liquibase.Liquibase.update(Liquibase.java:214); 	at liquibase.Liquibase.update(Liquibase.java:192); 	at cromwell.database.migration.liquibase.LiquibaseUtils$.updateSchema(",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:2384,message,message,2384,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['message'],['message']
Integrability," ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken""). The first is harder because you represent more things (more containers), but the second isn't reproducible because if you lose ""by chicken"" you've lost the entire workflow. Does that make sense?. ## What can/should we do now?. So there are two things to think about. With the current representation of a workflow, we would want Singularity to be OCI compliant, and I would propose a plan to move forward is to expect this, and contribute to Singularity itself with the mindset of ""I want this to plug into AWS"" or ""I want this to plug into Kubernetes,"" etc. The backends for HPC are going to be good to go with just a SLURM or SGE backend, and then commands to load and run/exec a Singularity container. When the time comes and Singularity supports services, then we can start to develop (I think) the singularity backend configuration for cromwell, with clean commands to get statuses, start and stop, and otherwise integrate into the software. You guys seem pretty busy, so likely your best bet would be to just wait, because the community is going in that direction anyway. The other representation is to rethink this. An approach that I like is to move away from micro managing the workflow / software, and to set requirements for the data. If you set standard formats (meaning everything from the organization of files down to the headers of a data file) on the data itself, then the software gets built around that. A researcher can have confidence that the data he is collecting will work with software because it's validated to the format. The developers can have confidence their tools will work with data because of that same format. A new graduate student knows how to develop a new tool because there are nicely defined rules. A good example is to look at the BIDS (brain imaging data structure) that (has severa",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:6283,integrat,integrate,6283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,1,['integrat'],['integrate']
Integrability," Somethign along the lines of ; ```; Workflow subWorkflow {. File f; String s; ...; ...; }; ```. ```; import ""file/path/subworkflow.wdl"" as sub. workflow root {; call sub.subWorkflow as aliasSub; }; ```. When I try to pass the values for `File f` and `String s` from the inputs json I get an failure message. To make sure I was giving the workflow the correct inputs json I first ran it with bad inputs on purpose and got expected failures; ```; status: ""Failed"",; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_pac' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.agg_preemptible_tries' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_ann' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.wgs_coverage_interval_list' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_fasta_index' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.unmapped_bam_suffix' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_ud' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_amb' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.preemptible_tries' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_sa' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.known_indels_sites_VCFs' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_mu' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.T",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2912:1075,message,message,1075,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2912,1,['message'],['message']
Integrability," WaitingForReturnCodeFile; [2017-10-04 06:07:31,28] [info] BackgroundConfigAsyncJobExecutionActor [bf90a37bhelloHaplotypeCaller.haplotypeCaller:NA:1]: Status change from WaitingForReturnCodeFile to Done; [2017-10-04 06:07:31,37] [error] WorkflowManagerActor Workflow bf90a37b-6ffa-4122-a12c-24aced32f3b6 failed (during ExecutingWorkflowState): Job helloHaplotypeCaller.haplotypeCaller:NA:1 exited with return code 1 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: /home/centos/cromwell-executions/helloHaplotypeCaller/bf90a37b-6ffa-4122-a12c-24aced32f3b6/call-haplotypeCaller/execution/stderr; [2017-10-04 06:07:31,37] [info] WorkflowManagerActor WorkflowActor-bf90a37b-6ffa-4122-a12c-24aced32f3b6 is in a terminal state: WorkflowFailedState; [2017-10-04 06:07:35,37] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2017-10-04 06:07:35,41] [info] Message [cromwell.engine.workflow.WorkflowManagerActor$RetrieveNewWorkflows$] without sender to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor#-1816723107] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; Workflow bf90a37b-6ffa-4122-a12c-24aced32f3b6 transitioned to state Failed; [2017-10-04 06:07:35,44] [info] Automatic shutdown of the async connection; [2017-10-04 06:07:35,44] [info] Gracefully shutdown sentry threads.; [2017-10-04 06:07:35,44] [info] Shutdown finished.; ```. Ans this is the output from the stderr file; ```; vi /home/centos/cromwell-executions/helloHaplotypeCaller/bf90a37b-6ffa-4122-a12c-24aced32f3b6/call-haplotypeCaller/execution/stderr; ^[[32m BwaSpark ^[[31m(BETA Tool) ^[[36mBWA on Spark^[[0m; ^[[32m CollectBaseDistributionByCycleSpark ^[[31m(BETA Tool) ^[[36mCollectBaseDistributionBy",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2673:6347,Message,Message,6347,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2673,1,['Message'],['Message']
Integrability," Which backend are you running? -->. Backend: AWS Batch. <!-- Paste/Attach your workflow if possible: -->. [Workflow](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-processing-for-variant-discovery-gatk4.wdl). [Input file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/FH-M40job.inputs.json). <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. [Configuration file](https://github.com/FredHutch/workflow-manager-hackathon/blob/issue/jobdef-error/Workflow/aws.conf). Running this workflow on AWS Batch (with cromwell-36.jar) consistently fails at the same point each time. . It gets through most (looks like all but one iteration) of the scatter loop that calls the `BaseRecalibrator` task. Then cromwell just sits for a long time (~1hr) with no Batch jobs running (or runnable or starting). Then cromwell calls the `RegisterJobDefinition` API of AWS Batch, and it always fails with the following error message:. ```; 2018-12-15 23:39:03,360 cromwell-system-akka.dispatchers.backend-dispatcher-258 ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; ERROR - AwsBatchAsyncBackendJobExecutionActor [UUID(8adb5141)PreProcessingForVariantDiscovery_GATK4.BaseRecalibrator:1:1]: Error attempting to Execute; software.amazon.awssdk.services.batch.model.ClientException: arn:aws:batch:us-west-2:064561331775:job-definition/PreProcessingForVariantDiscovery_GATK4-BaseRecalibrator not found or versions do not match (Service: null; Status Code: 404; Request ID: 9914238b-00c2-11e9-a13d-cdc28a8016c8); ```. Looking at cloudtrail, here is the event associated with that request ID:. [Event](https://gist.github.com/dtenenba/909f16e720a01b00a736cf6e60f7083a). If I pull out just the contents of the `requestParameters` section and call RegisterJobDefinition using t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4496:1658,message,message,1658,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4496,1,['message'],['message']
Integrability," `womtool validate` (and it validated fine on Terra with the automatic validation they do). But the job would run about halfway and then automatically switch to ""Aborting"" status with no explanation or error message. The workflow would eventually fail after a huge delay (about 22 hours), and there would be no real error message. All tasks that ran were successful (but not all tasks ran). # Minimal WDL example. Here is a working example:. ```wdl; version 1.0. workflow my_workflow {; call my_task; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. And here is a non-working example that still validates fine using `womtool validate`:. ```wdl; version 1.0. workflow my_workflow {; input {; Boolean run_task; }. if (run_task) {; call my_task; }. output {; File out = select_first([my_task.out, stdout()]); }; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. The above gives; ```console; (cromwell) [sfleming@laptop:~/cromwell]$ womtool validate test.wdl ; Success!; ```. # The problem. The problem is that the non-working WDL example above should not validate successfully, as it is NOT a valid WDL. The `stdout()` built-in inside the `select_first()` in the `output` block of the `workflow` is not actually allowed. It will cause a very bizarre error when this WDL is run. # What am I asking for?. 1. Fix `womtool validate` to catch these kinds of errors. Also happens with `stderr()`.; 2. Provide an actionable error message when this kind of edge case ends up being run by Cromwell. Right now it automatically moves to ""Aborting"" status with no error message at all. Very hard to diagnose!. # Other information. I found this error using `miniwdl check`, which correctly identified the error, just FYI. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6976:2171,message,message,2171,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6976,2,['message'],['message']
Integrability," about time out operation. It looks that some tasks that take longer does not get a response for the container (although it is still running) and thus cromwell assumes a failure (because docker returns -1 although it is still running) and the workflow finishes with errors. In the logs for the task, embedded into the standard error from the operations, I get the following signature:. ```; time=""2018-03-07T14:17:55+01:00"" level=error msg=""error waiting for container: read tcp 192.168.99.1:56961->192.168.99.101:2376: read: operation timed out""; ```. And the `rc` file is marked with `-1`. I cannot continue on this return code, because the task is still running on the container and continuing assumes that the operation is finished. My local configuration file looks like this:. ```; include required(classpath(""application"")). ## keep always the workflow logs; workflow-options.workflow-log-temporary: false. backend.providers.Local.config {; ## limit the number of jobs; concurrent-job-limit = 10; filesystems.local {; ## do not allow copy (huge files); ## prefer hard-links; localization: [""hard-link"", ""soft-link""]; caching.duplication-strategy: [""hard-link"", ""soft-link""]; }; }; ```. And the cromwell command is (using a `brew` installed wrapper):. ```bash; JAVA_OPTS=""-Dconfig.file=local.conf"" cromwell run --inputs inputs.json --metadata-output metadata-output.json workflow.wdl; ```. This error is happening for different workflows and tasks, so it is very difficult to account for it. In addition, a long-run workflow stops for this and requires a retry of the whole pipeline in my system, so it is really a problem when trying to run a time-consuming workflow that requires re-start for non-real failures. Is there any way that the local backend (or any backend) catch the docker timeout failures and re-attach? Or maybe that the `script.submit` or `script.backgound` checks that the container is really stop and finished before returning a misleading error code?. Thank you in advance!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3370:1438,wrap,wrapper,1438,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3370,1,['wrap'],['wrapper']
Integrability," actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 500; root = ""cromwell-executions""; dockerRoot = ""/cromwell-executions"". runtime-attributes = """"""; Int cpus = 1; String mem = ""2g""; String dx_timeout; String? docker; """"""; check-alive = ""squeue -j ${job_id}""; exit-code-timeout-seconds = 500; job-id-regex = ""Submitted batch job (\\d+).*"". submit = """"""; sbatch \; --partition ind-shared \; --nodes 1 \; --job-name=${job_name} \; -o ${out} -e ${err} \; --ntasks-per-node=${cpus} \; --mem=${mem} \; -c ${cpus} \; --time=$(echo ${dx_timeout} | sed -e 's/ //g' -e 's/\([0-9]\+\)h\([0-9]\+\)m/\1:\2:00/' -e 's/\([0-9]\+\)h/\1:00:00/' -e 's/\([0-9]\+\)m/\1:00/') \; --chdir ${cwd} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}"". # We're asking bash-within-singularity to run the script, but the script's location on the machine; # is different then the location its mounted to in the container, so need to change the path with sed; submit-docker = """"""; sbatch \; --partition ind-shared \; --nodes 1 \; --job-name=${job_name} \; -o ${out} -e ${err} \; --ntasks-per-node=${cpus} \; --mem=${mem} \; -c ${cpus} \; --time=$(echo ${dx_timeout} | sed -e 's/ //g' -e 's/\([0-9]\+\)h\([0-9]\+\)m/\1:\2:00/' -e 's/\([0-9]\+\)h/\1:00:00/' -e 's/\([0-9]\+\)m/\1:00/') \; --chdir ${cwd} \; --wrap ""; singularity exec --containall --bind ${cwd}:${docker_cwd} docker://${docker} bash \; \""$(echo ${script} | sed -e 's@.*cromwell-executions@/cromwell-executions@')\""; ""; """"""; kill-docker = ""scancel ${job_id}"". filesystems {; local {; localization: [""hard-link""]; caching {; duplication-strategy: [""hard-link""]; check-sibling-md5: true; hasing-strategy: ""fingerprint""; fingerprint-size: 1048576 # 1 MB ; }; }; }. }; }; }}; ```. Note: there are some WDL parameters relevant to DNANexus's dxCompiler. I'm hoping this code will be able to run on that system eventually, but I understand that those parameters are not relevant to cromwell.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7108:6447,wrap,wrap,6447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7108,1,['wrap'],['wrap']
Integrability, akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResultWith$2(BasicDirectives.scala:72); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.ExecutionDirectives.$anonfun$handleExceptions$2(ExecutionDirectives.scala:32); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.Route$.$anonfun$asyncHandler$1(Route.scala:86); 	at akka.stream.impl.fusing.MapAsyncUnordered$$anon$26.onPush(Ops.scala:1303); 	at akka.stream.impl.fusing.GraphInterpreter.processPush(GraphInterpreter.scala:519); 	at akka.stream.impl.fusing.GraphInterpreter.execute(GraphInterpreter.scala:411); 	at akka.stream.impl.fusing.GraphInterpret,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:3350,Rout,RouteConcatenation,3350,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,2,['Rout'],"['RouteConcatenation', 'RouteWithConcatenation']"
Integrability," as well as the ability to specify the log root. This also allows for both stdout and file-logging simultaneously. This will allow us to have an API endpoint which just reads this the particular file for the workflow and sends it back over HTTP. Cromwell now accepts three Java Properties:; - `LOG_ROOT` - location where log files go (default `.`); - `LOG_MODE` - `server`, `console`, or `server,console` (default `console`); - `LOG_LEVEL` - info, debug, etc (default `info`). **Standard out logging and the main Cromwell log were not changed by this PR**. If the command `sbt -DLOG_MODE=server,console -DLOG_ROOT=log ""run run 3step.wdl 3step.json""` were run three times, we'd see this in the `log` directory:. ```; log; â”œâ”€â”€ cromwell.2015-10-26.log; â”œâ”€â”€ workflow.319df202-a60f-47c8-b886-bd4821747c68.log; â”œâ”€â”€ workflow.36e07688-9e47-45bd-9930-aff58471541e.log; â””â”€â”€ workflow.7dad065d-9d7a-4450-91c8-1f7ece184851.log; ```. FAQ:. > Scott, why did you not use the `application.conf` file for things like log root, log mode? Seems obvious, right?. Glad you asked. I tried, for a very long time, to allow for the values to be set in `application.conf`. However, I was unable to get it to work. It seems that by querying for a value in `application.conf`, it causes the `logback.xml` to be read and processed. The logback.xml file does not recognize values in `application.conf` natively, it must have Java Properties set. > What the hell were you thinking with the `WorkflowLogger` class?. I'd be very happy to discuss how to do this better. The problem is this:; - We need to log to the Akka logger whenever we can because the tests depend on it; - Akka logger descends from the root logger (`logback.xml`); - A new logger needs to be created for each workflow.; - Both the Akka logger and the workflow logger descend from the root logger so they both can't propagate messages or we'll get doubles of everything in the root logger.; - Workflow logger is set to not propagate messages (`setAdditive(false)`)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/254:1667,depend,depend,1667,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/254,3,"['depend', 'message']","['depend', 'messages']"
Integrability," available on an OSS mirror, even if the image contains the exact same content. Also it should be decided if the BCS backend should behave like the JES/PAPI backend and only allow jobs that specify a `docker` runtime attribute, or if the behavior should continue to be like the `Local`/`SFS` backends and allow running jobs on the bare VM without a docker container. Links regarding BCS/OSS and docker:; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F28022.html)) https://help.aliyun.com/document_detail/28022.html; - ([EN translation](https://translate.google.com/translate?hl=en&sl=zh-CN&tl=en&u=https%3A%2F%2Fhelp.aliyun.com%2Fdocument_detail%2F42402.html)) https://help.aliyun.com/document_detail/42402.html; - https://docs.docker.com/registry/storage-drivers/; - https://github.com/docker/distribution/tree/v2.6.2/registry/storage/driver/oss; - https://stackoverflow.com/questions/45533005/why-digests-are-different-depend-on-registry; - http://cromwell.readthedocs.io/en/develop/CallCaching/; - https://github.com/broadinstitute/cromwell/blob/31/docs/backends/BCS.md#user-content-docker. A/C:. - Document if an image copied to OSS has the same sha256 as docker hub; - Pull the `ubuntu:latest` image from docker hub; - Record the sha256 from docker hub; - Obtain the OSS credentials from `secret/dsde/cromwell/common/cromwell-bcs` in vault; - Push the image to a private OSS bucket; - Check if the image contains the same hash in OSS as was in docker hub; - Document the results; - Create tickets as appropriate for:; - Splitting the docker runtime attribute used by OSS into `docker` and `dockerRegistry`; - Cromwell engine hashing of docker images stored in OSS; - (Optional) Require the `docker` runtime attribute to be mandatory for the BCS backend; - The A/C of the above tickets should include:; - Restore centaur docker testing to `testCentaurBcs.sh`; - Add a centaur `call_cache_capoeira_bcs`; - Updatin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3518:2116,depend,depend-on-registry,2116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3518,1,['depend'],['depend-on-registry']
Integrability," by backend. Unsupported attributes will not be part of; > job executions.; > 2023-03-06 17:17:22,220 WARN - Local [UUID(7e3f9b56)]: Key/s [cpu, memory]; > is/are not supported by backend. Unsupported attributes will not be part of; > job executions.; > 2023-03-06 17:17:22,220 WARN - Local [UUID(7e3f9b56)]: Key/s [cpu, memory]; > is/are not supported by backend. Unsupported attributes will not be part of; > job executions.; > 2023-03-06 17:17:22,220 WARN - Local [UUID(7e3f9b56)]: Key/s [cpu, memory]; > is/are not supported by backend. Unsupported attributes will not be part of; > job executions.; > 2023-03-06 17:17:22,220 WARN - Local [UUID(7e3f9b56)]: Key/s [cpu, memory]; > is/are not supported by backend. Unsupported attributes will not be part of; > job executions.; > 2023-03-06 17:17:22,221 WARN - Local [UUID(7e3f9b56)]: Key/s [cpu, memory]; > is/are not supported by backend. Unsupported attributes will not be part of; > job executions.; > 2023-03-06 17:17:24,401 INFO -; > WorkflowExecutionActor-7e3f9b56-790b-481b-a8d9-f24e88883ed5; > [UUID(7e3f9b56)]: Starting pb_assembly_hifi.generate_config; > 2023-03-06 17:17:28,549 INFO -; > 13f1ea7a-4f35-41ea-9afa-f8fc84d083b0-SubWorkflowActor-SubWorkflow-prepare_input; > ðŸ‘Ž1 [UUID(13f1ea7a)]: Starting prepare_input.dataset_filter; > 2023-03-06 17:17:30,919 WARN - BackgroundConfigAsyncJobExecutionActor; > [UUID(7e3f9b56)pb_assembly_hifi.generate_config:NA:1]: Unrecognized runtime; > attribute keys: cpu, memory; > 2023-03-06 17:17:30,919 WARN - BackgroundConfigAsyncJobExecutionActor; > [UUID(13f1ea7a)prepare_input.dataset_filter:NA:1]: Unrecognized runtime; > attribute keys: cpu, memory; >; > â€”; > Reply to this email directly, view it on GitHub; > <https://github.com/broadinstitute/cromwell/issues/7085>, or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/ABCR6IDB7UG3LW47K526FULW22MK5ANCNFSM6AAAAAAVR4KY5U>; > .; > You are receiving this because you are subscribed to this thread.Message; > ID: ***@***.***>; >",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7085#issuecomment-1457413667:6266,Message,Message,6266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7085#issuecomment-1457413667,1,['Message'],['Message']
Integrability," field seems to have an inconsistent format:. Compare the failures sections for the following:. ```; {; ""workflowName"": ""echo_strings"",; ""submittedFiles"": {; ""inputs"": ""{...},; ""calls"": {; ""echo_strings.echo_files"": [{; ""preemptible"": false,; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files/echo_files-stdout.log"",; ""backendStatus"": ""Failed"",; ""shardIndex"": -1,; ""jes"": {; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""machineType"": ""us-central1-c/n1-standard-1"",; ""googleProject"": ""broad-dsde-dev"",; ""executionBucket"": ""gs://cromwell-dev/cromwell-executions"",; ""zone"": ""us-central1-c"",; ""instanceName"": ""ggp-3462354720519617596""; },; ""runtimeAttributes"": {...},; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""CallCachingOff"",; ""inputs"": {...; },; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }],; ""jobId"": ""operations/EJiq_oWfKxi8-N-X4qiwhjAgw7vetLsXKg9wcm9kdWN0aW9uUXVldWU"",; ""backend"": ""JES"",; ""end"": ""2017-01-30T19:14:19.708Z"",; ""stderr"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files/echo_files-stderr.log"",; ""callRoot"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files"",; ""attempt"": 1,; ""executionEvents"": [...],; ""backendLogs"": {; ""log"": ""gs://fc-2d3fd356-e3be-4953-92f1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:1066,message,message,1066,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,1,['message'],['message']
Integrability," from the cache, Cromwell seems to lock up after the first handful of cache hits(~30). Cromwell will stop responding to api requests and after some time with logs being written the workflow that was getting the cache hits will hit 503 and timeout errors. When running the workflow with `read_from_cache=false` we run into none of these errors. Timeout Error. ```; 2016-05-05 17:37:02,285 cromwell-system-akka.actor.default-dispatcher-25 WARN - Configured registration timeout of 1 second expired, stoppingw; ```. 503 Error. ```; Exception occurred while attempting to copy outputs from gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/ccba2c79-c998-4f03-b736-af097391db66/call-SplitGvcf/shard-50 to gs://broad-gotc-dev-storage/cromwell_execution/JointGenotyping/7164dc88-af61-4ea6-8a73-f0b79594ae9a/call-SplitGvcf/shard-50. com.google.api.client.googleapis.json.GoogleJsonResponseException: 503 Service Unavailable. {. ""code"" : 503,. ""errors"" : [ {. ""domain"" : ""global"",. ""message"" : ""Backend Error"",. ""reason"" : ""backendError"". } ],. ""message"" : ""Backend Error"". }. at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.AbstractGoogleClientRequest$1.interceptResponse(AbstractGoogleClientRequest.java:321) ~[cromwell.jar:0.19]. at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1056) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]. at com.google.api.client.googleapis.services.AbstractGoog",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/794:1163,message,message,1163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/794,1,['message'],['message']
Integrability," full diff in <a href=""https://github.com/FasterXML/jackson/commits"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.fasterxml.jackson.core:jackson-databind&package-manager=maven&previous-version=2.13.4.1&new-version=2.13.4.2)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). You can trigger a rebase of this PR by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/broadinstitute/cr",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7110:864,Depend,Dependabot,864,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7110,9,"['Depend', 'depend']","['Dependabot', 'dependabot']"
Integrability," incompatible Akka versions on the classpath. Please note that a given Akka version MUST be the same across all modules of Akka that you are using, e.g. if you use akka-actor [2.5.3 (resolved from current classpath)] all other core Akka modules MUST be of the same version. External projects like Alpakka, Persistence plugins or Akka HTTP etc. have their own version numbers - please make sure you're using a compatible set of libraries. ; Uncaught error from thread [default-akka.actor.default-dispatcher-5]: akka.actor.ActorCell.addFunctionRef(Lscala/Function2;)Lakka/actor/FunctionRef;, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for for ActorSystem[default]; java.lang.NoSuchMethodError: akka.actor.ActorCell.addFunctionRef(Lscala/Function2;)Lakka/actor/FunctionRef;; ...; ```; I'm essentially seeing exactly the behaviour described in reference [1] below, which is eviction warnings at compile time and then the runtime blow-up. The root cause seems to be that akka-http depends on an older version of akka-actor (2.4.19) than that specified for the project (2.5.3). Running `dependencyTree` task confirms:; ```; [info] +-com.typesafe.akka:akka-http-spray-json_2.12:10.0.9 [S]; [info] | +-com.typesafe.akka:akka-http_2.12:10.0.9 [S]; [info] | | +-com.typesafe.akka:akka-http-core_2.12:10.0.9 [S]; [info] | | +-com.typesafe.akka:akka-parsing_2.12:10.0.9 [S]; [info] | | | +-com.typesafe.akka:akka-actor_2.12:2.4.19 (evicted by: 2.5.3); ```; If I explicitly add dependency on the latest akka-stream as suggested in [2] and [3], the problem goes away:; ```; diff --git a/project/Dependencies.scala b/project/Dependencies.scala; index 0d77e2d3..7254fc61 100644; --- a/project/Dependencies.scala; +++ b/project/Dependencies.scala; @@ -141,6 +141,7 @@ object Dependencies {; ; val cromwellApiClientDependencies = List(; ""com.typesafe.akka"" %% ""akka-actor"" % akkaV,; + ""com.typesafe.akka"" %% ""akka-stream"" % akkaV,; ""com.typesafe.akka"" %% ""akka-http-spray-json"" % akkaHttpV,; ""co",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2579:1267,depend,depends,1267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2579,1,['depend'],['depends']
Integrability," instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transient errors as ; > preemptible when they're not people can wind up on a standard instance ; > before it's necessary. ; > ; > If it's not 100%, is there any way for the error to include a message that ; > can indicate it's really a preemption? As an example, error code 2 will ; > sometimes indicate it was a preemption. ; > ; > J . @hjfbynara tagging you if you have anything you want to add",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:11731,Message,Message,11731,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,4,"['Message', 'message']","['Message', 'message']"
Integrability," is not; > completely implemented yet. This comes down to OCI compliance, and having a; > way for some host to manage all of its Singularity containers. Right now we; > just have start and stop, but we can't connect containers, define ports, or; > even easily get a PID. It could (sort of?) be hacked, but we would be; > better off waiting for that nice standard.; > Reproducible Binary (Workflow Step) vs. Environment; >; > There is also a distinction that I haven't completely wrapped my head; > around. Docker is very commonly used as an environment - you put a bunch of; > software (e.g., samtools, bwa aligner, etc.) and then issue commands to the; > container with custom things. Singularity, in my mind, to be truly a; > reproducible thing is more of the workflow step or script. It will have the; > software inside, but better should have those same commands represented; > with internal modularity. I could arguably completely do away with the; > external workflow dependency if a single binary told me how to run itself,; > and then had more than one entrypoint defined for each step. I wouldn't; > need to care about the software or components inside because my host just; > needs to run Singularity. A container should almost be more like a hard; > coded binary step instead of a ""come into the environment and play around,; > the water's fine!"" It's a little bit like the ICD 10 decision to give a; > unique id to every combination of things (e.g., ""got hit on the road by a; > chicken"") instead of combinations of them, eg. (""got hit"" + ""by chicken"").; > The first is harder because you represent more things (more containers),; > but the second isn't reproducible because if you lose ""by chicken"" you've; > lost the entire workflow. Does that make sense?; > What can/should we do now?; >; > So there are two things to think about. With the current representation of; > a workflow, we would want Singularity to be OCI compliant, and I would; > propose a plan to move forward is to expect",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046:6947,depend,dependency,6947,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-441126046,1,['depend'],['dependency']
Integrability," minutes"",; ""writeBatchSize"" : 10000,; ""writeThreshold"" : 10000; }; Uncaught error from thread [cromwell-system-akka.dispatchers.engine-dispatcher-4]: Uncaught error from thread [cromwell-system-akka.dispatchers.service-dispatcher-7]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-systemunable to create new native thread, Uncaught error from thread [cromwell-system-akka.dispatchers.io-dispatcher-15]; ]: unable to create new native thread, shutting down JVM since 'akka.jvm-exit-on-fatal-error' is enabled for ActorSystem[cromwell-system]; [...]; ```. So I tried following the HPC/SLURM instructions and made a conf file:; ```; include required(classpath(""application"")). webservice {; port = 8080; }. backend {; providers {; Sherlock {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 2; Int cpus = 1; Int requested_memory_mb_per_core = 1000; String queue = ""short""; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} \; ${""-c "" + cpus} \; --mem-per-cpu ${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }. default = Sherlock; }; ```. But I get the same error on `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl `. Any ideas what is going on? Perhaps this is some restriction for login nodes? I suppose I could submit a SLURM job to run Cromwell to then submit my actual jobs but that seems very clunky. Edit: can confirm that if I submit `java -Dconfig.file=/home/users/tbenst/cromwell/sherlock.conf -jar ~/cromwell/cromwell-48.jar run hello.wdl ` as a slurm job then it runs fine. Would be great to be able to submit jobs from using cromwell from login node, though!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5395:2346,wrap,wrap,2346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5395,1,['wrap'],['wrap']
Integrability," name-for-call-caching-purposes: PAPI; slow-job-warning-time: ""24 hours""; genomics-api-queries-per-100-seconds = 1000; maximum-polling-interval = 600; request-workers = 3; genomics {; auth = ""application-default""; endpoint-url = ""https://genomics.googleapis.com/""; location = ""us-west1""; restrict-metadata-access = false; localization-attempts = 3; parallel-composite-upload-threshold=""150M""; }; filesystems {; gcs {; auth = ""application-default""; project = ""xxxx""; caching {; duplication-strategy = ""copy""; }; }; http { }; }; default-runtime-attributes {; cpu: 1; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""us-west1-a"", ""us-west1-b""]; }; include ""papi_v2_reference_image_manifest.conf""; }; }; }; }; ```; When I run with the above config using:; ```; java -Dconfig.file=genomics.conf -jar cromwell-66.jar run cumulus.wdl -i cumulus_inputs.json; ```; I am getting the following error message:; ```; [2021-08-24 22:05:33,60] [info] WorkflowManagerActor: Workflow 6cc303b4-295d-49fa-a996-b5cf7ec9beea failed (during ExecutingWorkflowState): java.lang.Exception: Task cumulus.cluster:NA:1 failed. The job was stopped before the command finished. PAPI error code 3. Execution failed: allocating: creating instance: inserting instance: Invalid value for field 'resource.networkInterfaces[0].network': ''. The referenced network resource cannot be found.; ```; I have tried passing the vpc and subnet id using the following config:; ```; virtual-private-cloud {; network-label-key = ""xxx""; subnetwork-label-key = ""xxx""; auth = ""application-default""; }; ```. The above values are my actual vpc and subnet id/name. However, it is still giving me that error message. Is there something I am missing from a configuration perspective. Any help would be greatly appreciated. Our VPC network's are not created in auto mode and that is not something we have control over unfortunately. Thanks,; -Simran",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6477:1765,message,message,1765,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6477,2,['message'],['message']
Integrability," older and has had time to develop a full API for services. It meets the criteria for both a backend and an executable, and this is because it can be conceptualized as both ""a thing that you run"" and ""the thing that is the container you run in."" But it's confusing. The distinction is that although Singularity is also a container, Singularity is **not** like Docker because it doesn't have the fully developed services API (yet!). This problem is hard because the language for Singularity containers communicating between one another, and even to the host, is not completely implemented yet. This comes down to OCI compliance, and having a way for some host to manage all of its Singularity containers. Right now we just have start and stop, but we can't connect containers, define ports, or even easily get a PID. It could (sort of?) be hacked, but we would be better off waiting for that nice standard. ## Reproducible Binary (Workflow Step) vs. Environment. There is also a distinction that I haven't completely wrapped my head around. Docker is very commonly used as an environment - you put a bunch of software (e.g., samtools, bwa aligner, etc.) and then issue commands to the container with custom things. Singularity, in my mind, to be truly a reproducible thing is more of the workflow step or script. It will have the software inside, but better should have those same commands represented with internal modularity. I could arguably completely do away with the external workflow dependency if a single binary told me how to run itself, and then had more than one entrypoint defined for each step. I wouldn't need to care about the software or components inside because my host just needs to run Singularity. A container should almost be more like a hard coded binary step instead of a ""come into the environment and play around, the water's fine!"" It's a little bit like the ICD 10 decision to give a unique id to every combination of things (e.g., ""got hit on the road by a chicken"") inst",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214:4321,wrap,wrapped,4321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-416418214,1,['wrap'],['wrapped']
Integrability," provided backends with different configurations containing different flavours of { master and deployMode } combinations are already set. Internally, we create a bash script containing a spark-submit (depending on the backend flavour selected at runtime) command using all the specified wdl runtime attributes which is then executed by Spark.â€‚â€‚. Current deploy modes supported for any spark job:; â€‚â€‚a - Client deploy mode using the spark standalone cluster manager; â€‚â€‚b - Cluster deploy mode using the spark standalone cluster manager; â€‚â€‚c - Client deploy mode using Yarn resource manager; â€‚â€‚d - Cluster deploy mode using Yarn resource manager; â€‚â€‚; Future PR Plans:; â€‚â€‚In this PR, the hadoop file system cannot be used as an input/output for the SBE because the Cromwell engine does not identify the protocol, and this results in the hdfs path being localized (soft-link, hard-link or copied).; â€‚â€‚This is not a problem until the SBE tries to evaluate the output after a successful execution, and because it cannot interpret the protocol, it tries to look for an hdfs output locally which results in an error. Note: This is only the case when the spark job writes the output to an hdfs location. Then cromwell cannot find the output file for evaluation. â€‚â€‚In the near **Future**, we plan to provide an hdfs client similar to that of the gcs to add support for the hdfs, primarily because hdfs is spark's natural file system.; â€‚â€‚Note that this doesn't actually prevent spark from writing to the hdfs, in order words, the spark application can write or read from the hdfs if given hdfs locations as arguments. Reason for restriction on environment:; â€‚â€‚In spark cluster mode, the assembly jar file containing the application has to exist in all the nodes of the cluster since the driver program can be started on any of the nodes in the cluster.; â€‚â€‚Known solution to this is to put the jar file in a shared file system like hdfs or a network file system, or a parallel distributed file system like lustre ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1339:1647,protocol,protocol,1647,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339,1,['protocol'],['protocol']
Integrability," quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memory_task1; Int memory_task2. #Start the call. Array[Array[String]] files_and_metadata = read_tsv(meta_data). scatter(files_and_metadata_row in files_and_metadata) {; String sampleName = files_and_metadata_row[0]; File f1 = files_and_metadata_row[1]; File f2 = files_and_metadata_row[2]; String? barcode = files_and_metadata_row[3]; #if the barcode is passed, proceed with it.; if (defined(barcode)) {; call trimCellBarcode {; input:; f1=f1,; f2=f2,; sampleName=sampleName,; barcode=barcode,; monitoring_script=monitoring_script,; command=command,; memory_task1=memory_task1,; bases=bases; }; }; #if the barcode is not passed, proceed with the trimming of the adapters only; if (!defined(barcode)) {; call trimAdaptersWithoutBarcodes{; input:; input_r1=f1,; input_r2=f2,; sampleName=sampleName,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; }; call trimAdapters {; input:; input_r1=trimCellBarcode.fastqDebarcodedR1,; input_r2=trimCellBarcode.fastqDebarcodedR2,; sampleName=sampleName,; barcode=barcode,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; monitoring_script=monitoring_script,; memory_task2=memory_task2,; TAG=TAG; }; } ; }. task trimCellBarcode {; File f1; File f2; String sampleName; String? barcode; File command; Int bases; File? monitoring_script; Int memory_task1. command <<<; set -euo pip",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:2658,adapter,adapters,2658,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,1,['adapter'],['adapters']
Integrability," ref_fai; File ref_dict; File input_vcf; File input_vcf_idx; String reference_version; String output_file_base_name; String output_format; Boolean compress; Boolean use_gnomad; # This should be updated when a new version of the data sources is released; # TODO: Make this dynamically chosen in the command.; File? data_sources_tar_gz = ""gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.6.20190124s.tar.gz""; String? control_id; String? case_id; String? sequencing_center; String? sequence_source; String? transcript_selection_mode; File? transcript_selection_list; Array[String]? annotation_defaults; Array[String]? annotation_overrides; Array[String]? funcotator_excluded_fields; Boolean? filter_funcotations; File? interval_list. String? extra_args. # ==============; Runtime runtime_params; Int? disk_space #override to request more disk than default small task params. # You may have to change the following two parameter values depending on the task requirements; Int default_ram_mb = 3000; # WARNING: In the workflow, you should calculate the disk space as an input to this task (disk_space_gb). Please see [TODO: Link from Jose] for examples.; Int default_disk_space_gb = 100; }. # ==============; # Process input args:; String output_maf = output_file_base_name + "".maf""; String output_maf_index = output_maf + "".idx""; String output_vcf = output_file_base_name + if compress then "".vcf.gz"" else "".vcf""; String output_vcf_idx = output_vcf + if compress then "".tbi"" else "".idx""; String output_file = if output_format == ""MAF"" then output_maf else output_vcf; String output_file_index = if output_format == ""MAF"" then output_maf_index else output_vcf_idx; String transcript_selection_arg = if defined(transcript_selection_list) then "" --transcript-list "" else """"; String annotation_def_arg = if defined(annotation_defaults) then "" --annotation-default "" else """"; String annotation_over_arg = if defined(annotation_overrides) then "" --annotation-override "" else """"; String filter_fun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5345:34661,depend,depending,34661,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5345,1,['depend'],['depending']
Integrability," runtime-attributes = """"""; Int time_minutes = 600; Int cpu = 4; #Int memory = 500; String queue = ""short""; String map_path = ""/shared/rna-seq""; String partition = ""compute""; String root = ""/shared/rna-seq/cromwell-executions""; """""". # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at this interval for every job; # - if a job is found to be not alive, and no RC file appears after this interval; # - Then it will be marked as Failed.; # Warning: If set, Cromwell will run 'check-alive' for every job at this interval. # exit-code-timeout-seconds = 120. submit = """"""; task=`echo ${job_name}|cut -d'_' -f3`; echo $task; image=`grep ""\b$task\b"" ${map_path}/map.txt |cut -d',' -f2`; echo $PWD; echo $image; if [ ! -z $image ]; then \; echo ""Inside Singularity exec""; \; echo ""CPU count: "" ${cpu}; \; echo ""time_minutes: "" ${time_minutes}; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""singularity exec -B /shared/rna-seq:/shared/rna-seq $image /bin/bash ${script}""; else \; echo ""No Singularity""; \; sbatch -J ${job_name} -D ${cwd} -c ${cpu} -o ${out} -e ${err} -t ${time_minutes} --wrap ""/bin/bash ${script}""; fi;; """"""; kill = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; }; }; }; }; ```. </details>. <details>; <summary>Error stack trace</summary>. ```; [2021-03-08 11:53:28,10] [ESC[38;5;1merrorESC[0m] Failed to instantiate Cromwell System. Shutting down Cromwell.; java.sql.SQLTransientConnectionException: db - Connection is not available, request timed out after 300000ms.; at com.zaxxer.hikari.pool.HikariPool.createTimeoutException(HikariPool.java:676); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:190); at com.zaxxer.hikari.pool.HikariPool.getConnection(HikariPool.java:155); at com.zaxxer.hikari.HikariDataSource.getConnection(HikariDataSource.java:100); at slick.jdbc.hikaricp.HikariCPJdbcDataSource.createConnection(HikariCPJdb",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6208:2033,wrap,wrap,2033,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6208,2,['wrap'],['wrap']
Integrability," showing up as ""Error code 10: Message 14:"" - and cromwell takes care of re-submitting and following the logic coded in our WDLs. Try pre-emptibles 3 times then try a non-preemptible instance. ; > ; > cromwell metadata output:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.SamToFastqAndBwaMemAndMba:1:1 failed. JES error code 10. Task 417bb61c-16cc-4fda-91d5-443ccba4da11:SamToFastqAndBwaMemAndMba was preempted for the 1st time. The call will be restarted with another preemptible VM (max preemptible attempts number is 3). Error code Status{code=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fz",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:1450,Message,Message,1450,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,2,['Message'],['Message']
Integrability," that one of the goals is reliability/scalability, I thought I'd make a PR out of it since it might provide a base for discussion. This branch has an IO Actor that handles *some* of the IO that has to be done both on the engine and the backend side. Specifically the script.sh upload, rc file reading, stderr file size reading, call cache copying (on JES), workflow outputs copying is done using this mechanism.; The actor is under the service registry umbrella, that was to be able to test it more rapidly (as the service registry is already wired up pretty much everywhere), but it should probably be it's own top level actor. Due to the Future-based approach we took in the backend interface, the IO messages (copy, read, write, delete file...) are declined into 2 different flavors:; - A classic Command -> Response; - A Promise based version, that takes a promise in the command message itself to be completed when the operation finishes. This allow for the actor to integrate with parts of the code that can't (easily) handle the response as a message. The underlying implementation of the IO Actor is a router, but could be swapped for something else. Each worker tries to perform the operation, and once it's complete (successfully or not) either sends a message back or completes the promise depending on the command flavor.; Retries are handled by keeping an exponential backoff object in the command itself. If the failure is retryable, the worker sends the command message back to the router after waiting for the appropriate backoff time. The message will then be rerouted when a worker is available.; Note that the actual time before the command is picked up again by another worker could be longer than intended if all workers are busy and the command spends time in the mailbox. ; A command will be retried as many times as possible (considering exponentially long waiting times in between retries) until a threshold amount of time has passed since the first try (10 minutes by default",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1831:1131,integrat,integrate,1131,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1831,2,"['integrat', 'message']","['integrate', 'message']"
Integrability," the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }. default-runtime-attributes {; cpu: 4; failOnStderr: false; continueOnReturnCode: 0; memory: ""2 GB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""us-central1-a"", ""us-central1-b""]; }. include ""papi_v2_reference_image_manifest.conf""; }; }; }; }; ```. WDL:. ```; task hello {; String addressee ; command {; echo ""Hello ${addressee}! Welcome to Cromwell . . . on Google Cloud!"" ; }; output {; String message = read_string(stdout()); }; runtime {; docker: ""ubuntu:latest""; }; }. workflow wf_hello {; call hello. output {; hello.message; }; }; ```. input. ```; {; ""wf_hello.hello.addressee"": ""World""; }; ```. Gcloud log (edited):. ```; done: true; metadata:; '@type': type.googleapis.com/google.cloud.lifesciences.v2beta.Metadata; createTime: '2021-08-03T15:21:55.984657Z'; endTime: '2021-08-03T15:24:03.533702405Z'; events:; - description: Worker released; timestamp: '2021-08-03T15:24:03.533702405Z'; workerReleased:; instance: google-pipelines-worker-xxxxxx; zone: us-central1-b; - containerStopped:; actionId: 19; description: Stopped running ""-c python -c 'import base64; print(base64.b64decode(\""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh""; timestamp: '2021-08-03T15:24:02.823519462Z'; - containerStarted:; actionId: 19; description: Started running ""-c python -c 'import base64; print(base64.b64decode(\""xxxxxx""));'; > /tmp/xxxxxx.sh && chmod u+x /tmp/xxxxxx.sh; && sh /tmp/xxxxxx.sh""; timestamp: '2021-08-03T15:23:57.785552960Z'; - containerStopped:; actionId: 18; description: Stopped running ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6462:6362,message,message,6362,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6462,1,['message'],['message']
Integrability," would probably be lower on the list of TODOs as there exists a workaround. This ""workaround"" works, where all three `output` variables are relatively simple:; ```wdl; task x {; command {; echo 0 > intFile.txt; echo hello > outFile.txt; }; runtime { docker: ""ubuntu"" }; output {; Int intermediateInt = read_int(""intFile.txt""); Array[File] intermediateOuts = glob(""outFile.txt""); File out = intermediateOuts[intermediateInt]; }; }. workflow glob_indexing { call x }; ```. Starting to compress the output block into two statements, where the latter is a compound expression, this still parses and runs:; ```wdl; output {; Int intermediateInt = read_int(""intFile.txt""); File out = glob(""outFile.txt"")[intermediateInt]; }; ```. Regarding the problems with `Map[,]` this _does_ work:; ```wdl; output {; Map[String, File] intermediateMap = {""a"": ""outFile.txt""}; File out = intermediateMap[""a""]; }; ```. HOWEVER, this doesn't work, currently failing with the error `Workflow input processing failed: <string:8:20 lbrace ""ew==""> (of class wdl4s.parser.WdlParser$Terminal)`:. ```wdl; output {; File out = {""a"": ""outFile.txt""}[""a""]; }; ```. And going back to globbing, the error with globs is _slightly_ better. This doesn't work, either:; ```wdl; output {; File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; }; ```. And fails with the ""prettier"" message at the moment:. ```; ERROR: Unexpected symbol (line 8, col 48) when parsing 'e'. Expected rsquare, got (. File out = glob(""outFile.txt"")[read_int(""intFile.txt"")]; ^. $e = :identifier <=> :lparen $_gen18 :rparen -> FunctionCall( name=$0, params=$2 ); ; ```. ---. <sup>1</sup> The ""medium"" estimate is assuming this only needs to be fixed in the ~wdl4s~ cromwell-wdl project. If this is a problem lower down in the parser/grammar, then it might be harder for a developer to do. My note here is because Winstanley is also highlighting these ""bad"" examples as problematic with red-underlines, hinting that this may be a lower level problem than I think.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829:1831,message,message,1831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2698#issuecomment-345410829,1,['message'],['message']
Integrability," {; input:; ref_fasta = ref_fasta,; ref_fasta_index = ref_fasta_index,; ref_dict = ref_dict,; input_bam = SortAndFixReadGroupBam.output_bam,; report_filename = sub(sub(unmapped_bam, sub_strip_path, """"), sub_strip_unmapped, """") + "".validation_report"",; disk_size = flowcell_medium_disk,; preemptible_tries = preemptible_tries; }; ```. error in server logs:; ```; 2017-01-23 15:09:09 [cromwell-system-akka.actor.default-dispatcher-89] ERROR c.b.i.j.JesAsyncBackendJobExecutionActor - JesAsyncBackendJobExecutionActor [UUID(8f35e32d)PairedEndSingleSampleWorkflow.Vali; dateReadGroupSamFile:1:1]: Error attempting to Execute; java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlIntegerType,None); at wdl4s.command.ParameterCommandPart.instantiate(ParameterCommandPart.scala:48); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at wdl4s.Task$$anonfun$instantiateCommand$1$$anonfun$apply$2.apply(Task.scala:108); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.IterableLike$class.foreach(IterableLike.scala:72); at scala.collection.AbstractIterable.foreach(Iterable.scala:54); at scala.collection.TraversableLike$class.map(TraversableLike.scala:234); at scala.collection.AbstractTraversable.map(Traversable.scala:104); at wdl4s.Task$$anonfun$instantiateCommand$1.apply(Task.scala:108); at wdl4s.Task$$anonfun$instantiateCommand$1.apply(Task.scala:108); at scala.util.Try$.apply(Try.scala:192); ```; in metadata:; ```; failures: [; {; causedBy: {; message: ""Could not find declaration for WdlOptionalValue(WdlIntegerType,None)""; },; message: ""java.lang.UnsupportedOperationException: Could not find declaration for WdlOptionalValue(WdlIntegerType,None)""; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1943:2640,message,message,2640,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1943,2,['message'],['message']
Integrability," },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.agg_preemptible_tries' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_ann' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.wgs_coverage_interval_list' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_fasta_index' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.unmapped_bam_suffix' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_ud' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_amb' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.preemptible_tries' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_sa' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.known_indels_sites_VCFs' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_mu' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_alt' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_bwt' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.known_indels_sites_indices' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_dict' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_bed' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlign",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2912:1675,message,message,1675,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2912,1,['message'],['message']
Integrability," }. # Controls how batched requests to PAPI are handled:; batch-requests {; timeouts {; # Timeout when attempting to connect to PAPI to make requests:; # read = 10 seconds. # Timeout waiting for batch responses from PAPI:; #; # Note: Try raising this value if you see errors in logs like:; # WARN - PAPI request worker PAPIQueryWorker-[...] terminated. 99 run creation requests, 0 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice.; # ERROR - Read timed out; # connect = 10 seconds; }; }; filesystems {; gcs {; # A reference to a potentially different auth for manipulating files via engine functions.; auth = ""service-account""; # Google project which will be billed for the requests; project = ""***-***"". caching {; # When a cache hit is found, the following duplication strategy will be followed to use the cached outputs; # Possible values: ""copy"", ""reference"". Defaults to ""copy""; # ""copy"": Copy the output files; # ""reference"": DO NOT copy the output files but point to the original output files instead.; # Will still make sure than all the original output files exist and are accessible before; # going forward with the cache hit.; duplication-strategy = ""copy""; }; }; }. default-runtime-attributes {; cpu: 2; failOnStderr: false; continueOnReturnCode: 0; memory: ""2048 MB""; bootDiskSizeGb: 10; # Allowed to be a String, or a list of Strings; disks: ""local-disk 10 SSD""; noAddress: false; preemptible: 0; zones: [""eu-west4-a"",""eu-west4-b"",""eu-west4-c""]; }. include ""papi_v2_reference_image_manifest.conf""; }; }; }; }; ```. Other info:; Debian GNU/Linux 10 (buster); openjdk version ""11.0.9.1-internal"" 2020-11-04 (through MiniConda, also tried with openjdk version ""11.0.12"" 2021-07-20, no difference to failure message). Permissions for service-account (quite liberal); ![image](https://user-images.githubusercontent.com/36060453/129350599-b68eee59-f08b-458f-b164-c48210b140de.png)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6469:15918,message,message,15918,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6469,1,['message'],['message']
Integrability,"![19grnh](https://cloud.githubusercontent.com/assets/791985/17954234/7ba0b698-6a47-11e6-873c-c0e60ca163e1.jpg). I'd be game if all `java.nio.Path`s across cromwell, including the engine, all backends, services, etc. were always absolute. A relative path appearing in a web response, a shell script, or even the logs would then be considered a bug. In JES, there are internal private methods such as `JesAsyncBackendJobExecutionActor.relativeLocalizationPath` that create relative paths, but these relative paths should not be externally visible. `JesAsyncBackendJobExecutionActor.jesInputsFromWdlFiles` should be creating absolute paths. Over in the SFS, the `SharedFileSystemAsyncJobExecutionActor` doesn't create relative paths, but still inconsistently calls `Path.toAbsolutePath` in various places. More usage of `better.files` instead of `java.nio.Paths.get()` would help us from omitting calls to `Path.toAbsolutePath`, since [better's `.path` member](https://github.com/pathikrit/better-files#java-interoperability) creates absolute `java.nio.Path`s (for [now](https://github.com/pathikrit/better-files/issues/48#issuecomment-157837460)). **TL;DR This might also be a problem elsewhere, including the SFS backend.**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1332#issuecomment-242266539:1005,interoperab,interoperability,1005,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1332#issuecomment-242266539,1,['interoperab'],['interoperability']
Integrability,"![image](https://user-images.githubusercontent.com/165320/46151480-da3c2080-c23c-11e8-97a4-ecfa39139c11.png). We're seeing intermittent connectivity issues w/ message of ""socket timeout, cannot connect to server"" in Pingdom. They last 1-3 minutes and seem to be off and on:; ![image](https://user-images.githubusercontent.com/165320/46151547-05267480-c23d-11e8-865a-f9c1fc1c4e4d.png). From the looks of things this looks to be between pingdom and the load balancer or proxy, as neither Cromwell nor proxy logs are showing signs of distress during these times.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4164:159,message,message,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4164,1,['message'],['message']
Integrability,"![screen shot 2015-06-14 at 8 36 19 am](https://cloud.githubusercontent.com/assets/58551/8148606/0f7b94f6-1273-11e5-8f6e-8fb7b23aa935.png). No rush to review this. This is ancillary to the sprint but it'd be nice if we could get it in by the end of the sprint. Changes:. 1) SLF4J logging hooked in with the actor system too. 2) Two modes of logging, set by the Java Property `CROMWELL_LOGGER=[SERVER|CONSOLE]`:; - In SERVER mode, it logs to a rolling file appender with all the bells and whistles. This will default to DEBUG level.; - In CONSOLE mode, there's code in `cromwell.logging` that handles these messages from SLF4J and prints them out to the console is as human-readable way as possible. I welcome comments about how to make it more readable. Though, if you are going to do that make sure you first run it so you can see the colors, which are an important aspect of this! CONSOLE logs on INFO, WARN, ERROR.; - The modes are toggled either by explicitly setting CROMWELL_LOGGER, or based on the CLI sub-command you chose: `server` will do SERVER logging and every other sub-command uses CONSOLE logging. 3) I've tried to establish some conventions for logging:; - INFO, WARN, ERROR is meant to be read by _users_ to debug their WDL executions. It should equally be helpful for _developers_ to debug many issues. We must keep in mind that these are also show up in the server logs so they could also help us add context to debugging an issue easier if we're used to these messages from the command line.; - Messages should contain the workflow UUID wherever appropriate. Anything that exists only in a context of a workflow execution: CallActors, WorkflowActors, SymbolStores, etc.; - Messages should be chosen to craft a story about how a workflow is progressing. Highlight the big points (something starts, something finishes, something is launched, symbol store entry is updated, etc)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/44:606,message,messages,606,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/44,4,"['Message', 'message']","['Messages', 'messages']"
Integrability,"![screenshot-2019-2-28 kibana](https://user-images.githubusercontent.com/1087943/53603858-d5c7bb00-3b80-11e9-9330-a9ac9f9032dc.png). [Kibana link](https://kibana.logit.io/app/kibana#/discover?_g=(refreshInterval:(display:Off,pause:!f,value:0),time:(from:now-90d,mode:quick,to:now))&_a=(columns:!(_source),index:'*-*',interval:auto,query:(query_string:(analyze_wildcard:!t,query:'host:%22gce-cromwell-prod601%22%20AND%20%22Communications%20link%20failure%22')),sort:!('@timestamp',desc))). The same error message showed up in #4360, #3387, and #2519 but in those the ""last packet"" time was short and more or less random, while here it's repeatedly 929,284 milliseconds - or precisely 15 minutes, 30 seconds.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4689:504,message,message,504,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4689,1,['message'],['message']
Integrability,""": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stdout.log"",; ""commandLine"": ""sleep 60 \necho \""Hello World! Welcome to Cromwell . . . on Google Cloud!\"""",; ""shardIndex"": -1,; ""jes"": {; ""executionBucket"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca"",; ""endpointUrl"": ""https://genomics.googleapis.com/"",; ""googleProject"": ""broad-dsde-alpha""; },; ""runtimeAttributes"": {; ""preemptible"": ""0"",; ""failOnStderr"": ""false"",; ""bootDiskSizeGb"": ""10"",; ""disks"": ""local-disk 10 SSD"",; ""continueOnReturnCode"": ""0"",; ""docker"": ""ubuntu:latest"",; ""maxRetries"": ""0"",; ""cpu"": ""1"",; ""cpuMin"": ""1"",; ""noAddress"": ""false"",; ""zones"": ""us-central1-b"",; ""memoryMin"": ""2.048 GB"",; ""memory"": ""2.048 GB""; },; ""callCaching"": {; ""allowResultReuse"": false,; ""effectiveCallCachingMode"": ""CallCachingOff""; },; ""inputs"": {; ""addressee"": ""World""; },; ""backendLabels"": {; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663"",; ""wdl-task-name"": ""hello""; },; ""labels"": {; ""wdl-task-name"": ""hello"",; ""cromwell-workflow-id"": ""cromwell-9cc9b141-b2fb-4277-94bd-80ad87a49663""; },; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""message"": ""java.lang.IllegalArgumentException: Unexpected execution handle: AbortedExecutionHandle""; }; ],; ""backend"": ""JES"",; ""end"": ""2018-12-11T16:07:04.207Z"",; ""stderr"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello/hello-stderr.log"",; ""callRoot"": ""gs://fc-391d77ef-2e8c-45e5-bfef-3d12554920ca/wf_hello/9cc9b141-b2fb-4277-94bd-80ad87a49663/call-hello"",; ""attempt"": 1,; ""executionEvents"": [; {; ""startTime"": ""2018-12-11T16:07:02.746Z"",; ""description"": ""RequestingExecutionToken"",; ""endTime"": ""2018-12-11T16:07:03.606Z""; },; {; ""startTime"": ""2018-12-11T16:07:03.648Z"",; ""description"": ""RunningJob"",; ""endTime"": ""2018-12-11T16:07:04.116Z""; },; {; ""startTime"": ""2018-12-11T16:07:04.116Z"",; ""des",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4484:1557,message,message,1557,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4484,2,['message'],['message']
Integrability,"""; }; output {; String o = read_string(stdout()); }; }. workflow w {; call t; String declarationDependingOnCallOutput = t.o; }; ```. ---. @meganshand commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261092137). Oh no! This actually makes using zips infeasible, since I'd imagine in most cases the things you want to zip will be outputs from previous tasks. I suppose I can use a workaround where inside of a scatter loop I can create a task that takes in a File and Array[File] and outputs a Pair, then scatter over the output of that task outside of the original scatter. ---. @meganshand commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261095003). I tried that workaround with a task like this:. ```; task ZipUpWorkaround {; File unmapped_bam; Array[File] fastqs. command {; #do nothing; }; output {; Pair[File, Array[File]] p = [unmapped_bam, fastqs]; }; }; ```. and got this error message (after it submitted that task):; `Failed to evaluate outputs.: WdlTypeException: Arrays/Maps must have homogeneous types`. ---. @Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261095284). I think `Pair`s are declared with parenthesis and not brackets. Does . ```; output {; Pair[File, Array[File]] p = (unmapped_bam, fastqs); }; ```. work ?. ---. @Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261096132). Also, as long as you don't declared the zip as a workflow variable you should be fine. For example, this should work:. ```; task t {; command {; echo ""hello""; echo ""world""; }; output {; Array[String] o = read_lines(stdout()); }; }. task t2 {; Array[Pair[String, String]] p; command {; #do something; }; output {; Array[Pair[String, String]] o = p; }; }. workflow w {; call t; call t as u; call t2 { input: zip(t.o, u.o) }; }; ```. ---. @meganshand commented on [Thu Nov 17 2016](https://github.com/br",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2692:1776,message,message,1776,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2692,1,['message'],['message']
Integrability,"""class\"":\""File\"",\""location\"":\""/efs/input-data/SBJ_seqcii_020.bam\""},\""normal_sample\"":\""seqcii_N020\"",\""output_dir\"":\""output\"",\""promiscuous_five_csv_linx\"":{\""class\"":\""File\"",\""location\"":\""/efs/gridss-refdata/External Resources/HMFTools-Resources/GRIDSS-Purple-Linx-Docker/hg19/dbs/knowledgebases/output/knownPromiscuousFive.csv\""},\""promiscuous_three_csv_linx\"":{\""class\"":\""File\"",\""location\"":\""/efs/gridss-refdata/External Resources/HMFTools-Resources/GRIDSS-Purple-Linx-Docker/hg19/dbs/knowledgebases/output/knownPromiscuousThree.csv\""},\""reference\"":{\""class\"":\""File\"",\""location\"":\""/efs/umccr-refdata/bwa/hg38.fa\""},\""replication_origins_file_linx\"":{\""class\"":\""File\"",\""location\"":\""/efs/gridss-refdata/External Resources/HMFTools-Resources/Linx/heli_rep_origins.bed\""},\""sample_name\"":\""SBJ_seqcii_020\"",\""snvvcf\"":{\""class\"":\""File\"",\""location\"":\""/efs/input-data/SBJ_seqcii_020.vcf.gz\""},\""tumor_bam\"":{\""class\"":\""File\"",\""location\"":\""/efs/input-data/SBJ_seqcii_020_tumor.bam\""},\""tumor_sample\"":\""seqcii_T020\"",\""viral_hosts_file_linx\"":{\""class\"":\""File\"",\""location\"":\""/efs/gridss-refdata/External Resources/HMFTools-Resources/Linx/viral_host_ref.csv\""}}"",; ""workflowUrl"": """",; ""labels"": ""{}""; },; ""calls"": {},; ""outputs"": {},; ""actualWorkflowLanguage"": ""CWL"",; ""id"": ""8681f8fa-7624-4bba-bc94-a697d1d2d179"",; ""inputs"": {},; ""labels"": {; ""cromwell-workflow-id"": ""cromwell-8681f8fa-7624-4bba-bc94-a697d1d2d179""; },; ""submission"": ""2020-09-02T09:23:04.304Z"",; ""status"": ""Failed"",; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""FileStepUUID(file:///tmp/tmp.Olzr17Zus8_cromwell/cwl_temp_dir_5247448030953921891/cwl_temp_file_8681f8fa-7624-4bba-bc94-a697d1d2d179.cwl,Some(main),out_vcf,gridss_step,) (of class cwl.FileStepUUID)""; }; ],; ""message"": ""Workflow input processing failed""; }; ],; ""workflowLog"": ""wf_logs/workflow.8681f8fa-7624-4bba-bc94-a697d1d2d179.log"",; ""end"": ""2020-09-02T09:23:06.270Z"",; ""start"": ""2020-09-02T09:23:04.925Z""; }; ```. </details>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5826:129935,message,message,129935,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5826,2,['message'],['message']
Integrability,"""failures"": [{""causedBy"": [{""causedBy"": [],""message"": ""the local copy message must have path set.""}],""message"": ""Unable to complete JES Api Request""}]. See workflow metadata at: https://cromwell-v29.dsde-methods.broadinstitute.org/api/workflows/v1/4ff9cb8a-cade-482a-8492-66ea3b7a2eaa/",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2791:44,message,message,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2791,3,['message'],['message']
Integrability,"# What Happened. On Friday September 14, a user noted that they were unable to retrieve metadata associated with their workflow. Subsequent calls were made to the endpoint directly to retrieve this metadata. During this time, New Relic reported memory exhaustion and extensive (~30 mins) of garbage collection. Ultimately, the instance stopped responding to requests but continued accepting connections, resulting in proxy timeout log messages. ![image](https://user-images.githubusercontent.com/165320/45637785-56827700-ba79-11e8-9176-1991692fcc76.png). # What should have happened. Crowell frontend should have either:. * returned the result in a timely manner ; * failed more gracefully. # What we did to fix it. Rebooted the instances. Subsequent calls to retrieve the metadata also timed out but did not put the frontend back into the ""zombie"" state. # Potential causes. The metadata is too large to fit in memory. The present situation is that there is some processing done between DB and user in order to provide a more structured response. # Potential fixes. The timeout on Cromwell should be increased beyond the current 20s. The metadata could always be larger than the instance has memory. Either a streaming response or deferred computation of the structured result would be better. # Technical Addendums:. Error Message when unresponsive:. ```; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.508796 2018] [proxy:error] [pid 162:tid 139866926597888] [client 130.211.0.195:49012] AH00898: Error reading from remote server returned by /engine/v1/version; September 14th 2018, 14:19:31.000 - Sep 14 14:19:31 gce-cromwell-prod801 cromwell-proxy[2525]: [Fri Sep 14 14:19:31.316500 2018] [proxy:error] [pid 162:tid 139867379803904] (110)Connection timed out: AH00957: HTTP: attempt to connect to 172.17.0.2:8000 (app) failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4105:435,message,messages,435,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4105,2,"['Message', 'message']","['Message', 'messages']"
Integrability,"# What happened:. On 8/25/18, Dockerhub performed some scheduled maintenance. Cromwell subsequently failed to start new jobs as PAPI reported 500 errors from Dockerhub. # What should have happened:. Cromwell should be resilient to outages in its dependencies, in this case docker hosts. It should *not* report as down, but instead should be in a ""degraded"" state where jobs may be submitted/finished/etc. but new jobs will not be started until the docker host is back to full health. This should be a nuanced status check. GCR images may still be pulled when Dockerhub is down, so those jobs should proceed as planned.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4056:246,depend,dependencies,246,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4056,1,['depend'],['dependencies']
Integrability,"# What we have today. All runnable jobs are ""submitted"" (started but do not automatically run). We then apply backpressure using a token system on runnable jobs. Jobs which are runnable but not given a token reside in memory. # What we want. We do not pull any new jobs to be run unless all of the following conditions are met:. * The number of running jobs is below a threshold; * Dockerhub is healthy; * PAPI is healthy; * Database is healthy; * GCR is healthy!; * [insert other dependent systems here please]. # What this will give us. * A more powerful pull-based architecture (no need for backpressure gymnastics); * More resiliency to failures of dependent systems; * Less memory pressure. # What the author of this issue does not yet know / needs investigation. * Whether new runnable jobs from a new workflow are started(""submitted"" might be better word) automatically or persisted to the DB before they are run. This issue generally assumes this algorithm applies at the time of retrieving runnable jobs from the DB. # Technical miscellanies. * [fs2 Signal](https://github.com/functional-streams-for-scala/fs2/blob/072776fc8ba5ec41c9e8cdd0c28b6e719375112a/core/shared/src/main/scala/fs2/concurrent/Signal.scala) Is useful to share mutable state between threads, in this case the health status of our dependent services.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4138:481,depend,dependent,481,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4138,3,['depend'],['dependent']
Integrability,"## About this PR; ðŸ“¦ Updates ; * [ch.qos.logback:logback-access](https://github.com/qos-ch/logback); * [ch.qos.logback:logback-classic](https://github.com/qos-ch/logback); * [ch.qos.logback:logback-core](https://github.com/qos-ch/logback). from `1.2.11` to `1.2.12`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""ch.qos.logback"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""ch.qos.logback"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7260:908,depend,dependency,908,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7260,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates ; * [com.dimafeng:testcontainers-scala-mariadb](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-mysql](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-postgresql](https://github.com/testcontainers/testcontainers-scala); * [com.dimafeng:testcontainers-scala-scalatest](https://github.com/testcontainers/testcontainers-scala). from `0.40.10` to `0.40.17`. ðŸ“œ [GitHub Release Notes](https://github.com/testcontainers/testcontainers-scala/releases/tag/v0.40.17) - [Version Diff](https://github.com/testcontainers/testcontainers-scala/compare/v0.40.10...v0.40.17). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.dimafeng"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.dimafeng"" }; }]; ```; </details>. <sup>; labels: test-library-update, early-semver-minor, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7270:1321,depend,dependency,1321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7270,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates ; * [io.circe:circe-core](https://github.com/circe/circe); * [io.circe:circe-generic](https://github.com/circe/circe); * [io.circe:circe-literal](https://github.com/circe/circe); * [io.circe:circe-parser](https://github.com/circe/circe); * [io.circe:circe-refined](https://github.com/circe/circe); * [io.circe:circe-shapes](https://github.com/circe/circe). from `0.14.1` to `0.14.6`. ðŸ“œ [GitHub Release Notes](https://github.com/circe/circe/releases/tag/v0.14.6) - [Version Diff](https://github.com/circe/circe/compare/v0.14.1...v0.14.6). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, version-scheme:early-semver, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7292:1208,depend,dependency,1208,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7292,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates ; * [org.http4s:http4s-ember-client](https://github.com/http4s/http4s); * [org.http4s:http4s-ember-server](https://github.com/http4s/http4s). from `0.21.31` to `0.21.34`. ðŸ“œ [GitHub Release Notes](https://github.com/http4s/http4s/releases/tag/v0.21.34) - [Version Diff](https://github.com/http4s/http4s/compare/v0.21.31...v0.21.34). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.http4s"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.http4s"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, version-scheme:early-semver, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7311:1002,depend,dependency,1002,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7311,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates ; * [org.junit.jupiter:junit-jupiter-api](https://github.com/junit-team/junit5); * [org.junit.jupiter:junit-jupiter-engine](https://github.com/junit-team/junit5); * [org.junit.jupiter:junit-jupiter-params](https://github.com/junit-team/junit5). from `5.9.3` to `5.10.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.junit.jupiter"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.junit.jupiter"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7312:941,depend,dependency,941,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7312,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates ; * [org.typelevel:alleycats-core](https://github.com/typelevel/cats); * [org.typelevel:cats-core](https://github.com/typelevel/cats). from `2.7.0` to `2.10.0`. ðŸ“œ [GitHub Release Notes](https://github.com/typelevel/cats/releases/tag/v2.10.0) - [Version Diff](https://github.com/typelevel/cats/compare/v2.7.0...v2.10.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.7.0).; You might want to review and update them manually.; ```; services/src/test/scala/cromwell/services/database/QueryTimeoutSpec.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.typelevel"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7320:1288,depend,dependency,1288,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7320,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [cglib:cglib-nodep](https://github.com/cglib/cglib) from `3.2.7` to `3.2.12`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""cglib"", artifactId = ""cglib-nodep"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""cglib"", artifactId = ""cglib-nodep"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7259:748,depend,dependency,748,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7259,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.azure.resourcemanager:azure-resourcemanager](https://github.com/Azure/azure-sdk-for-java) from `2.18.0` to `2.33.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure.resourcemanager"", artifactId = ""azure-resourcemanager"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure.resourcemanager"", artifactId = ""azure-resourcemanager"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7269:792,depend,dependency,792,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7269,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-core-http-okhttp](https://github.com/Azure/azure-sdk-for-java) from `1.11.10` to `1.11.17`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core-http-okhttp"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core-http-okhttp"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7262:779,depend,dependency,779,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7262,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-core-management](https://github.com/Azure/azure-sdk-for-java) from `1.7.1` to `1.11.9`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.7.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core-management"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core-management"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7263:857,Depend,Dependencies,857,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7263,5,"['Depend', 'depend']","['Dependencies', 'dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-core-test](https://github.com/Azure/azure-sdk-for-java) from `1.18.0` to `1.18.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.18.0).; You might want to review and update them manually.; ```; cloud-nio/cloud-nio-impl-drs/src/test/scala/cloud/nio/impl/drs/DrsPathResolverSpec.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core-test"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core-test"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7264:1084,depend,dependency,1084,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7264,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-core](https://github.com/Azure/azure-sdk-for-java) from `1.40.0` to `1.45.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7261:765,depend,dependency,765,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7261,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-identity-extensions](https://github.com/azure/azure-sdk-for-java) from `1.1.4` to `1.1.10`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-identity-extensions"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-identity-extensions"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7266:779,depend,dependency,779,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7266,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-identity](https://github.com/Azure/azure-sdk-for-java) from `1.9.1` to `1.9.2`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-identity"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-identity"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7265:767,depend,dependency,767,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7265,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-storage-blob](https://github.com/Azure/azure-sdk-for-java) from `12.23.0-beta.1` to `12.23.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-storage-blob"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-storage-blob"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-pre-release, semver-spec-pre-release, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7267:782,depend,dependency,782,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7267,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.azure:azure-storage-common](https://github.com/Azure/azure-sdk-for-java) from `12.22.0-beta.1` to `12.22.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.azure"", artifactId = ""azure-storage-common"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.azure"", artifactId = ""azure-storage-common"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-pre-release, semver-spec-pre-release, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7268:784,depend,dependency,784,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7268,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.eed3si9n:sbt-assembly](https://github.com/sbt/sbt-assembly) from `1.1.1` to `2.1.5` âš . ðŸ“œ [GitHub Release Notes](https://github.com/sbt/sbt-assembly/releases/tag/v2.1.5) - [Version Diff](https://github.com/sbt/sbt-assembly/compare/v1.1.1...v2.1.5). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.1.1).; You might want to review and update them manually.; ```; womtool/src/test/resources/validate/wdl_draft3/valid/arrays_v1/arrays_v1.inputs.json; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" }; }]; ```; </details>. <sup>; labels: sbt-plugin-update, early-semver-major, semver-spec-major, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7271:1232,depend,dependency,1232,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7271,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.fasterxml.jackson.dataformat:jackson-dataformat-xml](https://github.com/FasterXML/jackson-dataformat-xml) from `2.13.3` to `2.13.5`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.fasterxml.jackson.dataformat"", artifactId = ""jackson-dataformat-xml"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.fasterxml.jackson.dataformat"", artifactId = ""jackson-dataformat-xml"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7272:808,depend,dependency,808,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7272,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.github.cb372:sbt-explicit-dependencies](https://github.com/cb372/sbt-explicit-dependencies) from `0.2.16` to `0.3.1`. ðŸ“œ [GitHub Release Notes](https://github.com/cb372/sbt-explicit-dependencies/releases/tag/v0.3.1) - [Version Diff](https://github.com/cb372/sbt-explicit-dependencies/compare/v0.2.16...v0.3.1). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.github.cb372"", artifactId = ""sbt-explicit-dependencies"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.github.cb372"", artifactId = ""sbt-explicit-dependencies"" }; }]; ```; </details>. <sup>; labels: sbt-plugin-update, early-semver-major, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7273:59,depend,dependencies,59,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7273,10,['depend'],"['dependencies', 'dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.github.sbt:junit-interface](https://github.com/sbt/junit-interface) from `0.13.2` to `0.13.3`. ðŸ“œ [GitHub Release Notes](https://github.com/sbt/junit-interface/releases/tag/v0.13.3) - [Version Diff](https://github.com/sbt/junit-interface/compare/v0.13.2...v0.13.3). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.13.2).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.github.sbt"", artifactId = ""junit-interface"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.github.sbt"", artifactId = ""junit-interface"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7274:50,interface,interface,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7274,11,"['Depend', 'depend', 'interface']","['Dependencies', 'dependency', 'dependencyOverrides', 'interface']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.google.api-client:google-api-client-jackson2](https://github.com/googleapis/google-api-java-client) from `2.1.4` to `2.2.0`. ðŸ“œ [GitHub Release Notes](https://github.com/googleapis/google-api-java-client/releases/tag/v2.2.0) - [Version Diff](https://github.com/googleapis/google-api-java-client/compare/v2.1.4...v2.2.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api-client"", artifactId = ""google-api-client-jackson2"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.api-client"", artifactId = ""google-api-client-jackson2"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7276:995,depend,dependency,995,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7276,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.google.api.grpc:proto-google-cloud-batch-v1](https://github.com/googleapis/google-cloud-java) from `0.18.0` to `0.30.0`. ðŸ“œ [GitHub Release Notes](https://github.com/googleapis/google-cloud-java/releases/tag/v0.30.0) - [Version Diff](https://github.com/googleapis/google-cloud-java/compare/v0.18.0...v0.30.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api.grpc"", artifactId = ""proto-google-cloud-batch-v1"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.api.grpc"", artifactId = ""proto-google-cloud-batch-v1"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7277:984,depend,dependency,984,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7277,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.google.api.grpc:proto-google-cloud-resourcemanager-v3](https://github.com/googleapis/google-cloud-java) from `1.17.0` to `1.32.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.17.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api.grpc"", artifactId = ""proto-google-cloud-resourcemanager-v3"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.api.grpc"", artifactId = ""proto-google-cloud-resourcemanager-v3"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7278:889,Depend,Dependencies,889,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7278,5,"['Depend', 'depend']","['Dependencies', 'dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.google.api:gax-grpc](https://github.com/googleapis/sdk-platform-java) from `2.25.0` to `2.38.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.25.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.api"", artifactId = ""gax-grpc"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.api"", artifactId = ""gax-grpc"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7275:855,Depend,Dependencies,855,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7275,5,"['Depend', 'depend']","['Dependencies', 'dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.google.auth:google-auth-library-oauth2-http](https://github.com/googleapis/google-auth-library-java) from `1.5.3` to `1.20.0`. ðŸ“œ [GitHub Release Notes](https://github.com/googleapis/google-auth-library-java/releases/tag/v1.20.0) - [Version Diff](https://github.com/googleapis/google-auth-library-java/compare/v1.5.3...v1.20.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.auth"", artifactId = ""google-auth-library-oauth2-http"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.auth"", artifactId = ""google-auth-library-oauth2-http"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7281:1003,depend,dependency,1003,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7281,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.google.cloud:google-cloud-batch](https://github.com/googleapis/google-cloud-java) from `0.18.0` to `0.30.0`. ðŸ“œ [GitHub Release Notes](https://github.com/googleapis/google-cloud-java/releases/tag/v0.30.0) - [Version Diff](https://github.com/googleapis/google-cloud-java/compare/v0.18.0...v0.30.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-batch"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-batch"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7282:972,depend,dependency,972,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7282,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.google.cloud:google-cloud-bigquery](https://github.com/googleapis/java-bigquery) from `2.25.0` to `2.34.2`. ðŸ“œ [GitHub Release Notes](https://github.com/googleapis/java-bigquery/releases/tag/v2.34.2) - [Version Diff](https://github.com/googleapis/java-bigquery/compare/v2.25.0...v2.34.2). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.25.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-bigquery"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-bigquery"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7283:1046,Depend,Dependencies,1046,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7283,5,"['Depend', 'depend']","['Dependencies', 'dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.google.cloud:google-cloud-resourcemanager](https://github.com/googleapis/google-cloud-java) from `1.17.0` to `1.32.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.17.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-resourcemanager"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-resourcemanager"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7284:877,Depend,Dependencies,877,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7284,5,"['Depend', 'depend']","['Dependencies', 'dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.google.cloud:google-cloud-storage](https://github.com/googleapis/java-storage) from `2.17.2` to `2.29.1`. ðŸ“œ [GitHub Release Notes](https://github.com/googleapis/java-storage/releases/tag/v2.29.1) - [Version Diff](https://github.com/googleapis/java-storage/compare/v2.17.2...v2.29.1). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.17.2).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.cloud"", artifactId = ""google-cloud-storage"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.cloud"", artifactId = ""google-cloud-storage"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7285:1042,Depend,Dependencies,1042,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7285,5,"['Depend', 'depend']","['Dependencies', 'dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [com.typesafe:config](https://github.com/lightbend/config) from `1.4.2` to `1.4.3`. ðŸ“œ [GitHub Release Notes](https://github.com/lightbend/config/releases/tag/v1.4.3) - [Version Diff](https://github.com/lightbend/config/compare/v1.4.2...v1.4.3). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.typesafe"", artifactId = ""config"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.typesafe"", artifactId = ""config"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7286:915,depend,dependency,915,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7286,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [commons-codec:commons-codec](https://github.com/apache/commons-codec) from `1.15` to `1.16.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.15).; You might want to review and update them manually.; ```; docs/developers/bitesize/workflowParsing/wdlToWdlom_wdlom.svg; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""commons-codec"", artifactId = ""commons-codec"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""commons-codec"", artifactId = ""commons-codec"" }; }]; ```; </details>. <sup>; labels: library-update, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7287:910,Depend,Dependencies,910,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7287,5,"['Depend', 'depend']","['Dependencies', 'dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [commons-io:commons-io](https://commons.apache.org/proper/commons-io/) from `2.11.0` to `2.15.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.11.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""commons-io"", artifactId = ""commons-io"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""commons-io"", artifactId = ""commons-io"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7288:851,Depend,Dependencies,851,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7288,5,"['Depend', 'depend']","['Dependencies', 'dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [commons-net:commons-net](https://commons.apache.org/proper/commons-net/) from `3.8.0` to `3.10.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""commons-net"", artifactId = ""commons-net"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""commons-net"", artifactId = ""commons-net"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7289:770,depend,dependency,770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7289,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [eu.timepit:refined](https://github.com/fthomas/refined) from `0.10.1` to `0.10.3`. ðŸ“œ [GitHub Release Notes](https://github.com/fthomas/refined/releases/tag/v0.10.3) - [Version Diff](https://github.com/fthomas/refined/compare/v0.10.1...v0.10.3). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""eu.timepit"", artifactId = ""refined"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""eu.timepit"", artifactId = ""refined"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7290:916,depend,dependency,916,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7290,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [io.circe:circe-config](https://github.com/circe/circe-config) from `0.8.0` to `0.10.1`. ðŸ“œ [GitHub Release Notes](https://github.com/circe/circe-config/releases/tag/v0.10.1). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"", artifactId = ""circe-config"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"", artifactId = ""circe-config"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, version-scheme:early-semver, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7291:845,depend,dependency,845,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7291,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [io.circe:circe-optics](https://github.com/circe/circe-optics) from `0.14.1` to `0.15.0`. ðŸ“œ [GitHub Release Notes](https://github.com/circe/circe-optics/releases/tag/v0.15.0) - [Version Diff](https://github.com/circe/circe-optics/compare/v0.14.1...v0.15.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.14.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.circe"", artifactId = ""circe-optics"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.circe"", artifactId = ""circe-optics"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-minor, version-scheme:early-semver, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7293:1011,Depend,Dependencies,1011,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7293,5,"['Depend', 'depend']","['Dependencies', 'dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [io.grpc:grpc-core](https://github.com/grpc/grpc-java) from `1.54.1` to `1.54.2`. ðŸ“œ [GitHub Release Notes](https://github.com/grpc/grpc-java/releases/tag/v1.54.2) - [Version Diff](https://github.com/grpc/grpc-java/compare/v1.54.1...v1.54.2). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.grpc"", artifactId = ""grpc-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.grpc"", artifactId = ""grpc-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7295:912,depend,dependency,912,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7295,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [io.projectreactor:reactor-test](https://github.com/reactor/reactor-core) from `3.4.29` to `3.4.34`. ðŸ“œ [GitHub Release Notes](https://github.com/reactor/reactor-core/releases/tag/v3.4.34) - [Version Diff](https://github.com/reactor/reactor-core/compare/v3.4.29...v3.4.34). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.projectreactor"", artifactId = ""reactor-test"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.projectreactor"", artifactId = ""reactor-test"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7296:943,depend,dependency,943,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7296,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [io.sentry:sentry-logback](https://github.com/getsentry/sentry-java) from `5.7.4` to `7.0.0` âš . ðŸ“œ [GitHub Release Notes](https://github.com/getsentry/sentry-java/releases/tag/7.0.0) - [Version Diff](https://github.com/getsentry/sentry-java/compare/5.7.4...7.0.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.sentry"", artifactId = ""sentry-logback"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.sentry"", artifactId = ""sentry-logback"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-major, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7297:934,depend,dependency,934,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7297,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [io.swagger:swagger-parser](https://github.com/swagger-api/swagger-parser) from `1.0.56` to `1.0.68`. ðŸ“œ [GitHub Release Notes](https://github.com/swagger-api/swagger-parser/releases/tag/v1.0.68) - [Version Diff](https://github.com/swagger-api/swagger-parser/compare/v1.0.56...v1.0.68). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""io.swagger"", artifactId = ""swagger-parser"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""io.swagger"", artifactId = ""swagger-parser"" }; }]; ```; </details>. <sup>; labels: test-library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7298:956,depend,dependency,956,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7298,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [net.minidev:json-smart](https://github.com/netplex/json-smart-v2) from `2.4.10` to `2.4.11`. ðŸ“œ [GitHub Release Notes](https://github.com/netplex/json-smart-v2/releases/tag/2.4.11) - [Version Diff](https://github.com/netplex/json-smart-v2/compare/2.4.10...2.4.11). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""net.minidev"", artifactId = ""json-smart"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""net.minidev"", artifactId = ""json-smart"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7300:935,depend,dependency,935,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7300,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [org.apache.commons:commons-lang3](https://commons.apache.org/proper/commons-lang/) from `3.12.0` to `3.14.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.commons"", artifactId = ""commons-lang3"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.apache.commons"", artifactId = ""commons-lang3"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7301:781,depend,dependency,781,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7301,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [org.apache.tika:tika-core](https://tika.apache.org/) from `2.3.0` to `2.9.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.3.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.aws.inputs.json; centaur/src/main/resources/integrationTestCases/germline/single-sample-workflow/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.json; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.tika"", artifactId = ""tika-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.apache.tika"", artifactId = ""tika-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7303:850,integrat,integrationTestCases,850,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7303,6,"['depend', 'integrat']","['dependency', 'dependencyOverrides', 'integrationTestCases']"
Integrability,"## About this PR; ðŸ“¦ Updates [org.codehaus.janino:janino](https://github.com/janino-compiler/janino) from `3.1.7` to `3.1.11`. ðŸ“œ [GitHub Release Notes](https://github.com/janino-compiler/janino/releases/tag/v3.1.11) - [Version Diff](https://github.com/janino-compiler/janino/compare/3.1.7...3.1.11) - [Version Diff](https://github.com/janino-compiler/janino/compare/v3.1.7...v3.1.11). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.codehaus.janino"", artifactId = ""janino"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.codehaus.janino"", artifactId = ""janino"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7307:1026,depend,dependency,1026,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7307,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [org.glassfish.jersey.inject:jersey-hk2](https://github.com/eclipse-ee4j/jersey) from `2.32` to `2.41`. ðŸ“œ [GitHub Release Notes](https://github.com/eclipse-ee4j/jersey/releases/tag/2.41) - [Version Diff](https://github.com/eclipse-ee4j/jersey/compare/2.32...2.41). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.32).; You might want to review and update them manually.; ```; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.glassfish.jersey.inject"", artifactId = ""jersey-hk2"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.glassfish.jersey.inject"", artifactId = ""jersey-hk2"" }; }]; ```; </details>. <sup>; labels: library-update, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7308:50,inject,inject,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7308,8,"['Depend', 'depend', 'inject']","['Dependencies', 'dependency', 'dependencyOverrides', 'inject']"
Integrability,"## About this PR; ðŸ“¦ Updates [org.gnieh:diffson-spray-json](https://github.com/gnieh/diffson) from `4.1.1` to `4.4.0`. ðŸ“œ [GitHub Release Notes](https://github.com/gnieh/diffson/releases/tag/v4.4.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.gnieh"", artifactId = ""diffson-spray-json"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.gnieh"", artifactId = ""diffson-spray-json"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7309:840,depend,dependency,840,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7309,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [org.hsqldb:hsqldb](http://hsqldb.org) from `2.6.1` to `2.7.2`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.6.1).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.hsqldb"", artifactId = ""hsqldb"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.hsqldb"", artifactId = ""hsqldb"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7310:816,Depend,Dependencies,816,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7310,5,"['Depend', 'depend']","['Dependencies', 'dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [org.mariadb.jdbc:mariadb-java-client](https://github.com/mariadb-corporation/mariadb-connector-j) from `2.7.4` to `2.7.11`. ðŸ“œ [GitHub Release Notes](https://github.com/mariadb-corporation/mariadb-connector-j/releases/tag/2.7.11) - [Changelog](https://github.com/mariadb-corporation/mariadb-connector-j/blob/master/CHANGELOG.md) - [Version Diff](https://github.com/mariadb-corporation/mariadb-connector-j/compare/2.7.4...2.7.11). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mariadb.jdbc"", artifactId = ""mariadb-java-client"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.mariadb.jdbc"", artifactId = ""mariadb-java-client"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7314:1100,depend,dependency,1100,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7314,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [org.mockito:mockito-core](https://github.com/mockito/mockito) from `4.11.0` to `5.7.0` âš . ðŸ“œ [GitHub Release Notes](https://github.com/mockito/mockito/releases/tag/v5.7.0) - [Version Diff](https://github.com/mockito/mockito/compare/v4.11.0...v5.7.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.mockito"", artifactId = ""mockito-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.mockito"", artifactId = ""mockito-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-major, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7315:921,depend,dependency,921,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7315,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [org.scala-graph:graph-core](https://github.com/scala-graph/scala-graph) from `1.13.1` to `1.13.6`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scala-graph"", artifactId = ""graph-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scala-graph"", artifactId = ""graph-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7316:770,depend,dependency,770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7316,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [org.scala-lang:scala-library](https://github.com/scala/scala) from `2.13.9` to `2.13.12`. ðŸ“œ [GitHub Release Notes](https://github.com/scala/scala/releases/tag/v2.13.12) - [Version Diff](https://github.com/scala/scala/compare/v2.13.9...v2.13.12). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scala-lang"", artifactId = ""scala-library"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scala-lang"", artifactId = ""scala-library"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7317:917,depend,dependency,917,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7317,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [org.scalatest:scalatest](https://github.com/scalatest/scalatest) from `3.2.15` to `3.2.17`. ðŸ“œ [GitHub Release Notes](https://github.com/scalatest/scalatest/releases/tag/release-3.2.17) - [Version Diff](https://github.com/scalatest/scalatest/compare/release-3.2.15...release-3.2.17). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scalatest"", artifactId = ""scalatest"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scalatest"", artifactId = ""scalatest"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7318:954,depend,dependency,954,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7318,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [org.scoverage:sbt-scoverage](https://github.com/scoverage/sbt-scoverage) from `2.0.4` to `2.0.9`. ðŸ“œ [GitHub Release Notes](https://github.com/scoverage/sbt-scoverage/releases/tag/v2.0.9) - [Version Diff](https://github.com/scoverage/sbt-scoverage/compare/v2.0.4...v2.0.9). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.scoverage"", artifactId = ""sbt-scoverage"" }; }]; ```; </details>. <sup>; labels: sbt-plugin-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7319:944,depend,dependency,944,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7319,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [org.typelevel:kittens](https://github.com/typelevel/kittens) from `2.3.2` to `3.1.0` âš . ðŸ“œ [GitHub Release Notes](https://github.com/typelevel/kittens/releases/tag/v3.1.0) - [Version Diff](https://github.com/typelevel/kittens/compare/v2.3.2...v3.1.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (2.3.2).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"", artifactId = ""kittens"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.typelevel"", artifactId = ""kittens"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-major, semver-spec-major, version-scheme:early-semver, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7321:1004,Depend,Dependencies,1004,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7321,5,"['Depend', 'depend']","['Dependencies', 'dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [org.typelevel:mouse](https://github.com/typelevel/mouse) from `1.0.11` to `1.2.2`. ðŸ“œ [GitHub Release Notes](https://github.com/typelevel/mouse/releases/tag/v1.2.2) - [Version Diff](https://github.com/typelevel/mouse/compare/v1.0.11...v1.2.2). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.0.11).; You might want to review and update them manually.; ```; .sdkmanrc; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.typelevel"", artifactId = ""mouse"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.typelevel"", artifactId = ""mouse"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, version-scheme:early-semver, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7322:1149,depend,dependency,1149,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7322,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [org.yaml:snakeyaml](https://bitbucket.org/snakeyaml/snakeyaml/src) from `1.33` to `2.2`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.33).; You might want to review and update them manually.; ```; core/src/test/resources/hello_goodbye_scattered_papiv2.json; project/Dependencies.scala; scripts/metadata_comparison/test/resources/comparer/papiv1_version3_good.json; scripts/metadata_comparison/test/resources/comparer/papiv2_version3_good.json; scripts/metadata_comparison/test/resources/comparer/version3_comparison_good.csv; src/ci/resources/papi_v2_reference_image_manifest.conf; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.yaml"", artifactId = ""snakeyaml"" }; }]; ```; </details>. <sup>; labels: test-library-update, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7324:902,Depend,Dependencies,902,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7324,5,"['Depend', 'depend']","['Dependencies', 'dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates [se.marcuslonnberg:sbt-docker](https://github.com/marcuslonnberg/sbt-docker) from `1.9.0` to `1.11.0`. ðŸ“œ [GitHub Release Notes](https://github.com/marcuslonnberg/sbt-docker/releases/tag/v1.11.0) - [Changelog](https://github.com/marcuslonnberg/sbt-docker/blob/master/CHANGELOG.md) - [Version Diff](https://github.com/marcuslonnberg/sbt-docker/compare/v1.9.0...v1.11.0). ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (1.9.0).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""se.marcuslonnberg"", artifactId = ""sbt-docker"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""se.marcuslonnberg"", artifactId = ""sbt-docker"" }; }]; ```; </details>. <sup>; labels: sbt-plugin-update, early-semver-minor, semver-spec-minor, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7325:1121,Depend,Dependencies,1121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7325,5,"['Depend', 'depend']","['Dependencies', 'dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates bio.terra:workspace-manager-client from `0.254.452-SNAPSHOT` to `0.254.966-SNAPSHOT`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>ðŸ” Files still referring to the old version number</summary>. The following files still refer to the old version number (0.254.452-SNAPSHOT).; You might want to review and update them manually.; ```; project/Dependencies.scala; ```; </details>; <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""bio.terra"", artifactId = ""workspace-manager-client"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""bio.terra"", artifactId = ""workspace-manager-client"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-patch, old-version-remains, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7258:851,Depend,Dependencies,851,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7258,5,"['Depend', 'depend']","['Dependencies', 'dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates com.google.apis:google-api-services-cloudkms from `v1-rev20230421-2.0.0` to `v1-rev20231012-2.0.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-cloudkms"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-cloudkms"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7279:770,depend,dependency,770,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7279,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates com.google.apis:google-api-services-lifesciences from `v2beta-rev20220916-2.0.0` to `v2beta-rev20230707-2.0.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.google.apis"", artifactId = ""google-api-services-lifesciences"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""com.google.apis"", artifactId = ""google-api-services-lifesciences"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7280:782,depend,dependency,782,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7280,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates mysql:mysql-connector-java from `8.0.28` to `8.0.33`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""mysql"", artifactId = ""mysql-connector-java"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""mysql"", artifactId = ""mysql-connector-java"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-patch, semver-spec-patch, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7299:724,depend,dependency,724,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7299,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates org.apache.commons:commons-text from `1.10.0` to `1.11.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.apache.commons"", artifactId = ""commons-text"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.apache.commons"", artifactId = ""commons-text"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7302:729,depend,dependency,729,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7302,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates org.broadinstitute.dsde.workbench:workbench-google from `0.21-5c9c4f6` to `0.30-2147824`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7304:760,depend,dependency,760,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7304,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates org.broadinstitute.dsde.workbench:workbench-google from `0.21-5c9c4f6` to `0.30-5781917`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/9ac858c7e61f43ed3648f0fabc7104d0951cce67/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-google"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7331:760,depend,dependency,760,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7331,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates org.broadinstitute.dsde.workbench:workbench-model from `0.15-f9f0d4c` to `0.19-8376167`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-model"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-model"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7305:759,depend,dependency,759,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7305,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates org.broadinstitute.dsde.workbench:workbench-util from `0.6-65bba14` to `0.10-8376167`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-util"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.broadinstitute.dsde.workbench"", artifactId = ""workbench-util"" }; }]; ```; </details>. <sup>; labels: library-update, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7306:757,depend,dependency,757,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7306,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates org.liquibase:liquibase-core from `4.8.0` to `4.25.0`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.liquibase"", artifactId = ""liquibase-core"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.liquibase"", artifactId = ""liquibase-core"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7313:725,depend,dependency,725,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7313,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## About this PR; ðŸ“¦ Updates org.webjars:swagger-ui from `4.5.2` to `4.19.1`. ## Usage; âœ… **Please merge!**. I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/57f91b22bf9b52c8cc7ea9474b188ac173019619/docs/repo-specific-configuration.md) file. _Have a fantastic day writing Scala!_. <details>; <summary>âš™ Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.webjars"", artifactId = ""swagger-ui"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""30 days"" },; dependency = { groupId = ""org.webjars"", artifactId = ""swagger-ui"" }; }]; ```; </details>. <sup>; labels: library-update, early-semver-minor, semver-spec-minor, commit-count:1; </sup>",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7323:719,depend,dependency,719,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7323,4,['depend'],"['dependency', 'dependencyOverrides']"
Integrability,"## Liquibase logging changes. This PR started out just fixing the leaking of Liquabase messages into stdout. Before this PR, from [logs from a recent run](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577494577):. | Application | Logger | Level | Message |; |---|---|---|---|; | cromwell | slf4j | INFO | 2022-07-22 13:23:56,018 INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:57 PM liquibase.lockservice |; | cromwell | stdout | INFO | INFO: Successfully acquired change log lock |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur |",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:87,message,messages,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,2,"['Message', 'message']","['Message', 'messages']"
Integrability,"### Description. After playing a while with GCP Batch:; 1. Batch can automatically retry preemption errors.; 2. When Batch retries, there is no signal in the Job status events, we need to check the VM logs.; 3. Cromwell does not get any details about Batch retries, hence, the same jobId is kept even if a VM is recreated.; 4. When the job status events mention that the job failed due to a preemption error, this is final, Batch already exhausted the retries. This removes all the code related to handling preemption errors and parses the job status events to derive the failure reason. Also, this tries detecting the other potential exit codes mapping them to a better error message. Refs:; - [Batch automated task retries](https://cloud.google.com/batch/docs/automate-task-retries); - [Batch exit codes](https://cloud.google.com/batch/docs/troubleshooting#reserved-exit-codes). <!-- What is the purpose of this change? What should reviewers know? -->. Fixes #7407. This is an example error log produced when getting a preemption error:. ```; [2024-06-21 12:30:09,28] [info] WorkflowManagerActor: Workflow 2cdef371-703c-4c1e-92b5-0e013dcda6c8 failed (during ExecutingWorkflowState): java.lang.Exception: Task myWorkflow.myTask:NA:1 failed: A Spot VM for the job was preempted during run time; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7457:677,message,message,677,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7457,1,['message'],['message']
Integrability,"### Description. After using it in practice for a while, here's a small round of changes:; - Remove JDK. This was by far the largest by megabytes and the most fickle build process. It was really only there in case I wanted to use `jstack` as a backup if I couldn't connect YourKit; but we now have a [blessed procedure](https://docs.google.com/document/d/1bmlrM3lpNP2c1_wnm2TzQmvtbsid2g-ZEdx41LcsECw/edit) to run YourKit in any environment.; - Message-of-the-day on container login. Enhanced situational awareness to make sure you're on the container you want, and the container is running the version you think it is. Without the JDK, the image is 634 MB, only 16% larger than baseline at 547 MB. MOTD example:; ```; > kubectl exec -it -n terra-dev cromwell1-runner-76f7b5d5df-qpwdl -c cromwell1-runner-app -- bash; Version 88-6e242af-DEBUG built at 2024-05-21 18:07:36; root@cromwell1-runner-76f7b5d5df-qpwdl:/# ; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7443:444,Message,Message-of-the-day,444,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7443,1,['Message'],['Message-of-the-day']
Integrability,"### Description. Resolves intermittent build breakage caused by 404s of `paleo-core` artifacts. `paleo-core` is deprecated, and so is the library that depends on it, `swagger2markup`. - Remove code and build components; - Clean up docs and provide reasonable replacements when necessary; - Removed the term ""REST"" as redundant because it has taken over as the dominant API type; - Reorganize current `CHANGELOG.md` into sections because we have a substantial number of release notes ðŸŽ‰ ; - Unrelated one-line change to add timezone to debug image. ```; > docker run -it --entrypoint /bin/bash broadinstitute/cromwell:88-648e536-DEBUG; Version 88-648e536-DEBUG built at 2024-08-08 15:04:21 EDT; root@4ec372b744a8:/# ; ```. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7488:151,depend,depends,151,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7488,1,['depend'],['depends']
Integrability,"### Description. The Batch integration test suite runs with the Local and GCPBatch backends. This means that any cases tagged `Local` would have been picked up and (perhaps counterintuitively) run on the GCPBatch backend, because that's the default. We do have some extra cases that are not compatible with the Local backend for whatever reason, and weren't running on Batch. There's a good chance that many of them do work/should work on Batch, since it's similar to PAPI in a way that Local is not. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7440:27,integrat,integration,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7440,1,['integrat'],['integration']
Integrability,"### Description. The PR exercises the ""retry with more memory"" Centaur tests on the GCP Batch backend. Minimal changes to production code, all of which are in the GCP Batch backend:. - The constant`RunnableUtils#MountPoint` was created with value `/mnt/disks/cromwell_root` and applied where appropriate.; - A copy/paste bug in code brought over from PAPIv2 was corrected (the `/cromwell_root` of PAPIv2 has become `/mnt/disks/cromwell_root` in Batch), using the constant described above.; - If a job fails, the *last* event message is now propagated rather than the first event message. The first event message is often a benign state transition, while the last event message is more likely to contain the actual reason for job failure.; ; Unfortunately Cromwell does not allow for dynamic backend selection (i.e. the backend name cannot be a variable), which necessitated copy/paste/renaming the Centaur test WDLs from their PAPIv2 versions, hence the magnitude of these diffs. The existing `preemptible_and_memory_retry ` Centaur test is heavily tailored to the quirks of Papi v2: a preemptible PAPI VM deletes itself and depends on the Lifesciences system mistaking that for a preemption event. tbh this is kind of a weird test and as I don't know how to induce a preemption on demand, I simply `ignore`d the GCPBATCH version. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7494:525,message,message,525,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7494,5,"['depend', 'message']","['depends', 'message']"
Integrability,### Description. Turn on 90ish Centaur tests for GCPBATCH. In all but one case this was just adding the GCPBATCH backend to the Centaur .test file. The one exception involved different error message text coming from the Batch system than what we get from Lifesciences. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7496:191,message,message,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7496,1,['message'],['message']
Integrability,"### Description. UPDATE: issues with special characters in passwords appear to be resolved. PR to demo broken private Docker repo support in GCP Batch. There are actually multiple existing PAPI v2 Centaur tests in this vein; the one test enabled here for GCP Batch seems to be the simplest and demonstrates the issues clearly enough. The crux of this test is that the Docker image that is specified for the task is in a private repo to which the Centaur service account has been granted access. This test passes on PAPI v2 but on GCP Batch jobs fail with messages like the following visible in `gcloud batch jobs describe`:. ```; Job state is set from RUNNING to FAILED for job projects/1005074806481/locations/us-central1/jobs/job-27607753-d2d5-404d-89af-a786da8ad383.Job; failed due to task failure. Specifically, task with index 0 failed due to the; following task event: ""Task state is updated from RUNNING to FAILED on zones/us-central1-b/instances/8098872438472929780; with exit code 125."". ```. Exit code 125 being a typical ""[something's wrong with that Docker invocation](https://stackoverflow.com/questions/53640424/exit-code-125-from-docker-when-trying-to-run-container-programmatically)"" error. in Cloud Logging I see the following, including what looks like a plaintext password which I have x'd out below:. ```; Executing runnable container:{image_uri:""broadinstitute/cloud-cromwell@sha256:0d51f90e1dd6a449d4587004c945e43f2a7bbf615151308cff40c15998cc3ad4"" commands:""/mnt/disks/cromwell_root/script"" entrypoint:""/bin/bash"" volumes:""/mnt/disks/cromwell_root:/mnt/disks/cromwell_root"" username:""firecloud"" password:""xxxxx""} labels:{key:""tag"" value:""UserRunnable""} for Task task/job-27607753-d2d5-132dc052-df92-4db100-group0-0/0/0 in TaskGroup group0 of Job job-27607753-d2d5-132dc052-df92-4db100.; ```. So it looks like the GCP Batch backend has acquired and plumbed through the required Docker credentials, but the login to Docker Hub doesn't seem to have happened. ### Release Notes Confi",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7515:555,message,messages,555,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7515,1,['message'],['messages']
Integrability,"### Description. We need to propagate the Google credentials while pulling metadata from private GCR repositories. This is likely fixes #7356. Before this change, we'd get a log error when cromwell tries pulling the metadata, this occurs because `GoogleRegistry` implementation does not have a valid auth token:. ```; [2024-06-28 01:14:19,56] [info] Assigned new job execution tokens to the following groups: 5fe16e0e: 1; [2024-06-28 01:14:20,38] [warn] BackendPreparationActor_for_5fe16e0e:myWorkflow.myTask:-1:1 [5fe16e0e]: Docker lookup failed; java.lang.Exception: Failed to get docker hash for gcr.io/<REDACTED>/debian:latest Request failed with status 403 and body {""errors"":[{""code"":""DENIED"",""message"":""Unauthenticated request. Unauthenticated requests do not have permission \""artifactregistry.repositories.downloadArtifacts\"" on resource \""projects/<REDACTED>/locations/us/repositories/gcr.io\"" (or it may not exist)""}]}; ```. <details>; <summary>An example Workflow.wdl to test this</summary>. ```; workflow myWorkflow {; call myTask; }. task myTask {; command {; echo ""hello world""; }. runtime {; docker: ""gcr.io/<REDACTED>/debian:latest""; bootDiskSizeGb: 50; preemptible: 0; }; }; ```. </details>. ### Release Notes Confirmation. #### `CHANGELOG.md`; - [x] I updated `CHANGELOG.md` in this PR; - [ ] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [ ] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7464:700,message,message,700,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7464,1,['message'],['message']
Integrability,### Description; Updated a few dependencies that we need in order to use the Cloud Billing SDK. #### `CHANGELOG.md`; - [ ] I updated `CHANGELOG.md` in this PR; - [x] I assert that this change shouldn't be included in `CHANGELOG.md` because it doesn't impact community users. #### Terra Release Notes; - [ ] I added a suggested release notes entry in this Jira ticket; - [x] I assert that this change doesn't need Jira release notes because it doesn't impact Terra users,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7490:31,depend,dependencies,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7490,1,['depend'],['dependencies']
Integrability,"### Motivation:. For motivation see the [metadata design doc](https://docs.google.com/document/d/1VYnzk97yTtllozO9ivZpZQTwrsY5T0wGqxlvAbrEQgg/edit?ts=5d5d601c#heading=h.iqo65dknl60s). . Briefly, the intention is to move the ""rendering"" process inside the `ServiceRegistryActor` so that in the future calls to the ServiceRegistry return JSON rather than event lists. This allows the ""pre-rendered JSON"" metadata service to fulfil the same service interface as the ""database-event driven"" metadata service. ### PR Review Guidance. Most of the PR is noise but the ""signal"" is very important to get right!. Things to consider when reviewing this (perhaps otherwise unwieldy) PR:. - Does the actor structure in the diagrams below make sense?; - ... and does it match reality as implemented in this PR?; - Have the newly introduced actors been implemented well? (ie please review these as though they were brand new actors); - `ReadMetadataRegulatorActor`; - `MetadataBuilderActor`; - `ReadDatabaseMetadataWorkerActor`; - Have the responsibilities of the replaced actors been taken care of appropriately?; - Has the API of Cromwell changed inappropriately?; - I had to refactor the `CallCacheDiffActor` because it was using the metadata service directly. Did I do a good job? And are its new tests appropriately equivalent to its old ones?; - Are there sufficient tests between unit, CI and ""perf"" to make you feel good about me merging this PR?; - Am I forgetting anything?. ### Structure before the changes:. ![Before BA-5842_ Metadata Service Actor (3)](https://user-images.githubusercontent.com/13006282/64040517-426d4380-cb2b-11e9-8a40-fa11edd33b58.png). ### Structure after the changes:. ![After BA-5842_ Metadata Service Actor](https://user-images.githubusercontent.com/13006282/64040066-24531380-cb2a-11e9-8a74-98d7c976e6ec.png). ### Concerns. This feels slightly more risky than normal because the refactor was pretty fiddly and I was ""test driven"" for a significant portion of the refactor - mainl",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5150:446,interface,interface,446,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5150,1,['interface'],['interface']
Integrability,"### What happened. On 10/10/2018, around 11:15 AM, there was a spike in backpressure and 403 copy failures. It was discovered that a user had submitted workflows attempting to access buckets it did not have access to. . ![image](https://user-images.githubusercontent.com/16748522/46764755-59087300-ccab-11e8-9163-afd953710adf.png); Purple line- backpressure; Light green line- 403 copy failures. ### What was done to fix it. The situation was discussed with the user, and once he aborted all his workflows, Cromwell slowly returned to its normal state. The issue was resolved around 1:50 PM. ### Potential causes. The user had reused a WDL from another user, but he didn't have access to their Google Cloud buckets. This workflow contained job that ran 5000 split intervals against dataset of approx 1300 samples. Each of the 5000 outputs would be copied, per workflow, per sample. Depending on the number of samples the other user had previously run, each interval-output-for-each-sample tried call caching to other user's workspace. This resulted in a lot of attempts to copy files and then failures.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4229:882,Depend,Depending,882,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4229,1,['Depend'],['Depending']
Integrability,"#### What's changed?. Updates the error message format and content if a call cache diff fails to find a set of metadata. #### Old Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found"",; ""errors"": {; ""JsArray"": {; ""elements"": [; {; ""JsString"": {; ""value"": ""Failed to calculate diff for call A and call B:\nFailed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No 'id' field found""; }; }; ]; }; }; }; ```. ### New Format and Content; ```; {; ""status"": ""error"",; ""message"": ""Failed to calculate diff for call A and call B"",; ""errors"": [; ""Failed to extract relevant metadata for call A (<<workflow ID>> / <<call name>>:<<index>>) (reason 1 of 1): No metadata was found for that workflow/call/index combination. Check that the workflow ID is correct, that the call name is formatted like 'workflowname.callname' and that an index is provided if this was a scattered task. (NOTE: the default index is -1, ie non-scattered)""; ]; }; ```. #### Commentary. ~~I'm not convinced the ""roll my own"" Json formatter is needed... if only there were an ""identity"" formatter for JsValue, rather than the default - which interprets the value more like a ADT.~~. ~~I'm open to suggestions.~~. UPDATE: it turns out rolling my own ""identity formatter"" was easier than rolling my own case class formatter.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5260:40,message,message,40,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5260,3,['message'],['message']
Integrability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->. endpoint-url = ""https://genomics.googleapis.com/""; Cromwell version 55. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; All job submissions stopped working today with errors:; Unable to complete PAPI request due to system or connection error (PipelinesApiRequestHandler actor termination caught by manager)"". Error messages from Cromwell logs:; cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anon$1: A batch of PAPI status requests failed. The request manager will retry automatically up to 10 times. The error was: 404 Not Found; POST https://genomics.googleapis.com/batch; <!DOCTYPE html>; <html lang=en>; <meta charset=utf-8>; <meta name=viewport content=""initial-scale=1, minimum-scale=1, width=device-width"">; <title>Error 404 (Not Found)!!1</title>; ...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6203:1428,message,messages,1428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6203,1,['message'],['messages']
Integrability,"###; ### IMPORTANT: Please file new issues over in our Jira issue tracker!; ###; ### https://broadworkbench.atlassian.net/projects/BA/issues; ###; ### You may need to create an account before you can view/create issues.; ###. <!--; Hi! Thanks for taking the time to report feedback. Before posting an issue over in Jira tracker, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're almost in the right place. -->. <!-- You'll want to go to https://broadworkbench.atlassian.net/projects/BA/issues and then tell us: -->. <!-- Which backend are you running? -->; The backend the workflow pipelines is https://genomics.googleapis.com/. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; Error message: ; The job was stopped before the command finished. PAPI error code 14. Execution failed: worker was terminated. The job was running on non-preemptible VM, with one instance of nvidia-tesla-t4 attached, nvidiaDriverVersion: 418.40.04. . What does ""PAPI error code 14"" mean? Can you suggest what we should do with it?. Thanks!",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6306:1239,message,message,1239,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6306,1,['message'],['message']
Integrability,"$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); ^C[2016-10-27 13:10:13,93] [info] WorkflowManagerActor: Received shutdown signal.; [2016-10-27 13:10:13,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:13,93] [info] WorkflowManagerActor Aborting all workflows; [2016-10-27 13:10:14,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:15,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:16,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:17,93] [info] Waiting for 1 workflows to abort...; ^C^C[2016-10-27 13:10:18,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:19,33] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:5827,message,message,5827,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,2,['message'],['message']
Integrability,"$common$api$PipelinesApiRequestWorker$$handleBatch(PipelinesApiRequestWorker.scala:53); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker$$anonfun$receive$1.applyOrElse(PipelinesApiRequestWorker.scala:36); at akka.actor.Actor.aroundReceive(Actor.scala:517); at akka.actor.Actor.aroundReceive$(Actor.scala:515); at cromwell.backend.google.pipelines.common.api.PipelinesApiRequestWorker.aroundReceive(PipelinesApiRequestWorker.scala:19); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:588); at akka.actor.ActorCell.invoke(ActorCell.scala:557); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:258); at akka.dispatch.Mailbox.run(Mailbox.scala:225); at akka.dispatch.Mailbox.exec(Mailbox.scala:235); at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); 2019-04-29 00:02:24,760 cromwell-system-akka.dispatchers.backend-dispatcher-139 WARN - PAPI request worker PAPIQueryWorker-aaa95e49-59b4-4de6-864d-22920eac6164 terminated. 99 run creation requests, 1 status poll requests, and 0 abort requests will be reconsidered. If any of those succeeded in the cloud before the batch request failed, they might be run twice. Exception details: cromwell.backend.google.pipelines.common.api.PipelinesApiRequestManager$$anonfun$1$$anon$1: PipelinesApiRequestHandler actor termination caught by manager; ```. Of note, I am running Cromwell 40 with the following `java -Xmx100g -Dconfig.file=google.conf -jar cromwell-40.jar server` on a 16-core highmem system that has 102g of RAM. Of those 102G, only 30G are in use per `htop` (including both active and cache). Cromwell does continue, but the concern, as noted in the error, is that 99 jobs might now be duplicated. If I run with just 1 or 2 jobs, I don't get this message.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4914:5310,message,message,5310,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4914,1,['message'],['message']
Integrability,"' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.preemptible_tries' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_sa' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.known_indels_sites_VCFs' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_mu' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_alt' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_bwt' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.known_indels_sites_indices' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_dict' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_bed' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_fasta' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.dbSNP_vcf_index' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.dbSNP_vcf' not specified.""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. But once I filled these out in my inputs json I then got this error. ```; [; {; causedBy: [; {; causedBy: [ ],; message: ""Missing inputs for subworkflow call SomaticRoot.TumorAlignment at index None: read_length, ref_fasta, agg_preemptible_tries, ref_dict, haplotype_database_file, ref_alt, ref_ann, known_indels_sites_indices, dbSNP_vcf, ref_sa, dbSNP_vcf_index, unmapped_bam_suffix, ref_amb, contamination_sites_ud, contamination_sites_bed, ref_bwt, ref_fasta_index, increase_disk_size, fingerpri",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2912:2502,message,message,2502,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2912,1,['message'],['message']
Integrability,(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResult$2(BasicDirectives.scala:66); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.uti,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:2158,Rout,RouteConcatenation,2158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,2,['Rout'],"['RouteConcatenation', 'RouteWithConcatenation']"
Integrability,"(FutureTask.java:266); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:409); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:42,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:43,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:44,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:45,94] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:46,38] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:14714,message,message,14714,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,2,['message'],['message']
Integrability,"(ToL, of course). I _still_ don't always know where to draw the line between creating a new `akka..Actor` and using a `scala..Future`. Something does ""smell"" funny though about the way we:; - Queue of things-to-do is on the `ec: ExecutionContext`; - A mailbox `message: AnyRef` is received off the `ec` by the dispatcher and passed to our `actor: Actor`.; - Instead of running a `runnable: Runnable` bit of code immediately, the `actor` chooses to throws the `runnable` onto the back of the `ec` queue and then say ""done processing `message`"". That said, [this blog](https://www.chrisstucchio.com/blog/2013/actors_vs_futures.html) seems to say that `Actor` and `Future` can work together, but maybe something is off about how we're composing them in our `BackendLifecycleActor` interfaces.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226573034:261,message,message,261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226573034,3,"['interface', 'message']","['interfaces', 'message']"
Integrability,"(during ExecutingWorkflowState): cromwell.backend.standard.StandardAsyncExecutionActor$$anon$2: Failed to evaluate job outputs:; Bad output 'print_nach_nachman_meuman.out': [Attempted 1 time(s)] - IOException: Could not read from s3://nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log: Cannot access file: s3://s3.amazonaws.com/nrglab-cromwell-genomics/cromwell-execution/run_multiple_tests/b6b9322c-3929-4b72-9598-45d97dfb858d/call-test_cromwell_on_aws/shard-61/SingleTest.test_cromwell_on_aws/f8ecf673-ed61-4b06-b1d6-c20f7efe986e/call-print_nach_nachman_meuman/print_nach_nachman_meuman-stdout.log; at cromwell.backend.standard.StandardAsyncExecutionActor.$anonfun$handleExecutionSuccess$1(StandardAsyncExecutionActor.scala:867); ```. The error occurs when running many sub-workflows within a single wrapping workflow.; The environment is configured correctly, and the test usually passes when running <30 subworkflows. Here are the workflows:. run_multiple_test.wdl; ```; import ""three_task_sequence.wdl"" as SingleTest. workflow run_multiple_tests {; scatter (i in range(30)){; call SingleTest.three_task_sequence{}; }; }; ```. three_task_sequence.wdl; ```; workflow three_task_sequence{; call print_nach. call print_nach_nachman {; input:; previous = print_nach.out; }. call print_nach_nachman_meuman{; input:; previous = print_nach_nachman.out; }; output{; Array[String] out = print_nach_nachman_meuman.out; }; }. task print_nach{; command{; echo ""nach""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; 	 docker: ""ubuntu:latest""; 	 maxRetries: 3; }; }. task print_nach_nachman{; Array[String] previous. command{; echo ${sep=' ' previous} "" nachman""; }; output{; Array[String] out = read_lines(stdout()); }; runtime {; docker: ""ubuntu:latest""; maxRetries",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687:1267,wrap,wrapping,1267,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687,1,['wrap'],['wrapping']
Integrability,"(noting here that a few Cromwellians & Workbenchers had a face conversation about this and general retry policy). My goal is to enable a user to say ""run my workflow and if JES has a random hiccup, try again and keep going"". The details of where in the stack we should do this, and under which conditions, are unclear to me. So I guess what I want here is to note one failure case for FireCloud. As for the actual action taken, that depends on the policy we decide on.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298721503:433,depend,depends,433,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2233#issuecomment-298721503,1,['depend'],['depends']
Integrability,"(ðŸ‘ assuming ""force-pushed the mlc_scala_steward_5640_5649 branch from aeaa1f3 to 7f28d1a yesterday"" represents rebasing onto the previously merged dependency bumps)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5711#issuecomment-671451870:147,depend,dependency,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5711#issuecomment-671451870,1,['depend'],['dependency']
Integrability,") = self._run(); [2018-11-04T19:02:19.373833Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 1121, in _run; [2018-11-04T19:02:19.373871Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] self.workflow.workflow(); [2018-11-04T19:02:19.373894Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 895, in workflow; [2018-11-04T19:02:19.373930Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] graphTasks = runLocusGraph(self,dependencies=graphTaskDependencies); [2018-11-04T19:02:19.373954Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/mantaWorkflow.py"", line 296, in runLocusGraph; [2018-11-04T19:02:19.373978Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] mergeTask = self.addTask(preJoin(taskPrefix,""mergeLocusGraph""),mergeCmd,dependencies=tmpGraphFileListTask,memMb=self.params.mergeMemMb); [2018-11-04T19:02:19.374002Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] File ""/usr/local/share/bcbio-nextgen/anaconda/share/manta-1.4.0-1/lib/python/pyflow/pyflow.py"", line 3689, in addTask; [2018-11-04T19:02:19.374023Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] raise Exception(""Task memory requirement exceeds full available resources""); [2018-11-04T19:02:19.374046Z] [2a8fea138cb7] [72_1] [WorkflowRunner] [ERROR] Exception: Task memory requirement exceeds full available resources; ```. The cwl [requests 4GB](https://github.com/bcbio/test_bcbio_cwl/blob/48ca2661644e01e2d4b7f8ad8f2588a31cf87537/gcp/somatic-workflow/steps/detect_sv.cwl#L25) of memory for this task, which I verified Cromwell did request from PAPI as well:; <pre>; resources:; projectId: broad-dsde-cromwell-perf; regions: []; virtualMachine:; accelerators: []; bootDiskSizeGb: 21; bootImage: projects/cos-cloud/global/images/family/cos-stable; cpuP",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856:1854,depend,dependencies,1854,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3724#issuecomment-435937856,1,['depend'],['dependencies']
Integrability,"); 	at cats.effect.internals.IORunLoop$RestartCallback.signal(IORunLoop.scala:355); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:376); 	at cats.effect.internals.IORunLoop$RestartCallback.apply(IORunLoop.scala:316); 	at cats.effect.internals.IOShift$Tick.run(IOShift.scala:36); 	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41); 	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:49); 	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); 	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); 	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); 	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); Caused by: com.google.api.client.googleapis.json.GoogleJsonResponseException: 403 Forbidden; POST https://storage.googleapis.com/upload/storage/v1/b/xxx/o?projection=full&userProject=xxx&uploadType=multipart; {; ""code"" : 403,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project."",; ""reason"" : ""forbidden""; } ],; ""message"" : ""xxx@xxx.iam.gserviceaccount.com does not have serviceusage.services.use access to the Google Cloud project.""; }; 	at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:150); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); 	at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:555); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:475)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594:3399,message,message,3399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594,2,['message'],['message']
Integrability,"* JES; * cromwell-30.jar. I have two `write_tsv()` calls in the command block. This code works fine locally. ```; task trim_adapter { # trim adapters and merge trimmed fastqs; 	# parameters from workflow; 	Array[Array[File]] fastqs 		# [merge_id][end_id]; 	Array[Array[String]] adapters 	# [merge_id][end_id]; 	Boolean paired_end; 	# mandatory; 	Boolean auto_detect_adapter		# automatically detect/trim adapters; 	# optional; 	Int? min_trim_len 		# minimum trim length for cutadapt -m; 	Float? err_rate			# Maximum allowed adapter error rate ; 							# for cutadapt -e	; 	# resource; 	Int? cpu; 	Int? mem_mb; 	Int? time_hr; 	String? disks. 	command {; 		python $(which encode_trim_adapter.py) \; 			${write_tsv(fastqs)} \; 			${""--adapters "" + write_tsv(adapters)} \; 			${if paired_end then ""--paired-end"" else """"} \; 			${if auto_detect_adapter then ""--auto-detect-adapter"" else """"} \; 			${""--min-trim-len "" + min_trim_len} \; 			${""--err-rate "" + err_rate} \; 			${""--nth "" + select_first([cpu,4])}; 	}; 	output {; 		# WDL glob() globs in an alphabetical order; 		# so R1 and R2 can be switched, which results in an; 		# unexpected behavior of a workflow; 		# so we prepend merge_fastqs_'end'_ (R1 or R2); 		# to the basename of original filename; 		# this prefix will be later stripped in bowtie2 task; 		Array[File] trimmed_merged_fastqs = glob(""merge_fastqs_R?_*.fastq.gz""); 	}; 	runtime {; 		cpu : select_first([cpu,2]); 		memory : ""${select_first([mem_mb,'10000'])} MB""; 		time : select_first([time_hr,24]); 		disks : select_first([disks,""local-disk 100 HDD""]); 	}; }; ```; with Google JES backend, second call of write_tsv() doesn't seem to correctly pass temporary tsv file into a docker container. `${write_tsv()}` works fine.; `${""some string "" + write_tsv()}` does not work. It still has URI prefix `gs://`. ```; [2017-12-07 13:37:45,35] [info] JesAsyncBackendJobExecutionActor [17f0658fatac.trim_adapter:1:1]: python $(which encode_trim_adapter.py) \; /cromwell_root/atac-seq-pipeline-w",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3032:141,adapter,adapters,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3032,4,['adapter'],"['adapter', 'adapters']"
Integrability,"* The ""no zip bundle"" set of import resolvers is probably the right one to use when trying to ""resolve"" the original file from String (unless a dependencies zip is *also* supplied?).; * Side comment: should we therefore make the set of import resolvers to use *Cromwell's* responsibility? (right now every language factory comes up with its own set ðŸ˜±); * We should try end up as similar to the WES equivalent as possible when adding this; * To find out: is this just `submit` with `workflowRoot` + no `workflowSource`, or is it a separate endpoint?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3849#issuecomment-403963315:144,depend,dependencies,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3849#issuecomment-403963315,1,['depend'],['dependencies']
Integrability,* Updates the TES backend to use the [v0.3 schema](https://github.com/ga4gh/task-execution-schemas/releases/tag/v0.3). ; * Updates the integration tests to use the latest Funnel binary,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3014:135,integrat,integration,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3014,1,['integrat'],['integration']
Integrability,"**Backend:** AWS. **Workflow:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-workflow.wdl; **First input json:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-parameters.json; **Second input json is LIKE this one, but refers to a batch of 100 input datasets:** https://github.com/FredHutch/reproducible-workflows/blob/master/WDL/unpaired-panel-consensus-variants-human/broad-containers-batchofOne.json. **Config:** ; Installed the cromwell version in PR #4790. . **Error:**; ```; ""callCaching"": {; ""allowResultReuse"": true,; ""hit"": false,; ""result"": ""Cache Miss"",; ""effectiveCallCachingMode"": ""ReadAndWriteCache"",; ""hitFailures"": [; {; ""dd860da7-bed8-4e70-812c-227f4e6fead8:Panel_BWA_GATK4_Samtools_Var_Annotate_Split.SamToFastq:0"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ],; ""message"": ""[Attempted 1 time(s)] - S3Exception: The specified copy source is larger than the maximum allowable size for a copy source: 5368709120 (Service: S3, Status Code: 400, Request ID: AE0D7E6A63C706E5)""; }; ```. This version of Cromwell does seem to successfully access and copy a cached file from a previous workflow at least on the first task in a shard. This workflow is essentially a batch in which each row of a batch file is passed to a shard and then the tasks run independently on each input dataset and they never gather. However, when the files get larger than the single test data set it seems it can't get to the previous file in order to determine if there's a hit.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4805:934,message,message,934,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4805,2,['message'],['message']
Integrability,"**Commit 1**; Stop invoking scalacheck during the sbt build by replacing a) specs2 with specs2-mock plus pegdown, and b) excluding cats dependencies (also in wdl4s).; Removed cromwell dependency duplications (see the verboseness in excising cats' duplicated dependencies).; Just in case, pass scalatest arguments only to scalatest. **Commit 2**; 3 seconds timeout (instead of the 1 second default) for each of the slick and liquibase databases being compared.; Removed dead docker case class.; Formatting updates for sbt-docker.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1589:136,depend,dependencies,136,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1589,3,['depend'],"['dependencies', 'dependency']"
Integrability,"**TL;DR Discussed in person with @ruchim. Going to ðŸ‘ , and perhaps dev choice a PR to change the syntax, using a secondary route that looks for `workflowInputs[]`.**. This current PR is very swagger spec friendly, using a fixed 2-based list of additional inputs:; - `workflowInputs`; - `workflowInputs_2`; - `workflowInputs_3`; - `workflowInputs_4`; - `workflowInputs_5`. Ideally we could use a PHP compatible syntax on a secondary spray route:; - `workflowInputs[]`; - `workflowInputs[]`; - `workflowInputs[]`; - etc. This array, using a [custom](https://groups.google.com/d/msg/spray-user/5kSZ87OnfkE/I_A_OcaIticJ) spray marshaller could be programmatically converted into an variable length `workflowInputs: Seq[String]`. Passing the sequence into the business logic would also allow storing the separated inputs in the metadata. For now the five inputs are merged into a single value in the web service and stored in the database as a merged clob. At this second I do not know if swagger would allow multiple form data elements with the same name. I'm assuming curl, HTTPie, and jvm clients such as spray-client would, as PHP supports the above syntax. [![Approved with PullApprove](https://img.shields.io/badge/pullapprove-approved-brightgreen.svg)](https://pullapprove.com/broadinstitute/cromwell/pull-request/1511/?utm_source=github-pr&utm_medium=comment-badge&utm_campaign=broadinstitute/cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1511#issuecomment-251432342:123,rout,route,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1511#issuecomment-251432342,2,['rout'],['route']
Integrability,"**TL;DR Use `sbt publish` to push all the non-fat jars, such as `cromwell-backend >>> _2.11-0.1 <<< .jar`, and do not custom upload the fat `cromwell-backend >>> -0.1 <<< .jar`.**. Via the sbt-assembly plugin [docs](https://github.com/sbt/sbt-assembly/tree/v0.14.1#publishing-not-recommended):. > Publishing fat JARs out to the world is discouraged because non-modular JARs cause much sadness. One might think non-modularity is convenience but it quickly turns into a headache the moment your users step outside of Hello World example code. The fat jars being generated for our sub-modules should ~~die in a fire~~ be removed via [`aggregate in assembly := false`](http://stackoverflow.com/a/30828390/3320205). Also be sure to clean out the proliferation in Settings.scala of `assemblyJarName in assembly` and the viral `val commonSettings = â€¦ ++ assemblySettings ++ â€¦`. `assemblySettings` and `assemblyJarName` only belong in the `root`!. I have also buried the lede a bit. Our cromwell versioning is... _incomplete_ at the moment, depending on if ""Add backend jar"" means releases-only or releases-and-snapshots. While we could technically publish releases as is, we shouldn't really publish any snapshots until #645 is fixed, or downstream devs are gonna have a bad time as unhashed snapshots continuously change with each re-publish.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1019#issuecomment-227342208:1033,depend,depending,1033,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1019#issuecomment-227342208,1,['depend'],['depending']
Integrability,"*/\""\n #String dir_pattern = \""/.*/\""\n Int revert_sam_disk_size = 400\n Int sort_sam_disk_size = 400\n Int validate_sam_file_disk_size = 200\n\n call RevertSam {\n input:\n input_bam = input_bam,\n revert_bam_name = sub(sub(input_bam, dir_pattern, \""\""), \"".bam$\"", \""\"") + \"".unmapped.bam\"",\n disk_size = revert_sam_disk_size\n }\n\n# call SortSam {\n# input:\n# input_bam = RevertSam.unmapped_bam,\n# sorted_bam_name = sub(sub(RevertSam.unmapped_bam, dir_pattern, \""\""), \"".bam$\"", \""\"") + \"".sorted.bam\"",\n# disk_size = sort_sam_disk_size\n# }\n\n call ValidateSamFile {\n input:\n input_bam = RevertSam.unmapped_bam,\n report_filename = sub(sub(RevertSam.unmapped_bam, dir_pattern, \""\""), \"".unmapped.bam$\"", \""\"") + \"".validation_report\"",\n disk_size = validate_sam_file_disk_size\n }\n\n output {\n RevertSam.*\n ValidateSamFile.*\n }\n}"",; ""options"": ""{\n \""default_runtime_attributes\"": {\n \""zones\"": \""us-central1-b us-central1-c us-central1-f\""\n },\n \""google_project\"": \""engle-macarthur-ccdd\"",\n \""auth_bucket\"": \""gs://cromwell-auth-engle-macarthur-ccdd\"",\n \""refresh_token\"": \""cleared\"",\n \""final_workflow_log_dir\"": \""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/c7af7e06-a435-44ec-8466-124ad8e1bcaf/workflow.logs\"",\n \""account_name\"": \""kcibul@broadinstitute.org\"",\n \""jes_gcs_root\"": \""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/c7af7e06-a435-44ec-8466-124ad8e1bcaf\""\n}""; },; ""calls"": {. },; ""outputs"": {. },; ""id"": ""a714b11b-0162-4585-afa5-abbd7433af51"",; ""inputs"": {; ""BamToUnmappedBams.input_bam"": ""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/batch04/S64-2_Illumina.bam""; },; ""submission"": ""2017-01-19T18:17:12.188Z"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Google credentials are invalid: connect timed out""; }],; ""workflowLog"": ""gs://fc-4c1c7765-2de2-4214-ac41-dc10bbcbb55b/c7af7e06-a435-44ec-8466-124ad8e1bcaf/workflow.logs/workflow.a714b11b-0162-4585-afa5-abbd7433af51.log"",; ""end"": ""2017-01-19T18:17:39.673Z"",; ""start"": ""2017-01-19T18:17:19.606Z""; }",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1886:4266,message,message,4266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1886,1,['message'],['message']
Integrability,"*See comment below on how to fix/address*. - cromwell-27-c89c83f-SNAP.jar; - JES backend; - server mode; - local mysql. I have a database block that looks exactly like the one in the example (from the error message), yet I still get the error message. I tried a diff on the database blocks, between the example and my database block, so I am sure that they match. Is this just a mistake in the error message itself? . This is blocking me. The error:. ```; Caused by: java.lang.Exception:; *******************************; ***** DEPRECATION MESSAGE *****; *******************************. Use of configuration path 'database.driver' has been deprecated. Replace with a ""profile"" element instead, e.g:. database {; #driver = ""slick.driver.MySQLDriver$"" #old; profile = ""slick.jdbc.MySQLProfile$"" #new; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://host/cromwell?rewriteBatchedStatements=true""; user = ""user""; password = ""pass""; connectionTimeout = 5000; }; }. Cromwell thanks you. at cromwell.services.SingletonServicesStore$.<init>(ServicesStore.scala:70); at cromwell.services.SingletonServicesStore$.<clinit>(ServicesStore.scala); ... 22 more. ```. My conf file for database:; ```; database {; #driver = ""slick.driver.MySQLDriver$""; profile = ""slick.jdbc.MySQLProfile$""; db {; driver = ""com.mysql.jdbc.Driver""; url = ""jdbc:mysql://localhost/cromwell_24?useSSL=false&rewriteBatchedStatements=true""; user = ""root""; password = ""blahblah""; connectionTimeout = 5000; }; }. ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2217:207,message,message,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2217,4,"['MESSAGE', 'message']","['MESSAGE', 'message']"
Integrability,"+---------+-------------------------------------------+--------+------------------------------------+; | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |; +----+--------------------+--------------------+------+---------------------------------------------+-------------------------------+---------+-------------------------------------------+--------+------------------------------------+; | 1 | PRIMARY | x2 | ALL | NULL | NULL | NULL | NULL | 185900 | Using where |; | 4 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,SYS_IDX_11226 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1070 | const,cromwell.x2.WORKFLOW_EXECUTION_UUID | 1 | Using index condition; Using where |; | 3 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,SYS_IDX_11226 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1070 | const,cromwell.x2.WORKFLOW_EXECUTION_UUID | 1 | Using index condition; Using where |; | 2 | DEPENDENT SUBQUERY | CUSTOM_LABEL_ENTRY | ref | UC_CUSTOM_LABEL_ENTRY_CLK_WEU,SYS_IDX_11226 | UC_CUSTOM_LABEL_ENTRY_CLK_WEU | 1070 | const,cromwell.x2.WORKFLOW_EXECUTION_UUID | 1 | Using index condition; Using where |; +----+--------------------+--------------------+------+---------------------------------------------+-------------------------------+---------+-------------------------------------------+--------+------------------------------------+; ```; The referenced index on `CUSTOM_LABEL_ENTRY` is `UC_CUSTOM_LABEL_ENTRY_CLK_WEU` which looks like:; ```; mysql> show index from CUSTOM_LABEL_ENTRY;; +--------------------+------------+-------------------------------+--------------+-------------------------+-----------+-------------+----------+--------+------+------------+---------+---------------+; | Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |; +--------------------+------------+-------------------------------+-----",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4598:1947,DEPEND,DEPENDENT,1947,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4598,1,['DEPEND'],['DEPENDENT']
Integrability,+1 to not wrapping successes. Also +1 to not disturbing the return values without telling blues ahead of time.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171075925:10,wrap,wrapping,10,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/368#issuecomment-171075925,1,['wrap'],['wrapping']
Integrability,"+1. I think this error message should read: `No coercion defined from Array[File]? to Array[File?]`. No need to print out the value of the entire array to the logs. @katevoss:; Impact: Low (a few hours of frustrating debugging the first time a user sees it); Probability: Medium => High (if people start using conditionals more, this is going to show up for more and more people); Fix: Easy (just change the error message)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016:23,message,message,23,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1998#issuecomment-280664016,2,['message'],['message']
Integrability,",035 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowManagerActor Restarting workflow IDs: 0bbd4dc6-bfa5-4ccb-bcdf-bb3a62bbc24b, 81182fb4-22b5-460b-b78e-72f7750aa598, abafd1cc-977a-47d6-acce-c1b2907829a8, ea0272fc-42ef-4852-8143-8b14d34bfd8a. 2016-04-26 18:26:08,700 cromwell-system-akka.actor.default-dispatcher-11 INFO - Invoking restartableWorkflow on ea0272fc; 2016-04-26 18:26:08,700 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowManagerActor submitWorkflow input id = Some(ea0272fc-42ef-4852-8143-8b14d34bfd8a), effective id = ea0272fc-42ef-4852-8143-8b14d34bfd8a; 2016-04-26 18:26:08,772 cromwell-system-akka.actor.default-dispatcher-10 INFO - Updating WorkflowManager state. New Data: (ea0272fc-42ef-4852-8143-8b14d34bfd8a,Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-ea0272fc-42ef-4852-8143-8b14d34bfd8a#787056469]); 2016-04-26 18:26:08,773 cromwell-system-akka.actor.default-dispatcher-3 INFO - WorkflowActor [UUID(ea0272fc)]: Restart message received; 2016-04-26 18:26:09,112 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: Initial symbols:. 2016-04-26 18:26:09,129 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: Initial executions:. 2016-04-26 18:26:09,156 cromwell-system-akka.actor.default-dispatcher-2 INFO - WorkflowActor [UUID(ea0272fc)]: ExecutionStoreCreated(Restart) message received; 2016-04-26 18:26:09,432 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: Beginning transition from Submitted to Running.; 2016-04-26 18:26:09,432 cromwell-system-akka.actor.default-dispatcher-11 INFO - WorkflowActor [UUID(ea0272fc)]: transitioning from Submitted to Running.; 2016-04-26 18:26:09,646 cromwell-system-akka.actor.default-dispatcher-7 INFO - WorkflowActor [UUID(ea0272fc)]: starting calls: GenomeStripBamWorkflow.ComputeMetadata, GenomeStripBamWorkflow.ComputeStatistics; 2016-04-26 18:26:09,646 cromwell-",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251:1159,message,message,1159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/588#issuecomment-215113251,1,['message'],['message']
Integrability,",; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(ForkJoinExecutorConfigurator.scala:43)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)""; },; {; causedBy: [ ],; message: ""akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. I think the culprit has something to do with both the root and sub workflow both having a task called `GatherbamFiles` because when I renamed the task in the subworkflow (and all subsequent necessary renames) the workflow ran fine. When I tried to make a simple example of this I couldn't get the error to pop up again so I'm definitely missing some nuances of the cause. The root workflow passes womtool-30.1.jar validation. Root workflow - [SomaticPairedSingleSampleWf.txt](https://github.com/broadinstitute/cromwe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143:3578,message,message,3578,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143,1,['message'],['message']
Integrability,"- 0.22; - local backend; - docker; - single workflow. Upshot: I still have jobs running and cromwell is not shutting down. ```; ^C[2016-10-19 18:29:22,42] [info] WorkflowManagerActor: Received shutdown signal. Aborting all running workflows...; [2016-10-19 18:29:22,42] [info] WorkflowManagerActor Aborting all workflows; [2016-10-19 18:29:22,42] [info] WorkflowExecutionActor [51ee236f]: Abort received. Aborting 8 EJEAs; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (2 remaining).; [2016-10-19 18:29:22,47] [info] WorkflowManagerActor Waiting for all workflows to abort (1 remaining).; [2016-10-19 18:29:50,48] [info] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] job aborted: case_gatk_acnv_workflow.HetPulldown:8:; 1; [2016-10-19 18:29:50,52] [warn] WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0 [51ee236f]: WorkflowExecutionActor [51ee236f] received an unhandled message: JobRunning(51ee236f-; c31a-48c2-bae7-9246439160b0:case_gatk_acnv_workflow.HetPulldown:12:1,Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-51ee236f-c31a-48c2-b; ae7-9246439160b0/WorkflowExecutionActor-51ee236f-c31a-48c2-bae7-9246439160b0/51ee236f-c31a-48c2-bae7-9246439160b0-EngineJobExecutionActor-case_gatk_acnv_workflow.HetPulldown:12:1/51ee236f-c; 31a-48c2-bae7-9246439160b0-BackendJobExecutionActor-51ee236f:case_gatk_acnv_workflow.HetPulldown:12:1#636728322])) in state: WorkflowExecutionAbortingState; [2016-10-19 18:29:50,53] [info] SharedFileSystemAsyncJobExecutionActor [51ee236fcase_gatk_acnv_workflow.HetPulldown:12:1]: java -Xmx4g -jar /root/gatk-protected.jar GetHetCoverage --referen; ce /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/data/ref/Homo_sapiens_assembly19.fasta \; --normal /root/case_gatk_acnv_workflow/51ee236f-c31a-48c2-bae7-9246439160b0/call-HetPulldown/shard-12/inputs/d",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1600:985,message,message,985,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1600,1,['message'],['message']
Integrability,"- 0.23; - SGE backend; - single workflow; - no docker. The wdl in question (a simpler WDL can be made easily). Look at how it scatters over a variable that does not exist. I would expect cromwell to give an error message and exit.; ```; # This is **broken.wdl**; # This simple, *unsupported* WDL takes in a VCF from M2 and a tumor bam file.; # It produces a new VCF with the filtering results. workflow test_ob_filter {; # tsv; # entity_id vcf tumor_bam_file; File input_table; Array[Array[String]] m2_vcfs = read_tsv(input_table); File db_snp; String gatk_jar; File ref_fasta. scatter (row in THIS_VAR_DOES_NOT_EXIST) {; call CollectSequencingArtifactMetrics {; input:; entity_id=row[0],; bam_file=row[2],; gatk_jar=gatk_jar,; ref_fasta=ref_fasta,; output_location_prepend=row[0]; }; call FilterByOrientationBias {; input:; entity_id=row[0],; gatk_jar=gatk_jar,; m2_vcf=row[1],; pre_adapter_detail_metrics=CollectSequencingArtifactMetrics.pre_adapter_detail_metrics; }; }. call MakeSummaryFileList {; input:; files=FilterByOrientationBias.orientation_bias_vcf_summary,; output_file=""summary_table.txt""; }; }. task CollectSequencingArtifactMetrics {; String entity_id; File bam_file; String output_location_prepend; String gatk_jar; # /seq/references/Homo_sapiens_assembly19/v1/Homo_sapiens_assembly19.fasta; File ref_fasta. command {; java -jar ${gatk_jar} CollectSequencingArtifactMetrics -I ${bam_file} -O ${output_location_prepend} -R ${ref_fasta} --VALIDATION_STRINGENCY SILENT; }. output {; File pre_adapter_detail_metrics = ""${output_location_prepend}.pre_adapter_detail_metrics""; File pre_adapter_summary_metrics = ""${output_location_prepend}.pre_adapter_summary_metrics""; File bait_bias_detail_metrics = ""${output_location_prepend}.bait_bias_detail_metrics""; File bait_bias_summary_metrics = ""${output_location_prepend}.bait_bias_summary_metrics""; }; }. task FilterByOrientationBias {; String entity_id; String gatk_jar; File m2_vcf; File pre_adapter_detail_metrics. command {; java -jar ${ga",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1774:213,message,message,213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1774,1,['message'],['message']
Integrability,"- 0.24; - SGE backend. I accidentally gave an Int parameter a String value in the json. I would prefer an error specific to parameter type, rather than a generic invalid runtime attribute error message (below). Proposed solution: ; ``Task m1_task was given an invalid type for cpu = ""${cpu}"". A String was given, though parameter is an Int``. Current error message:; ```; [ERROR] [02/08/2017 10:38:57.225] [cromwell-system-akka.dispatchers.engine-dispatcher-8] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow 07a3f007-8c62-4cd4-8668-6ac034ff42f1 failed (during InitializingWorkflowState): Task m1_task has an invalid runtime attribute cpu = ""${cpu}""; java.lang.IllegalArgumentException: Task m1_task has an invalid runtime attribute cpu = ""${cpu}""; at cromwell.backend.BackendWorkflowInitializationActor$class.validateRuntimeAttributes(BackendWorkflowInitializationActor.scala:156); at cromwell.backend.BackendWorkflowInitializationActor$class.initSequence(BackendWorkflowInitializationActor.scala:171); at cromwell.backend.sfs.SharedFileSystemInitializationActor.initSequence(SharedFileSystemInitializationActor.scala:37); ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1963:194,message,message,194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1963,2,['message'],['message']
Integrability,"- 0.24; - Single workflow mode; - SGE backend. Can you eliminate or reword the message below? I see this a lot and it is hard to decipher. Can't it just say, ""Dispatching workflow 2c9b89c2-27ef-448a-b5af-1d090e76ada1 task sub-workflow: dl_ob_training shard 213"" ?. ```; [INFO] [01/20/2017 09:08:22.441] [cromwell-system-akka.dispatchers.backend-dispatcher-225] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-2c9b89c2-27ef-448a-b5af-1d090e76ada1/WorkflowExecutionActor-2c9b89c2-27ef-448a-b5af-1d090e76ada1/SubWorkflowExecutionActor-SubWorkflow-dl_ob_training:213:1/ea0b6d84-117f-4cfe-88b4-26b621e60b8c-SubWorkflowActor-SubWorkflow-dl_ob_training:213:1/ea0b6d84-117f-4cfe-88b4-26b621e60b8c-EngineJobExecutionActor-dl_ob_training.dl_ob_training.CreateObIntervalList:NA:1/ea0b6d84-117f-4cfe-88b4-26b621e60b8c-BackendJobExecutionActor-ea0b6d84:dl_ob_training.dl_ob_training.CreateObIntervalList:-1:1/DispatchedConfigAsyncJobExecutionActor] DispatchedConfigAsyncJobExecutionActor [UUID(ea0b6d84)dl_ob_training.dl_ob_training.CreateObIntervalList:NA:1]: DispatchedConfigAsyncJobExecutionActor [UUID(ea0b6d84):dl_ob_training.dl_ob_training.CreateObIntervalList:NA:1] Status change from SharedFileSystemRunStatus(false) to SharedFileSystemRunStatus(true). ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1883:79,message,message,79,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1883,1,['message'],['message']
Integrability,"- 0.24; - single workflow mode; - JES backend. When I run the workflow, I get a localization permission error, but when I try again from the command line, there is no issue.; From cromwell:; ```; ....snip....; java.lang.RuntimeException: Task 773d051e-2e93-4248-bca4-e40292e0e59d:generate_true_positives failed: error code 5. Message: 9: Failed to localize files: failed to copy the following files: ""gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list -> /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list (cp failed: gsutil -q -m cp gs://firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list /mnt/local-disk/firecloud-tcga-open-access/tutorial/reference/whole_exome_agilent_1.1_refseq_plus_3_boosters_plus_10bp_padding_minus_mito.Homo_sapiens_assembly19.targets.interval_list, command failed: AccessDeniedException: 403 Caller does not have storage.objects.list access to bucket firecloud-tcga-open-access.\nCommandException: 1 file/object could not be transferred.\n)""; at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:489); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.handleExecutionFailure(JesAsyncBackendJobExecutionActor.scala:61); ....snip....; ```; ; BUT I would think this next operation would fail and it does not:; ```; lichtens@lichtens-big:~/test_dl_oxoq/create_bs$ gsutil ls gs://firecloud-tcga-open-access/tutorial/reference/; gs://firecloud-tcga-open-access/tutorial/reference/CNV.hg19.bypos.111213.txt; gs://firecloud-tcga-open-access/tutorial/reference/Homo_sapiens_assembly19.dict; gs://firecloud-tcga-open-access/tutori",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1960:326,Message,Message,326,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1960,1,['Message'],['Message']
Integrability,"- 0.25; - Local+docker backend (though I doubt it matters); - single workflow (though I doubt it matters). Workflow output is optional and sometimes both ``oncotate_m2_ob.oncotated_m2_vcf`` and ``oncotate_m2_no_ob.oncotated_m2_vcf`` are not populated. Proposed solution: select_first returns null if no inputs are populated. Offending workflow output:; ```; File? oncotated_m2_vcf = select_first([oncotate_m2_ob.oncotated_m2_vcf, oncotate_m2_no_ob.oncotated_m2_vcf]); ```. Error message:; ```; [ERROR] [02/17/2017 14:18:45.923] [cromwell-system-akka.dispatchers.engine-dispatcher-5] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow e241e2bc-95cd-4a8c-a814-20bb8852c6b5 failed (during ExecutingWorkflowState): select_first failed. All provided values were empty.; java.lang.IllegalArgumentException: select_first failed. All provided values were empty.; at wdl4s.expression.WdlStandardLibraryFunctions$$anonfun$select_first$1$$anonfun$apply$10.apply(WdlStandardLibraryFunctions.scala:180); at wdl4s.expression.WdlStandardLibraryFunctions$$anonfun$select_first$1$$anonfun$apply$10.apply(WdlStandardLibraryFunctions.scala:180); at scala.Option.getOrElse(Option.scala:121); at wdl4s.expression.WdlStandardLibraryFunctions$$anonfun$select_first$1.apply(WdlStandardLibraryFunctions.scala:180); at wdl4s.expression.WdlStandardLibraryFunctions$$anonfun$select_first$1.apply(WdlStandardLibraryFunctions.scala:175); at scala.util.Success.flatMap(Try.scala:231); at wdl4s.expression.WdlStandardLibraryFunctions$class.select_first(WdlStandardLibraryFunctions.scala:175). ....snip....; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2005:479,message,message,479,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2005,1,['message'],['message']
Integrability,"- Add load status logging where previously there was none for `PipelinesApiRequestManager.scala`; - Add high load logging to `IOActor`, which previously only had [back-to-normal logging](https://github.com/broadinstitute/cromwell/compare/develop...aen_wx_1333#diff-0be95c10972997df38906d44327436c1149e0c1a3df513bb49a43b9916ecd505R212); - Add load logging to `ServiceRegistryActor` which collects the load messages from their various sources and routes them to the sinks like `JobTokenDispenserActor`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7253:405,message,messages,405,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7253,2,"['message', 'rout']","['messages', 'routes']"
Integrability,- Adds a `JesError` class that maps some known JES errors to custom Exceptions to provide better error messages. Simplistic for now but avoid unnecessary stacktrace and give more explicit error messages.; - Tries to read the return code regardless of the final status of the JES job (even if it failed). If it can read it then the return code will be available in metadata.; - Sets the exec.sh content-type to `text/plain` in gcs so it opens in the browser instead of downloading a files.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1856:103,message,messages,103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1856,2,['message'],['messages']
Integrability,"- Break as many eggs as you want en route.; - Need to make any assumptions to simplify things? Make them!; - We don't necessarily need to merge the result into develop if it's hideous, we just want to find out how close/far we are from making this happen.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2570:36,rout,route,36,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2570,1,['rout'],['route']
Integrability,"- Bypass aging Scala Mockito wrappers with new custom wrapper, mostly; - For hard to port Mockito wrappers instead use Java API; - Only run ScalaTest, not Specs2 nor ScalaCheck; - Ported generator specs to scalatestplus-scalacheck; - Unignored and fixed up tests taggeed with PostWomTest; - Turned off ScalaTest HTML reports & removed out of date rendering dependencies",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6474:29,wrap,wrappers,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6474,4,"['depend', 'wrap']","['dependencies', 'wrapper', 'wrappers']"
Integrability,- Closes #4158 ; - [x] @ruchim could you confirm that [this error message](https://github.com/broadinstitute/cromwell/pull/4174/files#diff-aade89887d9abbfe15dc3bf8b809aec5R31) meets your AC?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4174:66,message,message,66,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4174,1,['message'],['message']
Integrability,- Fixes #4081 ; - Fixes the draft-2 error and makes slight readability improvements to the `1.0` and later messages,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4175:107,message,messages,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4175,1,['message'],['messages']
Integrability,"- Forked dbms tests into earliest and latest tests, with platform as a separate enum; - Run additional docker containers for latest dbms versions; - Run dbms tests as a separate travis job due to more containers & tests; - Generate dbms test configs, and ""how to"" messages to run docker and reset db; - Fixed dbms tests that were not closing their connections; - Verify that most projects are aggregated, and therefore tested by `sbt test`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5175:264,message,messages,264,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5175,1,['message'],['messages']
Integrability,"- JES backend; - 0.23; - single workflow. This workflow used to complete successfully (though cromwell did not exit), but with release 0.23, the workflow itself fails; Looks like cromwell can no longer handle spaces in the output file name. I believe that @kshakir had a similar issue in one of the develop builds. Did the fix make it into release 0.23? . ```; ...snip...; java.lang.RuntimeException: Task 5d13ddf0-dcf9-4b99-bd13-40b4321a954a:aggregate_results_html failed: error code 5. Message: 9: Failed to localize files: failed to copy; the following files: ""gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity; _series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png -> /mnt/local-disk/broad-dsde-methods/cromwell-executions-eval-gatk-protect; ed/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small Amplificati; ons.png (cp failed: gsutil -q -m cp gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-r; un_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png /mnt/local-disk/broad-dsde-methods/cromwell-executions-eval-g; atk-protected/crsp_validation_workflow/5d13ddf0-dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small; Amplifications.png, command failed: CommandException: No URLs matched: gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected/crsp_validation_workflow/5d13ddf0; -dcf9-4b99-bd13-40b4321a954a/call-run_plot_purity_series/glob-2a33a5ba399f044203396c79c9f80928/purity_series_small_Small%20Amplifications.png\nCommandException: 1 file/; object could not be transferred.\n); gs://broad-dsde-methods/cromwell-executions-eval-gatk-protected",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1754:488,Message,Message,488,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1754,1,['Message'],['Message']
Integrability,"- JES backend; - cromwell server; - localhost mysql; - cromwell-27-c89c83f-SNAP.jar; - I set the database queue size to 3000.; - I have *not* changed the metadata batch size. *Should I attempt to restart this workflow?* This took over 4 hours to get this error message and I do not want to incur the cost if it will fail the same way again. Side issues:; - My workflow failed and yet cromwell is still *mauling* the mysql server.; - The call cache lookups are taking >1 hour per task. Main issue:. I do not understand the error messages, but my workflow has entered a Failed state and I am not sure why. First, I see a bunch of NPE:; ```; [ERROR] [05/01/2017 17:36:00.055] [cromwell-system-akka.dispatchers.engine-dispatcher-84] [akka.dispatch.Dispatcher] null; java.lang.NullPointerException; at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor.receiver(CallCacheWriteActor.scala:17); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:21); at cromwell.engine.workflow.lifecycle.execution.callcaching.CallCacheWriteActor$$anonfun$1.apply(CallCacheWriteActor.scala:19); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2228:261,message,message,261,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2228,2,['message'],"['message', 'messages']"
Integrability,- Job Outputs not being printed; - Removed some log messages marked as PBE candidates,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1062:52,message,messages,52,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1062,1,['message'],['messages']
Integrability,- MariaDB liquibase and tests.; - Re-synchronize PostgreSQL schema to other DBMS.; - Use client side datetime DATETIME(6) vs. server side TIMESTAMP.; - Test that the DBMS attribute is set on all changelogs.; - Test that the quoting strategy is set for PostgreSQL changelogs.; - Test that sequences have the same width as columns.; - Database tests by default run on all DBMS.; - Add tests to ensure schemas stay synced from now on.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5058:37,synchroniz,synchronize,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5058,1,['synchroniz'],['synchronize']
Integrability,- Moved all expression related code into `cromwell.binding.expression`.; - Defined interface `Evaluator` which evaluates into a `type T`. The 3 currently implemented evaluators are:; - `ValueEvaluator` - Evaluate a `WdlExpression` into a `WdlValue`; - `TypeEvaluator` - Evaluate a `WdlExpression` into a `WdlType`; - `FileEvaluator` - Evaluate a `WdlExpression` into a `Seq[WdlFile]`; - Public API for evaluating expressions is not changed much. Basically:; - `WdlExpression.evaluate(...): Try[WdlValue]`; - `WdlExpression.evaluateType(...): Try[WdlType]`; - `WdlExpression.evaluateFiles(...): Try[Seq[WdlValue]]`; - `JesBackend` now uses the `WdlExpression.evaluateFiles()` for calculating all `JesOutput`s; - Statically type-check the `output` section of a task defintion!; - Coercion of input types happens when locally qualified inputs are created,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/174:83,interface,interface,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/174,1,['interface'],['interface']
Integrability,"- No frills github action that either passes, or fails with a list of files that need to be fixed. ; - Formatted `ContinuousIntegration.scala` since that slipped in before this github action did. ; - `scalafmt` can be executed locally in a number of ways:; - IntelliJ Integration: Works as long as the `scala` plugin is installed. `Option + Command + L` formats the current file.; - `sbt scalafmtCheckAll`; - Install the `scalafmt` CLI tool directly via [brew](https://scalameta.org/scalafmt/docs/installation.html) and [coursier](https://get-coursier.io/docs/cli-installation).",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7337:268,Integrat,Integration,268,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7337,1,['Integrat'],['Integration']
Integrability,"- Refactoring the `TaskExecutionContext` (and children) into `BackendCall`. `BackendCall` represents the marriage of a (you guessed it...) `Backend` and a `Call`. The `BackendCall` also stores the `Map[LocallyQualifiedName, WdlValue]`. A `BackendCall` is basically a way to package up a (Call + Backend + Inputs) so you can simply do `.execute` with no parameters and it can start running.; ; This actually sets us up nicely for tasks to define which backend they run on (if we choose to support that). This could be implemented simply by honoring a runtime section like this.; ; ```; task sge_task {; command { ... }; runtime {; backend: ""sge""; }; }; ```; ; At the time we're creating the `BackendCall`, we just switch on the 'backend' value on the task, and either return a `SgeBackendCall`, `LocalBackendCall`, or `JesBackendCall` (depending on what we support); - Add SGE backend based off of Local Backend; - Created a `LocalFileSystemOperations` trait which fulfills some of the `Backend` API. SGE and Local backends currently make an assumption: the Cromwell process writes everything to filesystem paths and jobs that Cromwell launch can see and write into those directories. So operations like initializing a workflow or post-processing a job that has completed are the same between SGE and Local backends.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/145:835,depend,depending,835,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/145,1,['depend'],['depending']
Integrability,- Remove the bit where supplied labels are injected into Google. ; - Remove the Google-related restrictions on label structure. These should just be allowed to be anything*; - Provide a new workflow option `google-labels` which **do** get injected into Google and enforces Google's restrictions; - Communicate that this changed to FC,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3233:43,inject,injected,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3233,2,['inject'],['injected']
Integrability,"- Removed one unnecessary capture each from the array and map regexps; - Factored out the backslash on escaped metacharacters in the map regexp; - Transformed the map regexp to standard, non-""free spacing"" form; - Added some docs on the `WdlValueSimpleton` encoding protocol",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1310:266,protocol,protocol,266,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1310,1,['protocol'],['protocol']
Integrability,"- Renamed existing ""upgrade"" tests to ""wdl_upgrade"".; - Refactored concept of `cron` as `y`/`n` to `centaur_type` of `standard`/`integration`/`engineUpgrade`.; - Before starting engine upgrade tests, run new sql checks for rows in metadata/jobKeyValue tables.; - Shutting down cromwell after wdl and engine upgrade tests.; - Rendering ci resources under `target`, instead of under `src`.; - Writing centaur logs under `target`.; - Logging the command used to start cromwell from centaur.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4132:129,integrat,integration,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4132,1,['integrat'],['integration']
Integrability,"- Replaced to-be-deprecated Credential (no 's') with Adapter around Credentials; - Removed dupe credentials adapting from PipelinesApiFactoryInterface; - Move service specific scopes (KMS, Genomics) out of GoogleAuthMode; - Changed credential creation methods to take scala collections",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5013:53,Adapter,Adapter,53,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5013,1,['Adapter'],['Adapter']
Integrability,"- SGE backend (though I bet backend does not matter); - server mode; - cromwell 29. WDL takes in a list of filenames and scatters over a read_lines call. Each line is a file.; If the list file has DOS line endings, read_lines preserves the `\r` character in the file name. After running dos2unix, the issue disappeared. Here is the error message and you can even see the appended `\r`... ```; Could not localize /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r -> /dsde/working/lichtens/sge_cromwell/cromwell-executions/m2_validation/3055776a-c32a-4309-a426-87f5730454b4/call-m1_basic_validator/shard-1/inputs/seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r:\n\t/seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r doesn't exists\n\tFile not found /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r\n\tFile not found /dsde/working/lichtens/sge_cromwell/cromwell-executions/m2_validation/3055776a-c32a-4309-a426-87f5730454b4/call-m1_basic_validator/shard-1/inputs/seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r -> /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r\n\tFile not found /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r""; ```. Hash error:; ```; ""Cannot hash file /seq/picard_aggregation/G20440/HSCX1989N/v2/HSCX1989N.bai\r because it can't be found"". ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2632:338,message,message,338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2632,1,['message'],['message']
Integrability,"- Single workflow mode; - Local backend (using throttling in custom application.conf); - 0.22; - using docker images; - call-caching enabled (localhost mysql instance); - all data is on local filesystem (not even shared filesystem); - N=2; - One time took 6 minutes before I did ctl-C. The second time it was left overnight and never completed. I did notice that (before I hit Ctl-C) cromwell got the shutdown signal and was aborting running jobs, even though there were none. If this was desired behavior, is there a flag to disable?. What other information can I provide? WDL? application.conf is attached. No other `-D` command line parameters were used. [local_application.conf.txt](https://github.com/broadinstitute/cromwell/files/539083/local_application.conf.txt). I am attempting to run cromwell as part of a larger shell script and I am positive that cromwell is not exiting (I still see MySQL warning messages). The workflow results appear to be there and no jobs are running (according to `top -c`)...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1594:911,message,messages,911,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1594,1,['message'],['messages']
Integrability,- To the changelog:; - Added entry for already released 62; - Added placeholder for next release 63; - For homebrew:; - Remove extra slash added to generated URLs; - Changed default publishing instructions to include homebrew; - Added validation of brew style according to guidelines; - Fixed casing of 'cromwell' in PR name; - For the publishing GitHub token scopes:; - updated instructions; - updated validation; - gracefully error with helpful messages; - added an example image; - Removed attempt to publish from dbms tests,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6338:447,message,messages,447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6338,1,['message'],['messages']
Integrability,- Truncate stack traces for logged-but-expected test exceptions.; - Add logback xml for all test artifacts that don't import core's copy.; - Make sure akka is routing logs through slf4j.; - Make sure log4j is routing logs through slf4j.; - Only print sbt warnings/errors when publishing/pushing artifacts.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4133:159,rout,routing,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4133,2,['rout'],['routing']
Integrability,"- [ ] Usage message; - Update the usage message to better describe how to use the commands. - [ ] Parameters; - Switch from positional parameters to parameter arguments so that users explicitly include inputs and other parameters (workflow options, metadata, imports, lables, etc).; - Make all parameters optional, so if a user doesn't include the parameter argument, then no error.; - Get rid of functionality that Cromwell looks for files with specific extensions, like `.imputs`, `.options`, etc. To Be Continued.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1939:12,message,message,12,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1939,2,['message'],['message']
Integrability,"- [ ] `missing_optional_output`:; The error message is now `""No input x.out_except_undeclared found evaluating inputs for expression x.out_except_undeclared""` which is significantly less friendly than the previous:; ```; out_except_undeclared is not declared as an output of the task x.; Make sure to declare it as an output to be able to use it in the workflow.; ```. - [x] `missing_input_failure`:; We used to get information saying which call, and which input, were given an invalid file. Now we just get `""Workflow Failed""` caused by: `""nonexistingbucket/path/doesnt/exist""`",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2871:44,message,message,44,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2871,1,['message'],['message']
Integrability,"- [x] Needs https://github.com/broadinstitute/wdl4s/pull/47; - [x] Needs https://github.com/broadinstitute/centaur/pull/114; - [x] Needs WDL doc; - [x] Needs Cromwell doc. What it does in a nutshell:. - Enables sub workflows execution; - Sub workflow metadata can be queried separately or injected in the main workflow metadata; - Restarts work; - Aborts should work (work meaning what abort is doing in develop now). To be addressed:; - ~~Sub Workflow Store cleanup~~; - ~~Workflow outputs copying~~ -> https://github.com/broadinstitute/cromwell/issues/1684; - ~~Call logs copying~~; - ~~Provenance: More related to imports, but right now the actual WDL content of a sub workflow is unknown to cromwell (it's in the `WdlNamespace` as a scala object but the actual text is not available).~~; - ~~Stats Endpoint~~",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1682:289,inject,injected,289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1682,1,['inject'],['injected']
Integrability,"- [x] Rebase on develop after #3439. Implements the ability to override inputs, even if they depend on upstream nodes. Another trick on the conversion into WOM. In this case we simplify/expand this:; ```wdl; input {; Int b = a; }; Int a = 55; ```. Into this:; ```wdl; input {; Int __b; }; Int a = 55; Int b = select_first([_b, a]); ```. But preserve the fact that the input being looked for in the input file is still just `b`. Also brings in a fix from the hermes grammar to fix the string regex (it wasn't allowing `(` or `)`)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3444:93,depend,depend,93,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3444,1,['depend'],['depend']
Integrability,- [x] Some of the error messages will change/improve following #3628 (or vice versa); - Red thumb required for the womtool changes,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3641:24,message,messages,24,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3641,1,['message'],['messages']
Integrability,"- cromwell 26; - JES backend; - call caching on local mysql instance; - server mode. Ran a bunch of the initial jobs, but once it really started fan out (thousands of jobs), I got this error message. Trying to replicate now, but not sure if I can. Might be transient. . Regardless, error message is not particularly helpful. Any ideas? . ```; cromwell.core.CromwellFatalException: com.google.cloud.storage.StorageException: 410 Gone; {; ""error"": {; ""errors"": [; {; ""domain"": ""global"",; ""reason"": ""backendError"",; ""message"": ""Backend Error""; }; ],; ""code"": 503,; ""message"": ""Backend Error""; }; }. at cromwell.core.CromwellFatalException$.apply(core.scala:17); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:37); at cromwell.core.retry.Retry$$anonfun$withRetry$1.applyOrElse(Retry.scala:36); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:346); at scala.concurrent.Future$$anonfun$recoverWith$1.apply(Future.scala:345); at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32); at akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at akka.dispatch.BatchingExecutor$BlockableBatch$$anonfun$run$1.apply(BatchingExecutor.scala:91); at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72); at akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:90); at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39); at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.for",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2215:191,message,message,191,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2215,4,['message'],['message']
Integrability,"- cromwell 30; - JES backend. ```""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""key not found: <cnv_somatic_pair_workflow.wdl:63:64 identifier \""UHJlcHJvY2Vzc0ludGVydmFscw==\"">""; }; ],; ""message"": ""Workflow input processing failed""; }; ],```. That WDL file (subworkflow) is packaged correctly as a subworkflow and being submitted to cromwell. I'm pretty sure that this worked just fine in cromwell 28.2. I've had to do a bunch of workarounds for issues in cromwell 30, so confounds abound.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3039:87,message,message,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3039,2,['message'],['message']
Integrability,"- cromwell pre-0.21 dev snaposhot; - JES backend; - command line execution (single workflow) . Some docker images are bigger than the default boot disk size for JES backend. There should be some safeguards against failure when the docker image is too big to fit in the default boot disk size. What happens?; 1. JES tries to download docker image that is bigger than the VM boot disk size. Disk full error message appears.; 2. Workflow fails. Proposed behavior:. After number 1 happens, attempt to spin the VM with additional boot disk storage and retry running the job.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1449:405,message,message,405,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1449,1,['message'],['message']
Integrability,"- cromwell-23-79f6e12-SNAPSHOT.jar; - SGE backend; - Broad Internal filesystem location: ``/dsde/working/lichtens/test_pon_cromwell/cromwell-executions/pon_gatk_workflow/3c28c49b-c243-4371-b80b-d14fb5286c43/``; - no docker; - Being run on Broad VM. The first task takes in a bam file and creates an entity_id, which is passed into the second task. This works fine on local backend. Feel free to contact me if you need more information. Error message:. ```; [ERROR] [11/03/2016 10:37:24.334] [cromwell-system-akka.dispatchers.engine-dispatcher-19] [akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor] WorkflowManagerActor Workflow 3c28c49b-c243-4371-b80b-d14fb5286c43 failed (during ExecutingWorkflowState): wdl4s.util.AggregatedException: Input evaluation for Call pon_gatk_workflow.CalculateTargetCoverage failedFailed to find index Success(WdlInteger(1)) on array:. Success([""/seq/picard_aggregation/C1850/GTEX-1A3MW-0004/current/GTEX-1A3MW-0004.bam""]). 1. ```. Relevant WDL:. ```; ...snip... scatter (row in bam_file_names) {. call GetBamFileName {; input:; input_bam=row[0]; }. call CalculateTargetCoverage {; input:; entity_id=GetBamFileName.name,; padded_target_file=PadTargets.padded_target_file,; input_bam=row[0],; bam_idx=row[1],; ref_fasta=ref_fasta,; ref_fasta_fai=ref_fasta_fai,; ref_fasta_dict=ref_fasta_dict,; gatk_jar=gatk_jar,; disable_sequence_dictionary_validation=disable_sequence_dictionary_validation,; disable_all_read_filters=disable_all_read_filters,; keep_duplicate_reads=keep_duplicate_reads,; transform=transform,; grouping=grouping,; isWGS=isWGS,; mem=calculate_target_coverage_memory; }; ...snip... # Helper task to get the name of the given bam file; task GetBamFileName {; File input_bam. command <<<; echo $(basename ""${input_bam}"" .bam); >>>. output {; String name=read_string(stdout()); }; }. # Calculate the target proportional coverage; task CalculateTargetCoverage {; String entity_id; File padded_target_file; String grouping; Boolean kee",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1646:442,message,message,442,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1646,1,['message'],['message']
Integrability,"- cromwell-25-c398490; - Local backend; - single workflow ; - using docker; - Google VM. This has happened multiple times. Three days apart. Reading google buckets from a local backend has worked fine in the past. The error states that a file cannot be found, yet the error messages look more like a HTTP 500. ``gsutil ls ...`` shows that the file is there. . Apologies if I am missing something obvious (this may just be a misleading error message)... ```; lichtens@lichtens-big:~/test_onco_m2$ gsutil ls gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam; lichtens@lichtens-big:~/test_onco_m2$ gsutil ls gs://broad-dsde-methods/takuto/na12878-crsp-ice/; gs://broad-dsde-methods/takuto/na12878-crsp-ice/; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V3.bai; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V3.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V4.bai; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V4.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V5.bai; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V5.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bai; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam; gs://broad-dsde-methods/takuto/na12878-crsp-ice/na12878-replicate-pairs-cloud.tsv. ```. ```; Could not localize gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam -> /home/lichtens/test_onco_m2/cromwell-executions/Mutect2ReplicateValidation/bf7e55a8-033b-4b36-9aa6-eeb2d77579d8/call-Mutect2/shard-11/Mutect2/0802e0bb-3231-4e14-a627-1ed839b213ae/call-CollectSequencingArtifactMetrics/inputs/broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam:; gs://broad-dsde-methods/takuto/na12878-crsp-ice/SM-612V6.bam doesn't exists; null; 500 Internal Server Error; Backend Error; 500 Internal Server Error; Backend Error; at cromwell.backend.standard.StandardAsyncExecutionActor$$anonfun$commandLinePreProcessor$1$$anonfun$a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2011:274,message,messages,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2011,2,['message'],"['message', 'messages']"
Integrability,"- cromwell-27-c89c83f-SNAP; - server mode; - JES backend; - call caching on localhost mysql server. Is this a matter of hitting some sort of ceiling in number of concurrent jobs? Can I increase this?. ```; ....snip....; ""failures"": [; {; ""causedBy"": [; {; ""causedBy"": [],; ""message"": ""Task slick.basic.BasicBackend$DatabaseDef$$anon$2@122f57e rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@35ca91f1[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 2293]""; }; ],; ""message"": ""JobStore write failure: Task slick.basic.BasicBackend$DatabaseDef$$anon$2@122f57e rejected from slick.util.AsyncExecutor$$anon$2$$anon$1@35ca91f1[Running, pool size = 20, active threads = 20, queued tasks = 1000, completed tasks = 2293]""; }; ],; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2219:274,message,message,274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219,2,['message'],['message']
Integrability,"- dev snapshot of cromwell pre-0.21; - local backend; - Specifying docker from options file; - Fails when running with sudo or without (same error); - `wdltool` validates successfully; - Being run on google cloud VM ; - And after error occurs, cromwell stays running. . I believe that this was working, as is, in cromwell 0.19. I believe that it is having trouble parsing the option file. Command:. ``` bash; java -Xmx4g -Dconfig.file=local_application.conf -jar \; /home/lichtens/test_eval/cromwell-0.20-028b74a-SNAPSHOT.jar run case_gatk_acnv_workflow.final.wdl \ ; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.final.json \; default_runtimes \; /home/lichtens/eval-gatk-protected/scripts/crsp_validation/crsp_validation_gatkp_run_local_paths.json.metadata.json; ```. Error message:. ```; [2016-09-21 17:51:25,15] [error] Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); java.lang.RuntimeException: Expression evaluation failed due to wdl4s.WdlExpressionException: Cannot perform operation: WdlString(broadinstitute) / WdlString(gatk): WdlExpression((Subtract: lhs=(Divide: lhs=<string:1:1 identifier ""YnJvYWRpbnN0aXR1dGU="">, rhs=<string:1:16 identifier ""Z2F0aw=="">), rhs=<string:1:21 identifier ""cHJvdGVjdGVk"">)); at cromwell.backend.validation.RuntimeAttributesValidation$class.validateOptionalExpression(RuntimeAttributesValidation.scala:319); at cromwell.backend.validation.RuntimeAttributesValidation$$anon$1.validateOptionalExpression(RuntimeAttributesValidation.scala:90); at cromwell.backend.sfs.SharedFileSystemInitializationActor$$anonfun$runtimeAttributeValidators$1$$anonfun$apply$1.apply(SharedFileSystemInitializationActor.scala:48); at cromwell.backend.sfs.Shar",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1465:832,message,message,832,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1465,1,['message'],['message']
Integrability,"- develop branch from 0.22; - local backend; - single workflow. I think one of the tasks failed, but got the message below. cromwell did not exit and I believe it should have. ```; [2016-10-24 14:44:19,47] [error] head of empty list; java.util.NoSuchElementException: head of empty list; at scala.collection.immutable.Nil$.head(List.scala:420); at scala.collection.immutable.Nil$.head(List.scala:417); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$8.apply(SingleWorkflowRunnerActor.scala:133); at scala.Option.getOrElse(Option.scala:121); at cromwell.engine.workflow.SingleWorkflowRunnerActor.cromwell$engine$workflow$SingleWorkflowRunnerActor$$issueReply(SingleWorkflowRunnerActor.scala:133); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:88); at cromwell.engine.workflow.SingleWorkflowRunnerActor$$anonfun$4.applyOrElse(SingleWorkflowRunnerActor.scala:85); at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36); at akka.actor.FSM$class.processEvent(FSM.scala:663); at cromwell.engine.workflow.SingleWorkflowRunnerActor.akka$actor$LoggingFSM$$super$processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.LoggingFSM$class.processEvent(FSM.scala:799); at cromwell.engine.workflow.SingleWorkflowRunnerActor.processEvent(SingleWorkflowRunnerActor.scala:34); at akka.actor.FSM$class.akka$actor$FSM$$processMsg(FSM.scala:657); at akka.actor.FSM$$anonfun$receive$1.applyOrElse(FSM.scala:651); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.server.CromwellRootActor.aroundReceive(CromwellRootActor.scala:27); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exe",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1615:109,message,message,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1615,1,['message'],['message']
Integrability,"- develop; - server mode. ```; SELECT MAX(aggregated) as group_concat_max_len FROM; (; SELECT cche.CALL_CACHING_ENTRY_ID, SUM(LENGTH(cche.HASH_VALUE)) AS aggregated; FROM CALL_CACHING_HASH_ENTRY cche; GROUP BY cche.CALL_CACHING_ENTRY_ID; ) aggregation; ```. yields a value of 1440. Yet, even when I set ``group_concat_max_len`` to 30000, I get the error message that the migration cannot be completed successfully.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2262:354,message,message,354,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2262,1,['message'],['message']
Integrability,- martha endpoint is specified in config; - martha_v3 has different response structure and supports Terra data repo; - Terra data repo integration tests are not included are in https://github.com/broadinstitute/cromwell/pull/5719. Closes https://broadworkbench.atlassian.net/browse/WA-180,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5710:135,integrat,integration,135,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5710,1,['integrat'],['integration']
Integrability,"- our operations staff reports that it is not unusual for this to happen up to dozen or so times a day where the ""Message 13:"" failures cause the entire workflow to fail and need to be re-submitted. I would only assume that at a task level it is happening more often and as long as it does happen three times in succession for the same task - our ops team may not even notice it. Since the retry covers it up. ; > ; > But it can cause considerable amount of delay on completing a sample. The time spent to do the 3 retries but then the time it takes for a human to notice the failure and re-submit the entire thing again. For ""normal"" preemption - we have codified things in our WDL such that when failures occur - it is usually something unusual. With the higher occurrence of ""Message 13"" cause workflow failures - there is a new added step that needs to be looked at first. Did the workflow fail due to ""Message 13""?; > ; > At a minimal it would be nice to understand what are the circumstances a ""Message 13"" failure happens - so the Red/Cromwell team can determine if there is anything they can or should do differently. ; > ; > -Henry. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #4 Jan 12, 2018 11:45AM ; > As I'm fielding questions about why there's a cromwell bug\ for not properly retrying preemptions in these cases I wanted to bump this a bit. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #5 Jan 16, 2018 03:59PM ; > This is occurring more and more. It is starting to impact our through-put for our production pipeline processing. > ------------------------------- ; > kemp@google.com <kemp@google.com> #6 Jan 17, 2018 10:44AM ; > Nothing has changed in Pipelines API in this regard. I suspect either a GCE preemption policy change or some other resourcing issue. Mike, can you reach out to the GCE team on this?; > ; > Garret, let's look at some of the operations in #1 and see if we can s",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:4557,Message,Message,4557,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,2,['Message'],['Message']
Integrability,- what version of cwltool is used and versions of transitive dependencies if known,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2718#issuecomment-394008620:61,depend,dependencies,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2718#issuecomment-394008620,1,['depend'],['dependencies']
Integrability,"---- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI servers don't see the instance shutting down then you'll see an error 13 as well.; > ; > I think the solution is to not differentiate your behavior on the content of the returned message, and always retry if the operation is showing as aborted and the instance was preemptible. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #16 Jan 18, 2018 07:20AM ; > Can Message 14's occur with non-preemptible instances? Like Message 13s cane?. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #17 Jan 18, 2018 10:26AM ; > hi - ; > ; > So is it the case that 100% of the time one receives a message 13 that it's ; > a preemption? ; > ; > The problem is that we keep them on separate counters so as to maximize the ; > number of preemptible tries a user gets (we try preemptibles up to N times ; > before falling back to a standard instance) but will retry other retryable ; > errors on their own count. If we're treating transient errors as ; > preemptible when they're not people can wind up on a standard instance ; > before it's necessary. ; > ; > If it's not 100%, is there any way for the error",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:11505,message,message,11505,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['message'],['message']
Integrability,"---. > All hoped-for things will come to you; > Who have the strength to watch and wait,; > Our longings spur the steeds of Fate,; > This has been said by one who knew. ---. - Don't set defaults for CI build variables ; - Removed many unused CI build variables ; - Renamed CI build variables based on generation and/or usage; - Ensure only passed values for CI build variables are used ; - Fixed tests that were using defaults and not passing in variables ; - Render centaur refresh tokens instead of rewriting json in memory ; - Don't use a bash wrapper-process for launching docker-compose from centaur ; - Pass centaur CI variables to docker-compose using env directly ; - Print docker-compose logs when centaur is unable to run `docker-compose up` ; - Better local docker-compose CI debugging with `crmdmm=y testFoo.sh` ; - Fix local CI debugging that was looking for `[force ci]` on prior commit ; - Allow force pushes to GitHub to use `[force ci]` syntax ; - Left `conformanceTesk` references while stubbing unused test ; - Consolidate tag generation for various docker images ; - Consolidate sbt invocation to build various docker images ; - On local tests, cache docker images just like assembly jars ; - Consistent level of verbosity on CI docker pushes and pulls ; - Moved conformance CI env-based-customization out of the reference.conf",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5738:547,wrap,wrapper-process,547,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5738,1,['wrap'],['wrapper-process']
Integrability,"-21 15:09:44,61] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-11-21 15:09:44,61] [info] WorkflowStoreActor stopped; [2018-11-21 15:09:44,61] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-11-21 15:09:44,62] [info] WorkflowLogCopyRouter stopped; [2018-11-21 15:09:44,62] [info] JobExecutionTokenDispenser stopped; [2018-11-21 15:09:44,62] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor All workflows finished; [2018-11-21 15:09:44,62] [info] WorkflowManagerActor stopped; [2018-11-21 15:09:44,62] [info] Connection pools shut down; [2018-11-21 15:09:44,62] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-11-21 15:09:44,62] [info] SubWorkflowStoreActor stopped; [2018-11-21 15:09:44,63] [info] JobStoreActor stopped; [2018-11-21 15:09:44,63] [info] DockerHashActor stopped; [2018-11-21 15:09:44,63] [info] IoProxy stopped; [2018-11-21 15:09:44,63] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] CallCacheWriteActor stopped; [2018-11-21 15:09:44,63] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-11-21 15:09:44,63] [info] ServiceRegistryActor stopped; [2018-11-21 15:09:44,65] [info] Database closed; [2018-11-21 15:09:44,65] [info] Stream materializer shut down; [2018-11-21 15:09:44,66] [info] WDL HTTP import resolver closed",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:6302,message,messages,6302,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,3,['message'],['messages']
Integrability,"-files). Thanks to all the great help you've provided we now have compatible WDL output that passes validation:. https://github.com/bcbio/test_bcbio_cwl/blob/master/run_info-cwl-wdl. This is brilliant, and I'd like to move into testing runs with Cromwell. Before starting this, there is one major area I know we're missing in the conversion, handling of secondary files and directories of files. CWL has the notion of secondaryFiles (http://www.commonwl.org/v1.0/Workflow.html#File) which you can use to block these and ensure they get staged/run next to each other. I use this in bcbio and wanted to figure out the best way to map it into WDL. There are two cases we use these for:. - Index files associated with compressed inputs, like BAM bai indices and bgzip VCF tbi indices. These are a single index file attached to the original file that should get staged in the same directory when running.; - Directories of index files like bwa or snpeff. These are a bit trickier since they can have many files and a variable number depending on the input. What is the recommended way to deal with these cases in WDL? I'll have to re-engineer bcbio to be able to represent and pass these and wanted to do so in a way that was forward compatible with WDL's thoughts and plans. I've seen recommendations on current hacks like explicitly declaring the indexes as separate files, or tarring up a directory of files and passing that as input. I'm not clear enough on staging files from WDL/Cromwell to understand if these are guaranteed to always go in the right place (bai next to bam, all indexes in the same directory). Thanks for any thoughts/suggestions/tips. This Issue was generated from your [forums] ; [forums]: http://gatkforums.broadinstitute.org/wdl/discussion/9299/secondary-index-files-and-directories-in-wdl/p1. ---. @vdauwera commented on [Thu May 04 2017](https://github.com/broadinstitute/dsde-docs/issues/1996#issuecomment-299359050). @katevoss this is a very common request from the Cromwel",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2269:2121,depend,depending,2121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269,1,['depend'],['depending']
Integrability,"-team/junit4/issues/1671"">#1671</a>)</li>; <li><a href=""https://github.com/junit-team/junit4/commit/a5d205c7956dbed302b3bb5ecde5ba4299f0b646""><code>a5d205c</code></a> Fix GitHub link in FAQ (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1672"">#1672</a>)</li>; <li><a href=""https://github.com/junit-team/junit4/commit/3a5c6b4d08f408c8ca6a8e0bae71a9bc5a8f97e8""><code>3a5c6b4</code></a> Deprecated since jdk9 replacing constructor instance of Double and Float (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1660"">#1660</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/junit-team/junit4/compare/r4.13...r4.13.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=junit:junit&package-manager=maven&previous-version=4.13&new-version=4.13.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5941:4121,depend,dependabot-security-updates,4121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5941,1,['depend'],['dependabot-security-updates']
Integrability,"-variant-discovery-gatk4.wdl.txt). Next, I am attaching the corresponding slurm output log to the command submitted (`NFRI_S003_M1.bam.sh.txt`) : `slurm-27492257.out.txt`. [slurm-27492257.out.txt](https://github.com/broadinstitute/cromwell/files/2540512/slurm-27492257.out.txt). As you can see in the slurm output log, the workflow stops making progress at the MergeBamAlignment step. Insterestingly, if you go into the `executions` directory in `cromwell-executions`: `path/to/cromwell-executions/PreProcessingForVariantDiscovery_GATK4/d41833e7-2c4a-4a92-a9d7-fd194d3059d3/call-MergeBamAlignment/shard-0/execution/`: you'll find in the `stderr` that in fact the jobs seems to have complete successfully and produced the desired output file. Unfortunately, for some reason this is where the job hags (the job continues running but the worfklow makes no progress). I checked with our Research Computing division and they closely investigated the issue. . Below is there message to me, which finds that something unexpected is going on with Cromwell. . ```; Hi Alon,; ; That ""child"" job is running on compute-p-17-32 along with few others:; ; JOBID USER PRIORITY PARTITION STATE TIME_LIMIT TIME NODELIST(REASON) ELIGIBLE_TIME START_TIME TRES; 27492251 ag457 514600 park RUNNING 30-00:00:00 4-15:26:38 compute-p-17-32 2018-10-25T22:10:29 2018-10-25T22:10:52 cpu=1,mem=10G,node=1; 27492313 ag457 405657 park RUNNING 25-00:00:00 4-10:32:08 compute-p-17-32 2018-10-25T22:11:33 2018-10-26T03:05:22 cpu=16,mem=224000M,node=1; 27518966 ag457 360221 park RUNNING 25-00:00:00 3-19:32:50 compute-p-17-32 2018-10-26T13:53:05 2018-10-26T18:04:40 cpu=1,mem=3500M,node=1; 27587803 ag457 357888 park RUNNING 25-00:00:00 2-15:03:00 compute-p-17-32 2018-10-27T17:37:01 2018-10-27T22:34:30 cpu=2,mem=10000M,node=1; 27612863 ag457 343479 park RUNNING 25-00:00:00 2-09:38:44 compute-p-17-32 2018-10-28T03:57:58 2018-10-28T03:58:46 cpu=1,mem=6000M,node=1; 27612723 ag457 343389 park RUNNING 25-00:00:00 2-11:30:58 compute-p-",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4347:2374,message,message,2374,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4347,1,['message'],['message']
Integrability,". If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/92f43ce5f010fa03daeb2625a3f9c1b1be80cca5/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.13.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/rnaseq-workflow/steps/prepare_sample.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/variantcall_batch_region.cwl; ```; </details>; <details>; <summary>Ignore future updates</summary>. Add this t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6410:1831,integrat,integrationTestCases,1831,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6410,1,['integrat'],['integrationTestCases']
Integrability,".""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_fasta_index' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.unmapped_bam_suffix' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_ud' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_amb' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.preemptible_tries' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_sa' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.known_indels_sites_VCFs' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_mu' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_alt' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_bwt' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.known_indels_sites_indices' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_dict' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_bed' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_fasta' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.dbSNP_vcf_index' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.dbSNP_vcf' not specified.""; }; ],; message: ""Workflow input processing failed""; }; ],; ```. But once I filled these out in my ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2912:2038,message,message,2038,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2912,1,['message'],['message']
Integrability,.SocketTimeoutException: Read timed out; 	at java.net.SocketInputStream.socketRead0(Native Method); 	at java.net.SocketInputStream.socketRead(SocketInputStream.java:116); 	at java.net.SocketInputStream.read(SocketInputStream.java:171); 	at java.net.SocketInputStream.read(SocketInputStream.java:141); 	at sun.security.ssl.InputRecord.readFully(InputRecord.java:465); 	at sun.security.ssl.InputRecord.read(InputRecord.java:503); 	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973); 	at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:105); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); 	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1569); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1474); 	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); 	at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); 	at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); 	at com.google.cloud.stora,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2229:4296,protocol,protocol,4296,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2229,1,['protocol'],['protocol']
Integrability,".actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Singularity/2.5.0-intel-2017.u2 || true; singularity pull docker://${docker}; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""singularity exec -B ${cwd}:${docker_cwd} docker://${docker} ${job_shell} ${script}"" ; """""". kill = ""scancel ${job_id}""; check-alive =",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577:3313,Message,Message,3313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577,1,['Message'],['Message']
Integrability,".client.googleapis.services.AbstractGoogleClient <init>; WARNING: Application name is not set. Call Builder#setApplicationName.; [2016-04-28 15:35:51,646] [info] JES Pipeline [1cb9c1d2:jes_task]: Inputs:; exec -> disk:local-disk relpath:exec.sh; [2016-04-28 15:35:51,647] [info] JES Pipeline [1cb9c1d2:jes_task]: Outputs:; jes_task-rc.txt -> disk:local-disk relpath:jes_task-rc.txt; [2016-04-28 15:35:51,648] [info] JES Pipeline [1cb9c1d2:jes_task]: Mounts:; c98942d68bf4c33728f1adef1bfd9ccc -> /mnt/mnt1 (3GB PERSISTENT_SSD); 4fd1d1e01455dfdd4eabcf02c1abaf55 -> /mnt/mnt2 (500GB PERSISTENT_HDD); local-disk -> /cromwell_root (10GB PERSISTENT_SSD); [2016-04-28 15:35:51,728] [warn] JesBackend [1cb9c1d2:jes_task]: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""status"" : ""INVALID_ARGUMENT""; }; com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request; {; ""code"" : 400,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""reason"" : ""badRequest""; } ],; ""message"" : ""Invalid value for field \""resources.disk.name\"": 4fd1d1e01455dfdd4eabcf02c1abaf55\nDisk names must follow rules at https://cloud.google.com/compute/docs/reference/latest/disks#name"",; ""status"" : ""INVALID_ARGUMENT""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.servic",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/757:1892,message,message,1892,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/757,1,['message'],['message']
Integrability,".forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107); [2016-10-27 13:10:24,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:25,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:26,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:27,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:28,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:29,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:30,93] [info] Waiting for 1 workflows to abort...; [2016-10-27 13:10:31,53] [error] Exception not convertible into handled response; com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found; {; ""code"" : 404,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Not Found"",; ""reason"" : ""notFound""; } ],; ""message"" : ""Not Found""; }; at com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:145); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:113); at com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:40); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:432); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352); at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469); at com.google.cloud.hadoop.util.AbstractGoogleAsyncWriteChannel$UploadOperation.call(AbstractGoogleAsyncWriteChannel.java:357); at jav",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631:10167,message,message,10167,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631,2,['message'],['message']
Integrability,".jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:113); 	at liquibase.database.AbstractJdbcDatabase.execute(AbstractJdbcDatabase.java:1277); 	at liquibase.database.AbstractJdbcDatabase.executeStatem",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:4828,message,message,4828,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['message'],['message']
Integrability,.scala:24); 	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24); 	... 6 more; Caused by: javax.net.ssl.SSLException: Connection has been shutdown: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe; 	at sun.security.ssl.SSLSocketImpl.checkEOF(SSLSocketImpl.java:1541); 	at sun.security.ssl.AppInputStream.read(AppInputStream.java:95); 	at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345); 	at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704); 	at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536); 	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441); 	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480); 	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338); 	at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37); 	at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94); 	at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:981); 	at com.google.cloud.storage.spi.DefaultStorageRpc.write(DefaultStorageRpc.java:564); 	... 24 more; Caused by: javax.net.ssl.SSLException: java.net.SocketException: Broken pipe; 	at sun.security.ssl.Alerts.getSSLException(Alerts.java:208); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1949); 	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1906); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1870); 	at sun.security.ssl.SSLSocketImpl.handleException(SSLSocketImpl.java:1815); 	at sun.security.ssl.AppOutputStream.write(AppOutputStream.java:128); 	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183:4898,protocol,protocol,4898,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183,1,['protocol'],['protocol']
Integrability,".scala:46); at sbt.std.Transform$$anon$4.work(System.scala:67); at sbt.Execute.$anonfun$submit$2(Execute.scala:269); at sbt.internal.util.ErrorHandling$.wideConvert(ErrorHandling.scala:16); at sbt.Execute.work(Execute.scala:278); at sbt.Execute.$anonfun$submit$1(Execute.scala:269); at sbt.ConcurrentRestrictions$$anon$4.$anonfun$submitValid$1(ConcurrentRestrictions.scala:178); at sbt.CompletionService$$anon$2.call(CompletionService.scala:37); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Cause: org.scalatest.exceptions.TestFailedException: isEmpty was false, and Some(false) did not contain true Instead, a.status.messages = List(Unknown status) and e.status.messages = List(womp womp); at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:528); at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:527); at cromwell.core.TestKitSuite.newAssertionFailedException(TestKitSuite.scala:16); at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$5(HealthMonitorServiceActorSpec.scala:48); at scala.collection.immutable.List.map(List.scala:283); at cromwell.services.healthmonitor.HealthMonitorServiceActorSpec.$anonfun$eventualStatus$1(HealthMonitorServiceActorSpec.scala:40); at org.scalatest.concurrent.Eventually.makeAValiantAttempt$1(Eventually.scala:395); at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:409); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382:5943,message,messages,5943,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4259#issuecomment-433056382,1,['message'],['messages']
Integrability,".services.ServicesStore$EnhancedSqlDatabase$.$anonfun$initialized$1$adapted(ServicesStore.scala:11); 	at cromwell.database.slick.SlickDatabase.$anonfun$withConnection$1(SlickDatabase.scala:155); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:70); 	at slick.jdbc.SimpleJdbcAction.run(StreamingInvokerAction.scala:69); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.liftedTree1$1(BasicBackend.scala:275); 	at slick.basic.BasicBackend$DatabaseDef$$anon$2.run(BasicBackend.scala:275); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: liquibase.exception.DatabaseException: Unknown column ':causedBy[]' in 'field list' [Failed SQL: INSERT INTO METADATA_ENTRY (WORKFLOW_EXECUTION_UUID, METADATA_KEY, CALL_FQN, JOB_SCATTER_INDEX, JOB_RETRY_ATTEMPT, METADATA_TIMESTAMP); SELECT t1.WORKFLOW_EXECUTION_UUID, CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[]""), t1.CALL_FQN, t1.JOB_SCATTER_INDEX, t1.JOB_RETRY_ATTEMPT, t1.METADATA_TIMESTAMP; FROM METADATA_ENTRY AS t1; WHERE METADATA_KEY LIKE '%failures[%]%:message'; AND NOT EXISTS (SELECT *; 	FROM METADATA_ENTRY AS t2; 	WHERE t2.WORKFLOW_EXECUTION_UUID = t1.WORKFLOW_EXECUTION_UUID; 	 AND (t2.CALL_FQN = t1.CALL_FQN OR (t2.CALL_FQN IS NULL AND t1.CALL_FQN IS NULL)); 	 AND (t2.JOB_SCATTER_INDEX = t1.JOB_SCATTER_INDEX OR (t2.JOB_SCATTER_INDEX IS NULL AND t1.JOB_SCATTER_INDEX IS NULL)); 	 AND (t2.JOB_RETRY_ATTEMPT = t1.JOB_RETRY_ATTEMPT OR (t2.JOB_RETRY_ATTEMPT IS NULL AND t1.JOB_RETRY_ATTEMPT IS NULL)); AND t2.METADATA_KEY LIKE CONCAT(TRIM(TRAILING ':message' FROM t1.METADATA_KEY), "":causedBy[%""); AND t2.METADATA_JOURNAL_ID <> t1.METADATA_JOURNAL_ID; )]; 	at liquibase.executor.jvm.JdbcExecutor$ExecuteStatementCallback.doInStatement(JdbcExecutor.java:309); 	at liquibase.executor.jvm.JdbcExecutor.execute(JdbcExecutor.java:55); 	at liquiba",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701:4634,message,message,4634,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4606#issuecomment-459609701,1,['message'],['message']
Integrability,".util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.$anonfun$validateWdlNamespace$13(MaterializeWorkflowDescriptorActor.scala:491)""; },; {; causedBy: [ ],; message: ""scala.util.Either.flatMap(Either.scala:338)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.validateWdlNamespace(MaterializeWorkflowDescriptorActor.scala:490)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor.cromwell$engine$workflow$lifecycle$materialization$MaterializeWorkflowDescriptorActor$$buildWorkflowDescriptor(MaterializeWorkflowDescriptorActor.scala:231)""; },; {; causedBy: [ ],; message: ""cromwell.engine.workflow.lifecycle.materialization.MaterializeWorkflowDescriptorActor$$anonfun$1.$anonfun$applyOrElse$1(MaterializeWorkflowDescriptorActor.scala:157)""; },; {; causedBy: [ ],; message: ""scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:304)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:37)""; },; {; causedBy: [ ],; message: ""scala.concurrent.impl.CallbackRunnable.run(Promise.scala:60)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$AbstractBatch.processBatch(BatchingExecutor.scala:55)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.$anonfun$run$1(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:12)""; },; {; causedBy: [ ],; message: ""scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:81)""; },; {; causedBy: [ ],; message: ""akka.dispatch.BatchingExecutor$BlockableBatch.run(BatchingExecutor.scala:91)""; },; {; causedBy: [ ],; message: ""akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:40)""; },; {; causedBy: [ ],; message: ""akka.dispatch.Fo",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3143:2475,message,message,2475,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3143,1,['message'],['message']
Integrability,"/broadinstitute_warp_development/tutorials/cromwell-slurm_5.config \; -jar /mainfs/wrgl/broadinstitute_warp_development/tutorials/cromwell-85.jar \; run /mainfs/wrgl/broadinstitute_warp_development/warp/ExomeGermlineSingleSample_v3.1.9.wdl \; -i /mainfs/wrgl/broadinstitute_warp_development/tutorials/Exom_test.json. #### Configuration file ###. include required(classpath(""application"")). system {; # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting; abort-jobs-on-terminate = false; }. backend {; default = SLURM. providers {; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {; temporary-directory = ""$(mktemp -d /tmp/tmp.XXXXXX)"". runtime-attributes = """"""; Int runtime_minutes = 60; Int cpu = 1; Int memory_mb = 3900; String? docker; """""". submit = """""" \; 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${out} \; -e ${err} \; -t ${runtime_minutes} \; -p batch,scavenger \; -c ${cpu} \; --mem $(( (${memory_mb} >= ${cpu} * 3900) ? ${memory_mb} : $(( ${cpu} * 3900 )) )) \; -N 1 \; --exclusive \; --wrap ""/bin/bash ${script}""'; """""". submit-docker = """""" \. # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default; # based on the users home.; module load apptainer; if [ -z $APPTAINER_CACHEDIR ];; then CACHE_DIR=$HOME/.apptainer/cache; else CACHE_DIR=$APPTAINER_CACHEDIR; fi; # Make sure cache dir exists so lock file can be created by flock; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/apptainer_pull_flock; # Create an exclusive filelock with flock. --verbose is useful for; # for debugging, as is the echo command. These show up in `stdout.submit`.; flock --exclusive --timeout 900 $LOCK_FILE \; apptainer exec --containall /mainfs/wrgl/broadinstitute_warp_development/warp/images/${docker}.sif \; echo ""successfully pulled ${docker}!"". # Submit the script to SLURM. 'sbatch \; --wait \; -J ${job_name} \; -D ${cwd} \; -o ${cwd}/execution/stdout \; -e ${cwd}/execu",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7086:1609,wrap,wrap,1609,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7086,1,['wrap'],['wrap']
Integrability,"/cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; This is a remark on https://github.com/broadinstitute/cromwell/blob/master/docs/tutorials/HPCSlurmWithLocalScratch.md there is a feature on slum config to edit the sbatch command. You could add in a find and replace in the config to do the same as the tutorial. you can skip the first part of the tutorial by editing the slurm backend config (somewhat hotpatching the scripts on submission time). old submit ; submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". new submit for slurm auto configured job dir: ; submit = """"""; perl -i.bak -wpe 's/^tmpDir=.*/tmpdir=""\$TMPDIR""/g' ${script} && \; sbatch -J ${job_name} --tmp=${disk} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". new submit for /genomics/local/ (not tested tough): ; submit = """"""; perl -i.bak -wpe 's/^tmpDir=.*/tmpdir=""$(mkdir -p ""\/genomics_local\/\$PID_\$HOSTNAME""\/"" && echo ""\/genomics_local\/\$PID_\$HOSTNAME""\/""/g' ${script} && \; sbatch -J ${job_name} --tmp=${disk} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; <!-- This is a clear feature cant you see -->. <!-- Which backend are you running? -->; The backend I'm running on is Slurm hpc with a version 1.0 workflow. This alternative workflow has its downsides but also benefits it is up to the hpc(user) to decide what works best in their own situation. ; <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7357:1213,wrap,wrap,1213,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7357,1,['wrap'],['wrap']
Integrability,/fc-jenkins.dsp-techops.broadinstitute.org/job/cromwell-test-runner/1025/. java.util.concurrent.TimeoutException: Futures timed out after [1 second] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:255) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:259) at scala.concurrent.Await$.$anonfun$result$1(package.scala:215) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:142) at akka.http.scaladsl.testkit.RouteTest.responseAs(RouteTest.scala:70) at akka.http.scaladsl.testkit.RouteTest.responseAs$(RouteTest.scala:68) at cromiam.webservice.SwaggerServiceSpec.responseAs(SwaggerServiceSpec.scala:17) at cromiam.webservice.SwaggerServiceSpec.$anonfun$new$2(SwaggerServiceSpec.scala:30) at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58) at akka.http.scaladsl.testkit.RouteTest.$anonfun$check$1(RouteTest.scala:56) at akka.http.scaladsl.testkit.RouteTestResultComponent$RouteTestResult.$tilde$greater(RouteTestResultComponent.scala:50) at cromiam.webservice.SwaggerServiceSpec.$anonfun$new$1(SwaggerServiceSpec.scala:27) at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1682) at org.scalatest.TestSuite.withFixture(TestSuite.scala:196) at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195) at org.scalatest.FlatSpec.withFixture(FlatSpec.scala:1685) at org.scalatest.FlatSpecLike.invokeWithFixture$1(FlatSpecLike.scala:1680) at org.scalatest.FlatSpecLike.$anonfun$runTest$1(FlatSpecLike.scala:1692) at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289) at org.scalatest.FlatSpecLike.runTest(FlatSpecLike.scala:1692) at org.scalatest.FlatSpecLike.runTest,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4357:987,Rout,RouteTestResultComponent,987,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4357,2,['Rout'],"['RouteTestResult', 'RouteTestResultComponent']"
Integrability,"/subworkflow.wdl"" as sub. workflow root {; call sub.subWorkflow as aliasSub; }; ```. When I try to pass the values for `File f` and `String s` from the inputs json I get an failure message. To make sure I was giving the workflow the correct inputs json I first ran it with bad inputs on purpose and got expected failures; ```; status: ""Failed"",; failures: [; {; causedBy: [; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_pac' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.agg_preemptible_tries' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_ann' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.wgs_coverage_interval_list' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_fasta_index' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.unmapped_bam_suffix' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_ud' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_amb' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.preemptible_tries' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_sa' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.known_indels_sites_VCFs' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_mu' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_alt' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlig",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2912:1194,message,message,1194,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2912,1,['message'],['message']
Integrability,"0,92] [info] Metadata summary refreshing every 2 seconds.; [2018-08-30 17:53:21,02] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-30 17:53:21,03] [info] KvWriteActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,03] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-30 17:53:21,89] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-30 17:53:21,95] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-30 17:53:21,97] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-30 17:53:22,05] [info] Unspecified type (Unspecified version) workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc submitted; [2018-08-30 17:53:22,16] [info] SingleWorkflowRunnerActor: Workflow submitted 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,16] [info] 1 new workflows fetched; [2018-08-30 17:53:22,16] [info] WorkflowManagerActor Starting workflow 4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,17] [info] WorkflowManagerActor Successfully started WorkflowActor-4dbd7d1c-e7e8-4f83-9750-5c638d1567bc; [2018-08-30 17:53:22,17] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-08-30 17:53:22,18] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-08-30 17:53:22,20] [warn] Couldn't find a suitable DSN, defaulting to a Noop one.; [2018-08-30 17:53:22,21] [info] Using noop to send events.; [2018-08-30 17:53:22,25] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-08-30 17:53:22,30] [info] MaterializeWorkflowDescriptorActor [4dbd7d1c]: Parsing workflow as WDL draft-2; [2018-08-30 17:53:23,48] [info] MaterializeWorkflowDescriptorActor [4dbd7d1c]: Call-to-Backend assignments: HelloWorld.WriteGreeting -> Local; [2018-08-30 17:53:24,95] [info] WorkflowExecutionActor-4db",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4062:2274,message,message,2274,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4062,1,['message'],['message']
Integrability,"00 Eukaryote Total RNA Nano Series II. Only total RNAs with a RIN score of more than 7 were used for RNA-Seq library construction\nRibosomal RNA (rRNA) was removed from total RNA using the RiboMinusâ„¢ Eukaryote Kit for RNA-Seq from Ambion. The ribosomal RNA depleted RNA fraction is termed the RiboMinusâ„¢ RNA fraction and is enriched in polyadenylated (polyA) mRNA, non-polyadenylated RNA, pre-processed RNA, tRNA, and may also contain regulatory RNA molecules such as microRNA (miRNA) and short interfering RNA (siRNA), snRNA, and other RNA transcripts of yet unknown function. Ambion RiboMinus rRNA depletion was performed as described in the manufacturerâ€™s protocol (Pub. Part no.: 100004590, Rev. date 2 December 2011) following the standard protocol.\nTruSeq RNA Sample Preparation was performed on the RiboMinusâ„¢ RNA fraction as described in the manufacturerâ€™s protocol (Pub. Part no.: 15026495 Rev. F March 2014) following the low sample protocol.\nThe libraries were sequenced on Illuminaâ€™s HiSeq 2000 instrument following standard protocol."",; ""processing"" : ""Data quality check using fastQC version 0.11.2.\nAlignment of unpaired unstranded reads using STAR version 2.4.0.\nQuantification of transcripts and isoforms using RSEM version 1.2.21 using rsem-calculate-expression, both alignment and quantification was done using the STAR_RSEM.sh pipeline (https://github.com/ENCODE-DCC/long-rna-seq-pipeline/blob/master/DAC/STAR_RSEM.sh)\nThe programe featurecounts version 1.4.6-p2 from the SourceForge Subread package was used to produce a summary file of counts from all the alignement .bam files.\nThe summary file of counts (RNAseq.counts) was used to plot the multidimensional scaling plot using edgeR version 3.1.3.\nThe *.osc.gz files were loaded into the genome browser ZENBU and was used visualize the transcripts. Screen shots were captured.\nGenome_build: hg19 with Gencode V19 annotation\nSupplementary_files_format_and_content: .osc files are simple tab delimited files. They were g",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4519:1949,protocol,protocol,1949,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4519,1,['protocol'],['protocol']
Integrability,"0163246050849367080, us-east1-d/n1-standard-16. > ------------------------------- ; > gdk@google.com <gdk@google.com> Jan 17, 2018 01:36PM; > Accepted by gdk@google.com. > ------------------------------- ; > gdk@google.com <gdk@google.com> #12 Jan 17, 2018 02:52PM ; > The difference between 13 and 14 here is simply when PAPI notices that the VM has been shut down. They mean essentially the same thing, and cromwell should be able to retry with the same logic.; > ; > It looks like this might have been exacerbated because changed the shutdown behavior of VMs so that they won't stay around for 24h for debugging before the holidays. This means that when a VM is preempted it shuts down faster than it used to, and so PAPI may see the shutdown at a different point. > ------------------------------- ; > ferrara@broadinstitute.org <ferrara@broadinstitute.org> #13 Jan 17, 2018 03:08PM ; > So, gdk - will Message 13 - only happen with pre-emptibles? Will a non-preemptible vm that is somehow shutdown also end up getting a Message 13 returned? If so - then how can one tell the difference? I thought Message 14 only happened on pre-emptibles. > ------------------------------- ; > jgentry@broadinstitute.org <jgentry@broadinstitute.org> #14 Jan 17, 2018 03:13PM ; > Hi - ; > ; > In the past we've been told that Message 13 was a generic catch all for ; > something unexpected happening. For instance I'm pretty sure (but don't ; > have data to back this up) that we see 13s when not running a preemptible ; > instance. ; > ; > Cromwell retries both messages, but treats them differently. It will simply ; > retry on a 13, but for preemptibles we will switch from using a preemptible ; > to a standard instance after N preemptions. ; > ; > J ; > ; > ------------------------------- ; > gdk@google.com <gdk@google.com> #15 Jan 17, 2018 05:01PM ; > Hi Henry, Jeff,; > Message 13 can occur with non-preemptible instances as well. In cases where the controller sees an error and exits, if the PAPI server",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:10238,Message,Message,10238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,3,['Message'],['Message']
Integrability,"0duB1nw. input fastq files can be retrieved here (they are small ~10000 reads each):. https://drive.google.com/file/d/1-c14Tja4zY3lyr6icFWT06stznR_-Zqr/view?usp=sharing; https://drive.google.com/file/d/1oJd_U9MjTllL0_kpNivw8I_LtSyvqpXH/view?usp=sharing. How can I solve this issue and make the workflow running smoothly?. ### Which backend are you running? ; I am running locally the workflow for now (because I am in the first phase of the development). ### Workflow is this:; ```; #workflow validated before running with: wdltool validate example.wdl and womtool validate scMeth_v2.wdl.sh -i scMeth_input_3.json. workflow scMeth {; # information for trimming the cell barcode; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName. # information for trimming the adapters and low quality reads; File file_format; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG; call trimCellBarcode {; input:; sampleName=sampleName,; bases=bases,; input_fastq1=input_fastq1,; input_fastq2=input_fastq2,; command=command; }; call trimAdapters {; input:; file_format=file_format,; input_r1 = trimCellBarcode.fastq_debarcoded_R1,; input_r2 = trimCellBarcode.fastq_debarcoded_R2,; low_quality_cutoff=low_quality_cutoff,; read_length_cutoff=read_length_cutoff,; adapters_1=adapters_1,; adapters_2=adapters_2,; trim_start_R1=trim_start_R1,; trim_end_R1=trim_end_R1,; trim_start_R2=trim_start_R2,; trim_end_R2=trim_end_R2,; TAG=TAG; }; }. task trimCellBarcode {; File command; Int bases; File input_fastq1; File input_fastq2; String sampleName; command {; perl ${command} paired ${input_fastq1} ${input_fastq2} ${bases} ${sampleName}.R1.debarcoded.fq.gz ${sampleName}.R2.debarcoded.fq.gz; }; output {; File fastq_debarcoded_R1 = ""${sampleName}.R1.debarcoded.fq.gz""; File fastq_debarcoded_R2 = ""${sampleName}.R2.debarcoded.fq.gz""; }; }. task trimAdapters {; File file_format",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5066:1490,adapter,adapters,1490,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5066,1,['adapter'],['adapters']
Integrability,"1-minute description of this: VariableReference had to become aware of where it was, because `p.left` might need a `Pair` called `p`'s `""left""` field, or it might need a `task` called `p`'s `""left""` output, depending on its scope, and the variable reference is different in each case (`p` and `p.left` respectively). Determining whether a reference is a member access or a task output reference is a bit inefficient right now, but I think it should be ok since it's only called during WDL instantiation (thereafter it's all just following the links in WOM objects)",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2947:207,depend,depending,207,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2947,1,['depend'],['depending']
Integrability,1. Added Cromwell-backend library as sub-project of Cromwell.; 2. Added base interfaces from Cromwell-backend.; 3. Tried to use SBT Git plugin for versioning but I couldn't get it work with sbt multi-projects. @geoffjentry and @scottfrazer could you please review this PR?,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/495:77,interface,interfaces,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/495,1,['interface'],['interfaces']
Integrability,1. Added cacheEnabled and cacheForceRw support in workflow options.; 2. Added missing keys during initialization to log a warning message if a key is not in that list. Reviewers: @jainh and @geoffjentry.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1184:130,message,message,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1184,1,['message'],['message']
Integrability,1. BCS supported backend.; 2. OSS storage nio interface.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3101:46,interface,interface,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3101,1,['interface'],['interface']
Integrability,"1. Looking specifically for feedback on what places JesCacheHitCopyingActor would require more error handling.; 2. Within the JesCacheHitCopyingActor, are there any messages not being sent to the metadata service that should be sent ?; 3. Currently, not re-saving the JobOutputs that JesCacheHitCopyingActor is copying, since we shouldn't require multiple copies of the same outputs",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1394:165,message,messages,165,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1394,1,['message'],['messages']
Integrability,"1.1.0...v1.1.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/2dc2301e75aea6c2d0c49b89d6092f7d4f134b40/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (1.1.0).; You might want to review and update them manually.; ```; CHANGELOG.md; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/variantcall_batch_region.cwl; cloud-nio/cloud-nio-impl-drs/src/main/scala/cloud/nio/impl/drs/DrsCloudNioFileSystemProvider.scala; cwl/src/test/resources/cwl/lodash.js; ```; </details>; <details>; <summary>Adjust future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" } ]; ```; Or, add this to slow down future updates of this dependency:; ```; dependencyOverrides = [{; pullRequests = { frequency = ""@monthly"" },; dependency = { groupId = ""com.eed3si9n"", artifactId = ""sbt-assembly"" }; }]; ```; </details>. labels: sbt-plugin-update, early-semver-patch, semver-spec-patch, old-version-remains, commit-count:1",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6850:1313,integrat,integrationTestCases,1313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6850,6,"['depend', 'integrat']","['dependency', 'dependencyOverrides', 'integrationTestCases']"
Integrability,"10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar1.wdl),Some(MetadataValue(task doIt1 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar9.wdl),Some(MetadataValue(task doIt9 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.777+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar10.wdl),Some(MetadataValue(task doIt10 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.778+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar2.wdl),Some(MetadataValue(task doIt2 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.779+10:00), ?)),java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/bar8.wdl); java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/bar8.wdl; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); 	at java.nio.file.Files.newByteChannel(Files.java:361); 	at java.nio.file.Files.newByteChannel(Files.java:407); 	at java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:384); 	at java.nio.file.Files.newInputStream(Files.java:152); 	at java.nio.file.Files.newBufferedReader(Files.java:2784); 	at java.nio.file.Files.readAllLines(Files.java:3202); 	at java.nio.file.Files.re",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:3897,message,message,3897,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['message'],['message']
Integrability,"10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar3.wdl),Some(MetadataValue(task doIt3 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar1.wdl),Some(MetadataValue(task doIt1 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar9.wdl),Some(MetadataValue(task doIt9 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.777+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar10.wdl),Some(MetadataValue(task doIt10 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.778+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar2.wdl),Some(MetadataValue(task doIt2 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.779+10:00), ?)),java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/bar8.wdl); java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/bar8.wdl; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107); 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214); 	at java.nio.file.Files.newByteChannel(Files.java:361); 	at java.nio.file.Files.newByteChannel(Files.java:407); 	at java.ni",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:3626,message,message,3626,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['message'],['message']
Integrability,10K copies of the same error message per Cloud NAT run gets old fast.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5589:29,message,message,29,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5589,1,['message'],['message']
Integrability,"11-21 15:09:05,44] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-11-21 15:09:05,51] [info] Unspecified type (Unspecified version) workflow 02306258-436a-4372-ab54-2dcd83c42b47 submitted; [2018-11-21 15:09:05,52] [info] SingleWorkflowRunnerActor: Workflow submitted 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,53] [info] 1 new workflows fetched; [2018-11-21 15:09:05,53] [info] WorkflowManagerActor Starting workflow 02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info] WorkflowManagerActor Successfully started WorkflowActor-02306258-436a-4372-ab54-2dcd83c42b47; [2018-11-21 15:09:05,54] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-11-21 15:09:05,57] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-11-21 15:09:05,58] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-11-21 15:09:06,80] [info] MaterializeWorkflowDescriptorActor [02306258]: Parsing workflow as WDL draft-2; [2018-11-21 15:09:07,34] [info] MaterializeWorkflowDescriptorActor [02306258]: Call-to-Backend assignments: test.hello -> AWSBATCH; [2018-11-21 15:09:08,72] [info] WorkflowExecutionActor-02306258-436a-4372-ab54-2dcd83c42b47 [02306258]: Starting test.hello; [2018-11-21 15:09:10,76] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: echo 'Hello World!' > ""helloWorld.txt""; [2018-11-21 15:09:10,80] [info] Submitting job to AWS Batch; [2018-11-21 15:09:10,80] [info] dockerImage: ubuntu:latest; [2018-11-21 15:09:10,80] [info] jobQueueArn: arn:aws:batch:us-east-1:267795504649:job-queue/GenomicsHighPriorityQue-ae4256f76f07d96; [2018-11-21 15:09:10,80] [info] taskId: test.hello-None-1; [2018-11-21 15:09:10,80] [info] hostpath root: test/hello/02306258-436a-4372-ab54-2dcd83c42b47/None/1; [2018-11-21 15:09:14,56] [info] AwsBatchAsyncBackendJobExecutionActor [02306258test.hello:NA:1]: job id: 77106e8d-c518-4c0d-82e9-3",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421:2447,message,message,2447,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440793421,1,['message'],['message']
Integrability,"14.1.; [GitHub Release Notes](https://github.com/circe/circe/releases/tag/v0.14.1) - [Version Diff](https://github.com/circe/circe/compare/v0.13.0...v0.14.1). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/scala-steward-org/scala-steward/blob/92f43ce5f010fa03daeb2625a3f9c1b1be80cca5/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Files still referring to the old version number</summary>. The following files still refer to the old version number (0.13.0).; You might want to review and update them manually.; ```; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/gvcf-joint-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/prealign-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/rnaseq-workflow/steps/prepare_sample.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/somatic-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/detect_sv.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/process_alignment.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/svcall-workflow/steps/variantcall_batch_region.cwl; centaur/src/main/resources/integrationTestCases/cwl/bcbio/wes_chr21_test-workflow-gcp/steps/detect_sv.cwl; ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6410:1505,integrat,integrationTestCases,1505,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6410,1,['integrat'],['integrationTestCases']
Integrability,"190d6-8640-4638-94cd-15f16b194f38/workflow.logs/workflow.c386672d-0248-4968-9b1a-114f5f5c4706.log"",; ""end"": ""2017-01-30T19:14:20.002Z"",; ""start"": ""2017-01-30T19:00:03.040Z""; }. ```; Here it's an array of ""message""s; ```; {; ""workflowName"": ""aggregate_data_workflow"",; ""submittedFiles"": {... },; ""calls"": {; ""aggregate_data_workflow.aggregate_data"": [{; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""/cromwell-executions/aggregate_data_workflow/3608d6ca-fbb4-4232-b197-268058470bfc/call-aggregate_data/execution/stdout"",; ""shardIndex"": -1,; ""runtimeAttributes"": {; ""docker"": ""broadgdac/aggregate_data:31"",; ""failOnStderr"": false,; ""continueOnReturnCode"": ""0""; },; ""cache"": {; ""allowResultReuse"": true; },; ""Effective call caching mode"": ""CallCachingOff"",; ""inputs"": {...; },; ""returnCode"": -1,; ""failures"": [{; ""message"": ""Call aggregate_data_workflow.aggregate_data: return code was -1""; }, {; ""message"": ""Call aggregate_data_workflow.aggregate_data: return code was -1""; }, {; ""message"": ""Call aggregate_data_workflow.aggregate_data: return code was -1""; }],; ""jobId"": ""2957"",; ""backend"": ""JES"",; ""end"": ""2016-12-02T15:05:42.655Z"",; ""stderr"": ""/cromwell-executions/aggregate_data_workflow/3608d6ca-fbb4-4232-b197-268058470bfc/call-aggregate_data/execution/stderr"",; ""callRoot"": ""/cromwell-executions/aggregate_data_workflow/3608d6ca-fbb4-4232-b197-268058470bfc/call-aggregate_data"",; ""attempt"": 1,; ""executionEvents"": [...]; },; ""outputs"": {. },; ""workflowRoot"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/9ea737cd-a512-4c62-820c-dd1505ea7676/aggregate_data_workflow/3608d6ca-fbb4-4232-b197-268058470bfc"",; ""id"": ""3608d6ca-fbb4-4232-b197-268058470bfc"",; ""inputs"": {...; },; ""submission"": ""2016-12-01T21:21:40.188Z"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Call aggregate_data_workflow.aggregate_data: return code was -1""; }, {; ""message"": ""Call aggregate_data_workflow.aggregate_data: return code was -1""; }, {; ""message"": ""Call aggregate_data_workflow.aggregate_",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:4070,message,message,4070,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,1,['message'],['message']
Integrability,"2); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339); at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979); at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107). [2016-10-28 14:38:11,89] [warn] 1 failures fetching JES statuses: {""domain"":""global"",""message"":""Deadline expired before operation could complete."",""reason"":""backendError""}; [2016-10-28 14:38:11,89] [warn] JesAsyncBackendJobExecutionActor [a3dd8163case_gatk_acnv_workflow.AllelicCNV:7:1]: Caught exception, retrying:; java.io.IOException: Google request failed: {; ""code"" : 504,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Deadline expired before operation could complete."",; ""reason"" : ""backendError""; } ],; ""message"" : ""Deadline expired before operation could complete."",; ""status"" : ""DEADLINE_EXCEEDED""; }; at cromwell.backend.impl.jes.statuspolling.JesPollingActorClient$$anonfun$pollingActorClientReceive$1.applyOrElse(JesPollingActorClient.scala:30); at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:170); at akka.actor.Actor$class.aroundReceive(Actor.scala:484); at cromwell.backend.impl.jes.JesAsyncBackendJobExecutionActor.aroundReceive(JesAsyncBackendJobExecutionActor.scala:82); at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526); at akka.actor.ActorCell.invoke(ActorCell.scala:495); at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257); at akka.dispatch.Mailbox.run(Mailbox.scala:224); at akka.dispatch.Mailbox.exec(Mailbox.scala:234); at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260); at scala.concurrent.forkjoin.ForkJoinPool$",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948:11273,message,message,11273,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1631#issuecomment-256938948,1,['message'],['message']
Integrability,"2+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar4.wdl),Some(MetadataValue(task doIt4 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.774+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar5.wdl),Some(MetadataValue(task doIt5 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.775+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar3.wdl),Some(MetadataValue(task doIt3 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar1.wdl),Some(MetadataValue(task doIt1 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar9.wdl),Some(MetadataValue(task doIt9 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.777+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar10.wdl),Some(MetadataValue(task doIt10 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.778+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar2.wdl),Some(MetadataValue(task doIt2 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.779+10:00), ?)),java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/b",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:3082,message,message,3082,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['message'],['message']
Integrability,"2467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:28,581 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Retrying job execution for PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; 2016-12-08 16:14:28,585 cromwell-system-akka.dispatchers.engine-dispatcher-145 INFO - WorkflowExecutionActor-0545f731-803b-4194-a74e-44cc5c208ce4 [UUID(0545f731)]: Starting calls: PairedEndSingleSampleWorkflow.SamToFastqAndBwaMem:5:2; ```. and this is one that was not pre-emptible(is my guess based on metadata from the workflow, only one task is ""failed"" in that workflow); ```; 2016-12-08 16:14:36,602 cromwell-system-akka.dispatchers.engine-dispatcher-289 ERROR - WorkflowManagerActor Workflow 0545f731-803b-4194-a74e-44cc5c208ce4 failed (during ExecutingWorkflowState): cromwell.core.package$CromwellFatalException: cromwell.core.package$CromwellFatalException: com.google.api.client.googleapis.json.GoogleJsonResponseException: 429 Too Many Requests; {; ""code"" : 429,; ""errors"" : [ {; ""domain"" : ""global"",; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""reason"" : ""rateLimitExceeded""; } ],; ""message"" : ""Insufficient tokens for quota group and limit 'defaultUSER-100s' of service 'staging-genomics.sandbox.googleapis.com', using the limit by ID '628662467800@1088569555438'."",; ""status"" : ""RESOURCE_EXHAUSTED""; }; 2016-12-08 16:14:36,604 cromwell-system-akka.dispatchers.engine-dispatcher-89 INFO - WorkflowManagerActor WorkflowActor-0545f731-803b-4194-a74e-44cc5c208ce4 is in a terminal state: WorkflowFailedState; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490:2098,message,message,2098,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1763#issuecomment-271640490,2,['message'],['message']
Integrability,2667. ```; org.scalatest.exceptions.TestFailedDueToTimeoutException: The code passed to eventually never returned normally. Attempted 210 times over 3.3447279390999998 minutes. Last failure message: Submitted did not equal Failed.; at org.scalatest.concurrent.Eventually.tryTryAgain$1(Eventually.scala:432); at org.scalatest.concurrent.Eventually.eventually(Eventually.scala:439); at org.scalatest.concurrent.Eventually.eventually$(Eventually.scala:391); at cromwell.CromwellTestKitSpec.eventually(CromwellTestKitSpec.scala:251); at cromwell.CromwellTestKitSpec.runWdl(CromwellTestKitSpec.scala:323); at cromwell.WorkflowFailSlowSpec.$anonfun$new$2(WorkflowFailSlowSpec.scala:18); at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85); at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83); at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104); at org.scalatest.Transformer.apply(Transformer.scala:22); at org.scalatest.Transformer.apply(Transformer.scala:20); at org.scalatest.WordSpecLike$$anon$1.apply(WordSpecLike.scala:1078); at org.scalatest.TestSuite.withFixture(TestSuite.scala:196); at org.scalatest.TestSuite.withFixture$(TestSuite.scala:195); at cromwell.CromwellTestKitWordSpec.withFixture(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.invokeWithFixture$1(WordSpecLike.scala:1076); at org.scalatest.WordSpecLike.$anonfun$runTest$1(WordSpecLike.scala:1088); at org.scalatest.SuperEngine.runTestImpl(Engine.scala:289); at org.scalatest.WordSpecLike.runTest(WordSpecLike.scala:1088); at org.scalatest.WordSpecLike.runTest$(WordSpecLike.scala:1070); at cromwell.CromwellTestKitWordSpec.runTest(CromwellTestKitSpec.scala:250); at org.scalatest.WordSpecLike.$anonfun$runTests$1(WordSpecLike.scala:1147); at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:396); at scala.collection.immutable.List.foreach(List.scala:389); at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:384); at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593:190,message,message,190,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4521#issuecomment-453539593,1,['message'],['message']
Integrability,"2] [info] WorkflowStoreActor stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-08-27 02:04:26,93] [info] JobExecutionTokenDispenser stopped; [2018-08-27 02:04:26,93] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-08-27 02:04:26,93] [info] WorkflowLogCopyRouter stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor stopped; [2018-08-27 02:04:26,94] [info] WorkflowManagerActor All workflows finished; [2018-08-27 02:04:26,94] [info] Connection pools shut down; [2018-08-27 02:04:26,94] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,95] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-08-27 02:04:26,96] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] ServiceRegistryActor stopped; [2018-08-27 02:04:26,96] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-08-27 02:04:26,96] [info] SubWorkflowStoreActor stopped; [2018-08-27 02:04:26,96] [info] DockerHashActor stopped; [2018-08-27 02:04:26,97] [info] IoProxy stopped; [2018-08-27 02:04:26,97] [info] JobStoreActor stopped; [2018-08-27 02:04:26,97] [info] CallCacheWriteActor stopped; [2018-08-27 02:04:27,00] [info] Database closed; [2018-08-27 02:04:27,00] [info] Stream materializer shut down; [2018-08-27 02:04:27,06] [info] Automatic shutdown of the async connection; [2",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:7137,message,messages,7137,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,1,['message'],['messages']
Integrability,2]; at java.net.SocketInputStream.read(SocketInputStream.java:170) ~[na:1.8.0_72]; at java.net.SocketInputStream.read(SocketInputStream.java:141) ~[na:1.8.0_72]; at sun.security.ssl.InputRecord.readFully(InputRecord.java:465) ~[na:1.8.0_72]; at sun.security.ssl.InputRecord.read(InputRecord.java:503) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) ~[na:1.8.0_72]; at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) ~[na:1.8.0_72]; at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_72]; at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_72]; at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_72]; at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704) ~[na:1.8.0_72]; at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647) ~[na:1.8.0_72]; at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536) ~[na:1.8.0_72]; at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441) ~[na:1.8.0_72]; at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37) ~[cromwell.jar:0.19]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.media.MediaHttpUploader.executeCurrentRequestWithoutGZip(MediaHttpUploader.java:545) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.media.MediaHttpUploader.executeCurrentRequest(MediaHttpUploader.java:562) ~[cromwell.jar:0.19]; at com.google,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/932:3115,protocol,protocol,3115,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/932,1,['protocol'],['protocol']
Integrability,"2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1#-1192719839] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2018-05-02 15:22:54,89] [info] Message [cromwell.backend.standard.callcaching.StandardFileHashingActor$FileHashResponse] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall:-1:1/CCHashingJobActor-bc4644da-batch_for_variantcall:NA:1/FileHashingActor_for_batch_for_variantcall:NA:1#-540594129] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-bc4644da-87f9-4765-9791-9011a2fae80f/WorkflowExecutionActor-bc4644da-87f9-4765-9791-9011a2fae80f/bc4644da-87f9-4765-9791-9011a2fae80f-EngineJobExecutionActor-batch_for_variantcall:NA:1/ejha_for_bc4644da-87f9-4765-9791-9011a2fae80f:BackendJobDescriptorKey_CommandCallNode_batch_for_variantcall",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3584:9858,Message,Message,9858,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3584,1,['Message'],['Message']
Integrability,36. I also edited the original message,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-466543684:31,message,message,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-466543684,1,['message'],['message']
Integrability,"3:40:18 UTC] Obtaining toolchain_env file from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain_env. real	0m0.126s; user	0m0.014s; sys	0m0.001s; [INFO 2020-08-04 23:40:18 UTC] Downloading toolchain from https://storage.googleapis.com/cos-tools/12871.1174.0/toolchain.tar.xz. real	0m11.907s; user	0m0.428s; sys	0m1.039s; [INFO 2020-08-04 23:41:17 UTC] Configuring environment variables for cross-compilation; [INFO 2020-08-04 23:41:17 UTC] Configuring installation directories; [INFO 2020-08-04 23:41:17 UTC] Updating container's ld cache; [INFO 2020-08-04 23:41:20 UTC] Configuring kernel sources; [INFO 2020-08-04 23:41:42 UTC] Modifying kernel version magic string in source files; [INFO 2020-08-04 23:41:42 UTC] Running Nvidia installer. ERROR: The kernel module failed to load, because it was not signed by a key; that is trusted by the kernel. Please try installing the driver; again, and set the --module-signing-secret-key and; --module-signing-public-key options on the command line, or run the; installer in expert mode to enable the interactive module signing; prompts. ERROR: Unable to load the kernel module 'nvidia.ko'. This happens most; frequently when this kernel module was built against the wrong or; improperly configured kernel sources, with a version of gcc that; differs from the one used to build the target kernel, or if another; driver, such as nouveau, is present and prevents the NVIDIA kernel; module from obtaining ownership of the NVIDIA GPU(s), or no NVIDIA; GPU installed in this system is supported by this NVIDIA Linux; graphics driver release. Please see the log entries 'Kernel module load error' and 'Kernel; messages' at the end of the file; '/usr/local/nvidia/nvidia-installer.log' for more information. ERROR: Installation has failed. Please see the file; '/usr/local/nvidia/nvidia-installer.log' for details. You may find; suggestions on fixing installation problems in the README available; on the Linux driver download page at www.nvidia.com.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5714:5941,message,messages,5941,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5714,1,['message'],['messages']
Integrability,"3d1cb48973da7f646a7de2 > /sandbox/users/foucal-a/test_atac-pipe/cromwell-executions/atac/f4fd93fa-6f3a-42a6-94f2-459901d245c4/call-trim_adapter/shard-0/execution/glob-4f26c666d13d1cb48973da7f646a7de2.list; ```; I have the error when the script tries to symlink all the files into the glob directory.; Here is the WDL code : ; ```; scatter( i in range(length(fastqs_)) ) {; # trim adapters and merge trimmed fastqs; call trim_adapter { input :; fastqs = fastqs_[i],; adapters = if length(adapters_)>0 then adapters_[i] else [],; paired_end = paired_end,; }; # align trimmed/merged fastqs with bowtie2s; call bowtie2 { input :; idx_tar = bowtie2_idx_tar,; fastqs = trim_adapter.trimmed_merged_fastqs, #[R1,R2]; paired_end = paired_end,; multimapping = multimapping,; }; }; ```; With the function :; ```; task trim_adapter { # trim adapters and merge trimmed fastqs; # parameters from workflow; Array[Array[File]] fastqs # [merge_id][read_end_id]; Array[Array[String]] adapters # [merge_id][read_end_id]; Boolean paired_end; # mandatory; Boolean? auto_detect_adapter # automatically detect/trim adapters; # optional; Int? min_trim_len # minimum trim length for cutadapt -m; Float? err_rate # Maximum allowed adapter error rate; # for cutadapt -e; # resource; Int? cpu; Int? mem_mb; Int? time_hr; #Commenting this line as a test. PRoblem with hard link; String? disks. command {; python $(which encode_trim_adapter.py) \; ${write_tsv(fastqs)} \; --adapters ${write_tsv(adapters)} \; ${if paired_end then ""--paired-end"" else """"} \; ${if select_first([auto_detect_adapter,false]) then ""--auto-detect-adapter"" else """"} \; ${""--min-trim-len "" + select_first([min_trim_len,5])} \; ${""--err-rate "" + select_first([err_rate,'0.1'])} \; ${""--nth "" + select_first([cpu,2])}; }; output {; # WDL glob() globs in an alphabetical order; # so R1 and R2 can be switched, which results in an; # unexpected behavior of a workflow; # so we prepend merge_fastqs_'end'_ (R1 or R2); # to the basename of original filename; # t",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3876:2499,adapter,adapters,2499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3876,4,['adapter'],"['adapter', 'adapters']"
Integrability,"4+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar5.wdl),Some(MetadataValue(task doIt5 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.775+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar3.wdl),Some(MetadataValue(task doIt3 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar1.wdl),Some(MetadataValue(task doIt1 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.776+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar9.wdl),Some(MetadataValue(task doIt9 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.777+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar10.wdl),Some(MetadataValue(task doIt10 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.778+10:00), MetadataEvent(MetadataKey(84a51727-cfda-41e7-a03c-9e3af35eb0dc,None,submittedFiles:imports:bar2.wdl),Some(MetadataValue(task doIt2 {; 	command { echo ""Help, world!"" }; 	output { String message = read_string(stdout()) }; },MetadataString)),2017-02-07T15:01:10.779+10:00), ?)),java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/bar8.wdl); java.nio.file.NoSuchFileException: /tmp/7849235605615896249.zip1398073512390398444/foo/bar8.wdl; 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86); 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102); 	at sun.nio.f",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1959:3353,message,message,3353,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1959,1,['message'],['message']
Integrability,"4-5c2b3bd209a9;shutdown=false;hsqldb.tx=mvcc; [2016-01-31 16:37:28,247] [info] WorkflowManagerActor submitWorkflow input id = None, effective id = 2a89a995-aa89-4172-a5e1-1054cbccd9e0; [2016-01-31 16:37:28,291] [info] WorkflowManagerActor Found no workflows to restart.; [2016-01-31 16:37:28,660] [info] WorkflowActor [2a89a995]: Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#-896492658])) message received; [2016-01-31 16:37:28,788] [info] WorkflowActor [2a89a995]: ExecutionStoreCreated(Start(Some(Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor#-896492658]))) message received; [2016-01-31 16:37:28,789] [info] SingleWorkflowRunnerActor: workflow ID 2a89a995-aa89-4172-a5e1-1054cbccd9e0; [2016-01-31 16:37:28,798] [info] WorkflowActor [2a89a995]: Beginning transition from Submitted to Running.; [2016-01-31 16:37:28,800] [warn] SingleWorkflowRunnerActor: received unexpected message: CurrentState(Actor[akka://cromwell-system/user/WorkflowManagerActor/WorkflowActor-2a89a995-aa89-4172-a5e1-1054cbccd9e0#2034772397],Submitted); [2016-01-31 16:37:28,800] [info] WorkflowActor [2a89a995]: transitioning from Submitted to Running.; [2016-01-31 16:37:28,801] [info] SingleWorkflowRunnerActor: transitioning to Running; [2016-01-31 16:37:28,804] [info] WorkflowActor [2a89a995]: starting calls: w.hello; [2016-01-31 16:37:28,805] [info] WorkflowActor [2a89a995]: persisting status of hello to Starting.; [2016-01-31 16:37:28,959] [info] WorkflowActor [2a89a995]: inputs for call 'hello':; addressee -> WdlString(String); [2016-01-31 16:37:28,962] [info] WorkflowActor [2a89a995]: created call actor for hello.; [2016-01-31 16:37:28,970] [info] WorkflowActor [2a89a995]: persisting status of hello to Running.; [2016-01-31 16:37:28,996] [info] LocalBackend [2a89a995:hello]: echo ""Hello String!"" && kill -SIGINT $BASHPID; [2016-01-31 16:37:29,19] [info] LocalBackend [2a89a995:hello]: command: ""/bin/bash"" ""-c"" ""cat cromwell-executions/w/2a89a995-aa89-4172-a5e1-1054c",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887:2346,message,message,2346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/404#issuecomment-177622887,1,['message'],['message']
Integrability,"4/compare/r4.13...r4.13.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=junit:junit&package-manager=maven&previous-version=4.13&new-version=4.13.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5941:4483,Depend,Dependabot,4483,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5941,9,"['Depend', 'depend']","['Dependabot', 'dependabot']"
Integrability,42b47 | callCaching:hashes:runtime attribute:docker | test.hello | NULL | 1 | 66E19F14150E71B0E42CA8557A69C5F9 | 2018-11-21 15:09:37.710000 | string |; | 4775 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hashes:runtime attribute:failOnStderr | test.hello | NULL | 1 | 68934A3E9455FA72420237EB05902327 | 2018-11-21 15:09:37.710000 | string |; | 4735 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | true | 2018-11-21 15:09:09.839000 | boolean |; | 4742 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hit | test.hello | NULL | 1 | false | 2018-11-21 15:09:10.555000 | boolean |; | 4741 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:causedBy[] | test.hello | NULL | 1 | NULL | 2018-11-21 15:09:10.486000 | NULL |; | 4740 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:causedBy[0]:message | test.hello | NULL | 1 | The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.486000 | string |; | 4739 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:hitFailures[0]:2f58eee9-1b0f-4436-a4ad-48eb305655e9\:test.hello\:-1[2043552529]:message | test.hello | NULL | 1 | [Attempted 1 time(s)] - NoSuchKeyException: The specified key does not exist. (Service: S3Client; Status Code: 404; Request ID: 677F4FE44C747A7E) | 2018-11-21 15:09:10.485000 | string |; | 4736 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Hit: 2f58eee9-1b0f-4436-a4ad-48eb305655e9:test.hello:-1 | 2018-11-21 15:09:09.839000 | string |; | 4743 | 02306258-436a-4372-ab54-2dcd83c42b47 | callCaching:result | test.hello | NULL | 1 | Cache Miss | 2018-11-21 15:09:10.555000 | string |; | 4759 | 02306258-436a-4372-ab54-2dcd83c42b47 | callRoot | test.hello | NULL | 1 | s3://s4-somaticgenomicsrd-val,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029:1082,message,message,1082,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4412#issuecomment-440701029,1,['message'],['message']
Integrability,45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResultWith$2(BasicDirectives.scala:72); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.ExecutionDirectives.$anonfun$handleExceptions$2(ExecutionDirectives.scala:32); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.Route$.$anonfun$asyncHandler$1(Route.scala:86); 	at akka.stream.impl.fusing.MapAsyncUnordered$$anon$26.onPush(Ops.scala:1303); 	at akka.stream.impl.fusing.GraphInterpreter.processPush(GraphInterpreter.scala:519); 	at akka.stream.impl.fusing.GraphInterpreter.execute(GraphInterpreter.scala:411); 	at akka.stream.impl.fusing.GraphInterpreterShell.runBatch(ActorGraphInterpreter.scala:588); 	at akka.stream.impl.fusing.GraphInterpreterShell$AsyncInput.execute(ActorGraphInterpreter.scala:472); 	at ak,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:3530,Rout,RouteConcatenation,3530,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,1,['Rout'],['RouteConcatenation']
Integrability,"4842beb\"",\""os\"":\""linux\"",\""parent\"":\""633c756fc95cb19efd13b84a949066c65f5b6683d0922d1980b6a19deb855fa0\"",\""throwaway\"":true}""; },; {; ""v1Compatibility"": ""{\""id\"":\""633c756fc95cb19efd13b84a949066c65f5b6683d0922d1980b6a19deb855fa0\"",\""created\"":\""2017-08-07T23:50:27.303876981Z\"",\""container_config\"":{\""Cmd\"":[\""/bin/sh -c #(nop) ADD file:fb17197475bb59bfb365c41f28d4bc15134b8dcb8907819e7be54bce53328c03 in / \""]}}""; }; ],; ""schemaVersion"": 1,; ""signatures"": [; {; ""header"": {; ""jwk"": {; ""crv"": ""P-256"",; ""kid"": ""4DPT:XWGC:ESR7:JVJY:2RON:CMML:IXIT:QXQ5:LGLL:LPJF:PWDL:AJSO"",; ""kty"": ""EC"",; ""x"": ""BjSTZs2e-5wP1bu4deBughI6YALM3vbLbZL-CGBRcmM"",; ""y"": ""Qj5Fr4Z1BQRe4EXMe-75dOkvIDzP-0u5cks8my7hkCA""; },; ""alg"": ""ES256""; },; ""signature"": ""ewZ_2lh2-uWSpw5tcprbhoFvjLoxGqsI06YSlvq4w2eXB5EEpMsk5Jo6WYRBeYeJxv0vnoe7SbN_1qarBAU9uQ"",; ""protected"": ""eyJmb3JtYXRMZW5ndGgiOjIxOTUsImZvcm1hdFRhaWwiOiJmUSIsInRpbWUiOiIyMDE3LTExLTA2VDE2OjE2OjE4WiJ9""; }; ]; }; ```. ```bash; $ curl -i -s -H 'Accept: application/vnd.docker.distribution.manifest.v2+json' https://gcr.io/v2/google-containers/ubuntu-slim/manifests/0.14; HTTP/1.1 200 OK; Docker-Distribution-API-Version: registry/2.0; Content-Type: application/vnd.docker.distribution.manifest.v2+json; Content-Length: 529; Docker-Content-Digest: sha256:1d5c0118358fc7651388805e404fe491a80f489bf0e7c5f8ae4156250d6ec7d8; Date: Mon, 06 Nov 2017 16:16:59 GMT; Server: Docker Registry; X-XSS-Protection: 1; mode=block; X-Frame-Options: SAMEORIGIN; Alt-Svc: quic="":443""; ma=2592000; v=""41,39,38,37,35"". {; ""schemaVersion"": 2,; ""mediaType"": ""application/vnd.docker.distribution.manifest.v2+json"",; ""config"": {; ""mediaType"": ""application/vnd.docker.container.image.v1+json"",; ""size"": 1528,; ""digest"": ""sha256:fba1281b32ffd9048881f99d8de0218c71552c4c91f09844b4b189b16e51cdca""; },; ""layers"": [; {; ""mediaType"": ""application/vnd.docker.image.rootfs.diff.tar.gzip"",; ""size"": 18278657,; ""digest"": ""sha256:1c4816548d6a2a08f89c304bf09503e791a338c4be90629610152124c7285d3f""; }; ]; }; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2826:5238,mediaT,mediaType,5238,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2826,3,['mediaT'],['mediaType']
Integrability,"5) ; at cromwell.services.ServiceRegistryActor.serviceProps(ServiceRegistryActor.scala:63) ; at cromwell.services.ServiceRegistryActor.<init>(ServiceRegistryActor.scala:65) ; at cromwell.services.ServiceRegistryActor$.$anonfun$props$1(ServiceRegistryActor.scala:25) ; at akka.actor.TypedCreatorFunctionConsumer.produce(IndirectActorProducer.scala:87) ; at akka.actor.Props.newActor(Props.scala:212) ; at akka.actor.ActorCell.newActor(ActorCell.scala:624) ; at akka.actor.ActorCell.create(ActorCell.scala:650) ; ... 9 more ; ```. If I add in a `services` stanza, though, it asks me to define the class of each service, even though they should probably have default values:; ```; [ERROR] [01/24/2019 11:09:59.741] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'LoadController' for which no service is configured. Message: LoadMetric(NonEmptyList(CallCacheWriteActor),NormalLoad) ; [ERROR] [01/24/2019 11:09:59.731] [cromwell-system-akka.dispatchers.service-dispatcher-10] [akka://cromwell-system/user/SingleWorkflowRunnerActor/ServiceRegistryActor] Received ServiceRegistryMessage requ; esting service 'Instrumentation' for which no service is configured. Message: InstrumentationServiceMessage(CromwellGauge(CromwellBucket(List(job),NonEmptyList(callcaching, read, $y, queue)),0)); ```. ***. Here's my config file for Cromwell 36 (that works):; ```; backend {; default = spartan. providers {; spartan {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; runtime-attributes = """"""; Int runtime_minutes = 600; Int cpus = 2; Int requested_memory_mb_per_core = 8000; String? docker; """""". submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} \; ${""-c "" + cpus} \; --mem-per-cpu=${requested_memory_mb_per_core} \; --wrap ""/bin/bash ${script}""; """""". submit-docker = """"""; module load Sin",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4577:2970,Message,Message,2970,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4577,1,['Message'],['Message']
Integrability,"5346 . @cmarkello, @illusional, I am sorry that I insisted that `path+modtime` did work. I was using less complex workflows that did not have this problem at the time. ## Call-caching problems with file strategy; The `file` strategy does work as it uses md5sums in order to calculate the file hash. An unfortunate side effect of this is that md5 uses massive system resources. On HPC systems that are the target for the sfs-backend, this is a big problem. Cromwell will be run from a submit node on the system and greedily grab all processing power on the submit node to calculate all the md5sums. . ## Md5sums; Md5sums are reliable hashes for file integrity, but this was not their intended purpose. Md5sum was intended as a cryptographic hash. A cryptographic hash has the following properties (wikipedia):; 1. it is deterministic, meaning that the same message always results in the same hash; 2. it is quick to compute the hash value for any given message; 3. it is infeasible to generate a message that yields a given hash value; 4. it is infeasible to find two different messages with the same hash value; 5. a small change to a message should change the hash value so extensively that the new hash value appears uncorrelated with the old hash value (avalanche effect). I contest point 2, in that many cryptographic explicitly strife for being slow to calculate in order to negate brute force attempts.; Anyway: for call caching we only need points 1. and 4. All the rest is unnecessary ballast. . ## xxHash; Luckily there is a hashing algorithm that is designed explicitly for content hashing only. It was made to generate reliably different hashes for file content as fast as possible. It's called [xxHash](https://www.xxhash.com). There are Java implementations available and I did [some extensive benchmarking](https://github.com/rhpvorderman/hashtest/) to find out which one was best. The xxh64 (xxhash for 64 bit machines) algorithm was 15 times faster than the java implementation of md5",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5450:1289,message,message,1289,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5450,1,['message'],['message']
Integrability,"5:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: executing: /bin/bash /home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-2/execution/script; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: command: ""/bin/bash"" ""/home/conradL/cromwell-executions/testMe/d6475258-0f55-449c-be0b-e08e1e0c5049/call-printPairStringString/shard-2/execution/script.submit""; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:1:1]: job id: 26767; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:2:1]: job id: 26770; [2016-11-24 15:22:45,23] [info] SharedFileSystemAsyncJobExecutionActor [d6475258testMe.printPairStringString:0:1]: job id: 26763; [2016-11-24 15:22:46,77] [info] WorkflowExecutionActor-d6475258-0f55-449c-be0b-e08e1e0c5049 [d6475258]: Starting calls: Collector-printPairStringString; [2016-11-24 15:22:46,80] [info] Message [cromwell.subworkflowstore.SubWorkflowStoreActor$SubWorkflowStoreCompleteSuccess] from Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/$b#-2119125994] to Actor[akka://cromwell-system/user/SingleWorkflowRunnerActor/WorkflowManagerActor/WorkflowActor-d6475258-0f55-449c-be0b-e08e1e0c5049#337013427] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2016-11-24 15:22:46,81] [error] WorkflowManagerActor Workflow d6475258-0f55-449c-be0b-e08e1e0c5049 failed (during ExecutingWorkflowState): WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); scala.MatchError: WdlPair(WdlString(foo1),WdlString(bar1)) (of class wdl4s.values.WdlPair); 	at cromwell.util.JsonFormatting.WdlValueJsonFormatter$WdlValueJsonFormat$.write(W",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1703:4103,Message,Message,4103,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1703,1,['Message'],['Message']
Integrability,6); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResultWith$2(BasicDirectives.scala:72); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.ExecutionDirectives.$anonfun$handleExceptions$2(ExecutionDirectives.scala:32); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); ,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:2983,Rout,RouteConcatenation,2983,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,2,['Rout'],"['RouteConcatenation', 'RouteWithConcatenation']"
Integrability,"6-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files/echo_files-stderr.log"",; ""callRoot"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files"",; ""attempt"": 1,; ""executionEvents"": [...],; ""backendLogs"": {; ""log"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files/echo_files.log""; },; ""start"": ""2017-01-30T19:00:03.896Z""; }]; },; ""outputs"": {. },; ""workflowRoot"": ""/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/"",; ""id"": ""c386672d-0248-4968-9b1a-114f5f5c4706"",; ""inputs"": {...; },; ""submission"": ""2017-01-30T19:00:00.796Z"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }],; ""workflowLog"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/workflow.logs/workflow.c386672d-0248-4968-9b1a-114f5f5c4706.log"",; ""end"": ""2017-01-30T19:14:20.002Z"",; ""start"": ""2017-01-30T19:00:03.040Z""; }. ```; Here it's an array of ""message""s; ```; {; ""workflowName"": ""aggregate_data_workflow"",; ""submittedFiles"": {... },; ""calls"": {; ""aggregate_data_workflow.aggregate_data"": [{; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""/cromwell-executions/aggregate_data_workflow/3608d6ca-fbb4-4232-b197-268058470bfc/call-aggregate_data/execution/stdout"",; ""shardIndex"": -1,; ""runtimeAttributes"": {; ""docker"": ""broadgdac/a",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:2641,Message,Message,2641,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,1,['Message'],['Message']
Integrability,"613). Admittedly, I have never written a parser before, so I don't know how feasible this is, but can you write the grammar/parser such that everything on a line after a `#` character is ignored?. The only edge cases I imagine are when the `#` character is in a quoted string. ---. @scottfrazer commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200505343). @tmdefreitas Yes, that is definitely possible. However, we try to not make assumptions about the type of characters that your script can have in it. I'm perhaps being a little overly cautious, but I'd hate for there to be a case where somebody wants to use a `#` in their command but it gets interpreted as a comment. That could lead to the same kind of confusion that we're seeing now. I vacillate on this because I also see the pragmatism in implementing your suggestion for the common case. In most cases I can think of, a `#` is a comment. Maybe some approach like Eddie's where I can have the parser give a better error message is the best solution. ---. @eddiebroad commented on [Wed Mar 23 2016](https://github.com/broadinstitute/wdltool/issues/7#issuecomment-200508578). Scott,. Not only can I imagine a command where # is used (and in a WDL no less),; but I have actually written such a command. In the merge step for mutect, I use ""#"" to select for header lines in; merging call_stats files and VCF files!. command <<<; #increase verbosity; set -x. #mutect1 call_stats merging; MUTECT1_CS=""MuTect1.call_stats.txt""; head --lines=2 ${mutect1_cs[0]} > $MUTECT1_CS; cat ${sep =' ' mutect1_cs} | grep -Pv '#'|grep -Pv '^contig' >> $MUTECT1_CS. #mutect2 call_stats merging; MUTECT2_CS=""MuTect2.call_stats.txt""; cat ${mutect2_cs[0]} |grep -P '^#' > $MUTECT2_CS ;; cat ${sep=' ' mutect2_cs} |grep -Pv '^#' >> $MUTECT2_CS ;; -eddie. On Wed, Mar 23, 2016 at 3:25 PM, Scott Frazer notifications@github.com; wrote:. > @tmdefreitas https://github.com/tmdefreitas Yes, that is definitely; > possible.; >",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2870:3012,message,message,3012,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2870,1,['message'],['message']
Integrability,66); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRouteResult$2(BasicDirectives.scala:66); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.util.FastFuture$.strictTransform$1(FastFuture.scala:41); 	at akka.http.scaladsl.util.FastFuture$.transformWith$extension1(FastFuture.scala:45); 	at akka.http.scaladsl.util.FastFuture$.flatMap$extension(FastFuture.scala:26); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$1(RouteConcatenation.scala:44); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$mapRequestContext$2(BasicDirectives.scala:43); 	at akka.http.scaladsl.server.directives.BasicDirectives.$anonfun$textract$2(BasicDirectives.scala:159); 	at akka.http.scaladsl.server.RouteConcatenation$RouteWithConcatenation.$anonfun$$tilde$2(RouteConcatenation.scala:47); 	at akka.http.scaladsl.uti,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187:2098,Rout,RouteConcatenation,2098,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4676#issuecomment-467133187,1,['Rout'],['RouteConcatenation']
Integrability,"7 02:04:06,81] [info] WriteMetadataActor configured to flush with batch size 200 and process rate 5 seconds.; [2018-08-27 02:04:06,91] [info] CallCacheWriteActor configured to flush with batch size 100 and process rate 3 seconds.; [2018-08-27 02:04:07,85] [info] JobExecutionTokenDispenser - Distribution rate: 50 per 1 seconds.; [2018-08-27 02:04:07,88] [info] SingleWorkflowRunnerActor: Version 34; [2018-08-27 02:04:07,90] [info] SingleWorkflowRunnerActor: Submitting workflow; [2018-08-27 02:04:07,91] [info] PAPIQueryManager Running with 3 workers; [2018-08-27 02:04:07,91] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,92] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,93] [info] JES batch polling interval is 33333 milliseconds; [2018-08-27 02:04:07,98] [info] Unspecified type (Unspecified version) workflow 967af8b6-0d68-44c4-b04e-204674333468 submitted; [2018-08-27 02:04:08,05] [info] SingleWorkflowRunnerActor: Workflow submitted 967af8b6-0d68-44c4-b04e-204674333468; [2018-08-27 02:04:08,05] [info] 1 new workflows fetched; [2018-08-27 02:04:08,05] [info] WorkflowManagerActor Starting workflow 967af8b6-0d68-44c4-b04e-204674333468; [2018-08-27 02:04:08,06] [warn] SingleWorkflowRunnerActor: received unexpected message: Done in state RunningSwraData; [2018-08-27 02:04:08,07] [info] WorkflowManagerActor Successfully started WorkflowActor-967af8b6-0d68-44c4-b04e-204674333468; [2018-08-27 02:04:08,07] [info] Retrieved 1 workflows from the WorkflowStoreActor; [2018-08-27 02:04:08,09] [info] WorkflowStoreHeartbeatWriteActor configured to flush with batch size 10000 and process rate 2 minutes.; [2018-08-27 02:04:08,17] [info] MaterializeWorkflowDescriptorActor [967af8b6]: Parsing workflow as WDL draft-2; [2018-08-27 02:04:08,86] [info] MaterializeWorkflowDescriptorActor [967af8b6]: Call-to-Backend assignments: wgbs.flatten_ -> singularity; [2018-08-27 02:04:12,30] [info] WorkflowExecutionActor-967af8b6-0d68-44c4-b0",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039:3544,message,message,3544,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039,1,['message'],['message']
Integrability,"7be934944""><code>510e906</code></a> Add sub headlines to class Javadoc</li>; <li><a href=""https://github.com/junit-team/junit4/commit/610155b8c22138329f0723eec22521627dbc52ae""><code>610155b</code></a> Merge pull request from GHSA-269g-pwp5-87pp</li>; <li><a href=""https://github.com/junit-team/junit4/commit/b6cfd1e3d736cc2106242a8be799615b472c7fec""><code>b6cfd1e</code></a> Explicitly wrap float parameter for consistency (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1671"">#1671</a>)</li>; <li><a href=""https://github.com/junit-team/junit4/commit/a5d205c7956dbed302b3bb5ecde5ba4299f0b646""><code>a5d205c</code></a> Fix GitHub link in FAQ (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1672"">#1672</a>)</li>; <li><a href=""https://github.com/junit-team/junit4/commit/3a5c6b4d08f408c8ca6a8e0bae71a9bc5a8f97e8""><code>3a5c6b4</code></a> Deprecated since jdk9 replacing constructor instance of Double and Float (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1660"">#1660</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/junit-team/junit4/compare/r4.13...r4.13.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=junit:junit&package-manager=maven&previous-version=4.13&new-version=4.13.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overw",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5941:3620,depend,dependabot,3620,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5941,1,['depend'],['dependabot']
Integrability,"7f71b554), I am recommending we just delete the test instead of spending any more time on this. ```; > gcloud beta lifesciences operations describe projects/1005074806481/locations/us-central1/operations/8650136336352694244 --format=json; {; ""done"": true,; ""error"": {; ""code"": 9,; ""message"": ""Execution failed: generic::failed_precondition: while running \""-c /bin/bash /cromwell_root/gcs_localization.sh\"": unexpected exit status 1 was not ignored""; },; ""metadata"": {; ""@type"": ""type.googleapis.com/google.cloud.lifesciences.v2beta.Metadata"",; ""createTime"": ""2023-12-04T20:36:45.056562Z"",; ""endTime"": ""2023-12-04T21:10:43.697318162Z"" # <- WTF!!; }; [...]; ```. ```; Long duration; Warning: arning] Using a password on the command line interface can be insecure.; +--------------------------------------+-----------------+----------------------------+----------------------------+; | name | RUNTIME_MINUTES | start | end |; +--------------------------------------+-----------------+----------------------------+----------------------------+; | localize_file_larger_than_disk_space | 35 | 2023-12-05 01:01:27.836000 | 2023-12-05 01:37:10.789000 |; | lots_of_inputs | 32 | 2023-12-05 01:02:03.292000 | 2023-12-05 01:34:26.490000 |; | draft3_call_cache_capoeira | 27 | 2023-12-05 01:03:01.338000 | 2023-12-05 01:30:34.171000 |; ```. ```; Late finishers; Warning: arning] Using a password on the command line interface can be insecure.; +------------------------------------------+-----------------+----------------------------+----------------------------+; | name | runtime_minutes | start | END |; +------------------------------------------+-----------------+----------------------------+----------------------------+; | restart_while_failing | 16 | 2023-12-05 01:33:04.179000 | 2023-12-05 01:49:36.795000 |; | localize_file_larger_than_disk_space | 35 | 2023-12-05 01:01:27.836000 | 2023-12-05 01:37:10.789000 |; | lots_of_inputs | 32 | 2023-12-05 01:02:03.292000 | 2023-12-05 01:34:26.490000 |; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7330:2123,interface,interface,2123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7330,1,['interface'],['interface']
Integrability,8.0_72]; 905154- at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) ~[na:1.8.0_72]; 905155- at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) ~[na:1.8.0_72]; 905156- at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_72]; 905157- at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_72]; 905158- at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_72]; 905159- at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704) ~[na:1.8.0_72]; 905160- at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647) ~[na:1.8.0_72]; 905161- at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536) ~[na:1.8.0_72]; 905162- at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441) ~[na:1.8.0_72]; 905163- at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) ~[na:1.8.0_72]; 905164- at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338) ~[na:1.8.0_72]; 905165- at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37) ~[cromwell.jar:0.19]; 905166- at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94) ~[cromwell.jar:0.19]; 905167- at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; 905168- at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:419) ~[cromwell.jar:0.19]; 905169- at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:352) ~[cromwell.jar:0.19]; 905170- at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:469) ~[cromwell.jar:0.19]; 905171- at cromwell.engine.backend.io.filesystem.gcs.GcsFileSystemProvider$$anonfun$crc32cHash$1.apply(Gcs,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102:1673,protocol,protocol,1673,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/738#issuecomment-214521102,1,['protocol'],['protocol']
Integrability,"9); at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:109); at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:597); at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:467); at akka.actor.LightArrayRevolverScheduler$$anon$8.executeBucket$1(Scheduler.scala:419); at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:423); at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375); at java.lang.Thread.run(Thread.java:745); [2015-12-18 08:43:19,174] [info] Message [cromwell.engine.workflow.WorkflowManagerActor$RestartWorkflows] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] was not delivered. [1] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,180] [info] Message [akka.actor.Status$Failure] from Actor[akka://cromwell-system/user/WorkflowManagerActor#-1616857312] to Actor[akka://cromwell-system/deadLetters] was not delivered. [2] dead letters encountered. This logging can be turned off or adjusted with configuration settings 'akka.log-dead-letters' and 'akka.log-dead-letters-during-shutdown'.; [2015-12-18 08:43:19,182] [error] WorkflowManagerActor: Workflow failed submission: cannot create children while terminating or terminated; java.lang.IllegalStateException: cannot create children while terminating or terminated; at akka.actor.dungeon.Children$class.makeChild(Children.scala:199); at akka.actor.dungeon.Children$class.actorOf(Children.scala:37); at akka.actor.ActorCell.actorOf(ActorCell.scala:369); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$11.apply(WorkflowManagerActor.scala:246); at cromwell.engine.workflow.WorkflowManagerActor$$anonfun$11.apply(WorkflowManagerActor.scala:245); at scala.util.Success$$anonfun",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/334:2525,Message,Message,2525,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/334,1,['Message'],['Message']
Integrability,"95-d3e9d7cd1423/call-download_normal/shard-1. /usr/bin/aws s3 cp s3://pipeline.poc/sampledata/PSNL/FASTQS/HCC-1187BL-replicate_CAATGAGC-TATCGCAC.merged_R2.fq.gz .; ) > ""$outed746149"" 2> ""$erred746149""; echo $? > /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp; (; # add a .file in every empty directory to facilitate directory delocalization on the cloud; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; find . -type d -exec sh -c '[ -z ""$(ls -A '""'""'{}'""'""')"" ] && touch '""'""'{}'""'""'/.file' \;; ); (; cd /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1; sync. ); mv /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt.tmp /gstore/cromwell_execution/FE_Somatic_Mutect2/ed746149-883f-4ef1-8b95-d3e9d7cd1423/call-download_normal/shard-1/download_normal-1-rc.txt; ```. In this example, shard-0 succeeds and shard-1 fails, with this error messages, retrieved from AWS batch cloud watch logs:. AWS log of failed container job:; ![image](https://user-images.githubusercontent.com/28019025/74864165-f3e37980-5303-11ea-9685-9c9cca8c56c3.png). AWS log of failed container job-proxy:; ![image](https://user-images.githubusercontent.com/28019025/74863921-8a636b00-5303-11ea-8d63-aa92d4540069.png). In other examples, both succeed, both fail, or shard-0 fails and shard-1 succeeds. It doesn't seem to matter. The error is always the same, from executing the script inside the container: ; INVALID ARGUMENT (as shown above). I don't think it has to do with the nature of the job (downloading a fastq) since the error isn't regarding the actual command. It's more about the communication of the job to temporary stdout / err files (I think). If anyone has seen this or has any advice, please help.; Thanks",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5421:3143,message,messages,3143,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5421,1,['message'],['messages']
Integrability,"9615b472c7fec""><code>b6cfd1e</code></a> Explicitly wrap float parameter for consistency (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1671"">#1671</a>)</li>; <li><a href=""https://github.com/junit-team/junit4/commit/a5d205c7956dbed302b3bb5ecde5ba4299f0b646""><code>a5d205c</code></a> Fix GitHub link in FAQ (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1672"">#1672</a>)</li>; <li><a href=""https://github.com/junit-team/junit4/commit/3a5c6b4d08f408c8ca6a8e0bae71a9bc5a8f97e8""><code>3a5c6b4</code></a> Deprecated since jdk9 replacing constructor instance of Double and Float (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1660"">#1660</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/junit-team/junit4/compare/r4.13...r4.13.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=junit:junit&package-manager=maven&previous-version=4.13&new-version=4.13.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen th",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5941:3941,depend,dependency-name,3941,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5941,1,['depend'],['dependency-name']
Integrability,"9kdWN0aW9uUXVldWU"",; ""backend"": ""JES"",; ""end"": ""2017-01-30T19:14:19.708Z"",; ""stderr"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files/echo_files-stderr.log"",; ""callRoot"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files"",; ""attempt"": 1,; ""executionEvents"": [...],; ""backendLogs"": {; ""log"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/call-echo_files/echo_files.log""; },; ""start"": ""2017-01-30T19:00:03.896Z""; }]; },; ""outputs"": {. },; ""workflowRoot"": ""/b6b190d6-8640-4638-94cd-15f16b194f38/echo_strings/c386672d-0248-4968-9b1a-114f5f5c4706/"",; ""id"": ""c386672d-0248-4968-9b1a-114f5f5c4706"",; ""inputs"": {...; },; ""submission"": ""2017-01-30T19:00:00.796Z"",; ""status"": ""Failed"",; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }],; ""workflowLog"": ""gs://fc-2d3fd356-e3be-4953-92f1-60af623e6fa5/b6b190d6-8640-4638-94cd-15f16b194f38/workflow.logs/workflow.c386672d-0248-4968-9b1a-114f5f5c4706.log"",; ""end"": ""2017-01-30T19:14:20.002Z"",; ""start"": ""2017-01-30T19:00:03.040Z""; }. ```; Here it's an array of ""message""s; ```; {; ""workflowName"": ""aggregate_data_workflow"",; ""submittedFiles"": {... },; ""calls"": {; ""aggregate_data_workflow.aggregate_data"": [{; ""retryableFailure"": false,; ""executionStatus"": ""Failed"",; ""stdout"": ""/cromwell-executions/aggregate_data_workflow/3608d6ca-fbb4-4232-b197-268058470bfc/cal",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037:2555,message,message,2555,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037,1,['message'],['message']
Integrability,": Evaluating files_and_metadata_row[3] failed: Failed to find index Success(WomInteger(3)); on array:\n\nSuccess([\""SRR5395068\"", \""SRR5395068_1.fastq.gz\"", \""SRR5395068_2.fastq.gz\""])\n\n3"",; ""causedBy"": []; ```; How can i avoid this? Or is there a way to accomplish what I am trying to do?. ### Which backend are you running? ; Unix terminal within slurm scheduler. ### Example meta_data files:; 1) without barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz	; ```; 2) with barcode; ```; SRR5395067	SRR5395067_1.fastq.gz	SRR5395067_2.fastq.gz ATCGCT	; SRR395068	SRR5395068_1.fastq.gz	SRR5395068_2.fastq.gz ATCGGA; ```; ### Below my workflow:. workflow scMethTask3 {. #information about the monitoring scrip and the number of samples; File? monitoring_script; File meta_data. #information for trimming the cell barcode; File command; Int bases; ; #information for trimming the adapters and low quality reads; Int low_quality_cutoff; Int read_length_cutoff; String adapters_1; String adapters_2; Int trim_start_R1; Int trim_end_R1; Int trim_start_R2; Int trim_end_R2; String TAG. #information memory for each task; Int memory_task1; Int memory_task2. #Start the call. Array[Array[String]] files_and_metadata = read_tsv(meta_data). scatter(files_and_metadata_row in files_and_metadata) {; String sampleName = files_and_metadata_row[0]; File f1 = files_and_metadata_row[1]; File f2 = files_and_metadata_row[2]; String? barcode = files_and_metadata_row[3]; #if the barcode is passed, proceed with it.; if (defined(barcode)) {; call trimCellBarcode {; input:; f1=f1,; f2=f2,; sampleName=sampleName,; barcode=barcode,; monitoring_script=monitoring_script,; command=command,; memory_task1=memory_task1,; bases=bases; }; }; #if the barcode is not passed, proceed with the trimming of the adapters only; if (!defined(barcode)) {; call trimAdaptersWithoutBarcodes{; input:; input_r1=f1,; input_r2=f2,; sampleName=sampleName,; low_quality",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5396:1755,adapter,adapters,1755,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5396,1,['adapter'],['adapters']
Integrability,":+1: . Yeah this is going to change a lot in the shadow world, but these changes do eliminate a lot of db stuff from the backend, integrate retries on the upserts, and test the heck out of restarts. ðŸ˜„",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210148650:130,integrat,integrate,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/704#issuecomment-210148650,1,['integrat'],['integrate']
Integrability,":+1: Although there is already another warning being issued a few lines below (`failMessage foreach { m => logger.warn(s""$m. $retryMessage"") }`), maybe the message could be incorporated in this one to try and limit the number of logs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/256#issuecomment-151227093:156,message,message,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/256#issuecomment-151227093,1,['message'],['message']
Integrability,:+1: a bunch. It didn't take long to make me question why we tried to resolve a problem which involved doing too much stuff up front by adding even more stuff up front. . I was talking about something a lot like this to one of our two main internal customers (I believe FC) and they were good with the idea - however I'd say that we should make sure to have official buy in from our internal policy makers just to be sure. The dept itself seems to be learning to wrap its collective head around models where not everything is immediate & 100% consistent fashion so it should be an easy sell now.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/564#issuecomment-197299959:463,wrap,wrap,463,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/564#issuecomment-197299959,1,['wrap'],['wrap']
Integrability,"://bitbucket.org/asomov/snakeyaml/src/master/Releases.rst) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/releases.md) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/releases.markdown) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/releases.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/fthomas/scala-steward/blob/4c999c871afc48b47ca01211b35bb70b849ae19a/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; </details>. labels: test-library-update",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5848:2659,depend,dependency,2659,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5848,1,['depend'],['dependency']
Integrability,"://bitbucket.org/asomov/snakeyaml/src/master/Releases.rst) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/releases.md) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/releases.markdown) - [Release Notes](https://bitbucket.org/asomov/snakeyaml/src/master/releases.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGELOG.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/Changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/changelog.rst) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.md) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.markdown) - [Changelog](https://bitbucket.org/asomov/snakeyaml/src/master/CHANGES.rst). I'll automatically update this PR to resolve conflicts as long as you don't change it yourself. If you'd like to skip this version, you can just close this PR. If you have any feedback, just mention me in the comments below. Configure Scala Steward for your repository with a [`.scala-steward.conf`](https://github.com/fthomas/scala-steward/blob/a2a47735b8b5ce3b0b0a9fa0a2cdf3b8405ff98d/docs/repo-specific-configuration.md) file. Have a fantastic day writing Scala!. <details>; <summary>Ignore future updates</summary>. Add this to your `.scala-steward.conf` file to ignore future updates of this dependency:; ```; updates.ignore = [ { groupId = ""org.yaml"", artifactId = ""snakeyaml"" } ]; ```; </details>. labels: test-library-update",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5687:2659,depend,dependency,2659,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5687,1,['depend'],['dependency']
Integrability,:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) ~[na:1.8.0_72]; at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) ~[na:1.8.0_72]; at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) ~[na:1.8.0_72]; at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) ~[na:1.8.0_72]; at java.io.BufferedInputStream.read1(BufferedInputStream.java:286) ~[na:1.8.0_72]; at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_72]; at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:704) ~[na:1.8.0_72]; at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:647) ~[na:1.8.0_72]; at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1536) ~[na:1.8.0_72]; at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1441) ~[na:1.8.0_72]; at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:480) ~[na:1.8.0_72]; at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338) ~[na:1.8.0_72]; at com.google.api.client.http.javanet.NetHttpResponse.<init>(NetHttpResponse.java:37) ~[cromwell.jar:0.19]; at com.google.api.client.http.javanet.NetHttpRequest.execute(NetHttpRequest.java:94) ~[cromwell.jar:0.19]; at com.google.api.client.http.HttpRequest.execute(HttpRequest.java:972) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.media.MediaHttpUploader.executeCurrentRequestWithoutGZip(MediaHttpUploader.java:545) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.media.MediaHttpUploader.executeCurrentRequest(MediaHttpUploader.java:562) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.media.MediaHttpUploader.resumableUpload(MediaHttpUploader.java:419) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.media.MediaHttpUploader.upload(MediaHttpUploader.java:336) ~[cromwell.jar:0.19]; at com.google.api.client.googleapis.services.AbstractGoogleClientRequest.ex,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/932:3420,protocol,protocol,3420,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/932,1,['protocol'],['protocol']
Integrability,":55:17,31] [info] Aborting all running workflows.; [2023-02-04 08:55:17,31] [info] JobExecutionTokenDispenser stopped; [2023-02-04 08:55:17,31] [info] WorkflowStoreActor stopped; [2023-02-04 08:55:17,32] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2023-02-04 08:55:17,32] [info] WorkflowLogCopyRouter stopped; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor All workflows finished; [2023-02-04 08:55:17,32] [info] WorkflowManagerActor stopped; [2023-02-04 08:55:17,32] [info] Connection pools shut down; [2023-02-04 08:55:17,33] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] SubWorkflowStoreActor stopped; [2023-02-04 08:55:17,33] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] JobStoreActor stopped; [2023-02-04 08:55:17,33] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2023-02-04 08:55:17,33] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] CallCacheWriteActor stopped; [2023-02-04 08:55:17,33] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2023-02-04 08:55:17,33] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2023-02-04 08:55:17,33] [info] KvWriteActor Shutting down: 0 queued messages to process; [2023-02-04 08:55:17,33] [info] DockerHashActor stopped; [2023-02-04 08:55:17,34] [info] IoProxy stopped; [2023-02-04 08:55:17,34] [info] ServiceRegistryActor stopped; [2023-02-04 08:55:17,37] [info] Database closed; [2023-02-04 08:55:17,37] [info] Stream materializer shut down; [2023-02-04 08:55:17,40] [info] Automatic shutdown of the async connection; [2023-02-04 08:55:17,40] [info] Gracefully shutdown sentry threads.; [2023-02-04 08:55:17,40] [info] Shutdown finish",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6999:17357,message,messages,17357,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6999,3,['message'],['messages']
Integrability,":56:30,264 cromwell-system-akka.dispatchers.engine-dispatcher-37 INFO - 384e88c5-eba8-400c-aaef-5d618ffdce88-SubWorkflowActor-SubWorkflow-SplitRG:-1:1 [UUID(384e88c5)]: Starting calls: SplitLargeRG.SamSplitter:NA:1; 2018-01-17 20:56:30,293 cromwell-system-akka.dispatchers.backend-dispatcher-43 INFO - JesAsyncBackendJobExecutionActor [UUID(384e88c5)SplitLargeRG.SamSplitter:NA:1]: `set -e; mkdir output_dir. total_reads=$(samtools view -c /cromwell_root/broad-dsp-spec-ops-cromwell-execution/CramToUnmappedBams/7db4d00c-0d04-43c5-b480-3cfe6080a3e3/call-SortSam/shard-0/0.1.unmapped.bam). java -Dsamjdk.compression_level=2 -Xms3000m -jar /usr/gitc/picard.jar SplitSamByNumberOfReads \; INPUT=/cromwell_root/broad-dsp-spec-ops-cromwell-execution/CramToUnmappedBams/7db4d00c-0d04-43c5-b480-3cfe6080a3e3/call-SortSam/shard-0/0.1.unmapped.bam \; OUTPUT=output_dir \; SPLIT_TO_N_READS=48000000 \; TOTAL_READS_IN_INPUT=$total_reads`; 2018-01-17 20:56:36,955 cromwell-system-akka.dispatchers.backend-dispatcher-43 INFO - JesAsyncBackendJobExecutionActor [UUID(384e88c5)SplitLargeRG.SamSplitter:NA:1]: job id: operations/EOvc7beQLBiwi6fk-aX5yBEgqeCbgo4VKg9wcm9kdWN0aW9uUXVldWU; 2018-01-17 20:56:48,780 cromwell-system-akka.dispatchers.backend-dispatcher-43 INFO - JesAsyncBackendJobExecutionActor [UUID(384e88c5)SplitLargeRG.SamSplitter:NA:1]: Status change from - to Running; ```. Here is the scattered [SomaticPairedSingleSampleWf.scattered.txt](https://github.com/broadinstitute/cromwell/files/1641317/SomaticPairedSingleSampleWf.scattered.txt) runnable version that gets stuck running. and the non scattered [SomaticPairedSingleSampleWf.single.txt](https://github.com/broadinstitute/cromwell/files/1641318/SomaticPairedSingleSampleWf.single.txt) runnable version that works great. Here is the dependencies zip [SomaticPairedSingleSampleWfDependencies.zip](https://github.com/broadinstitute/cromwell/files/1641320/SomaticPairedSingleSampleWfDependencies.zip). @kcibul i was asked to ping you on this issue",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3156:12063,depend,dependencies,12063,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3156,1,['depend'],['dependencies']
Integrability,":59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Creating database history table with name: cromwell_test.DATABASECHANGELOG |; | cromwell | stdout | INFO | Jul 22, 2022 1:23:59 PM liquibase.changelog |; | cromwell | stdout | INFO | INFO: Reading from cromwell_test.DATABASECHANGELOG |; | centaur | slf4j | INFO | 13:24:00.375 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 13:24:00.376 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | Jul 22, 2022 1:24:00 PM liquibase.changelog |; | cromwell | stdout | WARN | WARNING: modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. From the [logs for this current PR](https://app.travis-ci.com/github/broadinstitute/cromwell/jobs/577574057):. | Application | Logger | Level | Message |; |---|-------|---|---|; | cromwell | slf4j | INFO | 2022-07-23 22:04:49 main INFO - Running with database db.url = jdbc:mysql://localhost:3306/cromwell_test?allowPublicKeyRetrieval=true&useSSL=false&rewriteBatchedStatements=true&serverTimezone=UTC&useInformationSchema=true |; | centaur | slf4j | INFO | 22:04:54.033 [ScalaTest-main] INFO centaur.CromwellManager$ - Cromwell server alive while waiting = false |; | centaur | slf4j | INFO | 22:04:54.034 [ScalaTest-main] INFO centaur.CromwellManager$ - Waiting for Cromwell... |; | cromwell | stdout | WARN | 2022-07-23 22:04:54 db-1 WARN - modifyDataType will lose primary key/autoincrement/not null settings for mysql. Use <sql> and re-specify all configuration if this is the case |. Differences:; - Liquibase calls to java.util.logging are now being routed to slf4j, including identifying the thread `db-1`.; - Liquibase no longer outputs INFO messages as was [previously configured](https://github.com/broadinstitute/cromwell/blob/82/server/src/main/resources/logback.xml#L94). #",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532:1703,Message,Message,1703,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6813#issuecomment-1193214532,1,['Message'],['Message']
Integrability,:frowning: Another argument for why we should integration test. Do we need to do a release?. :+1:,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/342#issuecomment-166664940:46,integrat,integration,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/342#issuecomment-166664940,1,['integrat'],['integration']
Integrability,"; [2018-10-23 17:49:32,18] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2018-10-23 17:49:32,19] [info] JobExecutionTokenDispenser stopped; [2018-10-23 17:49:32,19] [info] WorkflowStoreActor stopped; [2018-10-23 17:49:32,20] [info] WorkflowLogCopyRouter stopped; [2018-10-23 17:49:32,20] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor All workflows finished; [2018-10-23 17:49:32,20] [info] Connection pools shut down; [2018-10-23 17:49:32,20] [info] WorkflowManagerActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] SubWorkflowStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] JobStoreActor stopped; [2018-10-23 17:49:32,21] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2018-10-23 17:49:32,21] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] KvWriteActor Shutting down: 0 queued messages to process; [2018-10-23 17:49:32,21] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2018-10-23 17:49:32,22] [info] ServiceRegistryActor stopped; [2018-10-23 17:49:32,22] [info] CallCacheWriteActor stopped; [2018-10-23 17:49:32,23] [info] DockerHashActor stopped; [2018-10-23 17:49:32,23] [info] IoProxy stopped; [2018-10-23 17:49:32,26] [info] Database closed; [2018-10-23 17:49:32,26] [info] Stream materializer shut down; [2018-10-23 17:49:32,27] [info] WDL HTTP import resolver closed; Workflow d186ca94-b85b-4729-befc-8ad28a05976c transitioned to state Failed; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856:8616,message,messages,8616,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4103#issuecomment-432449856,3,['message'],['messages']
Integrability,"; [2019-01-07 16:21:33,03] [info] WorkflowStoreActor stopped; [2019-01-07 16:21:33,03] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-01-07 16:21:33,05] [info] JobExecutionTokenDispenser stopped; [2019-01-07 16:21:33,05] [info] WorkflowLogCopyRouter stopped; [2019-01-07 16:21:33,05] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor All workflows finished; [2019-01-07 16:21:33,05] [info] WorkflowManagerActor stopped; [2019-01-07 16:21:33,05] [info] Connection pools shut down; [2019-01-07 16:21:33,07] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,07] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-01-07 16:21:33,08] [info] SubWorkflowStoreActor stopped; [2019-01-07 16:21:33,08] [info] JobStoreActor stopped; [2019-01-07 16:21:33,08] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,08] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-01-07 16:21:33,09] [info] DockerHashActor stopped; [2019-01-07 16:21:33,09] [info] CallCacheWriteActor stopped; [2019-01-07 16:21:33,09] [info] ServiceRegistryActor stopped; [2019-01-07 16:21:33,10] [info] IoProxy stopped; [2019-01-07 16:21:33,14] [info] Database closed; [2019-01-07 16:21:33,14] [info] Stream materializer shut down; [2019-01-07 16:21:33,15] [info] WDL HTTP import resolver closed; Workflow 18de8166-5f29-4288-9fa4-6741565446fd transitioned to state Failed; ```",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4526:8593,message,messages,8593,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4526,3,['message'],['messages']
Integrability,"; causedBy: [; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_pac' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.agg_preemptible_tries' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_ann' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.wgs_coverage_interval_list' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_fasta_index' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.unmapped_bam_suffix' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_ud' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_amb' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.preemptible_tries' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_sa' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.known_indels_sites_VCFs' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.contamination_sites_mu' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_alt' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_bwt' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.known_indels_sites_indices' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.ref_dict' not specified.""; },; {; causedBy: [ ],; message: ""Required workflow input 'SomaticRoot.TumorAlignment.c",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2912:1554,message,message,1554,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2912,1,['message'],['message']
Integrability,"; }; }. # Here is where you can define the backend providers that Cromwell understands.; # The default is a local provider.; # To add additional backend providers, you should copy paste additional backends; # of interest that you can find in the cromwell.example.backends folder; # folder at https://www.github.com/broadinstitute/cromwell; # Other backend providers include SGE, SLURM, Docker, udocker, Singularity. etc.; # Don't forget you will need to customize them for your particular use case.; backend {; # Override the default backend.; default = slurm. # The list of providers.; providers {; # Copy paste the contents of a backend provider in this section; # Examples in cromwell.example.backends include:; # LocalExample: What you should use if you want to define a new backend provider; # AWS: Amazon Web Services; # BCS: Alibaba Cloud Batch Compute; # TES: protocol defined by GA4GH; # TESK: the same, with kubernetes support; # Google Pipelines, v2 (PAPIv2); # Docker; # Singularity: a container safe for HPC; # Singularity+Slurm: and an example on Slurm; # udocker: another rootless container solution; # udocker+slurm: also exemplified on slurm; # HtCondor: workload manager at UW-Madison; # LSF: the Platform Load Sharing Facility backend; # SGE: Sun Grid Engine; # SLURM: workload manager. # Note that these other backend examples will need tweaking and configuration.; # Please open an issue https://www.github.com/broadinstitute/cromwell if you have any questions; slurm {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; # Root directory where Cromwell writes job results in the container. This value; # can be used to specify where the execution folder is mounted in the container.; # it is used for the construction of the docker_cwd string in the submit-docker; # value above.; dockerRoot = ""/cromwell-executions"". concurrent-job-limit = 10; # If an 'exit-code-timeout-seconds' value is specified:; # - check-alive will be run at ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6933:5526,protocol,protocol,5526,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6933,1,['protocol'],['protocol']
Integrability,"<!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; ![2018-10-26 23 24 43](https://user-images.githubusercontent.com/4966343/47572651-79585300-d976-11e8-8027-a9bade3f91d4.png). <!-- Which backend are you running? -->; aws. <!-- Paste/Attach your workflow if possible: -->; https://github.com/broadinstitute/cromwell/blob/develop/centaur/src/main/resources/integrationTestCases/germline/haplotype-caller-workflow/HaplotypeCallerWF.aws.wdl. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->; AWS Access Key ID [****************R62Q]: ; AWS Secret Access Key [****************uDg5]:; Default region name [ap-northeast-2]:; Default output format [None]:",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4321:401,integrat,integrationTestCases,401,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4321,1,['integrat'],['integrationTestCases']
Integrability,<!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->; Submitting an empty file as a `workflowInputs` causes cromwell to return an unhelpful ; `There was an internal server error.%`. <!-- Which backend are you running? -->; It happens on both PAPIv1 and PAPIv2 (tested on cromwell 33 and 34) . <!-- Paste/Attach your workflow if possible: -->; running the following:. ` ``; curl -s -v -F workflowSource=empty -F workflowInputs=empty https://cromwell-v34.dsde-methods.broadinstitute.org/api/workflows/v1; ```; results in:; ```; There was an internal server error.%; ```. The file `empty` is an empty file. It would be nice if it returned a more useful error message to tell you what the problem was.,MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4086:699,message,message,699,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4086,1,['message'],['message']
Integrability,"<!--; Hi! Thanks for taking the time to report feedback. Before posting an issue here, please check whether your question is already answered in our:; forum https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; documentation http://cromwell.readthedocs.io/en/develop/. Other forums:; FireCloud https://gatkforums.broadinstitute.org/firecloud/categories/ask-the-firecloud-team; WDL https://gatkforums.broadinstitute.org/wdl/categories/ask-the-wdl-team; CWL https://www.biostars.org/; -->. <!-- Are you seeing something that looks like a bug? Then great! You're in the right place. -->. <!-- Which backend are you running? -->; Backend:; Local, no conf file. <!-- Paste/Attach your workflow if possible: -->. Workflow: Files are here:; https://github.com/FredHutch/reproducible-workflows/tree/master/CWL/SingleStepWorkflow. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->. Details (see also [this post](https://gatkforums.broadinstitute.org/wdl/discussion/23265/cwl-workflow-fails-running-locally#latest)):. I can run this workflow just fine using cwltool/cwl-runner as follows:. ```; cwl-runner bwa-memWorkflow.cwl localInputs.yml; ```. When I try and run it with cromwell I get an error that ""The job was aborted from outside Cromwell"" but I definitely did not abort it myself. Here is the command I used to run this workflow in Cromwell:. ```; java -jar cromwell-36.jar run bwa-memWorkflow.cwl -i localInputs.yml -p bwa-pe.cwl.zip; ```. (`bwa-pe.cwl.zip` just contains the dependency `bwa-pe.cwl`). And here's the full output of it:. https://gist.github.com/dtenenba/61bcf60f129b817cd894ee222789369a. My ultimate goal is to switch over to the AWS Batch back end (in case you are wondering why I don't just stick with cwltool) but first I wanted to get the workflow running locally in cromwell. Any ideas about this?",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4587:1556,depend,dependency,1556,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4587,1,['depend'],['dependency']
Integrability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. # Description. I believe this is a bug. I tried to use `stderr()` in the `output` section of a `workflow`, rather than the output section of a `task`. The resulting WDL validated fine using `womtool validate` (and it validated fine on Terra with the automatic validation they do). But the job would run about halfway and then automatically switch to ""Aborting"" status with no explanation or error message. The workflow would eventually fail after a huge delay (about 22 hours), and there would be no real error message. All tasks that ran were successful (but not all tasks ran). # Minimal WDL example. Here is a working example:. ```wdl; version 1.0. workflow my_workflow {; call my_task; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. And here is a non-working example that still validates fine using `womtool validate`:. ```wdl; version 1.0. workflow my_workflow {; input {; Boolean run_task; }. if (run_task) {; call my_task; }. output {; File out = select_first([my_task.out, stdout()]); }; }. task my_task {; command {; echo ""hello world""; }; output {; File out = stdout(); }; }; ```. The above gives; ```console; (cromwell) [sfleming@laptop:~/cromwell]$ womtool validate test.wdl ; Success!; ```. # The problem. The problem is that the non-working WDL example above should not validate successfully, as it is NOT a valid WDL. The `stdout()` built-in inside the `select_first()` in the `output` block of the `workflow` is not actually allowed. It will cause a very bizarre err",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6976:868,message,message,868,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6976,2,['message'],['message']
Integrability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; Hi!. We've been looking into migrating from PAPIv2 backend to GCPBATCH backend. Callcaching fails on GCPBATCH but not on PAPIv2 when using a private docker image in gcr.io. ; Is this a missing feature or a bug? The documentation on the subject could go either way, depending on whether GCPBATCH is part of the other backends or a subset of the pipelines backend (https://cromwell.readthedocs.io/en/latest/cromwell_features/CallCaching/). ; I do not think this is a configuration error, since the same config works with PAPIv2 backend, but if it is, what configuration options would be necessary for configuring gcr.io authentication when using GCPBATCH?. Errors from cromwell logs when task is being callcached:; ```; cromwell_1 | 2024-01-11 11:09:38 pool-9-thread-9 INFO - Manifest request failed for docker manifest V2, falling back to OCI manifest. Image: DockerImageIdentifierWithoutHash(Some(eu.gcr.io),Some(project),image_name,tag); cromwell_1 | cromwell.docker.registryv2.DockerRegistryV2Abstract$Unauthorized: 401 Unauthorized {""errors"":[{""code"":""UNAUTHORIZED"",""message"":""You don't have the needed permissions to perform this operation, and you may have invalid credentials. To authenticate your request, follow the steps in: https://cloud.google.com/container-registry/docs/advanced-authentication""}]}; cromwell_1 | 	at cromwell.docker.registryv2.DockerRegistryV2Abstract.$anonfun$getDigestFromResponse$1(DockerRegistryV2Abstract.scala:321); cromwell_1 | 	at map @ fs2.internal.CompileScope.$anonfun$close$9(CompileScope.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7356:736,depend,depending,736,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7356,1,['depend'],['depending']
Integrability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->. Hi,. Since last week, our cromwell server instance on GCP started to encounter the following error in all the jobs:. ```; 2024-07-31 19:08:59 cromwell-system-akka.dispatchers.backend-dispatcher-35 WARN - PAPI request worker had 1 failures making 1 requests: ; Unable to complete PAPI request due to system or connection error (Unknown Error.); 2024-07-31 19:09:33 cromwell-system-akka.dispatchers.backend-dispatcher-56 WARN - PAPI request worker had 1 failures making 1 requests: ; Unable to complete PAPI request due to system or connection error (Unknown Error.); 2024-07-31 19:10:06 cromwell-system-akka.dispatchers.backend-dispatcher-56 WARN - PAPI request worker had 1 failures making 1 requests: ; Unable to complete PAPI request due to system or connection error (Unknown Error.); ```. However, with `Unknown Error` message, I don't know where to go for help. Do you have any suggestion?. Here are the configurations:. * Cromwell v85; * Genomics API; * PAPIv2 with `actor-factory = ""cromwell.backend.google.pipelines.v2alpha1.PipelinesApiLifecycleActorFactory""`. Many thanks!. <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7482:1189,message,message,1189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7482,1,['message'],['message']
Integrability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; I think the minimum to reproduce the bug is just. ```; Array[File] foo = []; Array[String]? bar = foo; ```. which fails with. ```; ""failures"": [; {; ""causedBy"": [; {; ""message"": ""Failed to evaluate 'bar' (reason 1 of 1): Evaluating foo failed: assertion failed: base member type WomMaybeEmptyArrayType(WomStringType) and womtype WomMaybeEmptyArrayType(WomSingleFileType) are not compatible"",; ""causedBy"": []; }; ],; ""message"": ""Workflow failed""; }; ],; ```. Interestingly enough, this passes if the array is non-empty, or if the target is not optional, or if the source is type `Array[String]`. I am running cromwell ""v85 (ish)"" according to the administrator. Backend is AWS batch.; <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->. <!-- Which backend are you running? -->. <!-- Paste/Attach your workflow if possible: -->. <!-- Paste your configuration if possible, MAKE SURE TO OMIT PASSWORDS, TOKENS AND OTHER SENSITIVE MATERIAL: -->",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7399:534,message,message,534,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7399,2,['message'],['message']
Integrability,"<!--; Hi! Thanks for taking the time to report feedback. Please check whether your question is already answered in our:; Documentation http://cromwell.readthedocs.io/en/develop/; Bioinformatics Stack Exchange https://bioinformatics.stackexchange.com/search?q=cromwell; Slack https://join.slack.com/t/cromwellhq/shared_invite/zt-dxmmrtye-JHxwKE53rfKE_ZWdOHIB4g; -->; This is a remark on https://github.com/broadinstitute/cromwell/blob/master/docs/tutorials/HPCSlurmWithLocalScratch.md there is a feature on slum config to edit the sbatch command. You could add in a find and replace in the config to do the same as the tutorial. you can skip the first part of the tutorial by editing the slurm backend config (somewhat hotpatching the scripts on submission time). old submit ; submit = """"""; sbatch -J ${job_name} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". new submit for slurm auto configured job dir: ; submit = """"""; perl -i.bak -wpe 's/^tmpDir=.*/tmpdir=""\$TMPDIR""/g' ${script} && \; sbatch -J ${job_name} --tmp=${disk} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". new submit for /genomics/local/ (not tested tough): ; submit = """"""; perl -i.bak -wpe 's/^tmpDir=.*/tmpdir=""$(mkdir -p ""\/genomics_local\/\$PID_\$HOSTNAME""\/"" && echo ""\/genomics_local\/\$PID_\$HOSTNAME""\/""/g' ${script} && \; sbatch -J ${job_name} --tmp=${disk} -D ${cwd} -o ${out} -e ${err} -t ${runtime_minutes} -p ${queue} ${""-c "" +; cpu} --mem ${memory_mb} --wrap ""/bin/bash ${script}""; """""". <!-- Are you seeing something that looks like a bug? Please attach as much information as possible. -->; <!-- This is a clear feature cant you see -->. <!-- Which backend are you running? -->; The backend I'm running on is Slurm hpc with a version 1.0 workflow. This alternative workflow has its downsides but also benefits it is up to the hpc(user) to decide ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7357:913,wrap,wrap,913,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7357,1,['wrap'],['wrap']
Integrability,"</li>; <li><a href=""https://github.com/junit-team/junit4/commit/b6cfd1e3d736cc2106242a8be799615b472c7fec""><code>b6cfd1e</code></a> Explicitly wrap float parameter for consistency (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1671"">#1671</a>)</li>; <li><a href=""https://github.com/junit-team/junit4/commit/a5d205c7956dbed302b3bb5ecde5ba4299f0b646""><code>a5d205c</code></a> Fix GitHub link in FAQ (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1672"">#1672</a>)</li>; <li><a href=""https://github.com/junit-team/junit4/commit/3a5c6b4d08f408c8ca6a8e0bae71a9bc5a8f97e8""><code>3a5c6b4</code></a> Deprecated since jdk9 replacing constructor instance of Double and Float (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1660"">#1660</a>)</li>; <li>Additional commits viewable in <a href=""https://github.com/junit-team/junit4/compare/r4.13...r4.13.1"">compare view</a></li>; </ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=junit:junit&package-manager=maven&previous-version=4.13&new-version=4.13.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/configuring-github-dependabot-security-updates). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cance",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5941:3842,Depend,Dependabot,3842,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5941,2,"['Depend', 'depend']","['Dependabot', 'dependabot-badges']"
Integrability,"</ul>; </details>; <br />. [![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=com.fasterxml.jackson.core:jackson-databind&package-manager=maven&previous-version=2.11.1&new-version=2.12.6.1)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores). Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`. [//]: # (dependabot-automerge-start); [//]: # (dependabot-automerge-end). ---. <details>; <summary>Dependabot commands and options</summary>; <br />. You can trigger Dependabot actions by commenting on this PR:; - `@dependabot rebase` will rebase this PR; - `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it; - `@dependabot merge` will merge this PR after your CI passes on it; - `@dependabot squash and merge` will squash and merge this PR after your CI passes on it; - `@dependabot cancel merge` will cancel a previously requested merge and block automerging; - `@dependabot reopen` will reopen this PR if it is closed; - `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually; - `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself); - `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself); - `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language; - `@de",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6743:954,Depend,Dependabot,954,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6743,9,"['Depend', 'depend']","['Dependabot', 'dependabot']"
Integrability,"<p><em>Sourced from <a href=""https://github.com/junit-team/junit4/releases"">junit's releases</a>.</em></p>; <blockquote>; <h2>JUnit 4.13.1</h2>; <p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.1.md"">release notes</a> for details.</p>; </blockquote>; </details>; <details>; <summary>Changelog</summary>; <p><em>Sourced from <a href=""https://github.com/junit-team/junit4/blob/main/doc/ReleaseNotes4.13.1.md"">junit's changelog</a>.</em></p>; <blockquote>; <h2>Summary of changes in version 4.13.1</h2>; <h1>Rules</h1>; <h3>Security fix: <code>TemporaryFolder</code> now limits access to temporary folders on Java 1.7 or later</h3>; <p>A local information disclosure vulnerability in <code>TemporaryFolder</code> has been fixed. See the published <a href=""https://github.com/junit-team/junit4/security/advisories/GHSA-269g-pwp5-87pp"">security advisory</a> for details.</p>; <h1>Test Runners</h1>; <h3>[Pull request <a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1669"">#1669</a>:](<a href=""https://github-redirect.dependabot.com/junit-team/junit/pull/1669"">junit-team/junit#1669</a>) Make <code>FrameworkField</code> constructor public</h3>; <p>Prior to this change, custom runners could make <code>FrameworkMethod</code> instances, but not <code>FrameworkField</code> instances. This small change allows for both now, because <code>FrameworkField</code>'s constructor has been promoted from package-private to public.</p>; </blockquote>; </details>; <details>; <summary>Commits</summary>; <ul>; <li><a href=""https://github.com/junit-team/junit4/commit/1b683f4ec07bcfa40149f086d32240f805487e66""><code>1b683f4</code></a> [maven-release-plugin] prepare release r4.13.1</li>; <li><a href=""https://github.com/junit-team/junit4/commit/ce6ce3aadc070db2902698fe0d3dc6729cd631f2""><code>ce6ce3a</code></a> Draft 4.13.1 release notes</li>; <li><a href=""https://github.com/junit-team/junit4/commit/c29dd8239d6b353e699397eb090a1fd27411fa24"">",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5941:1114,depend,dependabot,1114,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5941,1,['depend'],['dependabot']
Integrability,"=ABORTED, description=null, cause=null}. Message: 14: VM ggp-15030877962490231612 stopped unexpectedly.""; > ; > However we have seen a new error response. ""Error code 10: Message 13"" metadata output showing:; > ; > ""message"": ""Task PairedEndSingleSampleWorkflow.HaplotypeCaller:46:3 failed. JES error code 10. Message: 13: VM ggp-9289873678241352278 shut down unexpectedly.""; > ; > From what Cromwell team indicates is that ""Message 13"" is not the same as Message 14 - as such a different logic occurs within cromwell. Cromwell will try the task three times and after that it will just ""Fail"" the task. So the ""try 3 pre-emptible then try non-preemptible"" logic is never followed.; > ; > So my question is what is ""Message 13"" and how is it different from ""Message 14""? Below are OpsIDs for a set of tasks - the first are the ""Message 14"" (which again are normal preemption but I wanted to provide some for comparison to Message 13) and the second list are the ""Message 13"". This is just a small sample of Message 13 failures.; > ; > MESSAGE 14: ; > operations/ENWy-aWLLBi89uiD6_uZzNABIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMzb1NeLLBj0jsHwufD1gHogpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EOn3vcOKLBibqZWQsay6xlUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EK3Nx_aKLBjUn5bp5oqJz9oBIJGGnffgCioPcHJvZHVjdGlvblF1ZXVl; > operations/EIyjs-eKLBiUx5LdqLi-kh8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU. > MESSAGE 13:; > operations/EMCgv6aLLBifhsPH4fzAufMBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EPOYsKiLLBib6JnQtvmKzPoBIL3p_s7RASoPcHJvZHVjdGlvblF1ZXVl; > operations/EL-QlNKLLBjeuPH9gd3Ck24gven-ztEBKg9wcm9kdWN0aW9uUXVldWU; > operations/EK6y-aWLLBjV36D2ueHGsKYBIMf5sPc2Kg9wcm9kdWN0aW9uUXVldWU; > operations/EMPd46GLLBj1iYrpkrCipPsBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl; > operations/ENTd46GLLBiN8JPluoXAzFUgpfe0-ecHKg9wcm9kdWN0aW9uUXVldWU; > operations/EMPehaqLLBiS7p7OzdzYu5wBIKX3tPnnByoPcHJvZHVjdGlvblF1ZXVl. > ------------------------------- ; > kcibul@broadinstitute.org <kcibul@broadinstitute.org> ",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3157:2031,Message,Message,2031,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3157,1,['Message'],['Message']
Integrability,"> ; `tee` will cache the standard output of the program into a buffer, after using `sync`, it will brush the data from the buffer to disk, after that, nfs will synchronize to the remote service (nfs itself has a delay of a few seconds); so no matter whether it is `tee` and `sync`, or nfs there may be a problem, the best thing is to turn the`rc.tmp` to` rc` file operation to give a delay of a few seconds is to do it in the case of not changing the source code. It's a good idea to delay the `rc.tmp` to `rc` file operation for a few seconds without changing the source code.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1710994350:160,synchroniz,synchronize,160,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/7094#issuecomment-1710994350,1,['synchroniz'],['synchronize']
Integrability,"> @grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`. Since it has been decided to keep support for older v2alpha1 version in addition to newer v2beta, this is no longer an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147:34,adapter,adapter,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-580044147,1,['adapter'],['adapter']
Integrability,> @kpierre13 these tests cases look great. Would it also make sense to add tests for below endpoints? Or do tests for these already exist?; > ; > * GET `/runs/{workflowId}/status`; > * POST `/runs/{workflowId}/cancel`; > * GET `/service-info`. There are existing tests for the first two in the same file. The tests for `service-info` are located in [ServiceInfo.spec](https://github.com/broadinstitute/cromwell/blob/21fc23f41b776d666a089b7c83b64a066bc730e8/engine/src/test/scala/cromwell/webservice/routes/wes/ServiceInfoSpec.scala),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096:499,rout,routes,499,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6833#issuecomment-1219887096,1,['rout'],['routes']
Integrability,"> Actually I'd like to add a Centaur integration test for this, I can hopefully get this done today. Thanks, I thought about doing this but wasn't sure how to do it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518663198:37,integrat,integration,37,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5103#issuecomment-518663198,1,['integrat'],['integration']
Integrability,"> Also I'm not the ticket author but I thought that was intended to cover integrating ""compressed at rest"" writes into carboniting?. oops, accidentally reverted that part during testing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551225460:74,integrat,integrating,74,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5269#issuecomment-551225460,1,['integrat'],['integrating']
Integrability,"> Cloudwatch logs contained the following message: ""/bin/bash: /var/scratch/fetch_and_run.sh: Is a directory"". Also have this error. Anyone figure out what the issue is?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-727246269:42,message,message,42,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-727246269,1,['message'],['message']
Integrability,"> Futures are fine, just not in the code which the actor itself directly controls. Hmm pretty much all our relevant code lives in actors so if the only place where we can use `Future`s is outside of actors, then I'd say `Future`'s days are counted xD. The _burden_ of verifying that new code doesn't break anything is part of the review process anyway IMHO, and adding N more actors instead of `Future`s with twice as many more messages, states, transitions, and classes doesn't reduce the burden of checking that everything is wired correctly, as far as I'm concerned. . Also, about futures being dangerous in actors because they can mutate state, this can only happen in a true `Actor` where your state pretty much has to be `var`s if you want to be able to mutate it, or in an `FSM` where you would also store some state as mutable `var`s inside the actor, instead of using the FSM data.; If you're using an FSM and all your mutable data is contained in the FSM data, then I don't see how creating futures would ever lead to mutating your state (and by state I mean mutable data, not FSM state) asynchronously. The FSM data is only updatable when the actor receives a message and decides to change state or stay in the same. A future `onComplete` could never force the FSM to mutate its data, which is why all we do is always send a message to someone when the future completes. And I don't see what difference it makes, from an actor perspective, if a message comes from a `Future` or another actor.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218595592:428,message,messages,428,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218595592,4,['message'],"['message', 'messages']"
Integrability,"> Hmm, that's an interesting problem - since centaur runs in server mode I don't think you'd see an exit code.; Does any failure data end up in Cromwell's metadata when this copy fails? If so, centaur can query for the metadata entry. @cjllanwarne I found the problem. I did not have a `final_workflow_outputs_dir` set in the options.json files for the centaur tests. If this path is not set `use_relative_output_paths` is of course not used... :man_facepalming: That is fixed now and it works as expected. Colliding outputs will return as a workflow failure. Since I got the local testing working I was able to add more advanced tests and make sure these are correct as well. The error message is tested when the outputs are colliding. In order for that test to work I had to make the output order of colliding paths in the error message deterministic. (Otherwise it would fail randomly). Also my colleague @DavyCats showed me some centaur tests were file outputs are tested. I used these as an example to also test for the outputs. ; All the behavior that this PR affects is now properly tested, which means that these tests should be able to discover regressions in the future.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482492876:687,message,message,687,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4815#issuecomment-482492876,2,['message'],['message']
Integrability,"> I am having the same error with the example ""Using Data on S3"" on https://docs.opendata.aws/genomics-workflows/orchestration/cromwell/cromwell-examples/ . I have changed the S3 bucket name in the .json file to my bucket name, but the run still failed. After reporting running failure, I have got the same error message. I am using cromwell-48. The S3 bucket has all public access, and I was logged in as the Admin in two terminal windows, one running the server and the other submitting the job. The previous two hello-world example were successful. There is no log file in the bucket and in the cromwell-execution, the only file create was the script. There is no rc or stderr or stdout created. @blindmouse Were you able to resolve your issue? I am encountering the same problem. Thanks.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662080275:313,message,message,313,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662080275,1,['message'],['message']
Integrability,"> I can follow how `forInput` is passed around, but I can't seem to discern where it is actually set - i.e. where the default value is overridden. Can you help me wrap my head around this?. [Here](https://github.com/broadinstitute/cromwell/blob/9bd4e90fdf999abc76d0ba8e801ad24eb99140b5/engine/src/main/scala/cromwell/engine/workflow/lifecycle/execution/job/preparation/JobPreparationActor.scala#L62) in the JobPreparationActor. I agree that it is confusing. This is mostly because of the structure of the code. It has to be set in the top level class and then passed down through multiple layers. During JobPreparation the evaluation must be different. After that evaluation of the expressions in the outputs is handled, and there the default works well. I had to give the SFSBackend some info that it was being used in this way, so I had to set it all the way at toplevel and add all this code. It would be nice if there was a more straightforward way of doing this.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618835746:163,wrap,wrap,163,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618835746,1,['wrap'],['wrap']
Integrability,"> I haven't fully thought it through yet, but I'll keep something sorta like this in mind when we're dealing with tracking the WomFile.value to (cloud, vm, container) path split. I think we could use a mini brainstorm session on this. I'm finding other areas where it would be very nice to clarify this cloud / vm /container separation.; Namely `JobPaths` has become a huge messy melting pot over time and it would be awesome to have a clean `cloudCallRoot`, `vmCallRoot`, `containerCallRoot` for example (some of those that could be the same depending on the backend, w/ docker or not etc..)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3629#issuecomment-389577205:543,depend,depending,543,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3629#issuecomment-389577205,1,['depend'],['depending']
Integrability,"> I think I've been viewing Singularity & Docker as more of an ""either/or"" in that perhaps a task would require a singularity container vs a docker container - but if that's not really the case I've definitely been overcomplicating the matter. I'll admit that I've never been comfortable in my understanding of Singularity. If you are using a container, it definitely is an ""either / or"" in the sense that getting one working inside the other is pretty challenging. The reason a Dockerized cromwell doesn't work on a host (to submit jobs to other docker or singularity containers) is because of having the docker/singularity submit come from inside the container. We don't really want to do that anyway, because there is a double dependency. But on the other hand, we want to provide reproducible solutions, meaning that things are container based. In an ideal setup, I would have some (still container based) cromwell acting as more of a docker-compose setup, and issuing commands to other containers. Ideally there would be one maintained Docker container for a step in a pipeline, and then if it's run on an HPC resource (where you can't have docker) it would just be dumped into singularity (`docker://<username>/<reponame>`). But this case is a little different - I'm just talking about the cromwell ""plugin"". I don't actually understand why this is necessary, at least given that singularity containers can act like executable. If I want to run a python script, I run it in the command section, as an executable. I don't require a python plugin. Now given that Singularity changes so that we want to take advantage of more of the instance commands (e.g., we can start, stop, get a status) this might make it more like docker and warrant a plugin. But for now, it's not quite there, and making a plugin would just be a really fancy interface to run an executable. Does this make sense?. > @vsoch you're obviously well versed in all things Singularity - do you see any utility to defining the use ",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685:730,depend,dependency,730,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-412171685,1,['depend'],['dependency']
Integrability,"> I'm almost wondering if they should be wrapped in distinct ""root"" and ""possibly not root"" types. Done, plus a squash and rebase. Will merge after tests pass unless the new types aren't what you were imagining.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4195#issuecomment-426398048:41,wrap,wrapped,41,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4195#issuecomment-426398048,1,['wrap'],['wrapped']
Integrability,"> I'm starting to wonder if it would be easier for me to just write out every CREATE statement to generate the current tables. I'd prefer to use liquibase syntax as much as possible, versus [custom crafted SQL](https://www.liquibase.org/documentation/changes/sql.html). > do you have a preference for 1) trying to make the current migrations work for Postgres too (without breaking the MD5s), or 2) make all existing migrations non-Postgres and add a single comprehensive Postgres-specific migration?. Of the two, I think it would be fantastic if we could do ""1)"". Minimum requirements are that existing MySQL users can startup cromwell w/o a liquibase error. Ultimately, if you can get updated changelogs that actually don't cause collisions with existing MD5s for those populated databases that's one avenue that might work. If not, and ""2)"" is uglier but doesn't break things for MySQL, then so be it. Side note: I suspect the existing Java/Scala changelogs can be a no-op / skip, assuming that anyone using Postgres will not need to migrate data for those specific changes. I believe we skipped those Java/Scala migrations for the in-memory HSQLDB instances. Also, you didn't ask, but in my dream world Cromwell would have changelogs that:; - Use liquibase syntax vs. sql as much as possible; - Work for a new database; - Work for all old/populated databases; - Work for HSQLDB + MySQL + PostgreSQL + MariaDB; - Can be updated to add other databases if/when our [Slick](http://slick.lightbend.com/doc/3.2.3/supported-databases.html) calls work or cromwell switches to another SQL adapter. To get to that last point I've wondered how one would best handle the liquibase MD5 issue in the future, either suppressing the warnings and / or resetting the MD5s as needed. **TL;DR Try 1), but as long as populated MySQL databases still startup with cromwell you're good!**",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475371156:1584,adapter,adapter,1584,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4759#issuecomment-475371156,1,['adapter'],['adapter']
Integrability,"> If you are calling Cromwell in run mode, can you wrap it in a script followed by an AWS CLI command to copy the workflow log?. Yes -- I could do that. I was just thinking that workflow log should be automatically copied over since this is a cromwell workflow level log? But if it's too much to implement, the workaround will be just manually copying over it at the end of the workflow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4446#issuecomment-450916617:51,wrap,wrap,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4446#issuecomment-450916617,1,['wrap'],['wrap']
Integrability,"> In most cases, the `-branch` build has a significantly shorter runtime than the `-pr` build: https://app.circleci.com/pipelines/github/broadinstitute/cromwell/367/workflows/7b1a2a51-80b7-432a-b883-4c28c15741d4. Is the `-branch` build doing the right thing?. Yes, it mimics current behaviour we have for Travis - for branch builds we actually only run sbt tests and other tests just succeed fast without actually running anything, unless there's a `[force ci]` in the commit message",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776210901:476,message,message,476,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6181#issuecomment-776210901,1,['message'],['message']
Integrability,"> Is this because Pipelines API version 1 does not support buckets with requester pays? If so, why cannot Cromwell just say so?. Granted it's not in the error message itself, but the [page I linked](https://cromwell.readthedocs.io/en/stable/filesystems/GoogleCloudStorage/#requester-pays) states. > Pipelines API version 1 does not support buckets with requester pays, so while Cromwell itself might be able to access bucket with RP, jobs running on Pipelines API V1 with file inputs and / or outputs will not work. Pipelines API v1 is deprecated by Google and documentation for it is not maintained; new projects should always use v2. ---. As for the `gcloud` issue I've never done this particular operation personally, but I suspect you may have luck looking at the GCP docs or Stack Overflow. You could opt for [Terra](https://app.terra.bio/) which is basically a fully managed version of Cromwell (it configures Cromwell and all of this project stuff for you). Hope this helps.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424:159,message,message,159,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5594#issuecomment-665264424,1,['message'],['message']
Integrability,"> It might be nice to doublecheck in the `newFileSystem` where the `put` happens instead of this one caller since that would make all code paths threadsafe, but I'm not sure if that would introduce any other issues. I agree. But the reason I didn't do this is that `newFileSystem` method has another logic for the case when filesystem with such key already exists - it throws exception instead of just returning the existing filesystem. And this is a contract stated in the core `FileSystemProvider` abstract class.; But I think I can do some refactoring to overcome this problem.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5328#issuecomment-566167823:451,contract,contract,451,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5328#issuecomment-566167823,1,['contract'],['contract']
Integrability,"> Looks good, what were the results of running this to the point that Cromwell did not return a successful response?. It either fails with OOM or becomes totally unresponsive for a long time, while writing different kinds of timeout messages to the log (like ""timeout while trying to fetch new workflows"" or something like that).; Regarding number of rows, I remember that it handled 1.500.000 easily (carbonited within minute or two). I didn't look for precise upper bound, but I think that for 15.000.000 it was failing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638307488:233,message,messages,233,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5525#issuecomment-638307488,1,['message'],['messages']
Integrability,"> Lot's of good stuff here on first glance. I'll dive deeper over the weekend. ok!. > For better or worse, depending on pricing, support, reliability, etc. etc. etc. we like to move around our CI. I personally also like being able to test scripts on my laptop as much as possible. Yes we definitely can! See my comment above - it just is above moving the little snippet where the test actually happens from a command block to running a script from that same command block. > To that end, I'm trying to advocate for bash scripts that are then invoked from whatever CI we choose. +1!. > I haven't RTFM'ed enough of this PR nor CircleCI's manual yet to fully grasp what specific Circle features are being used here. Could a lot of the logic be separated from the .circleci/config.yml into a script, or multiple scripts if necessary?. I think the part we would want to take out are the testing commands, just executed via some primary file (that calls the individual ones, and which could be run on a host). > On a related note, based on your expertise I may want to pick your brain to go over our existing CI scripts too as we move to Circle, or perhaps something even shinier newer. Sure! I'm always around :). > Re your build failing: it wasn't anything in your PR. Based on the logs there was a weird connection issue between Travis and Github returning HTTP 5xx errors during the tests. Yeah, I've unfortunately been there :P Good luck this weekend!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-414000388:107,depend,depending,107,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4015#issuecomment-414000388,1,['depend'],['depending']
Integrability,"> Should we include a backwards compatibility adapter so that existing configuration still work? It would be sad if someone upgraded Cromwell without reading the release noted and suddenly they have no (recognized) backends configured. I'm not sure about that, since with `v2beta` there was introduced an additional mandatory parameter: `location` (currently supported `us-central1` and `europe-west2`). If we decide to coerce configured v2alpha1 actor factory to v2beta, then we'll also have choose some default value for `location`, which I'd be cautious to do due to possible issues (e.g., someone runs Cromwell in Europe and doesn't want their data to leave Europe)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715:46,adapter,adapter,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579385715,1,['adapter'],['adapter']
Integrability,"> So if I fix that in my conf, the messages should go away,; right?. Yes. > Can I specify docker.hash-lookup.method in the workflow_options?. No, if that's something you'd like feel free to create a github issue :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321876895:35,message,messages,35,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2538#issuecomment-321876895,1,['message'],['messages']
Integrability,"> So was the error report useful? Is there anything else I need to provide? Thank you for replying sooo quickly! -Giulio. Yes, I've heard about people still having this issue but you are the first to provide the full error message (and also provide confirmation by being another data point). I'll try to my fix idea of shortening the pattern in the regular release, if you're not likely to repro I won't do the custom JAR.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760333655:223,message,message,223,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/6154#issuecomment-760333655,1,['message'],['message']
Integrability,"> TOL2: is it worth another ad-hoc hash/UUID here to connect the ""sending"" and ""result"" messages?. But there is hash in there:; `logger.info(s""Failed to execute GCS Batch request $batchCommandsHash"", failure)`. Or do you mean something else?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900:88,message,messages,88,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5989#issuecomment-718246900,1,['message'],['messages']
Integrability,"> TOL: To me this feels like itâ€™d be way neater if the EGIN had a field or def called inputFileName nameInInputSet, to encapsulate all this into the egin itself rather than having to add it later externally?. Good point, I was just reticent to the idea of jamming yet another attribute injected by the language that will only ever be used once during the lifetime of the workflow but I agree it's still neater. > FWIW in my ideal world weâ€™d have pluggable languages which should only need to define one function like â€œreadWorkflowIntoWom(content: String, l: Set[ImportResolver]): WomExecutableâ€ and everything else would be included/encapsulated in that result. Yes but there would be some non-DRYness by having each language implement entirely how they ingest inputs (most of the logic is the same), plus having it in WOM guarantees that all EGIN are handled the same way w.r.t coercion, validation etc.. It could use some refactoring though I agree",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2988#issuecomment-349344402:286,inject,injected,286,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2988#issuecomment-349344402,1,['inject'],['injected']
Integrability,"> The `singleWorkflowRunner` and `dockerDeadlock` sub-builds both failed on the last PR run. I was seeing weird errors with `dockerDeadlock` on my builds yesterday that I eventually got past by restarting the builds, but the `singleWorkflowRunner` errors look more suspicious to me. The problem was caused by the fact that singleWorkflowRunner tests rely on application's log messages for validation and the first fix attempt broke logging: `CromwellEntryPoint.buildCromwellSystem` was calling `initLogging` method to tamper with system properties before logback initialization. Then I moved `validateRunArguments` call to happen before the `buildCromwellSubsystem`, but turned out that `validateRunArguments` triggered logback initialization before system properties have been modified, thus making logback misconfigured.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556:376,message,messages,376,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5236#issuecomment-544104556,1,['message'],['messages']
Integrability,"> The messages are logging the size of the list being (re-)added to the BatchRequest, not what's inside the possibly stale ArrayList inside the BatchRequest object. Yeah okay maybe don't mention that then since it will force those future maintainers to imagine what was happening before this variable became a local...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800620139:6,message,messages,6,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6218#issuecomment-800620139,1,['message'],['messages']
Integrability,"> There is one I'm having trouble googling a fix for. I can't figure out how to shut off PostgreSQL exceptions printing possibly sensitive row contents via their messages. I wouldn't be surprised if this is baked into the JDBC layer. We could try something like this:; ```; diff --git a/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala b/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; index 5d28cf1..5b0e227 100644; --- a/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; +++ b/database/sql/src/main/scala/cromwell/database/slick/SlickDatabase.scala; @@ -11,6 +11,7 @@ import net.ceedubs.ficus.Ficus._; import org.slf4j.LoggerFactory; import slick.basic.DatabaseConfig; import slick.jdbc.{JdbcCapabilities, JdbcProfile, TransactionIsolation}; +import org.postgresql.util.{PSQLException, ServerErrorMessage}. import scala.concurrent.duration._; import scala.concurrent.{Await, ExecutionContext, Future}; @@ -199,6 +200,8 @@ abstract class SlickDatabase(override val originalDatabaseConfig: Config) extend; case _ => /* keep going */; }; throw rollbackException; + case pe: PSQLException =>; + throw new PSQLException(new ServerErrorMessage(s""Oh no, a postgres error occurred! ${pe.getMessage}"")); }; }(actionExecutionContext); }; ```; only with some on-the-fly modification of the error message instead of my dummy string. This compiles for me, but I'm not sure how to test it.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504487606:162,message,messages,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4919#issuecomment-504487606,2,['message'],"['message', 'messages']"
Integrability,"> This can happen if the job fails meaning that an rc.txt file isnâ€™t created. It would be worth looking at the CloudWatch log for the batch job.; > [â€¦](#); > On Tue, Jul 21, 2020 at 4:07 PM Sri Paladugu ***@***.***> wrote: Is there any progress on this issue? I am the getting the following exception: IOException: Could not read from s3:///results/ReadFile/5fec5c4a-2e3f-49ed-8f9e-6d9d2d759449/call-read_file/read_file-rc.txt Caused by: java.nio.file.NoSuchFileException: s3:// s3.amazonaws.com/s3bucketname/results/ReadFile/5fec5c4a-2e3f-49ed-8f9e-6d9d2d759449/call-read_file/read_file-rc.txt â€” You are receiving this because you are subscribed to this thread. Reply to this email directly, view it on GitHub <[#4687 (comment)](https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662079379)>, or unsubscribe <https://github.com/notifications/unsubscribe-auth/AF2E6EMJZ66Z5PIAEUX3IBLR4XYPZANCNFSM4G23FFUQ> . Cloudwatch logs contained the following message: ""/bin/bash: /var/scratch/fetch_and_run.sh: Is a directory""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662170952:965,message,message,965,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4687#issuecomment-662170952,1,['message'],['message']
Integrability,"> This change looks safe to me but before merging:; > ; > * As the owners of this code has someone from the BT team(?) cloned this and run it through whatever test(s) are in Travis and/or Jenkins?; Are there some special tests needs to be run for this? If so, no. The tests that were run with the build(travis) passed.; > * Mainly out of curiosity: any idea if the whole AWS backend stopped working where/when/what broke? For example: did the recent dependency upgrades in 68 break something the existing test(s) didn't catch? There wasn't much background in the ticket as to why this fix was suddenly needed, so again just curious.; On EFS backend, the script for each cromwell task gave permission denied error before this fix. It's nothing to do with 68 dependency upgrade. This is caused by changes made for CROM-6682. It affects only the AWS-EFS backend.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094:450,depend,dependency,450,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6431#issuecomment-918183094,2,['depend'],['dependency']
Integrability,"> This fixes the problem at the point of expression evaluation... it seems like it might be easier (and a lot less fiddly?) to do the relative file resolution much earlier, at the point that inputs are being read in to the workflow in the first place. > The ValidatedWomNamespace produced as part of workflow materialization contains a womValueInputs field... I wonder whether performing this mapping as part of creating that validated set of inputs would work?. Great suggestion. I will take a look at this. I can checkout the test case on a new branch and try to hack there. One of the catches will be that this resolving will be backend dependent. In the current situation the input expressions are evaluated first, and after that the inputs are resolved. (This makes sense because input can also be something like `baseDir + ""/my_file.txt""`, which needs to be evaluated). But indeed this could be bypassed by doing this already at the workflow level, before it gets passed down to the task level. I will take a look at this. If it does not work, (or work easily) then I will report back here.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024:640,depend,dependent,640,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5478#issuecomment-618838024,1,['depend'],['dependent']
Integrability,"> What actually gets printed here, do we see a useful error from the underlying HTTP request? (I've made this mistake before...). So we're wrapping the response with a layer of IO, so this was what I was able to come up with to ensure the response information was piped to the log, but let me know if you think there is a better way. I verified locally with the ubuntu example that we get the 404 not found status, as well as the more informational MANIFEST_UNKNOWN body, so I hope thats enough to capture what we're seeing with Quay",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/7135#issuecomment-1550048978:139,wrap,wrapping,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/7135#issuecomment-1550048978,1,['wrap'],['wrapping']
Integrability,"> Why not use something like miniwdl?; > ; > run mode was originally created for **cromwell** development purposes, although for most of time there wasn't really an alternative for workflow development. Hi Geoff thanks for your suggestion. I have checked miniwdl but it has a docker dependency which does not fit my need for a sudo-less installation. The devs will certainly benefit from a quick REPL for WDL.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-599309808:283,depend,dependency,283,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5451#issuecomment-599309808,1,['depend'],['dependency']
Integrability,">If you're ok with waiting until the next release (likely 31, potentially 30.3), it'll also be fixed for you. Everything depends on how soon you are agoing to publish 30.3 If it is a week, I am ok to wait, if it will take longer - I will build from source",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367445273:121,depend,depends,121,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3306#issuecomment-367445273,1,['depend'],['depends']
Integrability,">The Guava in Cromwell appears to have it though, could add that to the dependencies. I think it'd be better/simpler to use Java 9 for this app rather than include another lib in the jar",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-663231595:72,depend,dependencies,72,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5587#issuecomment-663231595,1,['depend'],['dependencies']
Integrability,"@EvanTheB thanks for this report! . I've [added a test](https://github.com/broadinstitute/cromwell/pull/3867) to make sure this check happens during static validation and amended the error message. I'll link this issue so that it gets closed when the PR merges, are you all set with how to fix the problem in your expression?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402842188:189,message,message,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3863#issuecomment-402842188,1,['message'],['message']
Integrability,@Horneth Could you update w/ a description of what's being fixed/improved/etc here and/or link to an issue? I can see the code but need help wrapping my head around what it's all doing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2077#issuecomment-288477222:141,wrap,wrapping,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2077#issuecomment-288477222,1,['wrap'],['wrapping']
Integrability,"@Horneth I believe the transformation is more like `StateData + DB Calls` to `DB Calls + more DB Calls`. Currently, the AbortAll route just sends a msg to all the WorkflowActorRefs in the state data, and then waits's until the all the actors have responded with a terminal state.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201068582:129,rout,route,129,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/606#issuecomment-201068582,1,['rout'],['route']
Integrability,@Horneth I totally missed your message here. It was on the methods cromwell 30 instance. I'm not sure how to find logs. My guess is that if they're produced by default they're still available somewhere.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-398134854:31,message,message,31,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3483#issuecomment-398134854,1,['message'],['message']
Integrability,@Horneth No problem. I *might* scale up to 200k jobs in this workflow. Depends on how many samples I need.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298060547:71,Depend,Depends,71,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2219#issuecomment-298060547,1,['Depend'],['Depends']
Integrability,"@Horneth The more I'm thinking about this I'm transitioning from ""throwing something out there"" to ""advocating"" :). What bothers me about this is that eventually we're going to want workflow submission also go through the same validation actor instead of doing it in multiple ways. Typically you want actor ownership to be hierarchical in nature but this way you'd have multiple parent types. Something to consider - what happens if a VA throws an exception under this model? You'll now need to have supervision code in multiple places. More abstractly what will be said is that validation is A Thing, but needs to exist independently in multiple places - if that's the case it should be pulled out into its own block. Furthermore the validation actor should be the sort of actor which is perfect for being its own concept - it's a completely idempotent, stateless operation. Work gets sent to it, it process the work and responds to the querier. My point about supervision is that by Right Now defining A Validation Actor (presumably owned by the kernel) what you're effectively doing is defining a validation interface. You can change how things are implemented in the future (e.g. it's really a bunch of VAs, it's firing up ephemeral VAs, whatever) and not need to change any code throughout the rest of the system as everything is still talking to the same actorRef that they were before. Alternatively if validation actors are being spun up on demand in multiple places and we decide that we somehow need to handle VAs in a special manner, it'll be a larger refactor. All that said, at the moment only the validation webservice is talking to the VA. I'm happy to shelve this until when something else is talking to VAs.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195419168:1111,interface,interface,1111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195419168,1,['interface'],['interface']
Integrability,"@Horneth There are certainly tradeoffs and I don't disagree w/ what you said. However consider the flip side - by defining A Validation Actor you're allowing for more granular control over performance and fault tolerance down the road, e.g. you could replace it with a router talking to a bank of VAs and doing load balancing, fiddle with its own threadpool, provide validation specific supervision in case of error, etc.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195399500:269,rout,router,269,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/548#issuecomment-195399500,1,['rout'],['router']
Integrability,"@Horneth check out DSDEEPB-2876. Having thought about this and having seen that new issue in our backlog (Add â€œreason(s) for failureâ€ to database, metadata) I think this would be better reported as a stringly-typed ""reason for failure"" error message rather than an explicit ""backend return code"" field. It would then be the job of the backend to convert its ""backend error code"" into a useable and sensible message to report.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184753747:242,message,message,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184753747,2,['message'],['message']
Integrability,@Horneth commented on [Mon Sep 11 2017](https://github.com/broadinstitute/wdl4s/issues/195). WOM currently uses the WDL CommandPart which contains WDL specific constructs (WdlFunctions); WOM should get its own CommandPart.; Also factor in the fact that tweaks will have to be made in the way the command is instantiated depending on the language or even language configuration (this might be better put in the TaskDefinition instead though),MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2717:320,depend,depending,320,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2717,1,['depend'],['depending']
Integrability,"@Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48). ---. @meganshand commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261072803). The attached wdl results in an error message:. `Workflow has invalid declarations: : AggregatedException: : VariableNotFoundException: Variable 'generateArray' not found`. [scratch_3.wdl.txt](https://github.com/broadinstitute/wdl4s/files/595927/scratch_3.wdl.txt). ---. @Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261089538). @meganshand . This is actually a different problem - Cromwell doesn't support (yet) Workflow Declarations that reference call outputs. This wouldn't work either:. ```; task t {; command {; echo ""hello""; }; output {; String o = read_string(stdout()); }; }. workflow w {; call t; String declarationDependingOnCallOutput = t.o; }; ```. ---. @meganshand commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261092137). Oh no! This actually makes using zips infeasible, since I'd imagine in most cases the things you want to zip will be outputs from previous tasks. I suppose I can use a workaround where inside of a scatter loop I can create a task that takes in a File and Array[File] and outputs a Pair, then scatter over the output of that task outside of the original scatter. ---. @meganshand commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48#issuecomment-261095003). I tried that workaround with a task like this:. ```; task ZipUpWorkaround {; File unmapped_bam; Array[File] fastqs. command {; #do nothing; }; output {; Pair[File, Array[File]] p = [unmapped_bam, fastqs]; }; }; ```. and got this error message (after it submitted that task):; `Failed to evaluate outputs.: WdlTypeException: Arrays/Maps must have homogeneous types`. ---. @Horneth commented on [Wed Nov 16 2016](https://github.com/broadinstitute/wdl4s/issues/48",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2692:252,message,message,252,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2692,1,['message'],['message']
Integrability,"@Horneth depends when the `Any` coercions happen. If they only happen reading the inputs files then it's actually already separated out I think (and ""coercion"" and ""input parsing"" could easily be separate functions IMO)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3430#issuecomment-373823553:9,depend,depends,9,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3430#issuecomment-373823553,1,['depend'],['depends']
Integrability,"@Horneth my concern on `endpoint` is its overloaded with the REST API endpoints. Realizing that it's also not an **akka** router, it seems to be serving a similar role. Proxy also seems viable (granted you're using that term in your description, but not in the code)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3091#issuecomment-354340319:122,rout,router,122,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3091#issuecomment-354340319,1,['rout'],['router']
Integrability,"@Horneth my suggestion for this PR (happy to be overruled - @geoffjentry @kcibul) would be to get the return code being ONLY the return code. And forget the ""backend return code"" entirely for now until that upcoming ticket (maybe enhance that other ticket to add ""JES return codes appear in failure message"" as another AC?)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184760682:299,message,message,299,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/447#issuecomment-184760682,1,['message'],['message']
Integrability,"@Horneth what do you think the effort would be to add retries to the WDL functions? Does it depend on the function? This might get prioritized as a part of Joint Calling, but for now I'll leave it out of the retries improvement.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288756294:92,depend,depend,92,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2084#issuecomment-288756294,1,['depend'],['depend']
Integrability,"@IsanEmory thanks for the update, that's good to know. ; @ruchim can you tell me a bit more about what's going on to cause the message to show up?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2340#issuecomment-332239786:127,message,message,127,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2340#issuecomment-332239786,1,['message'],['message']
Integrability,"@LeeTL1220 ; - Is this .21, .22 or develop?; - Did you capture the 'CMD \' thread dump from the JVM?; - Was there an error message from Cromwell before it got stuck?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258434307:123,message,message,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1649#issuecomment-258434307,1,['message'],['message']
Integrability,@MartonKN This is almost certainly not a real error but rather some annoying/alarming yet harmless Cromwell messages. Other than this does it appear that your workflow successfully completed?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388891613:108,message,messages,108,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3618#issuecomment-388891613,1,['message'],['messages']
Integrability,"@TMiguelT . I separated out the part that is failing into a separate WDL and tried running just that WDL with different inputs, it all failed with the same error message. The WDL is attached so that you can just run to see the error. [cromwell_4356.zip](https://github.com/broadinstitute/cromwell/files/2555124/cromwell_4356.zip)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436419756:162,message,message,162,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4356#issuecomment-436419756,1,['message'],['message']
Integrability,"@TMiguelT @geoffjentry I've been following the conversation and we're pretty keen to use some container system with Cromwell on our cluster. At the moment I'm trying to use udocker with Cromwell with the following conf, but the docker param [is looked up](https://cromwell.readthedocs.io/en/stable/cromwell_features/CallCaching/#Docker+Tags) and injected as a [digest](https://docs.docker.com/engine/reference/commandline/pull/#pull-an-image-by-digest-immutable-identifier) which udocker [doesn't appear to support](https://github.com/indigo-dc/udocker/issues/112). . ```; backend {; default: udocker; providers: {; udocker {; # The backend custom configuration.; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"". config {. # The list of possible runtime custom attributes.; runtime-attributes = """"""; String? docker; String? docker_user; """""". # Submit string when there is a ""docker"" runtime attribute.; submit-docker = """"""; udocker run \; --rm -i \; ${""--user "" + docker_user} \; # Edit: future Michael here, entrypoint in udocker starts interactive shell so exclude it; #--entrypoint ${job_shell} \; -v ${cwd}:${docker_cwd} \; ${docker} ${script}; """"""; }; }; }; }; ```. which results in the script.submit:; ```bash; udocker run \; --rm -i \; # --entrypoint /bin/bash \ # Edit: Don't include this line it causes interactive shell; -v /path/to/call-untar:/cromwell-executions/path/to/call-untar \; ubuntu@sha256:868fd30a0e47b8d8ac485df174795b5e2fe8a6c8f056cc707b232d65b8a1ab68 \; /cromwell-executions/path/to/call-untar/execution/script; ```. and fails with the error:; ```; Error: invalid repo name syntax; Error: must specify image:tag or repository/image:tag; ```. I can't find some way to disable the docker lookup by Cromwell, nor some non-digest runtime variable that Cromwell exposes. Just wondering how you're achieving this on docker or singularity. . Edit: `entrypoint` in udocker starts interactive shell, suspending the execution of the program.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364:346,inject,injected,346,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4039#issuecomment-454569364,1,['inject'],['injected']
Integrability,"@TMiguelT I looked into this and we are using the latest version of the configuration library, so short of someone submitting a PR to fix their parsing issue, there is not much we can do. Lightbend recommends a linting tool, http://www.hoconlint.com/, which when run on your sample file gives the correct error message!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4549#issuecomment-465173266:311,message,message,311,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4549#issuecomment-465173266,1,['message'],['message']
Integrability,"@TMiguelT for suggesting flock. Together with `singularity exec` I think it can solve this particular use case. The `SINGULARITY_CACHEDIR` environment variable needs to be set to a location on the cluster. Then the following config can work:. ```; SLURM {; actor-factory = ""cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory""; config {; concurrent-job-limit = 200; exit-code-timeout-seconds = 120; # 4G Memory by default; runtime-attributes= """"""; Int cpu = 1; Int? memory; String? docker; Int time_minutes = 120; """"""; submit-docker = """"""; # Singularity pull image. ; if [ -z $SINGULARITY_CACHEDIR ]; ; then CACHE_DIR=$HOME/singularity/cache; else CACHE_DIR=$SINGULARITY_CACHEDIR; fi; mkdir -p $CACHE_DIR; LOCK_FILE=$CACHE_DIR/singularity_pull_flock; # flock should work as this is executed at the same node as cromwell.; flock --verbose --exclusive --timeout 900 $LOCK_FILE singularity exec --containall docker://${docker} echo ""succesfully pulled ${docker}!"". # Partition selection; PARTITION=all; MEMORY=${default=""4294967296"" memory}; if [ ${time_minutes} -lt 60 ]; then PARTITION=short; fi; if [ $MEMORY -gt 107374182400 ] ; then PARTITION=highmem ; fi. # Job submission; sbatch \; --partition=$PARTITION \; --job-name=""${job_name}"" \; --chdir=""${cwd}"" \; --time=""${time_minutes}"" \; --cpus-per-task=""${cpu}"" \; --mem=$(echo ""$MEMORY / 1024^2"" | bc) \; --output=""${out}"" \; --error=""${err}"" \; --wrap \; 'singularity exec --containall --bind /shared_cluster_dir,${cwd}:${docker_cwd} docker://${docker} sh ${script}; rc=$?; if [ ! -f ${cwd}/execution/rc ]; then; echo ""$rc"" > ${cwd}/execution/rc; fi'; """"""; kill = ""scancel ${job_id}""; kill-docker = ""scancel ${job_id}""; check-alive = ""squeue -j ${job_id}""; job-id-regex = ""Submitted batch job (\\d+).*""; ``` . EDIT: I changed the config. Instead of using multiple locks (one lock per image) there is now one universal lock. This is because pulling two images at the same time that have a shared layer might also corrupt the cache.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430:1954,wrap,wrap,1954,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5063#issuecomment-627379430,1,['wrap'],['wrap']
Integrability,"@adamstruck I still need to do a more in depth review if you're looking for scala syntax feedback (ex: `case â€¦ => { â€¦ }` could be `case â€¦ => â€¦`). Early feedback:. - PR 1930 contains a few more changes to the standard backend api. I tested cherry-picking 1930 onto this PR to see what would be left to patch up. Using an ""Obsolete"" set of bridge code for now, [these](https://github.com/kshakir/cromwell/commit/19f3bad4ca752ac47ab6f37b694dbdaec8850b36) are the minimal changes for the updated path api, plus changes for standardized command line generation. NOTE: 1930 is still under review and may change, plus the linked github commit will be deleted once these two PRs are merged. - The standard backend will continue to change for a while as we move more common code. For example, the script generation for globs is now centralized as of PR 1930. The only CI testing I am aware of at the moment is `sbt tesBackend/test` that runs under travis. Is there a dockerized solution yet for TES that we could use with travis centaur, like we have for JES and Local? Otherwise, the minimal patches above pass the very, very basic sbt test unit tests. - Your PR is ok as is, but I need to think about necessity of `Async.await` a bit more. The standard backend api is synchronous, requiring the `Async.await`. But the underlying ""basic"" backend trait is using scala futures, and I need more insight into how those are interacting with the akka actors. For example, I wouldn't want the actors receiving multiple akka poll messages in the mailbox and then queuing up dozens of overlapping poll futures in the thread pool. I also really like that your awaits have timeouts and aren't infinite futures. More to come. Thanks!",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295:338,bridg,bridge,338,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1816#issuecomment-276378295,2,"['bridg', 'message']","['bridge', 'messages']"
Integrability,"@aednichols . I implemented the changes. The lock should have a negligible performance impact. It only locks when cached-copy strategy is used, threads which use other strategies are not blocked by this. (This was already true before the changes). A dictionary is used to keep track which files are being copied within the process. Filesystem lock files are too slow for this because during scatters cromwell creates a lot of threads that need exactly the same file at the same time. . A lock file is now used so other cromwell processes know the file is being copied. The chances of two processes needing the same file at exactly the same time is negligible, so lock files are fast enough here. Due to the `synchronized` lock, there can not be race conditions where more than one thread modifies the dictionary and/or creates the lock file at the same time. (The lock is absolutely necessary for cached-copy to work). The amount of time spent in the lock per thread is negligible. . I implemented a waitOnCopy function which lets a thread wait for the locks in the dictionary on the filesystem to clear. This structure allows a lot of paths to be copied to the cache at the same time. So the copying is still multithreaded, maintaining high performance. Are these changes satisfactory?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703:708,synchroniz,synchronized,708,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-488295703,1,['synchroniz'],['synchronized']
Integrability,"@aednichols . Thanks for your response. We're in a University and the sys admins are worried that with users submitting thousands of jobs, depending on what cromwell actually sends MySQL there may be quite a bit of overhead. Do you have a link to what Cromwell stores in the MySQL database? That may assuage some of their concerns. Using SQLite would just be easier, users can create a local instance and be on with it. @rhpvorderman . That sounds like a workable option. That sounds exactly like our situation, it would be great if you could keep us updated! It would definitely be very useful for us!. Thanks,; Bobbie.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564721678:139,depend,depending,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5271#issuecomment-564721678,1,['depend'],['depending']
Integrability,"@aednichols ; 1. You can either use blessed images or your own! All we need is to integrate the Trivy check that runs on every PR, and in case when it does find Critical vulns, those should be fixed on a 2-week timeline (it doesn't have to block your release, however!); 2. We don't have to have ""many"" checks, but we're currently running it on every Dockerfile in this repo that is deployed to production - please correct me if that's inaccurate. I did notice that a lot of these images are essentially the same (+/- the JAR), so we don't have to scan those twice (so we can just pick a representative one from the set). However, at least one image (**cromwell-drs-localizer**) is different, so that one should be scanned separately. Does that make sense?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6147#issuecomment-758206921:82,integrat,integrate,82,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6147#issuecomment-758206921,1,['integrat'],['integrate']
Integrability,"@aednichols @cjllanwarne, I had looked into adding log message inside `withRetryForTransactionRollback` method before. But I was not able to find a logger class that can be used. But I can take a look at it again. Agreed that having some kind of indication that retrying is happening will be good.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6363#issuecomment-860684404:55,message,message,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6363#issuecomment-860684404,1,['message'],['message']
Integrability,"@aednichols tested that just now. The experience is similar to the ""I don't have git hooks installed"" case (ie see the two final `[error]` messages):; ```; $ sbt compile; [...]; [info] Executing in batch mode. For better performance use sbt's shell; [info] Executing pre-compile script...; [error] You are not running our custom git commit hooks. If you are making changes to the codebase, we recommend doing this (by running 'git config --add core.hooksPath hooks/') to ensure that your cryptographic secrets are not committed to our repository by accident.; [error] If you don't want to set up hooks (if you never intend to commit to the cromwell repo, can be sure that you won't commit secrets by accident, or have already installed git-secrets in this repo separately), you can suppress this error by running with: 'sbt -Dignore-hooks-check=true [...]'; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820:139,message,messages,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5060#issuecomment-510938820,1,['message'],['messages']
Integrability,"@aednichols, rewrote using the ""dependency injection pattern"".",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511603946:32,depend,dependency,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5061#issuecomment-511603946,2,"['depend', 'inject']","['dependency', 'injection']"
Integrability,"@alexagrf Would it be possible to add some tests here? I realize that it can be difficult to do that w/ auth code, so if this seems like a challenge perhaps we can work out a way w/ you to develop some integration tests we could fold into our internal system",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5088#issuecomment-516670868:202,integrat,integration,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5088#issuecomment-516670868,1,['integrat'],['integration']
Integrability,"@aofarrel I recommend setting a [`concurrent-job-limit`](https://cromwell.readthedocs.io/en/stable/backends/Backends/) value of 1 so that Cromwell only starts one job at once. . The interface between Cromwell and its backends is designed so that resource management happens entirely within the backend. As such, Cromwell never knows how much memory/CPU a backend has; rather the backend is expected to start as many jobs as it can safely handle and stop when it reaches the limit. What you're finding is that the local backend, implemented with Docker, doesn't support that self-management because it is a non-goal of the Docker product itself. Docker tries to start whatever containers you request, immediately. Since I _think_ `concurrent-job-limit` should fully address your problem, I am going to close the issue. If that's not the case feel free to reopen.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272:182,interface,interface,182,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2365#issuecomment-803082272,1,['interface'],['interface']
Integrability,"@buchanae Looks like it's still failing, looking at the error message seems like it gets the path to the config file wrong. Should it be `--config` instead of `-config` ?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999:62,message,message,62,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2116#issuecomment-290765999,1,['message'],['message']
Integrability,"@cahrens Yes you are right. If we could somehow add the list of files that were deleted in that message or some other log message, in my opinion, it would be useful for debugging to figure out which files being deleted were associated with which workflow. But if you think it could be a lot of messages to add then I am fine with not adding it ðŸ‘",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934765674:96,message,message,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/6538#issuecomment-934765674,3,['message'],"['message', 'messages']"
Integrability,"@ccarrizo A call wraps a single command line and thus can only be run on a single backend, correct? Composed backends wouldn't apply for a call. We have a need to be able to report back certain backend specific values and from an operational/regulatory perspective we need to track other things for provenance and such. That information needs to be persisted somewhere - should the backends know about the DB (let's ignore for them oment that one backend does indeed talk directly to the DB)? If not that implies now need to know about the internal persistence. What if we decide to bulkhead all DB access behind a specialized actor/router? You can point to separating out the persistence but that's far beyond the scope of this work and is a separate block entirely. I viewed this as a transitional PR. There'd been a request from team members on our side for a while now that things be done as bit by bit as possible. To do that implies not having full & sweeping changes as the only way to do _that_ is one monolithic PR which causes a ton of problems. The whole process would be a bit smoother if you all could find other portions which could be folded directly into develop _now_ which started to move the ball in the right direction instead of requiring that everything be perfect in one fell swoop.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-182388303:17,wrap,wraps,17,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/427#issuecomment-182388303,2,"['rout', 'wrap']","['router', 'wraps']"
Integrability,"@cjllanwarne :. > Everything has access to the ServiceRegistry. This premise still holds. Why do you think otherwise?. > Anyone (including the Engine) can send a MetadataPut message to the ServiceRegistry, which will forward it automagically to the MetadataService. This is how the magic is currently happening as well. The engine _is_ sending MetadataPut messages to the ServiceRegistry.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219110124:174,message,message,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219110124,2,['message'],"['message', 'messages']"
Integrability,"@cjllanwarne @ruchim I've been going back and forth as to whether or not this retry count should replace or add on to the ""we know that's flakey so retried it for ya"" situation vs. adding on top of it. Chris, you seemed pretty sure it should be the latter, why do you view it that way? Ruchi, I know this was spec'd out to go the former route, but I think the person speccing it out didn't know that we already did some stuff - do you actually have an opinion here or is this just how you did it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3490#issuecomment-379372182:337,rout,route,337,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3490#issuecomment-379372182,1,['rout'],['route']
Integrability,"@cjllanwarne As you mentioned ""order is defined in WDL and DAG gets created based on inter-dependencies"" is true during execution of the task but not during initialization phase, hence that order is required during initialization as well. . @geoffjentry If i understand correctly there is no graph being received by the backend during initialization it is `Map[Call, String]` to identifies all calls @ backend and then ultimately `Seq[Call]`, what is expected from engine if user specifies calls as @cjllanwarne mentioned in above wdl example (a then b) that order is preserved and pass through. . RP backend in CCC depends upon order of calls during initialization as well therefore that change is required.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235959600:91,depend,dependencies,91,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235959600,2,['depend'],"['dependencies', 'depends']"
Integrability,"@cjllanwarne Do you need a red review on this ? If so could you elaborate just a little on what ; > Allows reading of WDL 1.0 and 1.1 Asts through a shared set of CheckedAtoB functions, with the flexibility to inject different transform behavior into each usage of the instantiations of the transforms. means for me poor mortal and / or point to relevant code that I should look at ? ðŸ˜„",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3852#issuecomment-402194081:210,inject,inject,210,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3852#issuecomment-402194081,1,['inject'],['inject']
Integrability,"@cjllanwarne GCS is not backend specific, which is actually what makes it possible to use GCS on local backend.; We could extract the ""File system logic"" from the WorkflowDescriptor but anything related to GCS will depend on a workflow descriptor because of all the auth stuff",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170675299:215,depend,depend,215,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/361#issuecomment-170675299,1,['depend'],['depend']
Integrability,"@cjllanwarne Hey Chris..in reference to your question from the previous PR.. checkout the Backend contract defined [here](https://github.com/broadinstitute/cromwell/blob/pluggable_backends/cromwell-backend/src/main/scala/cromwell/backend/BackendActor.scala). We'll need the individual backends to implement that contract, for instance how it's been done to [LocalBackend](https://github.com/broadinstitute/cromwell/blob/pluggable_backends/cromwell-backend/src/main/scala/cromwell/backend/provider/local/LocalBackend.scala). The older methods, such as adjustInputPaths() etc. can be clubbed together in, say, prepare() step. If you think we'll need more methods that can be defined in the BackendActor which are generic enough, please let us know.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174084987:98,contract,contract,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174084987,2,['contract'],['contract']
Integrability,"@cjllanwarne I agree now that fix will not work, but what i don't see we can perform Topological sort because we have seq of calls instead dependency graph ( Correct me if i am wrong?).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236050063:139,depend,dependency,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236050063,1,['depend'],['dependency']
Integrability,"@cjllanwarne I certainly don't think this needs a unit test for a hotfix. And as there was no unit test added for the introduction of the queue, and many unit and virtually all integration tests exercise this indirectly I'm not sure I see the necessity.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371:177,integrat,integration,177,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5510#issuecomment-624861371,1,['integrat'],['integration']
Integrability,"@cjllanwarne I know you've made some error message improvements, was this one you fixed? Or is it still To-Do?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1933#issuecomment-330627584:43,message,message,43,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1933#issuecomment-330627584,1,['message'],['message']
Integrability,"@cjllanwarne I pointed to the wrong line of code, sorry for that. I have identified the bug. The refactoring produced **better** code. The code written before the refactoring created a rc file with exit code `9`(Which was probably a mistake as the comments above the code said that 137 was chosen, for kill -9). `137` for SIGKILL would have been the better value. The current refactored code uses SIGTERM (`143`). This looks nicer, but unfortunately the functionality of the code depended on the choice for `9`. . If cromwell gets SIGINT (`130`) , SIGKILL (`137`) or SIGTERM(`143`) as exit codes for a job, it assumes that cromwell was the one that aborted them and the jobs should NOT be retried. This makes perfect sense. . The refactored code now returns a return code(`143`) that makes cromwell believe that the job should not be retried. My solution would be to write a non-sensical return code in the case exit-timeout-seconds is used. I am working on a pr now. EDIT: This change indeed fixes the problem. PR coming.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4998#issuecomment-496236589:480,depend,depended,480,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4998#issuecomment-496236589,1,['depend'],['depended']
Integrability,"@cjllanwarne I'm probably misreading the convo but I was reading this to imply that a cromwell-singleton data access object would be getting hit harder from running our unit tests in terms of connections than real life, but in the latter we could conceivably have many thousands of workflows (and thus many, many thousands of tasks) banging on the DB simultaneously. A teensy threadpool isn't going to be able to handle the latter case. Another possibility (which we originally looked at but discarded for non-singleton data access) is to have an actual data access actor, and then that actor can scale horizontally as needed via a router actor. those actors can even be on different machines if CPU load is an issue.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143033225:632,rout,router,632,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/198#issuecomment-143033225,1,['rout'],['router']
Integrability,"@cjllanwarne Indeed different containers might have different requirements, as it will depend on the container what mount points are available. I would like to point out, however, that you can already define this per task using a custom runtime attribute. For example, in my config I could put something like:; ```; runtime-attributes= """"""; String? docker; String? dockerMountPoint = ""/data""; """"""; dockerRoot = ""${dockerMountPoint}""; ```. EDIT: Hmm, nevermind, looks like that wouldn't work. In the submission command, it would, but you'll just end up with `${dockerMountPoint}` in the execution script.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420927684:87,depend,depend,87,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4088#issuecomment-420927684,1,['depend'],['depend']
Integrability,@cjllanwarne Might this have been addressed in the recent (c28?) EJEA/better error messages push? We had some FC reports of similar things that were reported resolved after the FC cromwell was bumped to 28 iirc.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-320523744:83,message,messages,83,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-320523744,1,['message'],['messages']
Integrability,"@cjllanwarne On a high level, you can use Intellij to create a new scala project, add the dependencies (wdl4s and cromwell-backend) in the build.sbt from Broad's artefactory repository, copy the JES code folder from develop to this new project, modify the JesBackend to extend from BackendActor, honor the subsequent intellij warnings to implement the abstract methods, and done. The CallActor will control the flow of the backend, going from prepare() -> execute() -> cleanUp() -> stop() etc..",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174085760:90,depend,dependencies,90,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174085760,1,['depend'],['dependencies']
Integrability,"@cjllanwarne Really it's just that they were developed simultaneously, and it's probably my fault for not going back and cleaning the other one up. In retrospect I knew it existed, but probably just lost track of it. I certainly wouldn't disapprove of converting future-based logic in actors into a more actor-y solution but that's because IMO it's easier to reason about multiple actors (and their messages) than composed Futures. My stance isn't one which is universally held, however",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1004#issuecomment-226012998:399,message,messages,399,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1004#issuecomment-226012998,1,['message'],['messages']
Integrability,"@cjllanwarne So if a runtime attribute is not supported, there is no warning message? What happens instead?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1674#issuecomment-325661331:77,message,message,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1674#issuecomment-325661331,1,['message'],['message']
Integrability,"@cjllanwarne Sorry for the delay in response, I was offline over the Xmas break.; ; > Question: Does the new test actually depend on connecting to external AWS resources? . The short answer is no, it doesn't depend on actually connecting to AWS resources. . However it does require the AWS_REGION environment variable to be set so that the SDK initializes. So if it is run without AWS_REGION the test fails, if it is run with AWS_REGION it works (but doesn't actually connect to AWS). Assuming that your regular CI builds don't set the AWS env variables, it is probably simplest to leave it tagged as an AwsTest.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509:123,depend,depend,123,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5216#issuecomment-575101509,2,['depend'],['depend']
Integrability,"@cjllanwarne Sort does not solve the purpose here because it is not what backend expecting and want,However it is expecting the right dependencies as specified in the wdl. . Because wdl writer may not right alphabetically or numeric order for call name it could be something logical name or else. I still don't get how sorting would work.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236009348:134,depend,dependencies,134,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-236009348,1,['depend'],['dependencies']
Integrability,@cjllanwarne Thanks for the quick response!. I agreed that it seems reasonable to have built-in support for FUSE mounts in Cromwell. Nevertheless this PR can be a neat addition to the existing Google Cloud integration. I've updated the docs with the FUSE filesystem usage limitations as you asked. Looking forward for the review. Thanks.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572980186:206,integrat,integration,206,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5343#issuecomment-572980186,1,['integrat'],['integration']
Integrability,"@cjllanwarne The ""causedBy"" nested thing is weird. I'm also not sure how many different formats there are. There's the ""message""; ```; ""failures"": [{; ""message"": ""Task c386672d-0248-4968-9b1a-114f5f5c4706:echo_files failed: error code 5. Message: 8: Failed to pull image ubuntu:latest: \""docker --config /tmp/.docker/ pull ubuntu:latest\"" failed: exit status 1: Pulling repository docker.io/library/ubuntu\nNetwork timed out while trying to connect to https://index.docker.io/v1/repositories/library/ubuntu/images. You may want to check your internet connection or if you are behind a proxy.\n""; }]; ```; and then there's the ""failure"" and timestamp"" :; ```; ""failures"": [{; ""timestamp"": ""2016-08-01T19:58:04.704000Z"",; ""failure"": ""com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request\n{\n \""code\"" : 400,\n \""errors\"" : [ {\n \""domain\"" : \""global\"",\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""reason\"" : \""badRequest\""\n } ],\n \""message\"" : \""Pipeline 9453747469251135900: Unable to evaluate parameters: %!(EXTRA string=parameter \\\""input_array-0\\\"" has invalid value: bar, baz)\"",\n \""status\"" : \""INVALID_ARGUMENT\""\n}""; }],; ```; and then the caused by: ; ```; ""failures"": [{; ""causedBy"": {; ""causedBy"": {; ""message"": ""connect timed out""; },; ""message"": ""Error getting access token for service account: ""; },; ""message"": ""Failed to upload authentication file""; }]; ```. So, if there are these 3 different ways to show the failures section, I'm not sure if there are more formats that I missed in my cursory examination. My dream is that there would be a consistent format for the failures section that we could reliably programmatically find and display.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064:120,message,message,120,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802064,8,"['Message', 'message']","['Message', 'message']"
Integrability,"@cjllanwarne The problem with not injecting, and creating separate tasks, is that you either have to clone the entire repo again for each of the tasks to extract the version information etc.., or clone it once and pass around the execution dir of the corresponding task which is even more horrible IMO.; Unless there's another way I'm missing",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-286472793:34,inject,injecting,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-286472793,1,['inject'],['injecting']
Integrability,"@cjllanwarne There's nothing stopping you from submitting a future PR proposing the changes you describe. @kshakir Don't forget about akka streams, which sit in between futures and actors on the generalized concurrency spectrum. . I recognized the URL of that blog post, I'll say that it has surprisingly useful comments at the end as well. Personally I find it a lot easier to reason about actors & messages than futures but that's not true for everyone.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226577404:400,message,messages,400,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1010#issuecomment-226577404,1,['message'],['messages']
Integrability,"@cjllanwarne Yes, but for example Workflow store has a dependency on WorkflowStoreSqlDatabase trait, so that means you can not use a NoSQLDatabase impl.; In order to allow WorkflowStore to do that you will need to implement a generic interface to work with different specializations of dbs and for that you will need to define a DAL and perform a bigger refactoring.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237936563:55,depend,dependency,55,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1254#issuecomment-237936563,2,"['depend', 'interface']","['dependency', 'interface']"
Integrability,"@cjllanwarne commented on [Fri Sep 15 2017](https://github.com/broadinstitute/wdl4s/issues/217). EG this can be converted from WDL to WOM:; ```; import ""import_me.wdl"" as import_me. workflow outer {; ; Array[Int] xs; scatter (x in xs) {; 	Boolean b; if (b) {; call import_me.inner as inner { input: i = x }; }; }; output {; Array[String?] outer_out = inner.out; }; }; ```. But if we move the `b` outside the scatter:; ```; import ""import_me.wdl"" as import_me. workflow outer {; Boolean b; Array[Int] xs; scatter (x in xs) {; if (b) {; call import_me.inner as inner { input: i = x }; }; }; output {; Array[String?] outer_out = inner.out; }; }; ```. Then we get an error:; ```; Exception in thread ""main"" java.lang.Exception: Can't build WOM executable from WDL namespace:; No input b found evaluating inputs for expression b; key not found: b; ```. ---. @Horneth commented on [Mon Sep 18 2017](https://github.com/broadinstitute/wdl4s/issues/217#issuecomment-330214878). This has implications in Cromwell. Namely if `b` was a Call instead of being a boolean, and `import_me.inner` depended on an output of `b`, when we evaluate the inputs of `import_me.inner` it will make a difference whether or not `b` is a sibling of `import_me.inner`. If it is we want to get the output with the same shard number from the output store, otherwise the output with no index (if we rule out nested scatters). We could simplify and say ""always look for the same index and if it's not there take the output with no index"" but it would be better to know for sure which one we need. ---. @mcovarr commented on [Fri Sep 22 2017](https://github.com/broadinstitute/wdl4s/issues/217#issuecomment-331568932). Sorry if this is a dumb question, but do you understand what's going wrong here? It's obvious looking at this statically what `b` is supposed to be whether it's inside or outside the scatter. Also it seems a little weird to me that `b` can even be a `GraphInputNode` inside the scatter...",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2724:1079,depend,depended,1079,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2724,1,['depend'],['depended']
Integrability,"@cjllanwarne new lines adjusted, so are my intellij settings thanks to Jose. I'm thinking the integrationTestCases should run weekly instead of nightly even. @jsotobroad as the original creator of the integrationTestCases -- does that sound okay?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352044523:94,integrat,integrationTestCases,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/3066#issuecomment-352044523,2,['integrat'],['integrationTestCases']
Integrability,@cjllanwarne the goal here is to move all backend-specific logic off the `BackendCall` and into the `Backend`. This is an evolutionary step that maintains the same `BackendCall` interface but delegates all meaningful work directly or indirectly to the backend. When this process is complete the `BackendCall` will become useless and methods can become parameterized by `JobDescriptor` instead. The combination of `JobDescriptor` and `Backend` should be able to accomplish any call-level work in the system.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/509#issuecomment-193368423:178,interface,interface,178,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/509#issuecomment-193368423,1,['interface'],['interface']
Integrability,"@cjllanwarne yes, comments were hidden due to file name change.; Ans 1. Adding hidden comment I did: Caching functionality is missing here. Shouldn't each backend implement caching and when engine ask for a jobExecutor, backend may return BackendCachedJobExecutor?; Doing that we can get rid of the engine responsibility to deal with cached data...; IMO, Cache should be encapsulated in each backend. The only thing I'm not sure if we should expose a standard message to force not to use cached data. So with that you tell to each backend to not use cached data but instead to process data again.; Ans 2. I'm not seeing any new msg for WorkflowBackendActor right now. That will depend on the UCs... for CCC backend those msgs are OK.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200953495:460,message,message,460,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200953495,2,"['depend', 'message']","['depend', 'message']"
Integrability,"@cjllanwarne you're good, thanks ðŸ™‚ The wording on the GitHub message made it sound like two reviews were missing but it was actually only one.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4632#issuecomment-469849765:61,message,message,61,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4632#issuecomment-469849765,1,['message'],['message']
Integrability,"@cjllanwarne. Yes, flattening the messages would definitely make things better. The other thing to address is the ""timestamp"" and ""failure"" format shown in my previous comment.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802926:34,message,messages,34,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2037#issuecomment-282802926,1,['message'],['messages']
Integrability,"@cpavanrun Your right here, but this can be improved. What can be done here is lower the number of parallel jobs submitted by cromwell. This depends really on the cluster. In our case I did a stress test with 10000 parallel jobs and it still acts fine. Only downside is that the log is getting spammed a bit but it still works like it should. Still in the past (on older hardware) the headnode could not deal with this number of jobs. If this is the case limiting the parallel jobs could be a fix.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425029773:141,depend,depends,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-425029773,1,['depend'],['depends']
Integrability,@danbills Could you give more information about the test cases you see for this issue? I thought about using [gatling-akka](https://github.com/chatwork/gatling-akka) in order to send a lot of messages to `WriteMetadataActor` and `MetadataSummaryRefreshActor`.; Do you thing this approach is suitable for this problem?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4762#issuecomment-522923560:192,message,messages,192,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4762#issuecomment-522923560,1,['message'],['messages']
Integrability,@danbills Sure. the point is that there's a built in way to handle it and we should be doing that instead of our ad hoc method of having some catch all on every `receive` method throughout the system that are at best logging a message and potentially slightly changing the stacktrace. . We should remove those catch alls and use the built in capabilities.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-467168303:227,message,message,227,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-467168303,1,['message'],['message']
Integrability,"@danbills The Orchestrator pattern as described above is what we discussed. . Per your other questions, the answer is that AWS Batch does not take a array of arbitrary scripts as an option, nor can you override a Docker container's `ENTRY_POINT` to supply your own script if the entry point of the container has been changed from the default shell. You can only specify an array to pass into Docker daemon's `CMD`. Speaking of default shells, the other arguments against a set of shell scripts is that it limits the set of containers that can be called from a WDL. For example, the current Cromwell scripts that are injected into the container assume Bash support, but by default Alpine Linux (and many containers that build off of it) do not have Bash installed. . Most of the time the above two items are safe assumptions, but not always, hence the current plan to implement data staging via a sibling container approach similar to how CI systems are deployed today. For inspiration, I refer to [Dave Hein's excellent article on running sibling containers in lieu of docker-in-docker](https://www.develves.net/blogs/asd/2016-05-27-alternative-to-docker-in-docker/)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-400025987:616,inject,injected,616,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3804#issuecomment-400025987,1,['inject'],['injected']
Integrability,"@danbills commented on [Fri Sep 15 2017](https://github.com/broadinstitute/wdl4s/issues/216). We have existing tests (in ExportCwlSamplesSpec) that are testing output as previously implemented, yet library collisions forced us to remove that capability. If and when [circe-yaml](https://github.com/circe/circe-yaml) updates their cats dependency we could use that.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2725:335,depend,dependency,335,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2725,1,['depend'],['dependency']
Integrability,"@danxmoran I'm trying to recreate this - could you list the `womtool` version that you're using and a minimal WDL that reproduces the issue?. Note: I tried to recreate on `develop` using this WDL:; ```wdl; version 1.0. import ""not/a/file.wdl"" as oops. workflow foo {; call oops.not_a_thing; }; ```. And received an error message and an exit code of 1:; ```; Failed to import 'not/a/file.wdl' (reason 1 of 3): Failed to resolve 'not/a/file.wdl' using resolver: 'relative to directory [...]/cromwell (without escaping Some([...]/cromwell))' (reason 1 of 1): Import file not found: not/a/file.wdl; Failed to import 'not/a/file.wdl' (reason 2 of 3): Failed to resolve 'not/a/file.wdl' using resolver: 'relative to directory [...]/bad_import (without escaping None)' (reason 1 of 1): Import file not found: not/a/file.wdl; Failed to import 'not/a/file.wdl' (reason 3 of 3): Failed to resolve 'not/a/file.wdl' using resolver: 'http importer' (reason 1 of 1): Cannot import 'not/a/file.wdl' relative to nothing; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3977#issuecomment-410845176:321,message,message,321,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3977#issuecomment-410845176,1,['message'],['message']
Integrability,"@davidbenjamin has an interesting proposal for user-defined / explicit sets of params for WDL: https://github.com/broadinstitute/wdl/issues/102. Depending on how ""[CWL support](https://github.com/broadinstitute/cromwell/milestone/20)"" addresses the [secondaryFiles](http://www.commonwl.org/v1.0/Workflow.html#File) mentioned above, it's supposed that similar WDL features will follow.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317839668:145,Depend,Depending,145,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2269#issuecomment-317839668,1,['Depend'],['Depending']
Integrability,"@delagoya your dependencies update might conflict with a round I just did in our `develop` branch. Namely we are using cats 1.0.1, and I'm not 100% sure 1.1 will work. So if you pull/rebase you will get most of what you posted minus the sttp update",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-382077268:15,depend,dependencies,15,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/3514#issuecomment-382077268,1,['depend'],['dependencies']
Integrability,"@delocalizer @kcibul we talked about this internally. As background we went down this path as our integration tests were frequently failing in travis - hte output files would be empty or incomplete. . It was noted that our tests use a lot of `echo` and `cat` and are quite short, so the theory is we're running into [this](https://www.turnkeylinux.org/blog/unix-buffering). **if** that turns out to be the culprit (and it does make a lot of sense) one could either take the stance that tools need to be well formed and have properly flushed, or we could try to bake something into our controller bash script (which IMO adding so much stuff to that bash script is a bomb waiting to happen, but ....), some [ideas](http://serverfault.com/questions/294218/is-there-a-way-to-redirect-output-to-a-file-without-buffering-on-unix-linux) are in that link. @kcibul what's your reaction to the above? does it ring true or still seem fishy?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175:98,integrat,integration,98,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-284812175,1,['integrat'],['integration']
Integrability,"@delocalizer We're starting to consider that the issue is in tooling, specifically in the tools we use for our integration tests. Since you are as far as I know the largest user of the shared file system backend(s), to what degree do you trust that tools are flushing when they're complete?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285268482:111,integrat,integration,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2057#issuecomment-285268482,1,['integrat'],['integration']
Integrability,"@dgtester I wouldn't avoid the scala shutdown hook if it makes sense (making this up as I go, but e.g. sending a message to WorkflowActor which then propagates outward), there've been a handful of things that I've been meaning to add to a hook anyways so there'll be one sooner or later anyways",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-174198348:113,message,message,113,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/397#issuecomment-174198348,1,['message'],['message']
Integrability,@dianekaplan could you run `gcloud alpha genomics operations describe operations/EKmIx96ALBjh373VhLH0ui8gkYad9-AKKg9wcm9kdWN0aW9uUXVldWU` and add in anything that looks like an error message or error code?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2970#issuecomment-348623050:183,message,message,183,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2970#issuecomment-348623050,1,['message'],['message']
Integrability,"@ffinfo my concern was more that the `isAlive` should be opt-in, not that that timeout should be opt in. . if I'm reading this right (EDIT: I think I got it a bit wrong first time):. - The job enters the `Running` state; - The first time we poll for it, we *always* check whether it's alive; - While it still is, we keep running `isAlive` every time we get polled; - Otherwise we enter the `WaitingForReturnCode`; - After the job is no longer alive, we abandon it after a given timeout and declare it failed; - If no timeout is configured, we keep waiting forever. I think this shouldn't be too much of a refactor:. - The job enters the `WaitingForReturnCode` state; - We immediately schedule an `CheckAlive` message to the actor at the configurable time; - If the cadence is not set, we never send that message (this would be the default); - When that CheckAlive arrives we can run `isAlive` and remember the result (and if we're still alive, schedule another `CheckAlive` again after the same delay); - If the `isAlive` failed, the next poll would return `Failed`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423569265:709,message,message,709,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4112#issuecomment-423569265,2,['message'],['message']
Integrability,"@francares Ah! I see. Yeah, I asked because I wasn't sure why we'd need another command type, we could either make a different type of ExecutorActor or have a branch in the existing ExecutorActor for caches. So we agree there. As for another message type, I believe this is all configured in (a) global config file and (b) workflow options so the actors should already have everything they need to decide whether to allow caching or not. So IMO there's no special case at this layer to handle caching.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200959492:242,message,message,242,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/604#issuecomment-200959492,1,['message'],['message']
Integrability,@francares Why do you view this as a bug? WDL explicitly states that there is no natural order outside of dependencies. Tagging our illustrious PO @kcibul just for record keeping,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235935544:106,depend,dependencies,106,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1215#issuecomment-235935544,1,['depend'],['dependencies']
Integrability,"@freeseek `firebase.developAdmin` is a pretty wide role (with 204 permissions), so it's not surprising that it gives some permissions that are needed here. What would be helpful is if Google showed the exact permissions in their error messages, though from what it seems, that's not always the case. Then if you have a list of permissions, you can find minimal role(s) that encompass those permissions, rather than through a blind hunt (please correct me if it wasn't entirely blind here..). Btw @freeseek, from my limited experience, GitHub issues here are not often-looked-through, it might be better to create an internal JIRA ticket instead ;)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680289141:235,message,messages,235,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4304#issuecomment-680289141,1,['message'],['messages']
Integrability,"@gauravs90 I would say this maybe could be done in smaller chunks. For this first step, maybe just implement enough for a single step workflow to pass through the WorkflowExecutionActor? I think that should be fairly small but will involve ironing out a lot of the issues with the interface.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/652#issuecomment-211918816:281,interface,interface,281,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/652#issuecomment-211918816,1,['interface'],['interface']
Integrability,@gauravs90 I'm happy to handle the tidy-up of this PR. Could you though review it (including my integration of your changes) and thumbs-up if you're happy? Cheers!,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/707#issuecomment-211403226:96,integrat,integration,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/707#issuecomment-211403226,1,['integrat'],['integration']
Integrability,"@gauravs90 I'm slightly confused here!. The model I thought we agreed on was:; - Everything has access to the `ServiceRegistry`; - Anyone (including the Engine) can send a `MetadataPut` message to the `ServiceRegistry`, which will forward it automagically to the `MetadataService`.; - ... That's it!. So, just send a bunch of `MetadataPut` messages to `ServiceRegistry` from the engine, and you're done, no need for all the extra stuff",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219083782:186,message,message,186,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/829#issuecomment-219083782,2,['message'],"['message', 'messages']"
Integrability,"@gauravs90 I've generally seen the akka folks recommend directly passing references to actors which need to be used. That has multiple benefits (e.g. makes it easy to switch out and/or dep injection, etc). My off the cuff reaction is that that seems simpler to just pass the required reference around, although I'll admit I'm basing that purely on your description and not having looked at the changes yet",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/822#issuecomment-218866972:189,inject,injection,189,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/822#issuecomment-218866972,1,['inject'],['injection']
Integrability,"@gemlam3 No they're different. This ticket is for the REST API, #2345 is for the command line interface.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2344#issuecomment-314125428:94,interface,interface,94,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2344#issuecomment-314125428,1,['interface'],['interface']
Integrability,"@gemmalam - I am trying to access the JIRA tickets for cromwell. I followed the link in the README and created an account, but I'm getting a message ""<my_email_address> doesn't have access to Jira on broadworkbench.atlassian.net.""",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-1095862028:141,message,message,141,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-1095862028,1,['message'],['message']
Integrability,"@genomics-geek Just ran into the same problem here.; ```; Unable to run job: failed receiving gdi request response for mid=1 (got syncron message receive timeout error)..; Exiting.; error: commlib error: got read error (closing ""vm-gridmaster/qmaster/1""); ```; This kind of glitch seems pretty common in SGE, unfortunately - it would be nice to have a workaround.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335:138,message,message,138,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2176#issuecomment-520629335,1,['message'],['message']
Integrability,"@geoffjentry ; I trying to write some kind of integration test for my fix of this task and me with @TimurKustov came to idea of executing two workflows sequential in order to get outputs, results and call logs copied after execution of the first workflow and assure that they are exist and correctly placed by running second workflow, which would check these files locations and existence.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-520024075:46,integrat,integration,46,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-520024075,1,['integrat'],['integration']
Integrability,"@geoffjentry Absolutely no worries, I totally understand but it is a bit weird to be aware of the concepts behind the following fault-tolerant scalable analysis pipelines and other distributed algorithms - which I'm sure you and many people are - and still be noticing that you have to deal with [20000 scatter/gather jobs](https://github.com/broadinstitute/cromwell/issues/1662) that might be causing issues when producing 10% of the world's genomic data:; - [Google's Continuous Pipelines](http://research.google.com/pubs/pub43790.html); - [Facebook's Real-Time Data Processing Pipelines](https://research.facebook.com/publications/realtime-data-processing-at-facebook/); - [Microsoft's Whole-Exome Workflows](https://www.microsoft.com/en-us/research/publication/scalable-and-efficient-whole-exome-data-processing-using-workflows-on-the-cloud/). Maybe it's my passion for high-throughput data integration, and knowing the potential of pipelined analysis that is achievable today through streamlined fault-tolerant scaling. I'm sure the Broad is already aware of these, as some of the fundamental scalability concepts have and are being implemented in [Hail](https://github.com/hail-is/hail). At least I'm comforted that you watch all the suggestions, and maybe in the future this might provide some helpful support :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956:895,integrat,integration,895,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1665#issuecomment-261687956,1,['integrat'],['integration']
Integrability,@geoffjentry Closing this issue. I think I figured out what happened and it is okay. Very hard to decipher from log messages.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1970#issuecomment-278996765:116,message,messages,116,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1970#issuecomment-278996765,1,['message'],['messages']
Integrability,"@geoffjentry Correct, I understand :) What I am suggesting that a uniform configuration file should exist for the user (runtime attributes, file behavior, etc.):; - If a configuration does is not present, then the user is presented with an message that it will assume reading from the path of the provided files, with an example of the path. A command ""cromwell describe/get config"" will show the current configuration.; - Then the program will provide them with an option like ""cromwell set-config defaults"" or something that makes sense for them to update the behavior to their choosing. Then this config file will be stored/read-from a ""well-known location"" for looking up a user's preference. If a program has multiple paths it can take - because of unset option - it will let the user know. Basically the less users have to type and deal with, the more they can concentrate on getting things done :) It will save you time in the way to update and configure new features, and it will provide user comments on preferred settings through use-cases.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849:240,message,message,240,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1260#issuecomment-238118849,1,['message'],['message']
Integrability,@geoffjentry I agree. I think we could easily replace the `promise.complete`s in `AwsSdkAsyncHandler` with appropriate messages to appropriate actors (NB we probably don't _need_ the `AwsSdkAsyncHandler` wrapper class at all),MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1570#issuecomment-253837416:119,message,messages,119,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1570#issuecomment-253837416,2,"['message', 'wrap']","['messages', 'wrapper']"
Integrability,@geoffjentry I had trouble building this pr:. ```; [error] /work/engine/src/main/scala/cromwell/webservice/SwaggerService.scala:3:35: imported `CromwellApiService' is permanently hidden by definition of object CromwellApiService in package webservice; [error] import cromwell.webservice.routes.CromwellApiService; [error] ^; ```. Any ideas?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-479678866:287,rout,routes,287,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4563#issuecomment-479678866,1,['rout'],['routes']
Integrability,@geoffjentry I removed lib dependencies in Cromwell-backend.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-192365563:27,depend,dependencies,27,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/495#issuecomment-192365563,1,['depend'],['dependencies']
Integrability,"@geoffjentry I think I understand what you mean. From my perspective it's a matter of where to stop.; We could ban entirely `Future`s from the codebase and decide that every asynchronous task deserves its own actor. That seems a bit extreme to me though.; I see your point about future being dangerous inside of actors. However I believe that small actors with 2 states and 3 internal messages are a small enough unit to be understood well enough to avoid the kind of problems we encountered before. This actor doesn't even have state data, there's no state to be shared or mutated.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218578385:385,message,messages,385,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218578385,1,['message'],['messages']
Integrability,"@geoffjentry I think what I was looking for is defined in the Scala interfaces for backends. I.e. I could create a custom backend to support AWS by implementing those (so it wouldn't be a web API, just an application PI).",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239899256:68,interface,interfaces,68,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1288#issuecomment-239899256,1,['interface'],['interfaces']
Integrability,"@geoffjentry I'm not sure if it's a 5 min change, but surely it's not a thing for 5 days. We are still working on some of the caching behaviors (we are still not clear on that yet).. and that's why a face-to-face with you guys will help us in understanding the implications of some of the refactoring. The change to implement the new backend interface in it's own will not be much, but the some functionalities like caching may still be lacking as of right now.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174086368:342,interface,interface,342,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/401#issuecomment-174086368,1,['interface'],['interface']
Integrability,"@geoffjentry Is there a mechanism for pub/sub when running Cromwell in Server mode? We're moving to running Cromwell more as a service and less interaction-y, but we're hoping for a way that we can find out about job status changes without writing a wrapper and polling the API every 5 seconds or so. / moderately related. Is there a way Cromwell can pub/sub for certain issues. If a Slurm job fails, I was hoping Cromwell could be notified that this has happened and relate the error back up the chain. Best I can come up with is submitting an `afternotok:jobid` dependent slurm job to write a non-zero rc file to where it's supposed to be.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-483900290:250,wrap,wrapper,250,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-483900290,2,"['depend', 'wrap']","['dependent', 'wrapper']"
Integrability,"@geoffjentry That makes sense, thanks. Given the current code structure it's not at all clear to me how Docker-dependent branching would fit in - maybe this would be easier as a boolean configuration option adjacent to `workflow-log-dir`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693:111,depend,dependent,111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4499#issuecomment-562687693,1,['depend'],['dependent']
Integrability,"@geoffjentry This is a proper text file format, IMHO ... Also, think about support down the road - the error message that goes with this is pretty cryptic (it's a giant stack trace buried in other error messages and the log message gets clobbered by stdout/stderr contention). Unless it is a lot of work, would you guys be willing to address it?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770:109,message,message,109,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1876#issuecomment-273793770,3,['message'],"['message', 'messages']"
Integrability,"@geoffjentry This issue has been in the WDL repo ([#20](https://github.com/openwdl/wdl/issues/20)) for nearly 2 years, and has the blessing of the originator of WDL (@scottfrazer). And we can see here that at least one of the current core team members (@patmagee) has voiced support. I think a good initial pass for OpenWDL should be to go through the outstanding issue list rather than start from scratch. Having to resubmit via the mailing list and go through the entire RFC procedure seems extremely heavy-handed for someone who's an end-user of WDL via either FireCloud or Cromwell. This is a capability I, and several others, have a strong use-case for. While I might have suggestions on how its implementation should look like (same as for inputs, just in the output section), in the grand scheme of things, all I want is a capability that will make FireCloud/Cromwell easier for me to use; I care about the ""what"", not the ""how"". There needs to continue to be a path for end-user-requested enhancements, rather than just developer-requested enhancements, which is what the [rfc protocol](https://github.com/openwdl/wdl/blob/master/GOVERNANCE.md#rfc-process), as outlined, really seems geared towards.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-340037503:1085,protocol,protocol,1085,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1838#issuecomment-340037503,1,['protocol'],['protocol']
Integrability,"@geoffjentry Very nice, thanks for the link! Wish I did know this earlier... :+1: ; Could this file then be provided to `cromwell` when running integration test via `centaur`?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519570433:144,integrat,integration,144,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5105#issuecomment-519570433,1,['integrat'],['integration']
Integrability,@geoffjentry What benefit does the AsyncAppender have for our logs? How realistic is the risk that some messages could be dropped?,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329481910:104,message,messages,104,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1809#issuecomment-329481910,1,['message'],['messages']
Integrability,"@geoffjentry commented on [Wed May 04 2016](https://github.com/broadinstitute/centaur/issues/38). Now that the centaur code base is starting to accumulate code that's not just tests, we should start having actual centaur unit tests as well. That'll be a little odd simply because sbt test will run both those and the integration tests but i'm sure we'll manage",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2886:317,integrat,integration,317,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2886,1,['integrat'],['integration']
Integrability,"@geoffjentry my understanding is that the logic here is what we want but the interface could be a little better, and a little more fully implemented. Correct me if I'm wrong, but I _think_ we want all `DataAccess` methods to automatically (and transparently) have deadlock retries. If we were to do that for all methods in `DataAccess` using the scheme laid out in this PR, then we'd be duplicating all of the methods in `DataAccess`. Perhaps we could construct `DataAccess` with an `ActorSystem`... though I can't remember if @kshakir objected to this for a reason that is currently escaping me. We could also just make the whole thing an actor, or put an actor in front of `DataAccess`.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/690#issuecomment-208340703:77,interface,interface,77,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/690#issuecomment-208340703,1,['interface'],['interface']
Integrability,"@geoffjentry the . > Extra logging around unexpected keys. commit was the key:. <img width=""1316"" alt=""screen shot 2017-04-05 at 12 13 07 pm"" src=""https://cloud.githubusercontent.com/assets/13006282/24715464/6515445e-19f9-11e7-9c54-34698bfe9d87.png"">. Before moving that message send I was seeing that programming error appear as a rare race condition (but often enough to fail a few sbt tests every time). I think my mistake was that the `createResponseSet` wasn't necessarily called from a `receive` method so akka was quite at liberty to interleave it with calls to `fulfillOrLog`, which I had assumed would be impossible.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185:271,message,message,271,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2102#issuecomment-291915185,1,['message'],['message']
Integrability,@geoffjentry this also feels relevant to our repo dependency discussion.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1934#issuecomment-327927401:50,depend,dependency,50,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1934#issuecomment-327927401,1,['depend'],['dependency']
Integrability,@geoffjentry this feels relevant to our repo dependency discussion.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1955#issuecomment-327927328:45,depend,dependency,45,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1955#issuecomment-327927328,1,['depend'],['dependency']
Integrability,"@geoffjentry what are unhandled messages, and why do we explicitly log them?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328885838:32,message,messages,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1808#issuecomment-328885838,1,['message'],['messages']
Integrability,"@geoffjentry yes, this is turning the knob higher and hoping for the best. The downstream tests succeeded except for the one that depended on cross-talking with the failed test...",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4525#issuecomment-452358458:130,depend,depended,130,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4525#issuecomment-452358458,1,['depend'],['depended']
Integrability,"@geoffjentry, thanks for the answer :); Actually, we tried to do something similar to how it was done in GCP (or TES) but it didn't work out. We added logging to the `mapCommandLineWomFile` method so that we can see what `womFiles` Cromwell passes to this method. And it turns out Cromwell never passes ""ad hoc"" files to this method, therefore `asAdHocFile`, for example, always returns `None`. In particular, in our integration test (PR #5057) it passes only two womFiles with values something like ; `s3://bucket-name/cromwell-execution/cwl_temp_file_some-numbers.cwl/some-numbers/call-test`; and; `s3://bucket-name/cromwell-execution/cwl_temp_file_some-numbers.cwl/some-numbers/call-test/tmp.59740063`; I'm not sure, but it looks like the first `womValue` somehow related to the `runtimeEnvironment` field in the `StandardAsyncExecutionActor`. The second value is something else too, since ""ad hoc"" files are placed in `call-test` directory.; It is possible that we misunderstood something, but for now, it looks like a dead-end.; By the way, we also tried to override `localizeAdHocValues` method `AwsBatchAsyncBackendJobExecutionActor` so that it would copy ""ad hoc"" files to the ""/cromwell_root"" directory. It fails with an AccessDeniedException.; I hope this gives you an understanding of why we came to the proposed solutions. As I said, perhaps we misunderstood something, so we will be happy if you can give us some hint.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509765587:417,integrat,integration,417,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4586#issuecomment-509765587,1,['integrat'],['integration']
Integrability,"@grsterin @aednichols if not an adapter from the old config, I do think a stub which throws an exception saying ""you need to update your config"" or something similar would be better than users suddenly getting cryptic errors like `""Class not found: x.y.z""`",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948:32,adapter,adapter,32,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5386#issuecomment-579501948,1,['adapter'],['adapter']
Integrability,"@grsterin perhaps arrows showing which actors send messages to others? perhaps ""sends messages to""?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/5255#issuecomment-549571920:51,message,messages,51,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/5255#issuecomment-549571920,2,['message'],['messages']
Integrability,"@horneth - so I see 2 Futures in here. One the little bit changing the state (around the Props) and the other was resolveAndEvaluate. As background for my statement I'll say the following:; - We've already seen firsthand the havoc which can erupt from having Futures rolling around inside an Actor. They break the Actor Model's abstraction that the internals of an actor are single threaded, meaning you now have to reason about shared mutable state, etc. We _can_ do that, but there are easier paths than actors to deal with that. We've been better about this recently but my concern is that it's too easy for stuff like that to sneak into what were previously pure Futures. Mixing Futures & Actors is not really a great idea.; - There are two async operations in the actor, which means that it is certainly doing two different things (I'll admit that the creation of an actor is a fairly lame 'thing'), disrupting Akka's mantra that actors should do one thing only. What I was suggesting was that the work being performed by these Futures be themselves pushed to their own actors. When they complete they can message back to this one, and those messages could be use to manage state transitions and such. (and to be clear, this is _not_ our little 'tol' code phrase - it's something I think we need to be much better about as we refactor cromwell)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678:1111,message,message,1111,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218573678,2,['message'],"['message', 'messages']"
Integrability,"@illusional ; I am happy you like this change. I have checked your other post in #4945 and your use case is similar to ours. We use a SGE cluster and run cromwell from the login node. The message is really easy to implement. But I am not sure what would be the right way to tackle this. I would like some consistency with the other localization methods, and I don't know if they message when a file is being copied. I haven't tested cached-copy in conjunction with call-caching and path+modtime yet. If I find issues with it I will create a new issue on the cromwell issue tracker, ping you, and see if I can fix it in a PR. We rely heavily on the path+modtime strategy as well.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507966522:188,message,message,188,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4900#issuecomment-507966522,2,['message'],['message']
Integrability,"@illusional The key part was this:; ```; runtime-attributes = """"""; Int cpu = 1; String? memory; """"""; ```; plus passing `memory` to the submit command - which in my case is a wrapper script that can interpret strings like ""4GB"". After another hour of trial-and-error I did finally get a working config using `memory_gb` in the config file instead of `memory`. But I have to say, a colleague and I read the documentation on memory repeatedly and we're still confused about what it's *trying* to say and how that relates to what actually happens. (In particular, it is not clear that you can't pass `memory` directly to non-SFS backends.) The fact that both `memory` and `memory_gb` can be task runtime parameters, yet one will presumably be overridden anyway, seems unwise.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284:174,wrap,wrapper,174,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5530#issuecomment-637924284,1,['wrap'],['wrapper']
Integrability,@iyanuobidele can you please pull it and try one more round of integration testing with the Spark cluster ? ; @geoffjentry : I rebased it with the develop branch let me know how does it look ? I will merge it then. Thank you.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1339#issuecomment-243985512:63,integrat,integration,63,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1339#issuecomment-243985512,1,['integrat'],['integration']
Integrability,"@jainh To echo what @cjllanwarne said, there is _no_ implied order in WDL, it's a pure dataflow. Any backend which requires an ordering beyond the dependency graph is implementing things incorrectly. If I'm misunderstanding what you're trying to do here, let me know - it's possible that @cjllanwarne totally biased my thinking :)",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235934992:147,depend,dependency,147,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/1214#issuecomment-235934992,1,['depend'],['dependency']
Integrability,"@jsotobroad I believe the integration tests you set up for Green are covered by this ticket, do you agree?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247:26,integrat,integration,26,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2112#issuecomment-329666247,1,['integrat'],['integration']
Integrability,@katevoss Assuming @Horneth thinks it'd be easy to add this I think we should. It's pretty irritating when it happens as it tends to come in storms and the message sounds a lot scarier than it really is.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2183#issuecomment-295010496:156,message,message,156,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2183#issuecomment-295010496,1,['message'],['message']
Integrability,"@katevoss I'm one of the developers of Singularity and I would like to +1 this request! I don't know scala, but if it comes down to making an equivalent folder [like this one for Docker](https://github.com/broadinstitute/cromwell/tree/9aff9f2957d303a4789801d6a482777faf47d48f/dockerHashing/src/main/scala/cromwell/docker) I can give a first stab at it. Or if it's more helpful I can give complete examples for all the steps to working with singularity images. We have both a registry ([Singularity Hub](https://singularity-hub.org) that is hooked up to the singularity command line client to work with images. So - to integrate into cromwell you could either just run the container via a singularity command, or implement your own connection to our API to download the image. Please let me know how I might be helpful, and I'd gladly help. If you want me to give a go at scala I would just ask for your general workflow to compile and test functionality.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968:618,integrat,integrate,618,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2177#issuecomment-295935968,1,['integrat'],['integrate']
Integrability,"@katevoss iirc we can't do anything with it directly (as i said, pubsub itself has been around for ages), but would need papi to be smart about it. I think there's a ticket, and sine you said you saw it in the tracker that's likely true. also note that what @Horneth said is true. i've resisted this request for quite some time because i don't like the idea of wiring in email sending into cromwell. I wouldn't be opposed to adding a service to the service registry for workflow notifications where we provide two out of the box implementations - one which is just a Noop and one which will push a message to a pubsub topic and then users can do whatever they want with it. Similarly if a user *really* wants to have email notifications they can write their own implementation and plug it in.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-329023606:598,message,message,598,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1678#issuecomment-329023606,1,['message'],['message']
Integrability,"@katevoss in your absence I've marked this as low. It's mainly a ""terrible error messages"" bug (you might consider this more important!). OTOH, the lack of failure recording and the EJEA crashing does concern me and might indicate a bigger problem under the surface.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-284013133:81,message,messages,81,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2050#issuecomment-284013133,1,['message'],['messages']
Integrability,@kbergin Having this feature will help us remove all of our private docker images from pipeline-tools and will make it much easier to write adapter workflows in the future,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4236#issuecomment-429117857:140,adapter,adapter,140,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4236#issuecomment-429117857,1,['adapter'],['adapter']
Integrability,@kcibul @geoffjentry I actually have a python cli for running Workflows (and a single task) against the cromwell rest interface which I can open source when I get a chance. The cli lets you run one or more workflow. Its missing some parameters but you guys can feel free to expand it. Ive run 1000's of workflows with this little tool,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-328192845:118,interface,interface,118,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1492#issuecomment-328192845,1,['interface'],['interface']
Integrability,"@kcibul Right now there are two problems:. - We're overloading the DB (largely metadata) in FC; - Nothing good happens when the DB is overloaded. The first one is solvable by a new metadata impl and will be the sort of route we go in CaaS. The second one should be fixed no matter what one is doing, so I'm specifically talking about all slick interaction which includes the stock metadata impl. The solution shouldn't be ""turn your buffers up"", but rather a more appropriate scheme instead of just dropping stuff to the floor. Related - as part of the CaaS milestone there are tickets (#1349 in particular) to make sure the standard impl isn't too tightly coupled with the resto f our DB stuff.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-320251484:219,rout,route,219,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2466#issuecomment-320251484,1,['rout'],['route']
Integrability,"@kcibul regarding issue #1804 .... Would my wdl and json look as follows?. ```wdl. workflow yo {; String msg; String? docker_image. call task1 {; input:; msg=msg,; docker_image=docker_image; }; }. # Run a message in an arbitrary docker container (e.g. ""broadinstitute/eval-gatk-protected:crsp_validation_latest""); task task1 {; String msg; String ? docker_image; ; command {; echo ${msg}; } ; ; runtime {; docker: ""${docker_image}""; memory: ""1GB""; }; }; ```; When I want a docker image:; ```; {; ""yo.msg"": ""foo""; ""yo.docker_image"": ""broadinstitute/eval-gatk-protected:crsp_validation_latest""; }; ```. No docker image:; ```; {; ""yo.msg"": ""foo""; }; ```",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-271612340:205,message,message,205,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/1832#issuecomment-271612340,1,['message'],['message']
Integrability,@kshakir I _think_ this covers your points although the test refactor isn't quite what we were talking about on hipchat. I think to go the route I think you were describing would be tough.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/158#issuecomment-136456530:139,rout,route,139,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/158#issuecomment-136456530,1,['rout'],['route']
Integrability,"@kshakir I just made the following changes:. - SBT is now run on pushes, to make sure that the artifact publishing still happens. Since it's ~30 minutes, not dependent on external services, and less flaky than the other tests I still think this is an improvement over today; - I switched the syntax to `[force ci]`. See the most recent commit message and it triggering the tests to run.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/4839#issuecomment-485822757:158,depend,dependent,158,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/4839#issuecomment-485822757,2,"['depend', 'message']","['dependent', 'message']"
Integrability,"@kshakir commented on [Mon Jan 23 2017](https://github.com/broadinstitute/centaur/issues/134). For each test name, it would be helpful to log the workflowId, as the name of the WDL workflow doesn't always match the name of the test. Additionally, a brief message of when the test was detected as starting & stopped would help debug stuck workflows. If this is deemed too verbose, the above could be logged at level debug.",MatchSource.ISSUE,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2891:255,message,message,255,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2891,1,['message'],['message']
Integrability,@lbergelson I just messaged our system administrators. I believe someone keeps changing a setting. It shouldnâ€™t matter if you are on the internal WiFi. Iâ€™m sorry for the inconvenience this is causing.,MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-501879498:19,message,messaged,19,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/5031#issuecomment-501879498,1,['message'],['messaged']
Integrability,"@lij41 - do you have any more error information, like a specific failure message? Also, were you creating the AMI from the CloudFormation templates? If so, which version - with or without VPC creation?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4435#issuecomment-445980888:73,message,message,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4435#issuecomment-445980888,1,['message'],['message']
Integrability,"@likeanowl - Took a look at the PR. Overall, looks good, but had a couple questions. Do the new integration tests you mention cover the points I brought up - i.e. mostly around default credentials use and default region config?",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-523568967:96,integrat,integration,96,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/4982#issuecomment-523568967,1,['integrat'],['integration']
Integrability,"@mcovarr ; because docker does not run without root on Linux. Although, there is a workaround:; ```; sudo usermod -aG docker $USER; ```; But after doing this it did not solve a problem. I am using the last release because I have dependency problems when building from master.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283005522:229,depend,dependency,229,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/issues/2034#issuecomment-283005522,1,['depend'],['dependency']
Integrability,"@mcovarr @cjllanwarne I made substantial changes to allow for automatic release number calculation and added the few things we talked about (pin centaur branch, add hotfix branch). It still has command injection though...; I tested it on a fork and as far as I can tell everything looked good.; If you don't mind re-giving it a look, otherwise I'll probably merge it as is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325:202,inject,injection,202,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/2065#issuecomment-287431325,1,['inject'],['injection']
Integrability,"@mcovarr And originally @Horneth had one fewer Future, listen to your own (well, my own) advice ;). Ok - so now that I have a chance to look at this more closely, it's unclear why there are any futures at all going on here. If I'm reading things right (as always, a big if) only one thing ever messages it (once) and then waits for a response. That response is either a success or failure. Why not just do the stuff it needs to do in the event loop (as nothing should be messaging it anyways)? The one argument I can come up with is that this would tie tie up one of the actorsystem dispatcher's threads but that's just as easily handled by giving this actor class its own dispatcher - that gets you the same effect as putting the Future in the global EC without all of the state changing and such. If you all want to make the claim that reasoning about the actor with futures kicking around inside is easier to reason about than multiple actors (a claim I vehemently disagree with), I'd put forth that the futures/states are themselves far more difficult to reason about than just simply doing the work straight up considering how simple this is.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218610073:294,message,messages,294,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/818#issuecomment-218610073,1,['message'],['messages']
Integrability,"@mcovarr I agree re testing, however IMO showing that there are outbound messages to start a CallActor at the same time is sufficient to demonstrate exactly what you just stated. At that point you know you have multiple CallActors running independently of each other which represent async computations.",MatchSource.ISSUE_COMMENT,broadinstitute,cromwell,87,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103308232:73,message,messages,73,https://cromwell.readthedocs.io/en/latest/,https://github.com/broadinstitute/cromwell/pull/20#issuecomment-103308232,1,['message'],['messages']
