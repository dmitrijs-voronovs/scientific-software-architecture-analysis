quality_attribute,sentence,source,author,repo,version,id,keyword,matched_word,match_idx,wiki,url,total_similar,target_keywords,target_matched_words
Performance,"There are some problems while running the ./build-prereq.sh:; ```; + python3.8 -m pip install absl-py parameterized protobuf==3.13.0 pyparsing==2.2.0; Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (1.4.0); Requirement already satisfied: parameterized in /usr/local/lib/python3.8/dist-packages (0.9.0); Requirement already satisfied: protobuf==3.13.0 in /usr/local/lib/python3.8/dist-packages (3.13.0); Collecting pyparsing==2.2.0; Using cached pyparsing-2.2.0-py2.py3-none-any.whl (56 kB); Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (1.16.0); Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from protobuf==3.13.0) (69.0.2); Installing collected packages: pyparsing; Attempting uninstall: pyparsing; Found existing installation: pyparsing 3.1.1; Uninstalling pyparsing-3.1.1:; Successfully uninstalled pyparsing-3.1.1; ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.; httplib2 0.21.0 requires pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2; python_version > ""3.0"", but you have pyparsing 2.2.0 which is incompatible.; matplotlib 3.7.3 requires pyparsing>=2.3.1, but you have pyparsing 2.2.0 which is incompatible.; Successfully installed pyparsing-2.2.0; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv; + DV_PLATFORM=ubuntu-20.04; + ln -sf /usr/bin/python3.8 /usr/local/bin/python3; + cd; + rm -rf clif; + proxychains git clone https://github.com/google/clif.git; ProxyChains-3.1 (http://proxychains.sf.net); Cloning into 'clif'...; |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK; remote: Enumerating objects: 5846, done.; remote: Counting objec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:472,cache,cached,472,,https://github.com/google/deepvariant/issues/739,1,['cache'],['cached']
Performance,"This might be an obvious question but i cannot work it out, what does the -B mean?. For example from your singularity guide: . ```; singularity run **-B** /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; ```. Also, if using a singularity container would you use it like this (fake link to the container):. ```; wget https://containers/deepvariant_1.3.0.sif ; module load singularity. singularity run deepvariant_1.3.0.sif -B --model_type=WES -ref=PolyposisExomeAnalysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna; --reads=PolyposisExomeAnalysis/samtoolssort/{}PE_samtoolssorted.bam; --output_vcf=PolyposisExomeAnalysis/deepvariant/vcf/PE_output.vcf.gz ; --output_gvcf=PolyposisExomeAnalysis/deepvariant/gvcf/PE_output.vcf.gz; --intermediate_results_dir PolyposisExomeAnalysis/deepvariant/intermediateresults/; ```. Sorry, still figuring it out! Thanks, Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/506:442,load,load,442,,https://github.com/google/deepvariant/issues/506,1,['load'],['load']
Performance,"Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 14030513477",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12385,tune,tune,12385,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.5519920587539673, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1_weighted=1.0, train/false_negatives=0.0, train/false_positives=0.0, train/learni",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12708,tune,tune,12708,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 290,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:10887,Tune,Tune,10887,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,Very slow performave for make_example,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/903:10,perform,performave,10,,https://github.com/google/deepvariant/issues/903,1,['perform'],['performave']
Performance,"What is the version of tensorflow for generating the checkpoint files (`index`, `meta`, `data`)?; And is there any way that I can load these checkpoints into a standalone tensorflow program and then dump it as a `.onnx` file?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/477:130,load,load,130,,https://github.com/google/deepvariant/issues/477,1,['load'],['load']
Performance,"When I run build_and_test.sh, I get the following isssues; ```. Extracting Bazel installation...; .............................; (12:58:42) INFO: Current date is 2018-03-20; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:03) ERROR: Analysis of target '//deepvariant:binaries' failed; build aborted: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream; (13:01:03) INFO: Elapsed time: 146.946s; (13:01:03) FAILED: Build did NOT complete successfully (60 packages loaded); Fetching https://mirror.bazel.build/ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz; 26,415b 43s; Fetching https://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz; 32,588b 42s; Fetching https://mirror.bazel.build/www.kurims.kyoto-u.ac.jp/~ooura/fft.tgz; 20,092b 40s; (13:01:03) ERROR: Couldn't start the build. Unable to run tests. ```. Please Help",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/59:1014,load,loaded,1014,,https://github.com/google/deepvariant/issues/59,1,['load'],['loaded']
Performance,"When I used version 1.6.1 for source code compilation, an error related to the numpy library occurred. I suspect this is due to incompatibility with TensorFlow. I tried using other versions of the numpy library, but the issue persisted. ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Misc setup' starting; ========== [Fri 02 Aug 2024 02:20:04 PM CST] Stage 'Update package list' starting; ========== [Fri 02 Aug 2024 02:20:06 PM CST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Fri 02 Aug 2024 02:20:10 PM CST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2213k 100 2213k 0 0 1634k 0 0:00:01 0:00:01 --:--:-- 1634k; Collecting pip; Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:328,Load,Load,328,,https://github.com/google/deepvariant/issues/859,3,"['Load', 'cache']","['Load', 'cached']"
Performance,YTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; build_and_test.sh: line 39: bazel: command not found; + PATH=/home/solokopi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018年 08月 24日 星期五 19:30:03 CST] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [2018年 08月 24日 星期五 19:30:03 CST] Stage 'Misc setup' starting; Hit:1 http://mirrors.aliyun.com/ubuntu xenial InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:3 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Ign:4 http://dl.google.com/linux/chrome/deb stable InRelease ; Hit:5 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Hit:6 http://dl.google.com/linux/chrome/deb stable Release ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Err:9 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:3954,Load,Load,3954,,https://github.com/google/deepvariant/issues/89,1,['Load'],['Load']
Performance,[THREAD 00] 430/483 COMPLETE (89%) [ELAPSED TIME: 4 Min 5 Sec]; [11-03-2021 14:17:11] INFO: [THREAD 00] 440/483 COMPLETE (91%) [ELAPSED TIME: 4 Min 5 Sec]; [11-03-2021 14:17:11] INFO: [THREAD 00] 450/483 COMPLETE (93%) [ELAPSED TIME: 4 Min 5 Sec]; [11-03-2021 14:17:11] INFO: [THREAD 00] 460/483 COMPLETE (95%) [ELAPSED TIME: 4 Min 5 Sec]; [11-03-2021 14:17:11] INFO: [THREAD 00] 470/483 COMPLETE (97%) [ELAPSED TIME: 4 Min 5 Sec]; [11-03-2021 14:17:11] INFO: [THREAD 00] 480/483 COMPLETE (99%) [ELAPSED TIME: 4 Min 5 Sec]; [11-03-2021 14:17:11] INFO: THREAD 0 FINISHED SUCCESSFULLY.; [11-03-2021 14:18:09] INFO: FINISHED IMAGE GENERATION; [11-03-2021 14:18:09] INFO: ELAPSED TIME: 5 Min 4 Sec; [11-03-2021 14:18:09] STEP 2: RUNNING INFERENCE; [11-03-2021 14:18:09] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/predictions_11032021_141305/; [11-03-2021 14:18:09] INFO: DISTRIBUTED CPU SETUP.; [11-03-2021 14:18:09] INFO: TOTAL CALLERS: 64; [11-03-2021 14:18:09] INFO: THREADS PER CALLER: 1; [11-03-2021 14:18:09] INFO: MODEL LOADING TO ONNX; [11-03-2021 14:19:18] INFO: BATCHES PROCESSED 5/66.; [11-03-2021 14:20:22] INFO: BATCHES PROCESSED 10/66.; [11-03-2021 14:21:25] INFO: BATCHES PROCESSED 15/66.; [11-03-2021 14:22:29] INFO: BATCHES PROCESSED 20/66.; [11-03-2021 14:23:34] INFO: BATCHES PROCESSED 25/66.; [11-03-2021 14:24:40] INFO: BATCHES PROCESSED 30/66.; [11-03-2021 14:25:45] INFO: BATCHES PROCESSED 35/66.; [11-03-2021 14:26:47] INFO: BATCHES PROCESSED 40/66.; [11-03-2021 14:27:51] INFO: BATCHES PROCESSED 45/66.; [11-03-2021 14:28:56] INFO: BATCHES PROCESSED 50/66.; [11-03-2021 14:29:59] INFO: BATCHES PROCESSED 55/66.; [11-03-2021 14:31:03] INFO: BATCHES PROCESSED 60/66.; [11-03-2021 14:31:57] INFO: BATCHES PROCESSED 65/66.; [11-03-2021 14:31:05] INFO: THREAD 46 FINISHED SUCCESSFULLY.; [11-03-2021 14:31:07] INFO: THREAD 19 FINISHED SUCCESSFULLY.; [11-03-2021 14:31:09] INFO: THREAD 38 FINISHED SUCCESSFULLY.; [11-03-2021 14:31:19] INFO: THREAD 3 FINISHED SUCCESSFULLY.; [1,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:15446,LOAD,LOADING,15446,,https://github.com/google/deepvariant/issues/491,1,['LOAD'],['LOADING']
Performance,\; --gvcf \; --phased_output \; --ont; ```; Relevant part of the log file (which is over 200MB):. ```; run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont; [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED; [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND; -------; mkdir -p /cromwell_root/pepper_output; ; mkdir -p /cromwell_root/pepper_output/logs; ; mkdir -p /cromwell_root/pepper_output/intermediate_files;; -------; [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND; -------; time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log; -------; [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED.; [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING.; [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041; [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/; [11-03-2021,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:2173,cache,cacheCopy,2173,,https://github.com/google/deepvariant/issues/491,1,['cache'],['cacheCopy']
Performance,"] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.5519920587539673, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1_weighted=1.0, train/false_negatives=0.0, train/false_positives=0.0, train/learning_rate=9.999999747378752e-05, train/loss=0.551992654800415, train/precision=1.0, train/precision_het=0.0, train/precision_homalt=0.0, train/precision_homref=1.0, t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12888,tune,tune,12888,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,] Tune step 300 / 3162 (10.0%); I0829 07:38:54.834164 140318776715072 train.py:366] Tune step 400 / 3162 (10.0%); I0829 07:40:47.319165 140318776715072 train.py:366] Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 270,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:10721,Tune,Tune,10721,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,] [13948] steps_per_sec=0.280075; I0829 07:28:44.568793 140305134778112 logging_writer.py:48] [13948] uptime=74874.6; I0829 07:31:25.151819 140318776715072 train.py:361] Running tune at step=13993 epoch=0; I0829 07:31:25.152109 140318776715072 train.py:366] Tune step 0 / 3162 (0.0%); I0829 07:33:17.573163 140318776715072 train.py:366] Tune step 100 / 3162 (0.0%); I0829 07:35:10.013494 140318776715072 train.py:366] Tune step 200 / 3162 (10.0%); I0829 07:37:02.497336 140318776715072 train.py:366] Tune step 300 / 3162 (10.0%); I0829 07:38:54.834164 140318776715072 train.py:366] Tune step 400 / 3162 (10.0%); I0829 07:40:47.319165 140318776715072 train.py:366] Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:10224,Tune,Tune,10224,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:15969,cache,cache,15969,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,"_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527.impute.vcf.gz; WI.20180527.impute.vcf.gz.tbi; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz; c_elegans.PRJEB28388.WS274.genomic.fa.gz.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz.gzi; ```. Now that I think I have all the appropriate input files in my `INPUT_DIR` I will try to run the code again:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; time=""2020-03-31T18:35:24-07:00"" level=warning msg=""\""/run/user/3019658\"" directory set by $XDG_RUNTIME_DIR does not exist. Either create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such file or directory: Trying to pull image in the event that it is a publi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:9167,load,load,9167,,https://github.com/google/deepvariant/issues/292,1,['load'],['load']
Performance,_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_array.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/utf.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@co,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:18535,cache,cache,18535,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced b,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:15678,cache,cache,15678,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,"_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}; ; 2023-04-13 03:58:35.887616: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.520424: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.431128: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.649384: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.283502: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.696454: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:2357,optimiz,optimized,2357,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"_type: zero, count: 0, combi_method: min, ignore_non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}; ##bcftools_viewVersion=1.10.2+htslib-1.10.2; ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022; ```. ### Variant line; ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband; X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:..; ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:; Pipeline; ```; # Load singularity; module load singularity; BIN_VERSION=""1.1.0"". # Load env for bcftools; ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/; source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh; conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped; export SINGULARITY_CACHEDIR=$PWD; singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads; NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered); cd $SLURM_SUBMIT_DIR; WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space; mkdir -p $WORKING_DIR; cd $WORKING_DIR. #### GRCh38 #### ; echo ""GRCh38 genome""; GENOME=GRCh38; FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/; FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS; BAM_DIR=$WORKING_DIR; Case_ID=Case1; FAMILY_ID=$Case_I",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518:6854,Load,Load,6854,,https://github.com/google/deepvariant/issues/518,2,"['Load', 'load']","['Load', 'load']"
Performance,"_weighted=1.0, train/false_negatives=0.0, train/false_positives=0.0, train/learning_rate=9.999999747378752e-05, train/loss=0.5519945025444031, train/precision=1.0, train/precision_het=0.0, train/precision_homalt=0.0, train/precision_homref=1.0, train/recall=1.0, train/recall_het=0.0, train/recall_homalt=0.0, train/recall_homref=1.0, train/true_negatives=12800.0, train/true_positives=6400.0; I0829 07:28:44.566404 140318776715072 local.py:41] Setting work unit notes: 0.3 steps/s, 61.4% (13948/22724), ETA: 8h42m; I0829 07:28:44.568708 140305134778112 logging_writer.py:48] [13948] steps_per_sec=0.280075; I0829 07:28:44.568793 140305134778112 logging_writer.py:48] [13948] uptime=74874.6; I0829 07:31:25.151819 140318776715072 train.py:361] Running tune at step=13993 epoch=0; I0829 07:31:25.152109 140318776715072 train.py:366] Tune step 0 / 3162 (0.0%); I0829 07:33:17.573163 140318776715072 train.py:366] Tune step 100 / 3162 (0.0%); I0829 07:35:10.013494 140318776715072 train.py:366] Tune step 200 / 3162 (10.0%); I0829 07:37:02.497336 140318776715072 train.py:366] Tune step 300 / 3162 (10.0%); I0829 07:38:54.834164 140318776715072 train.py:366] Tune step 400 / 3162 (10.0%); I0829 07:40:47.319165 140318776715072 train.py:366] Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:9650,Tune,Tune,9650,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,a98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:15) Analyzing: 242 targets (37 packages loaded); (09:27:17) Analyzing: 242 targets (45 packages loaded); (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitstate.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/compile.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/dfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.cc' contains an error and its package is in error and referenced by,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:8521,cache,cache,8521,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,able config definition build:monolithic in file /opt/tensorflow/.bazelrc: --define framework_shared_object=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true; #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30; #16 1490.1 (21:51:01) Loading:; #16 1490.1 (21:51:01) Loading: 0 packages loaded; #16 1491.1 (21:51:02) Loading: 0 packages loaded; #16 1492.2 (21:51:03) Loading: 0 packages loaded; #16 1493.2 (21:51:04) Loading: 0 packages loaded; #16 1494.2 (21:51:05) Loading: 0 packages loaded; #16 1495.2 (21:51:06) Loading: 0 packages loaded; #16 1496.2 (21:51:07) Loading: 0 packages loaded; #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:; #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.0 Repository rule _tf_http_archive defined at:; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44d,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:4862,Load,Loading,4862,,https://github.com/google/deepvariant/issues/608,2,"['Load', 'load']","['Loading', 'loaded']"
Performance,"aedyl01/disk1/yangyxt/test_tmp"". I0826 20:44:28.894064 47737984214848 call_variants.py:317] From /paedyl01/disk1/yangyxt/test_tmp/make_examples.tfrecord-00000-of-00014.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0826 20:44:28.898550 47737984214848 call_variants.py:317] From /opt/models/wgs/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; 2022-08-26 20:44:28.903729: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2022-08-26 20:44:28.905866: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 3. Tune using inter_op_parallelism_threads for best performance.; WARNING:tensorflow:Using temporary folder as model directory: /tmp/pbs.1173981.omics/tmpag6nq5vt; W0826 20:44:28.952679 47737984214848 estimator.py:1864] Using temporary folder as model directory: /tmp/pbs.1173981.omics/tmpag6nq5vt; INFO:tensorflow:Using config: {'_model_dir': '/tmp/pbs.1173981.omics/tmpag6nq5vt', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_i",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:2542,Tune,Tune,2542,,https://github.com/google/deepvariant/issues/564,2,"['Tune', 'perform']","['Tune', 'performance']"
Performance,"ain(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 362, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252. ```. Here is the supported CPU instructions of host:. ```sh; # cat /proc/cpuinfo; processor : 0; vendor_id : GenuineIntel; cpu family : 6; model : 30; model name : Intel(R) Core(TM) i7 CPU 870 @ 2.93GHz; stepping : 5; microcode : 0xa; cpu MHz : 1197.018; cache size : 8192 KB; physical id : 0; siblings : 8; core id : 0; cpu cores : 4; apicid : 0; initial apicid : 0; fpu : yes; fpu_exception : yes; cpuid level : 11; wp : yes; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm sse4_1 sse4_2 popcnt lahf_lm pti ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid dtherm ida flush_l1d; bugs : cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs; bogomips : 5851.92; clflush size : 64; cache_alignment : 64; address sizes : 36 bits physical, 48 bits virtual; power management:; ```. I also tried create a env & install on host by `conda install -c bioconda deepvariant`, but it pop-up the same error.; And Deepvariant v0.10.0 also have the same error. Please kindly give me some advice about this thank you. Best,; Jerry",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345:3626,cache,cache,3626,,https://github.com/google/deepvariant/issues/345,1,['cache'],['cache']
Performance,"all last):; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768, in main; call_variants(; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 598, in call_variants; model_example_shape = dv_utils.get_shape_and_channels_from_json(; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/dv_utils.py"", line 367, in get_shape_and_channels_from_json; example_info = json.load(f); File ""/usr/lib/python3.8/json/__init__.py"", line 293, in load; return loads(fp.read(),; File ""/usr/lib/python3.8/json/__init__.py"", line 357, in loads; return _default_decoder.decode(s); File ""/usr/lib/python3.8/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/usr/lib/python3.8/json/decoder.py"", line 355, in raw_decode; raise JSONDecodeError(""Expecting value"", s, err.value) from None; json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0). Process ForkProcess-1:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 454, in post_processing; item = output_queue.get(timeout=180); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 108, in get; raise Empty; _queue.Empty. real 3m2.335s; user 0m7.450s; sys 0m4.274s`. **Does the quick start test work on your system?**; Yes",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:6734,load,loads,6734,,https://github.com/google/deepvariant/issues/869,2,"['load', 'queue']","['loads', 'queues']"
Performance,"ance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants; output_queue = multiprocessing.Queue(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__; self._rlock = ctx.Lock(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock; return Lock(ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__; SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__; sl = self._semlock = _multiprocessing.SemLock(; FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s; user 0m6.224s; sys 0m3.683s. ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, the error happens with the quick start. . **Any additional context:**. Files generated ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733:2721,Queue,Queue,2721,,https://github.com/google/deepvariant/issues/733,1,['Queue'],['Queue']
Performance,"arnold proj-pbarc 54K Aug 6 22:51 ckpt-14902.index; > -rw-r----- 1 haley.arnold proj-pbarc 250M Aug 6 22:51 ckpt-14902.data-00000-of-00001; > -rw-r----- 1 haley.arnold proj-pbarc 266 Aug 6 22:51 checkpoint. and finally, here are the contents of ckpt-14902: . > total 7.6M; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 1 22:49 ..; > drwxr-s--- 2 haley.arnold proj-pbarc 4.0K Jul 1 22:49 variables; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 21 23:11 .; > -rw-r----- 1 haley.arnold proj-pbarc 6.9M Aug 6 22:51 saved_model.pb; > -rw-r----- 1 haley.arnold proj-pbarc 677K Aug 6 22:51 keras_metadata.pb; > -rw-r----- 1 haley.arnold proj-pbarc 55 Aug 6 22:51 fingerprint.pb; > -rw-r----- 1 haley.arnold proj-pbarc 80 Aug 6 22:51 example_info.json. Here is the error log file: . > 2024-08-09 20:05:25.101938: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; > To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; > I0809 20:05:40.093672 139993880950592 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmp4wzl_5p3; > Traceback (most recent call last):; > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>; app.run(main); > File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); > File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 693, in main; commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir); > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 572, in create_all_commands_and_logfiles; check_flags(); > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 544, in check_flags; raise RuntimeError(; > RuntimeError: The model f",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/866:2163,optimiz,optimized,2163,,https://github.com/google/deepvariant/issues/866,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"b/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement already satisfied, skipping upgrade: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (3.0.0); Requirement already satisfied, skipping upgrade: google-auth-httplib2>=0.0.3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 24日 星期五 19:54:15 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:03:04 --:--:-- 0; curl: (56)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:18016,cache,cachetools,18016,,https://github.com/google/deepvariant/issues/89,1,['cache'],['cachetools']
Performance,"b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':; #16 1497.0 Traceback (most recent call last):; #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl; #16 1497.0 ctx.download_and_extract(; #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:; #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.1 Repository rule _tf_http_archive defined at:; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:7299,cache,cache,7299,,https://github.com/google/deepvariant/issues/608,1,['cache'],['cache']
Performance,by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.cc' contains an error and its package is in ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:14513,cache,cache,14513,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,"c2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam"" --examples ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord@2.gz"" --channels ""insert_size"" --gvcf ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/gvcf.tfrecord@2.gz"" --regions ""NC_037590.1:200,000-950,000"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2024-01-05 15:53:39.096475: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:39.096611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:39.226747: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:39.226871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, ple",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:4681,load,load,4681,,https://github.com/google/deepvariant/issues/761,1,['load'],['load']
Performance,"cable); - ; - `***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/paedyl01/disk1/yangyxt/test_tmp/call_variants_output.tfrecord.gz"" --examples ""/paedyl01/disk1/yangyxt/test_tmp/make_examples.tfrecord@14.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/paedyl01/disk1/yangyxt/test_tmp"". I0826 20:44:28.894064 47737984214848 call_variants.py:317] From /paedyl01/disk1/yangyxt/test_tmp/make_examples.tfrecord-00000-of-00014.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0826 20:44:28.898550 47737984214848 call_variants.py:317] From /opt/models/wgs/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; 2022-08-26 20:44:28.903729: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2022-08-26 20:44:28.905866: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 3. Tune using inter_op_parallelism_threads for best performance.; WARNING:tensorflow:Using temporary folder as model directory: /tmp/pbs.1173981.omics/tmpag6nq5vt; W0826 20:44:28.952679 47737984214848 estimator.py:1864] Using temporary folder as model directory: /tmp/pbs.1173981.omics/tmpag6nq5vt; INFO:tensorflow:Using config: {'_model_dir': '/tmp/pbs.1173981.omics/tmpag6nq5vt', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimen",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:2165,optimiz,optimized,2165,,https://github.com/google/deepvariant/issues/564,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"com_google_deepvariant/deepvariant/make_examples.py"", line 1510, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1358, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1264, in processing_regions_from_options; options.reference_filename).header.contigs; File ""/tmp/Bazel.runfiles_2jmg0cod/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__; fasta_path, fai_path, options); ValueError: Not found: could not load fasta and/or fai for fasta /input/hg19.fa.gz; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/hg19.fa.gz --reads /input/2009617.cram --examples /tmp/tmpr2hdz82_/make_examples.tfrecord@16.gz --gvcf /tmp/tmpr2hdz82_/gvcf.tfrecord; @16.gz --task 1; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/hg19.fa.gz --reads /input/2009617.cram --examples /tmp/tmpr2hdz82_/make_examples.tfrecord@16.gz --gvcf /tmp/tmpr2hdz82_/gvcf.tfrecord; @16.gz --task 4; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/hg19.fa.gz --reads /input/2009617.cram --examples /tmp/tmpr2hdz82_/make_examples.tfrecord@16.gz --gvcf /tmp/tmpr2hdz82_/gvcf.tfrecord; @16.gz --task 7; real 0m5.299s; user 0m12.412s; sys 0m3.014s; I0509 06:55:25.059437 140033813915392 run_deepvariant.py:321] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/307:2256,load,load,2256,,https://github.com/google/deepvariant/issues/307,1,['load'],['load']
Performance,com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_goo,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:16539,cache,cache,16539,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_array.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/utf.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_g,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:18249,cache,cache,18249,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/deepvariant/deepvariant/testing/BUILD:19:1: Target '@com_googlesource_code_re2//:re2' contains an error and its package is in error and referenced by '//deepvariant/testing:gunit_extras'; (09:27:18) ERROR: Analysis of target '//deepvariant/testing:gunit_extras_test' failed; build aborted: Loading failed; (09:27:18) INFO: Elapsed time: 14.618s; (09:27:18) FAILED: Build did NOT complete successfully (48 packages loaded); (09:27:18) ERROR: Couldn't start the build. Unable to run tests; ```; Could anyone shed some light on this issue? Interestingly this was working a few days ago but possibly on a different host. Could it be hardware dependent?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:20232,cache,cache,20232,,https://github.com/google/deepvariant/issues/19,3,"['Load', 'cache', 'load']","['Loading', 'cache', 'loaded']"
Performance,ction to raise exceptions.; performance hint: _common.pyx:592:36: Exception check after calling 'f0' will always require the GIL to be acquired. Declare 'f0' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:596:36: Exception check after calling 'f1' will always require the GIL to be acquired. Declare 'f1' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:600:36: Exception check after calling 'f2' will always require the GIL to be acquired. Declare 'f2' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:604:36: Exception check after calling 'f3' will always require the GIL to be acquired. Declare 'f3' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:638:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:675:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:712:63: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:754:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:785:31: Exception check after ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:4957,perform,performance,4957,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance,ction to raise exceptions.; performance hint: _common.pyx:785:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:903:40: Exception check after calling 'f0' will always require the GIL to be acquired. Declare 'f0' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:907:40: Exception check after calling 'fd' will always require the GIL to be acquired. Declare 'fd' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:911:41: Exception check after calling 'fdd' will always require the GIL to be acquired. Declare 'fdd' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:916:40: Exception check after calling 'fi' will always require the GIL to be acquired. Declare 'fi' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:920:41: Exception check after calling 'fdi' will always require the GIL to be acquired. Declare 'fdi' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:924:38: Exception check after calling 'fiii' will always require the GIL to be acquired. Declare 'fiii' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:960:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:1002:32: Exception,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:6869,perform,performance,6869,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance,d by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitstate.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/compile.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/dfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/mimics_pcre.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/nfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/onepass.cc' contains an error and its package is in error and referenced b,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:9373,cache,cache,9373,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,d referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:13090,cache,cache,13090,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,dedupped.bam.bai; ```. I noticed there are a few more input files in the sample example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:8411,load,load,8411,,https://github.com/google/deepvariant/issues/292,1,['load'],['load']
Performance,define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true; #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30; #16 1490.1 (21:51:01) Loading:; #16 1490.1 (21:51:01) Loading: 0 packages loaded; #16 1491.1 (21:51:02) Loading: 0 packages loaded; #16 1492.2 (21:51:03) Loading: 0 packages loaded; #16 1493.2 (21:51:04) Loading: 0 packages loaded; #16 1494.2 (21:51:05) Loading: 0 packages loaded; #16 1495.2 (21:51:06) Loading: 0 packages loaded; #16 1496.2 (21:51:07) Loading: 0 packages loaded; #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:; #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.0 Repository rule _tf_http_archive defined at:; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:5112,Load,Loading,5112,,https://github.com/google/deepvariant/issues/608,2,"['Load', 'load']","['Loading', 'loaded']"
Performance,"dir ""/tmp/tmpiy9bfzyx"" --use_openvino; I1214 06:10:30.972710 140363019278144 call_variants.py:317] From /tmp/tmpiy9bfzyx/make_examples.tfrecord-00000-of-00002.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I1214 06:10:30.995939 140363019278144 call_variants.py:317] From /opt/models/wes/model.ckpt.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; 2022-12-14 06:10:31.060396: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2022-12-14 06:10:31.101084: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I1214 06:10:31.288279 140363019278144 openvino_estimator.py:55] Processing ckpt=/opt/models/wes/model.ckpt, tensor_shape=[100, 221, 7]; /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:1083: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:678: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs, training=is_training); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:2441: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.; outputs = layer.apply(inputs); /usr/local/lib/python3.8/dist-packages/tf_slim/layers/layers.py:118: UserWarning: `layer.apply` is deprecated and will be remo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/597:7704,Tune,Tune,7704,,https://github.com/google/deepvariant/issues/597,2,"['Tune', 'perform']","['Tune', 'performance']"
Performance,dk-243.0.0-0/platform/gsutil/third_party/socksipy-branch/socks.pyc'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/rpm/mapping/command_mapping.yaml'; specified in the package manifest cannot be found. CondaVerificationError: The package for google-cloud-sdk located at /home/pedge/anaconda3/pkgs/google-cloud-sdk-243.0.0-py27_0; appears to be corrupted. The path 'share/google-cloud-sdk-243.0.0-0/rpm/mapping/component_mapping.yaml'; specified in the package manifest cannot be found.; ```; (there are many similar CondaVerificationErrors before this); Conda info:; ```; active environment : longshot; active env location : /home/pedge/anaconda3/envs/longshot; shell level : 2; user config file : /home/pedge/.condarc; populated config files : /home/pedge/.condarc; conda version : 4.6.9; conda-build version : 3.17.6; python version : 3.7.1.final.0; base environment : /home/pedge/anaconda3 (writable); channel URLs : https://conda.anaconda.org/conda-forge/linux-64; https://conda.anaconda.org/conda-forge/noarch; https://conda.anaconda.org/bioconda/linux-64; https://conda.anaconda.org/bioconda/noarch; https://repo.anaconda.com/pkgs/main/linux-64; https://repo.anaconda.com/pkgs/main/noarch; https://repo.anaconda.com/pkgs/free/linux-64; https://repo.anaconda.com/pkgs/free/noarch; https://repo.anaconda.com/pkgs/r/linux-64; https://repo.anaconda.com/pkgs/r/noarch; https://conda.anaconda.org/OpenMDAO/linux-64; https://conda.anaconda.org/OpenMDAO/noarch; package cache : /home/pedge/anaconda3/pkgs; /home/pedge/.conda/pkgs; envs directories : /home/pedge/anaconda3/envs; /home/pedge/.conda/envs; platform : linux-64; user-agent : conda/4.6.9 requests/2.21.0 CPython/3.7.1 Linux/2.6.32-696.18.7.el6.x86_64 centos/6.6 glibc/2.12; UID:GID : 511626:8162; netrc file : None; offline mode : False. ```,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/177:5229,cache,cache,5229,,https://github.com/google/deepvariant/issues/177,1,['cache'],['cache']
Performance,dog_bam_trim_bwaMEM_sort_dedupped.bam.bai; ```. I noticed there are a few more input files in the sample example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.ann,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:8388,load,load,8388,,https://github.com/google/deepvariant/issues/292,1,['load'],['load']
Performance,"e --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pip 24.0 from /usr/local/lib/python3.10/site-packages/pip (python 3.10); ========== [Fri 02 Aug 2024 02:20:22 PM CST] Stage 'Install python3 packages' starting; error: subprocess-exited-with-error; ; × Preparing metadata (pyproject.toml) did not run successfully.; │ exit code: 1; ╰─> [78 lines of output]; Running from numpy source directory.; setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates; run_build = parse_setuppy_commands(); performance hint: _common.pyx:275:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:299:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:322:50: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:426:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:465:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _c",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:2767,perform,performance,2767,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance,e step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [139,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:11136,Tune,Tune,11136,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"e test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Here is the complete error msg:; #############################################; **Any additional context:**; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in d",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678:3275,optimiz,optimized,3275,,https://github.com/google/deepvariant/issues/678,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"e(; ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /input/chr19_new.fa; [E::idx_find_and_load] Could not retrieve index file for '/input/A_J.chr19.bam'; I1114 07:56:46.139744 140275925796672 genomics_reader.py:222] Reading /input/A_J.chr19.bam with NativeSamReader; I1114 07:56:46.142346 140275925796672 make_examples_core.py:243] Task 0/2: Preparing inputs; [E::fai_load3_core] Failed to open FASTA index /input/chr19_new.fa.fai: No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_1n42qf4z/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_1n42qf4z/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_1n42qf4z/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_1n42qf4z/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170, in main; make_examples_core.make_examples_runner(options); File ""/tmp/Bazel.runfiles_1n42qf4z/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1746, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_1n42qf4z/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1653, in processing_regions_from_options; ref_contigs = fasta.IndexedFastaReader(; File ""/tmp/Bazel.runfiles_1n42qf4z/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 105, in __init__; self._reader = reference.IndexedFastaReader.from_file(; ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /input/chr19_new.fa; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/chr19_new.fa --reads /input/A_J.chr19.bam --examples /tmp/tmpr4ux434h/make_examples.tfrecord@2.gz --channels insert_size --gvcf /tmp/tmpr4ux434h/gvcf.tfrecord@2.gz --task 1. real 0m2.768s; user 0m2.534s; sys 0m0.541s. Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/585:4094,load,load,4094,,https://github.com/google/deepvariant/issues/585,1,['load'],['load']
Performance,"e-linux_x86_64.whl; ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ export DV_TF_NIGHTLY_BUILD=0; ++ DV_TF_NIGHTLY_BUILD=0; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ DV_COPT_FLAGS='--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ export DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; ++ DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; ..................; (09:27:04) INFO: Current date is 2017-12-21; (09:27:04) Loading: ; (09:27:04) Loading: 0 packages loaded; (09:27:05) Loading: 0 packages loaded; (09:27:06) Loading: 7 packages loaded; currently loading: deepvariant/core/genomics ... (6 packages); (09:27:07) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:08) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:09) Analyzing: 242 targets (15 packages loaded); (09:27:11) Analyzing: 242 targets (16 packages loaded); (09:27:12) Analyzing: 242 targets (18 packages loaded); (09:27:14) Analyzing: 242 targets (31 packages loaded); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.; (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:98:1: name 'r",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:3337,Load,Loading,3337,,https://github.com/google/deepvariant/issues/19,8,"['Load', 'load']","['Loading', 'loaded', 'loading']"
Performance,e2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:131:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:136:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:141:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:146:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:151:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:15) Analyzing: 242 targets (37 packages loaded); (09:27:17) Analyzing: 242 targets (45 packages loaded); (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitstate.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/compile.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:7931,cache,cache,7931,,https://github.com/google/deepvariant/issues/19,3,"['cache', 'load']","['cache', 'loaded']"
Performance,e_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and refer,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:15386,cache,cache,15386,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,ed by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/mimics_pcre.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/nfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/onepass.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/parse.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/perl_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.h' contains an error and its package is in error and reference,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:10518,cache,cache,10518,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,ed by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_array.h' contains an error and its package is in error and referenced by '@,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:16824,cache,cache,16824,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,"ef_annotation_Geneset/11.variant_calling/deepvariant/input; OUTPUT_DIR=/home/data/ref_annotation_Geneset/11.variant_calling/deepvariant/outdir; singularity run --nv -B /home/data/ref_annotation_Geneset/11.variant_calling/deepvariant/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""${reference}""/GRCh38_no_alt_analysis_set.fasta \; --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ; - Error trace: INFO: Using cached SIF image; WARNING: Could not find any nv files on this host!; 2023-04-22 17:10:50.025707: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 51, in <module>; from ._api.v2 import compat; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compat/__init__.py"", line 37, in <module>; from . import v1; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py"", line 30, in <module>; from . import compat; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py"", line 38, in <module>; from . import v2; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/_api/v2/compat/v1/compat/v2/__init__.py"", line 28, in <module>; from tensorflow._api.v2.compat.v2 import __internal__; File ""/usr/local/lib/python3.8/dist-packa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/634:1044,optimiz,optimized,1044,,https://github.com/google/deepvariant/issues/634,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"el.ckpt \; --number_of_steps=50000 \; --save_interval_secs 300; ```. Here is the run info for just one sample's examples set (only a single chromosome, for testing purposes, from the .run_info.pbtxt file):. ```; labeling_metrics {; n_truth_variant_sites: 3469; n_truth_variant_alleles: 3474; n_candidate_variant_sites: 9778; n_candidate_variant_alleles: 9943; n_non_confident_candidate_variant_sites: 2219; n_true_positive_sites: 3468; n_true_positive_alleles: 3845; n_false_negative_sites: 1; n_false_negative_alleles: 1; n_false_positive_sites: 6309; n_false_positive_alleles: 6469; n_inexact_position_matches: 1; n_exact_position_matches: 3469; n_exact_position_and_allele_matches: 3443; n_exact_position_and_allele_and_genotype_matches: 3443; }; ```. Training runs just fine, with loss starting at ~1.2 and dropping to 0.04. Batch size is relatively small (memory error on the GPU with any larger). Is it simply my patience or is something else going on? I can provide tensorboard stats as well, but taking any model and performing make_examples(calling) -> postprocess results in only refcalls. Thanks, and let me know what other info I can provide. Edit: Here is some of the output from model_eval; ```; Saving dict for global step 0: Accuracy/All = 0.17285156, Accuracy/Indels = 0.078431375, Accuracy/SNPs = 0.19634147, F1/All = 0.39246467, F1/Het = 0.0, F1/HomRef = 0.39246467, F1/HomVar = 0.2947544, FNs/All = 0.0, FNs/Indels = 0.0, FNs/SNPs = 0.0, FPs/All = 774.0, FPs/Indels = 164.0, FPs/SNPs = 610.0, Precision/All = 0.24414062, Precision/Het = 0.0, Precision/HomRef = 0.24414062, Precision/HomVar = 0.17285156, Precision/Indels = 0.19607843, Precision/SNPs = 0.25609756, Recall/All = 1.0, Recall/Het = 0.0, Recall/HomRef = 1.0, Recall/HomVar = 1.0, Recall/Indels = 1.0, Recall/SNPs = 1.0, TNs/All = 0.0, TNs/Indels = 0.0, TNs/SNPs = 0.0, TPs/All = 250.0, TPs/Indels = 40.0, TPs/SNPs = 210.0, global_step = 0, loss = 1.3173751; I1209 06:57:08.677582 46912496317632 estimator.py:2039] Savin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/251:2409,perform,performing,2409,,https://github.com/google/deepvariant/issues/251,1,['perform'],['performing']
Performance,enced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/nfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/onepass.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/parse.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/perl_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.cc' contains an error and its package is in error and ref,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:10801,cache,cache,10801,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,enced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/utf.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.h' contains an error and its package is in error and referenced by '@com_goo,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:19101,cache,cache,19101,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,enced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/utf.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.h' contains an error and its package is in error and referenced by '@com_,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:19383,cache,cache,19383,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,"er-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=gpu # standard node(s); #SBATCH --ntasks=48; #SBATCH --job-name=""deepvariant_training""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \; --strategy=mirrored \; --config.batch_size=512 ; `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --ntasks-per-node=1 # 20 processor cor",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:2453,load,load,2453,,https://github.com/google/deepvariant/issues/797,1,['load'],['load']
Performance,"er.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-03 17:21:57.549571: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-07-03 17:21:57.644332: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2024-07-03 17:21:58.247052: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-07-03 17:21:58.247080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; Runs all 3 steps to go from input DNA reads to output VCF/gVCF files.; (... then -- the list of all options follows). If run on a proper BAM file with all options provided, all TF-TRT warning messages are periodically repeated as well as ; CUDA Version 11.3.1; 2024-07-02 22:47:07.493311: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-02 22:47:12.386498: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:2187,load,load,2187,,https://github.com/google/deepvariant/issues/844,1,['load'],['load']
Performance,erenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:13938,cache,cache,13938,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,erenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_array.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_set.h' contains an error and its package is in error and referenced,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:17105,cache,cache,17105,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,erenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/deepvariant/deepvariant/testing/BUILD:19:1: Target '@com_googlesource_code_re2//:re2' contains an error and its package is in error and referenced by '//deepvariant/testing:gunit_extras'; (09:27:18) ERROR: Analysis of target '//deepvariant/testing:gunit_extras_test' failed; build aborted: Loading failed; (09:27:18) INFO: Elapsed time: 14.618s; (09:27:18) FAILED: Build did NOT complete successfully (48 packages loaded); (09:27:18) ERROR: Couldn't start the build. Unable to run tests; ```; Could anyone sh,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:19952,cache,cache,19952,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,"ess_variants.py:1316] Transforming call_variants_output to variants.; I0218 00:48:12.163960 139719065552704 postprocess_variants.py:1318] Using 16 CPUs for parallelization of variant transformation.; I0218 00:48:12.684920 139719065552704 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: HG001; I0218 00:48:18.996037 139719065552704 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.06664579312006633 minutes; I0218 00:48:39.012242 139719065552704 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.33359973033269247 minutes. real	0m59.941s; user	0m58.218s; sys	0m5.086s. ***** Running the command:*****; time /opt/deepvariant/bin/vcf_stats_report --input_vcf ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --outfile_base ""output_apptainer_gpu/HG001.apptainer.gpu.output"". 2024-02-18 00:48:50.006549: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:48:50.008250: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-18 00:48:57.417490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0218 00:48:57.421117 139673283618624 genomics_reader.py:222] Reading output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz with NativeVcfReader. real	0m23.982s; user	0m12.056s; sys	0m2.006s. ```. ----------------------------------------------------------------------------------; ----------------------------------------------------------------------------------. My sys",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:19912,load,load,19912,,https://github.com/google/deepvariant/issues/774,1,['load'],['load']
Performance,"examples: [100, 221, 7]; I0822 07:52:10.813338 127086447671104 call_variants.py:592] Use saved model: True; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768, in main; call_variants(; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 598, in call_variants; model_example_shape = dv_utils.get_shape_and_channels_from_json(; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/dv_utils.py"", line 367, in get_shape_and_channels_from_json; example_info = json.load(f); File ""/usr/lib/python3.8/json/__init__.py"", line 293, in load; return loads(fp.read(),; File ""/usr/lib/python3.8/json/__init__.py"", line 357, in loads; return _default_decoder.decode(s); File ""/usr/lib/python3.8/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/usr/lib/python3.8/json/decoder.py"", line 355, in raw_decode; raise JSONDecodeError(""Expecting value"", s, err.value) from None; json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0). Process ForkProcess-1:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 454, in post_processing; item = output_queue.get(timeout=180); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 108, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:6580,load,load,6580,,https://github.com/google/deepvariant/issues/869,1,['load'],['load']
Performance,"f output]; Running from numpy source directory.; setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates; run_build = parse_setuppy_commands(); performance hint: _common.pyx:275:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:299:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:322:50: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:426:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:465:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:509:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:592:36: Exception check after calling 'f0' will always require the GIL to be acquired. Declare 'f0' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:596:36: Exception check afte",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:3283,perform,performance,3283,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance,"f=1.0, train/true_negatives=12800.0, train/true_positives=6400.0; I0829 07:28:44.566404 140318776715072 local.py:41] Setting work unit notes: 0.3 steps/s, 61.4% (13948/22724), ETA: 8h42m; I0829 07:28:44.568708 140305134778112 logging_writer.py:48] [13948] steps_per_sec=0.280075; I0829 07:28:44.568793 140305134778112 logging_writer.py:48] [13948] uptime=74874.6; I0829 07:31:25.151819 140318776715072 train.py:361] Running tune at step=13993 epoch=0; I0829 07:31:25.152109 140318776715072 train.py:366] Tune step 0 / 3162 (0.0%); I0829 07:33:17.573163 140318776715072 train.py:366] Tune step 100 / 3162 (0.0%); I0829 07:35:10.013494 140318776715072 train.py:366] Tune step 200 / 3162 (10.0%); I0829 07:37:02.497336 140318776715072 train.py:366] Tune step 300 / 3162 (10.0%); I0829 07:38:54.834164 140318776715072 train.py:366] Tune step 400 / 3162 (10.0%); I0829 07:40:47.319165 140318776715072 train.py:366] Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:9978,Tune,Tune,9978,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,ferenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_array.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.cc' contains an error and its package is in error and referen,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:17388,cache,cache,17388,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true; #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30; #16 1490.1 (21:51:01) Loading:; #16 1490.1 (21:51:01) Loading: 0 packages loaded; #16 1491.1 (21:51:02) Loading: 0 packages loaded; #16 1492.2 (21:51:03) Loading: 0 packages loaded; #16 1493.2 (21:51:04) Loading: 0 packages loaded; #16 1494.2 (21:51:05) Loading: 0 packages loaded; #16 1495.2 (21:51:06) Loading: 0 packages loaded; #16 1496.2 (21:51:07) Loading: 0 packages loaded; #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:; #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.0 Repository rule _tf_http_archive defined at:; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found; #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.0 (21:51:08) ERROR: An error oc,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:5635,cache,cache,5635,,https://github.com/google/deepvariant/issues/608,1,['cache'],['cache']
Performance,"g, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pip 24.0 from /usr/local/lib/python3.10/site-packages/pip (python 3.10); ========== [Fri 02 Aug 2024 02:20:22 PM CST] Stage 'Install python3 packages' starting; error: subprocess-exited-with-error; ; × Preparing metadata (pyproject.toml) did not run successfully.; │ exit code: 1; ╰─> [78 lines of output]; Running from numpy source directory.; setup.py:470: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates; run_build = parse_setuppy_commands(); performance hint: _common.pyx:275:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:299:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:322:50: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:426:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:2509,perform,performance,2509,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance,"g: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LC_CTYPE = ""C.UTF-8"",; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2024-01-05 15:53:39.096475: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:39.096611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:39.226747: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:39.226871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:49.941043: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:53:49.987410 140173517489984 genomics_reader.py:222] Reading /public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam with NativeSamReader; W0105 15:53:49.988560 140173517489984 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:5283,load,load,5283,,https://github.com/google/deepvariant/issues/761,1,['load'],['load']
Performance,"g_rate=9.999999747378752e-05, train/loss=0.5519945025444031, train/precision=1.0, train/precision_het=0.0, train/precision_homalt=0.0, train/precision_homref=1.0, train/recall=1.0, train/recall_het=0.0, train/recall_homalt=0.0, train/recall_homref=1.0, train/true_negatives=12800.0, train/true_positives=6400.0; I0829 07:28:44.566404 140318776715072 local.py:41] Setting work unit notes: 0.3 steps/s, 61.4% (13948/22724), ETA: 8h42m; I0829 07:28:44.568708 140305134778112 logging_writer.py:48] [13948] steps_per_sec=0.280075; I0829 07:28:44.568793 140305134778112 logging_writer.py:48] [13948] uptime=74874.6; I0829 07:31:25.151819 140318776715072 train.py:361] Running tune at step=13993 epoch=0; I0829 07:31:25.152109 140318776715072 train.py:366] Tune step 0 / 3162 (0.0%); I0829 07:33:17.573163 140318776715072 train.py:366] Tune step 100 / 3162 (0.0%); I0829 07:35:10.013494 140318776715072 train.py:366] Tune step 200 / 3162 (10.0%); I0829 07:37:02.497336 140318776715072 train.py:366] Tune step 300 / 3162 (10.0%); I0829 07:38:54.834164 140318776715072 train.py:366] Tune step 400 / 3162 (10.0%); I0829 07:40:47.319165 140318776715072 train.py:366] Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 316",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:9732,Tune,Tune,9732,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,gainst_concurrent_changes; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true; #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30; #16 1490.1 (21:51:01) Loading:; #16 1490.1 (21:51:01) Loading: 0 packages loaded; #16 1491.1 (21:51:02) Loading: 0 packages loaded; #16 1492.2 (21:51:03) Loading: 0 packages loaded; #16 1493.2 (21:51:04) Loading: 0 packages loaded; #16 1494.2 (21:51:05) Loading: 0 packages loaded; #16 1495.2 (21:51:06) Loading: 0 packages loaded; #16 1496.2 (21:51:07) Loading: 0 packages loaded; #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:; #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.0 Repository rule _tf_http_archive defined at:; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found; #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:5337,cache,cache,5337,,https://github.com/google/deepvariant/issues/608,1,['cache'],['cache']
Performance,"gets (15 packages loaded); (09:27:11) Analyzing: 242 targets (16 packages loaded); (09:27:12) Analyzing: 242 targets (18 packages loaded); (09:27:14) Analyzing: 242 targets (31 packages loaded); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.; (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:4667,cache,cache,4667,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,glesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.cc' contains an error and its package is in error and referenced by '@com_goo,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:11955,cache,cache,11955,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/perl_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.h' contains an error and its package is in error and referenced b,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:11663,cache,cache,11663,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:14799,cache,cache,14799,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,"gz \; --num_shards=2`. Error messages:; `==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available.; Use the NVIDIA Container Toolkit to start this container with GPU support; see; https://docs.nvidia.com/datacenter/cloud-native/ . 2024-01-05 15:52:56.748367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-01-05 15:52:57.864310: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2024-01-05 15:53:10.688853: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:10.692890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing librarie",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:1830,optimiz,optimized,1830,,https://github.com/google/deepvariant/issues/761,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"hap phase- whatshap haplotag-deepvariant2; Now I also want to use DeepTrio.I used the haplotagged.bam(generated from whatshap haplotag) as input bam.When I ran DeepTrio,the output.vcf.gz was generated normally.However,the log file showed the following warning message:; ------------------------; I0926 14:26:35.659228 47028170803008 call_variants.py:336] Shape of input examples: [140, 221, 9]; W0926 14:26:35.665323 47028170803008 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An **error** will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; 2021-09-26 14:26:35.668419: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-09-26 14:26:35.669638: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2021-09-26 14:26:35.671197: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2600000000 Hz; WARNING:tensorflow:Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; W0926 14:26:35.690017 47028170803008 estimator.py:1846] Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; ------------------------; The version I used:; DeepVariant 1.1.0; glnexus v1.3.1; whatshap 1.0; DeepTrio 1.2.0. Does the warning message affect the results or can be ignored?; Should I use a higher version of DeepVariant(1.2.0)?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488:1151,optimiz,optimized,1151,,https://github.com/google/deepvariant/issues/488,4,"['Tune', 'optimiz', 'perform']","['Tune', 'optimized', 'performance', 'performance-critical']"
Performance,he function to raise exceptions.; performance hint: _common.pyx:426:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:465:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:509:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:592:36: Exception check after calling 'f0' will always require the GIL to be acquired. Declare 'f0' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:596:36: Exception check after calling 'f1' will always require the GIL to be acquired. Declare 'f1' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:600:36: Exception check after calling 'f2' will always require the GIL to be acquired. Declare 'f2' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:604:36: Exception check after calling 'f3' will always require the GIL to be acquired. Declare 'f3' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:638:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:675:31: Exception check ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:4237,perform,performance,4237,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance,he function to raise exceptions.; performance hint: _common.pyx:604:36: Exception check after calling 'f3' will always require the GIL to be acquired. Declare 'f3' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:638:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:675:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:712:63: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:754:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:785:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:903:40: Exception check after calling 'f0' will always require the GIL to be acquired. Declare 'f0' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:907:40: Exception check after calling 'fd' will always require the GIL to be acquired. Declare 'fd' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:911:41: Exception check af,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:5671,perform,performance,5671,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance,he function to raise exceptions.; performance hint: _common.pyx:712:63: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:754:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:785:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:903:40: Exception check after calling 'f0' will always require the GIL to be acquired. Declare 'f0' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:907:40: Exception check after calling 'fd' will always require the GIL to be acquired. Declare 'fd' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:911:41: Exception check after calling 'fdd' will always require the GIL to be acquired. Declare 'fdd' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:916:40: Exception check after calling 'fi' will always require the GIL to be acquired. Declare 'fi' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:920:41: Exception check after calling 'fdi' will always require the GIL to be acquired. Declare 'fdi' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:924:38: Exception ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:6387,perform,performance,6387,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance,"hecked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: CentOS Linux release 7.9.2009; - DeepVariant version: deepvariant:0.9.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); - Illumina, HG38, standard capture panel. **Steps to reproduce:**; - Command: Snakemake command:; - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12; - actual command (XXXXX = removed for security purposes) ; - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/550:1864,perform,performance,1864,,https://github.com/google/deepvariant/issues/550,1,['perform'],['performance']
Performance,"iants.py:583] Predicted 52224 examples in 51 batches [0.463 sec per 100].; I0218 00:42:59.116032 140119155529536 call_variants.py:583] Predicted 103424 examples in 101 batches [0.468 sec per 100].; I0218 00:46:58.822113 140119155529536 call_variants.py:583] Predicted 154624 examples in 151 batches [0.468 sec per 100].; I0218 00:47:39.156648 140119155529536 call_variants.py:623] Complete: call_variants. real	13m21.231s; user	118m36.634s; sys	25m56.983s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpd74of138/gvcf.tfrecord@16.gz"". 2024-02-18 00:47:52.195457: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:47:52.196245: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-18 00:48:10.043945: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0218 00:48:10.133844 139719065552704 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: HG001; I0218 00:48:12.163552 139719065552704 postprocess_variants.py:1313] CVO sorting took 0.03374857902526855 minutes; I0218 00:48:12.163919 139719065552704 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0218 00:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:17990,load,load,17990,,https://github.com/google/deepvariant/issues/774,1,['load'],['load']
Performance,"ical_crossentropy=0.5519936680793762, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1_weighted=1.0, train/false_negatives=0.0, train/false_positives=0.0, train/learning_rate=9.999999747378752e-05, train/loss=0.5519945025444031, train/precision=1.0, train/precision_het=0.0, train/precision_homalt=0.0, train/precision_homref=1.0, train/recall=1.0, train/recall_het=0.0, train/recall_homalt=0.0, train/recall_homref=1.0, train/true_negatives=12800.0, train/true_positives=6400.0; I0829 07:28:44.566404 140318776715072 local.py:41] Setting work unit notes: 0.3 steps/s, 61.4% (13948/22724), ETA: 8h42m; I0829 07:28:44.568708 140305134778112 logging_writer.py:48] [13948] steps_per_sec=0.280075; I0829 07:28:44.568793 140305134778112 logging_writer.py:48] [13948] uptime=74874.6; I0829 07:31:25.151819 140318776715072 train.py:361] Running tune at step=13993 epoch=0; I0829 07:31:25.152109 140318776715072 train.py:366] Tune step 0 / 3162 (0.0%); I0829 07:33:17.573163 140318776715072 train.py:366] Tune step 100 / 3162 (0.0%); I0829 07:35:10.013494 140318776715072 train.py:366] Tune step 200 / 3162 (10.0%); I0829 07:37:02.497336 140318776715072 train.py:366] Tune step 300 / 3162 (10.0%); I0829 07:38:54.834164 140318776715072 train.py:366] Tune step 400 / 3162 (10.0%); I0829 07:40:47.319165 140318776715072 train.py:366] Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:9490,Tune,Tune,9490,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,improve call_variants performance,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/152:22,perform,performance,22,,https://github.com/google/deepvariant/pull/152,2,['perform'],['performance']
Performance,"in/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1_weighted=1.0, train/false_negatives=0.0, train/false_positives=0.0, train/learning_rate=9.999999747378752e-05, train/loss=0.5519945025444031, train/precision=1.0, train/precision_het=0.0, train/precision_homalt=0.0, train/precision_homref=1.0, train/recall=1.0, train/recall_het=0.0, train/recall_homalt=0.0, train/recall_homref=1.0, train/true_negatives=12800.0, train/true_positives=6400.0; I0829 07:28:44.566404 140318776715072 local.py:41] Setting work unit notes: 0.3 steps/s, 61.4% (13948/22724), ETA: 8h42m; I0829 07:28:44.568708 140305134778112 logging_writer.py:48] [13948] steps_per_sec=0.280075; I0829 07:28:44.568793 140305134778112 logging_writer.py:48] [13948] uptime=74874.6; I0829 07:31:25.151819 140318776715072 train.py:361] Running tune at step=13993 epoch=0; I0829 07:31:25.152109 140318776715072 train.py:366] Tune step 0 / 3162 (0.0%); I0829 07:33:17.573163 140318776715072 train.py:366] Tune step 100 / 3162 (0.0%); I0829 07:35:10.013494 140318776715072 train.py:366] Tune step 200 / 3162 (10.0%); I0829 07:37:02.497336 140318776715072 train.py:366] Tune step 300 / 3162 (10.0%); I0829 07:38:54.834164 140318776715072 train.py:366] Tune step 400 / 3162 (10.0%); I0829 07:40:47.319165 140318776715072 train.py:366] Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:9569,Tune,Tune,9569,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"in; sys.exit(main(argv)); File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants; predictions = model.signatures['serving_default'](batch[1]); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__; return self._call_impl(args, kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl; return self._call_with_flat_signature(args, kwargs,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature; return self._call_flat(args, self.captured_inputs, cancellation_manager); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat; return super(_WrapperFunction, self)._call_flat(args, captured_inputs,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat; return self._build_call_outputs(self._inference_function.call(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 378, in call; outputs = execute.execute(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/execute.py"", line 52, in quick_execute; tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,; tensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:. input depth must be evenly divisible by filter depth: 7 vs 6; [[{{node StatefulPartitionedCall/inceptionv3/activation/Relu}}]] [Op:__inference_signature_wrapper_14413]`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:8843,load,load,8843,,https://github.com/google/deepvariant/issues/797,1,['load'],['load']
Performance,ion to raise exceptions.; performance hint: _common.pyx:903:40: Exception check after calling 'f0' will always require the GIL to be acquired. Declare 'f0' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:907:40: Exception check after calling 'fd' will always require the GIL to be acquired. Declare 'fd' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:911:41: Exception check after calling 'fdd' will always require the GIL to be acquired. Declare 'fdd' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:916:40: Exception check after calling 'fi' will always require the GIL to be acquired. Declare 'fi' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:920:41: Exception check after calling 'fdi' will always require the GIL to be acquired. Declare 'fdi' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:924:38: Exception check after calling 'fiii' will always require the GIL to be acquired. Declare 'fiii' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:960:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:1002:32: Exception check after calling 'f1' will always require the GIL to be acquired. Declare 'f1' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; ; Error compiling Cython file:; ---------------,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:7109,perform,performance,7109,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance,"ion_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets; WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter; W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay; W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum; W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum; ..... In the final check point folder, there is nothing in the assets folder. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:3043,optimiz,optimizer,3043,,https://github.com/google/deepvariant/issues/722,6,['optimiz'],['optimizer']
Performance,"iousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/python3.6/dist-packages/googleapiclient/discovery.py discovery.patch. CMD [""/opt/deepvariant/bin/run_deepvariant"", ""--help""]; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469:4403,load,loader,4403,,https://github.com/google/deepvariant/issues/469,1,['load'],['loader']
Performance,"iter.py:48] [13900] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.5519936680793762, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1_weighted=1.0, train/false_negatives=0.0, train/false_positives=0.0, train/learning_rate=9.999999747378752e-05, train/loss=0.5519945025444031, train/precision=1.0, train/precision_het=0.0, train/precision_homalt=0.0, train/precision_homref=1.0, train/recall=1.0, train/recall_het=0.0, train/recall_homalt=0.0, train/recall_homref=1.0, train/true_negatives=12800.0, train/true_positives=6400.0; I0829 07:28:44.566404 140318776715072 local.py:41] Setting work unit notes: 0.3 steps/s, 61.4% (13948/22724), ETA: 8h42m; I0829 07:28:44.568708 140305134778112 logging_writer.py:48] [13948] steps_per_sec=0.280075; I0829 07:28:44.568793 140305134778112 logging_writer.py:48] [13948] uptime=74874.6; I0829 07:31:25.151819 140318776715072 train.py:361] Running tune at step=13993 epoch=0; I0829 07:31:25.152109 140318776715072 train.py:366] Tune step 0 / 3162 (0.0%); I0829 07:33:17.573163 140318776715072 train.py:366] Tune step 100 / 3162 (0.0%); I0829 07:35:10.013494 140318776715072 train.py:366] Tune step 200 / 3162 (10.0%); I0829 07:37:02.497336 140318776715072 train.py:366] Tune step 300 / 3162 (10.0%); I0829 07:38:54.834164 140318776715072 train.py:366] Tune step 400 / 3162 (10.0%); I0829 07:40:47.319165 140318776715072 train.py:366] Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I082",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:9410,tune,tune,9410,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"ither '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.; (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:112:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:114:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:116:1: name 're2_test' is not defined ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:5051,cache,cache,5051,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,"k: . **Setup**; - Operating system: Linux HPC; - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WES. **Steps to reproduce:**; ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p c_compute_wgp; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=02:00:00; #SBATCH --time=072:00:00; #SBATCH --mem-per-cpu=32GB. module purge; module load singularity; module load parallel. set -eu. cd /scratch/c.c21087028/; BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \; --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,; Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/522:1950,cache,cache,1950,,https://github.com/google/deepvariant/issues/522,1,['cache'],['cache']
Performance,"kpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \; --strategy=mirrored \; --config.batch_size=512 ; `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=atlas # standard node(s); #SBATCH --job-name=""deepvariant_modeltest""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2/checkpoints/ckpt-58"" \; --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/idBacDors_rearing_male_chr_unpl_mt.fasta"" \; --reads ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/DTWP-03_F1_M1_Chromosome4_sorted.bam"" \; --regions ""Chromosome4"" \; --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:3953,LOAD,LOAD,3953,,https://github.com/google/deepvariant/issues/797,1,['LOAD'],['LOAD']
Performance,"kpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token; I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]; I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6].; 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz; WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0; W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0; INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true; graph_options {; rewrite_options {; meta_optimizer_iterations: ONE; }; }; , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': Tr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:1956,Tune,Tune,1956,,https://github.com/google/deepvariant/issues/537,2,"['Tune', 'perform']","['Tune', 'performance']"
Performance,"l first):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 500 in _import_graph_de; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 414 in import_graph_def; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/function_def_to_graph.py"", line 87 in func; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/function_deserialization.py"", line 416 i; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 154 in __init__; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 958 in load_partial; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 828 in load; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 596 in call_; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768 in main; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789 in <modu. real 0m5.038s; user 0m3.921s; sys 0m1.122s; Process ForkProcess-1:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 454, in post; File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 108, in get; raise Empty; _queue.Empty; INFO: Cleaning up image...; ```. Please let me know if I could provide the input BAM for testing/debugging. Thank you for your time. Best regards,; Louis",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:4740,queue,queues,4740,,https://github.com/google/deepvariant/issues/833,1,['queue'],['queues']
Performance,"l_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found; #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':; #16 1497.0 Traceback (most recent call last):; #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl; #16 1497.0 ctx.download_and_extract(; #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:; #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:6808,cache,cache,6808,,https://github.com/google/deepvariant/issues/608,1,['cache'],['cache']
Performance,"l_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""; `; - Error trace: (if applicable); ```; ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-10 12:07:21.275077: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0710 12:07:24.889796 139944337696576 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: sample1; I0710 12:09:25.874185 139944337696576 postprocess_variants.py:1313] CVO sorting took 2.0161957065264384 minutes; I0710 12:09:25.874843 139944337696576 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0710 12:09:25.874915 139944337696576 postprocess_variants.py:1318] Using 19 CPUs for parallelization of variant transformation.; I0710 12:09:45.096508 139944337696576 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: sample1; multiprocessing.pool.RemoteTraceback:; """"""; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 125, in worker; result = (True, func(*args, **kwds)); File ""/usr/lib/python3.8/multiprocessing/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849:2029,optimiz,optimized,2029,,https://github.com/google/deepvariant/issues/849,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"ld (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PYTHON_BIN_PATH=/opt/at11.0/bin/python \; PYTHON_LIB_PATH=/opt/at11.0/lib64/python3.6/site-packages \; TF_CONFIGURE_IOS=0 \; TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.0,7.0 \; TF_CUDA_VERSION=10.0 \; TF_CUDNN_VERSION=7 \; TF_NEED_CUDA=1 \; /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/external/org_tensorflow/tensorflow/tools/git/gen_git_source --generate external/local_config_git/gen/spec.json ext",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/356:1393,cache,cache,1393,,https://github.com/google/deepvariant/issues/356,1,['cache'],['cache']
Performance,"le: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-07-03 17:21:58.247080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; Runs all 3 steps to go from input DNA reads to output VCF/gVCF files.; (... then -- the list of all options follows). If run on a proper BAM file with all options provided, all TF-TRT warning messages are periodically repeated as well as ; CUDA Version 11.3.1; 2024-07-02 22:47:07.493311: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-02 22:47:12.386498: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ...; and processing is performed on CPUs. . Also, these details are put in the log hudreds of times:; 2024-07-03 18:27:31.862526: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-03 18:27:31.862557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 8308a7bb3067; 2024-07-03 18:27:31.862563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 8308a7bb3067; 2024-07-03 18:27:31.862607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 555.42.6; 2024-07-03 18:27:31.862621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 555.42.6; 2024-07-03 18:27:31.862626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 555.42.6; (then repeated with every call). **",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:3289,perform,performed,3289,,https://github.com/google/deepvariant/issues/844,1,['perform'],['performed']
Performance,"les.tfrecord-00000-of-00002.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0105 15:56:51.296850 140372734228288 call_variants.py:583] Predicted 1024 examples in 1 batches [4.670 sec per 100].; I0105 16:00:45.139408 140372734228288 call_variants.py:623] Complete: call_variants. real 5m27.431s; user 6m58.490s; sys 0m19.033s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/refgenome/GCF_003073045.1_ASM307304v1_genomic.fna"" --infile ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/call_variants_output.tfrecord.gz"" --outfile ""./outputgpu/output.vcf.gz"" --cpus ""2"" --gvcf_outfile ""./outputgpu/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/gvcf.tfrecord@2.gz"". 2024-01-05 16:00:59.661436: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 16:00:59.661893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 16:01:06.236791: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 16:01:06.304423 140416700553024 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0105 16:01:06.676597 140416700553024 postprocess_variants.py:1313] CVO sorting took 0.006136405467987061 minutes; I0105 16:01:06.677379 140416700553024 postprocess_variants.py:1316] Transforming call_variants_outp",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:15670,load,load,15670,,https://github.com/google/deepvariant/issues/761,1,['load'],['load']
Performance,lesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_go,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:16256,cache,cache,16256,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,"loaded; (09:27:05) Loading: 0 packages loaded; (09:27:06) Loading: 7 packages loaded; currently loading: deepvariant/core/genomics ... (6 packages); (09:27:07) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:08) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:09) Analyzing: 242 targets (15 packages loaded); (09:27:11) Analyzing: 242 targets (16 packages loaded); (09:27:12) Analyzing: 242 targets (18 packages loaded); (09:27:14) Analyzing: 242 targets (31 packages loaded); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.; (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:4284,cache,cache,4284,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,"location, but I would also very much appreciate if someone could take a glance at the code I am submitting to make sure there are no obvious causes for this in the deepvariant commands that I'm just completely missing. . Thank you very much! . Best, . Haley. Here is the code: ; `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=5-48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=gpu-a100 # standard node(s); #SBATCH --ntasks=1; #SBATCH --job-name=""deepvariant_modeltraining""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. # LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export APPTAINER_CACHEDIR=$TMPDIR ; export APPTAINER_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/training_set_channelsize_F1F1shuffle.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/validation_set_channelsize_F1F2shuffled.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/d",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/840:2231,LOAD,LOAD,2231,,https://github.com/google/deepvariant/issues/840,1,['LOAD'],['LOAD']
Performance,"lugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2023-10-25 17:00:55.064391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format.; WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets; WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the followin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:1276,load,loaded,1276,,https://github.com/google/deepvariant/issues/722,1,['load'],['loaded']
Performance,"lysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.; Instructions for updating:; Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089; W0828 10:40:49.488072 140318776715072 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be rem>; Instructions for updating:; Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089; 2024-08-28 10:40:49.893797: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.; I0828 10:40:49.947995 140318776715072 train.py:316]; ```. And here is an excerpt of from a later portion of the log file including some training and tuning steps, where you can see the 0.0 for het and homalt eval stats. . ```; I0829 07:13:59.098341 140305134778112 logging_writer.py:48] [13700] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.552009105682373, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1_; weighted=1.0, train/false_negatives=0.0, train/false_positives=0.0, train/learning_rate=9.999999747378752e-05, train/loss=0.5520098805427551, train/precision=1.0, train/precision_het=0.0, train/precision_homalt=0.0, train/precision_homref=1.0, train/recall=1.0, train/recall_het=0.0,; train/recall_homalt=0.0, train/recall_homref=1.0, train/true_negatives=12800.0, train/true_positives=6400.0; I0829 07",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:6083,optimiz,optimized,6083,,https://github.com/google/deepvariant/issues/876,2,['optimiz'],"['optimizations', 'optimized']"
Performance,"m_google_deepvariant/deepvariant/data_providers.py:374: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; I0424 15:59:50.681574 139872277903104 data_providers.py:376] self.input_map_threads=48; W0424 15:59:50.681832 139872277903104 deprecation.py:323] From /tmp/Bazel.runfiles_sszxydhb/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0424 15:59:51.794167 139872277903104 estimator.py:1147] Calling model_fn.; W0424 15:59:51.800228 139872277903104 deprecation.py:323] From /tmp/Bazel.runfiles_sszxydhb/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0424 15:59:51.806498 139872277903104 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0424 16:00:02.682547 139872277903104 estimator.py:1149] Done calling model_fn.; I0424 16:00:06.021238 139872277903104 monitored_session.py:240] Graph was finalized.; I0424 16:00:06.037272 139872277903104 saver.py:1284] Restoring para",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/304:4213,optimiz,optimizations,4213,,https://github.com/google/deepvariant/issues/304,1,['optimiz'],['optimizations']
Performance,"m_google_deepvariant/deepvariant/data_providers.py:374: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.; I0703 17:18:45.810746 140322304501504 data_providers.py:376] self.input_map_threads=48; W0703 17:18:45.810852 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/data_providers.py:381: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.; I0703 17:18:46.328745 140322304501504 estimator.py:1147] Calling model_fn.; W0703 17:18:46.330687 140322304501504 deprecation.py:323] From /tmp/Bazel.runfiles_m65fk12g/runfiles/com_google_deepvariant/deepvariant/modeling.py:885: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.; Instructions for updating:; Deprecated in favor of operator or tf.math.divide.; W0703 17:18:46.333346 140322304501504 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.; Instructions for updating:; Please use `layer.__call__` method instead.; I0703 17:18:50.712157 140322304501504 estimator.py:1149] Done calling model_fn.; I0703 17:18:51.788142 140322304501504 monitored_session.py:240] Graph was finalized.; ```; We are on Docker 19.03.11 on Debian 10 and executed deepvarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321:5078,optimiz,optimizations,5078,,https://github.com/google/deepvariant/issues/321,1,['optimiz'],['optimizations']
Performance,m_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_goog,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:12528,cache,cache,12528,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,"me/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```; sudo sh run_deepvariant.sh; I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0; 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA; 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters; W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ; I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_global_id_in_cluster': 0, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd40e875e50>, '_model_dir': '/tmp/tmpuBleAQ', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_session_config': allow_soft_placement: true; graph_options {; rewrite_options {; meta_optimizer_iterations: ONE; }; }; , '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_co",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:1580,Tune,Tune,1580,,https://github.com/google/deepvariant/issues/166,2,"['Tune', 'perform']","['Tune', 'performance']"
Performance,"model: True; 2024-06-19 14:57:57.916727: F tensorflow/tsl/platform/env.cc:391] Check failed: -1 != path_length (-1 vs. -1); Fatal Python error: Aborted. Current thread 0x00002b1ce03a6740 (most recent call first):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 500 in _import_graph_de; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 414 in import_graph_def; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/function_def_to_graph.py"", line 87 in func; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/function_deserialization.py"", line 416 i; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 154 in __init__; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 958 in load_partial; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 828 in load; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 596 in call_; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768 in main; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789 in <modu. real 0m5.038s; user 0m3.921s; sys 0m1.122s; Process ForkProcess-1:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 454, in post; File ""/usr/lib/python3.8/multiprocessing/queues.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:3744,load,load,3744,,https://github.com/google/deepvariant/issues/833,1,['load'],['load']
Performance,"models/wgs/model.ckpt; I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]; I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]; I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s; user 0m37.643s; sys 0m6.426s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878; 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz; 2023-05-19 16:22:51.173038: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 305; I0519 16:22:51.173760 140640470185792 postprocess_variants.py:1037] CVO sorting took 3.902912139892578e-05 minutes; I0519 16:22:51.173915 140640470185792 postprocess_variants.py:1040] Transforming call_variants_output to variants.; I0519 16:22:51.204555 140640470185792 postprocess_variants.py:1080] Processing variants (and writing to temporary file) took 0.0005078872044881184 minutes; I0519 16:22:51.582558 140640470185792 postprocess_variants.py:1093] Finished writing V",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:13267,optimiz,optimized,13267,,https://github.com/google/deepvariant/issues/653,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"mples ""intermediate_results_dir/make_examples.tfrecord@32.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""intermediate_results_dir/gvcf.tfrecord@32.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}; ; 2023-04-13 03:58:35.887616: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.520424: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.431128: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.649384: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.283502: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:1994,optimiz,optimized,1994,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"n c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds.; Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Launcher: Job 6 completed in 0 seconds.; Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); Launcher: Job 7 completed in 0 seconds.; FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:2793,optimiz,optimized,2793,,https://github.com/google/deepvariant/issues/717,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,n to raise exceptions.; performance hint: _common.pyx:907:40: Exception check after calling 'fd' will always require the GIL to be acquired. Declare 'fd' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:911:41: Exception check after calling 'fdd' will always require the GIL to be acquired. Declare 'fdd' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:916:40: Exception check after calling 'fi' will always require the GIL to be acquired. Declare 'fi' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:920:41: Exception check after calling 'fdi' will always require the GIL to be acquired. Declare 'fdi' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:924:38: Exception check after calling 'fiii' will always require the GIL to be acquired. Declare 'fiii' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:960:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:1002:32: Exception check after calling 'f1' will always require the GIL to be acquired. Declare 'f1' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; ; Error compiling Cython file:; ------------------------------------------------------------; ...; self.rng_state.ctr.v[i] = counter[i]; ; self._reset_state_variables(); ; self._bitgen.state = <void *>&self.rng_state; self._bitgen.next_uint64 = &philox_uint64; ^; -------------------------,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:7351,perform,performance,7351,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance,"n to raise exceptions.; performance hint: _common.pyx:916:40: Exception check after calling 'fi' will always require the GIL to be acquired. Declare 'fi' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:920:41: Exception check after calling 'fdi' will always require the GIL to be acquired. Declare 'fdi' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:924:38: Exception check after calling 'fiii' will always require the GIL to be acquired. Declare 'fiii' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:960:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:1002:32: Exception check after calling 'f1' will always require the GIL to be acquired. Declare 'f1' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; ; Error compiling Cython file:; ------------------------------------------------------------; ...; self.rng_state.ctr.v[i] = counter[i]; ; self._reset_state_variables(); ; self._bitgen.state = <void *>&self.rng_state; self._bitgen.next_uint64 = &philox_uint64; ^; ------------------------------------------------------------; ; _philox.pyx:195:35: Cannot assign type 'uint64_t (*)(void *) except? -1 nogil' to 'uint64_t (*)(void *) noexcept nogil'. Exception values are incompatible. Suggest adding 'noexcept' to the type of the value being assigned.; Processing numpy/random/_bounded_integers.pxd.in; Processing numpy/random/_common.pyx; Processing numpy/random/_philox.pyx; Traceback (most recent call last):; File ""/tmp/pip-install-u4pd848o/numpy_1b14eb8cc70c49928ac1e",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:7833,perform,performance,7833,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance,n.pyx:275:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:299:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:322:50: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:426:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:465:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:509:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:592:36: Exception check after calling 'f0' will always require the GIL to be acquired. Declare 'f0' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:596:36: Exception check after calling 'f1' will always require the GIL to be acquired. Declare 'f1' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:600:36: Exception check af,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:3521,perform,performance,3521,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance,nced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_goog,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:12809,cache,cache,12809,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,nced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.h' contains an error and its package is in error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:14224,cache,cache,14224,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,ne step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 310,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:11053,Tune,Tune,11053,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,ne=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true; #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30; #16 1490.1 (21:51:01) Loading:; #16 1490.1 (21:51:01) Loading: 0 packages loaded; #16 1491.1 (21:51:02) Loading: 0 packages loaded; #16 1492.2 (21:51:03) Loading: 0 packages loaded; #16 1493.2 (21:51:04) Loading: 0 packages loaded; #16 1494.2 (21:51:05) Loading: 0 packages loaded; #16 1495.2 (21:51:06) Loading: 0 packages loaded; #16 1496.2 (21:51:07) Loading: 0 packages loaded; #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:; #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.0 Repository rule _tf_http_archive defined at:; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found; #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab46,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:5483,cache,cache,5483,,https://github.com/google/deepvariant/issues/608,1,['cache'],['cache']
Performance,"ng package metadata (repodata.json): ...working... done; 6.190 Solving environment: ...working... failed; 6.260; 6.260 PackagesNotFoundError: The following packages are not available from current channels:; 6.260; 6.260 - bioconda::samtools==1.15; 6.260 - bioconda::bcftools==1.15; 6.260; ```. I resolved this error by removing the version numbers. i.e., removed the `==1.15` from both the lines. #### Error in the build-prerunreq.sh script. Once, I cross the previous error, I get this error -. ```; > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:; 0.101 ========== This script is only maintained for Ubuntu 22.04.; 0.101 ========== Load config settings.; 0.103 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Install the runtime packages' starting; 0.104 ========== This script is only maintained for Ubuntu 22.04.; 0.104 ========== Load config settings.; 0.105 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Misc setup' starting; 1.955 W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ub",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/902:1549,Load,Load,1549,,https://github.com/google/deepvariant/issues/902,1,['Load'],['Load']
Performance,"ntime.; if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; export DV_CPP_TENSORFLOW_TAG=""master""; else; export DV_CPP_TENSORFLOW_TAG=""r1.12""; fi; export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing; # value in the environment (allowing command line control of the build),; # defaulting to 0 (CPU only build).; export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file; # compiled with MKL support for corei7 or better chipsets, which; # significantly speeds up execution when running on modern CPUs. The default; # TensorFlow wheel files don't contain these instructions (and thereby run on a; # broader set of CPUs). Using this optimized wheel reduces the runtime of; # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud; # Platform) optimized wheel because all GCP instances have at least Sandy Bridge; # or better chipsets, so this wheel should run anywhere on GCP.; export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl""; export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow""; export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries.; # If you already have CUDA installed, such as on a properly provisioned; # Docker image, it shouldn't be necessary.; export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python); export USE_DEFAULT_PYTHON_LIB_PATH=1; export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings"". function note_build_stage {; echo ""========== [$(date)] Stage '${1}' starting""; }; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:5809,optimiz,optimized,5809,,https://github.com/google/deepvariant/issues/145,1,['optimiz'],['optimized']
Performance,"oad samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module load nixpkgs/16.09; module load gcc/7.3.0; module load htslib/1.9; tabix -p vcf WI.20180527.impute.vcf.gz; ```. Now my input directory looks like:. ```; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; c_elegans.PRJEB28388.WS274.annotations.bed; WI.20180527.impute.vcf.gz; WI.20180527.impute.vcf.gz.tbi; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz; c_elegans.PRJEB28388.WS274.genomic.fa.gz.fai; c_elegans.PRJEB28388.WS274.genomic.fa.gz.gzi; ```. Now that I think I have all the appropriate input files in my `INPUT_DIR` I will try to run the code again:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; time=""2020-03-31T18:35:24-07:00"" level=warning msg=""\""/run/user/3019658\"" directory set by $XDG_RUNTIME_DIR does not exist. Either create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/3019658: no such file or directory: Trying to pull image in the",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:9144,load,load,9144,,https://github.com/google/deepvariant/issues/292,1,['load'],['load']
Performance,object=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true; #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30; #16 1490.1 (21:51:01) Loading:; #16 1490.1 (21:51:01) Loading: 0 packages loaded; #16 1491.1 (21:51:02) Loading: 0 packages loaded; #16 1492.2 (21:51:03) Loading: 0 packages loaded; #16 1493.2 (21:51:04) Loading: 0 packages loaded; #16 1494.2 (21:51:05) Loading: 0 packages loaded; #16 1495.2 (21:51:06) Loading: 0 packages loaded; #16 1496.2 (21:51:07) Loading: 0 packages loaded; #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:; #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.0 Repository rule _tf_http_archive defined at:; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.0,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:4962,Load,Loading,4962,,https://github.com/google/deepvariant/issues/608,2,"['Load', 'load']","['Loading', 'loaded']"
Performance,"odel.ckpt"". ```. the tail of my noup.out has not changed in over a day; ```; packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.; Instructions for updating:; Use standard file APIs to check for files with this prefix.; I0208 09:29:54.405941 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:55.469674 139859027293952 session_manager.py:491] Running local_init_op.; I0208 09:29:55.510524 139859027293952 session_manager.py:493] Done running local_init_op.; I0208 09:29:55.864006 139859027293952 modeling.py:410] Reloading EMA...; I0208 09:29:55.864634 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]; ```; top. Looks like there is a lot of under utilized compute resources; ```; top - 00:16:22 up 1 day, 23:24, 1 user, load average: 16.03, 16.02, 16.00; Tasks: 621 total, 1 running, 620 sleeping, 0 stopped, 0 zombie; %Cpu(s): 25.0 us, 0.0 sy, 0.0 ni, 75.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st; KiB Mem : 52262400+total, 48358937+free, 8878616 used, 30156016 buff/cache; KiB Swap: 0 total, 0 free, 0 used. 51216832+avail Mem . PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND ; 23437 root 20 0 27.319g 3.709g 136440 S 800.0 0.7 18646:29 python ; 23944 root 20 0 27.259g 3.687g 137396 S 799.7 0.7 18613:17 python ; 1 root 20 0 119604 5788 4112 S 0.0 0.0 0:03.37 systemd ; 2 root 20 0 0 0 0 S 0.0 0.0 0:00.01 kthreadd ; 3 root 20 0 0 0 0 S 0.0 0.0 0:00.06 ksoftirqd/0 ; 4 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0 ; ```; search for recently modified files; ```; ubuntu@ip-172-31-21-181:/deepTmp$ sudo find . -type f -printf '%T@ %p\n' | sort -n | tail -1 | cut -f2- -d"" ""; ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; ubuntu@ip-172-31-21-181:/deepTmp$ ls",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/269:3824,load,load,3824,,https://github.com/google/deepvariant/issues/269,1,['load'],['load']
Performance,oglesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googl,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:12246,cache,cache,12246,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,"onnection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 136, in worker; put((job, i, (False, wrapped))); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 404, in _send_bytes; self._send(header); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe; Process ForkPoolWorker-42:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 131, in worker; put((job, i, result)); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 368, in put; self._writer.send_bytes(obj); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 200, in send_bytes; self._send_bytes(m[offset:offset + size]); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 405, in _send_bytes; self._send(buf); File ""/usr/lib/python3.8/multiprocessing/connection.py"", line 368, in _send; n = write(self._handle, buf); BrokenPipeError: [Errno 32] Broken pipe; ```; There are many BrokenPipeErorr below, I just snapshot part of it. Do you have any idea why this error happens?. **Setup**; - Operating system: Ubuntu 22.04; - DeepVariant version: v1.6.1; - Installation method : singularity; - Type of data: ONT sequencing data",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/804:4055,queue,queues,4055,,https://github.com/google/deepvariant/issues/804,1,['queue'],['queues']
Performance,"ons, rebuild TensorFlow with the appropriate compiler flags.; I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2023-07-15 14:07:10.226851: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled; WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpl_zjylv7; W0715 14:07:10.308488 47821886322496 estimator.py:1864] Using temporary folder as model directory: /tmp/tmpl_zjylv7; INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 10000; I0715 14:07:10.309390 47821886322496 estimator.py:202] Using config: {'_model_dir': '/tmp/tmpl_zjylv7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_c; I0715 14:07:10.309847 47821886322496 call_variants.py:446] Writing calls to /tmp/call_variants_output.tfrecord.gz; INFO:tensorflow:Calling model_fn.; I0715 14:07:10.758",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:2310,Tune,Tune,2310,,https://github.com/google/deepvariant/issues/679,2,"['Tune', 'perform']","['Tune', 'performance']"
Performance,"opt/models/wes/model.ckpt"". I0424 15:59:50.266534 139872277903104 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-04-24 15:59:50.321136: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations: AVX2 FMA; To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.; 2020-04-24 15:59:50.376605: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2904000000 Hz; 2020-04-24 15:59:50.378224: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56a1fd0 executing computations on platform Host. Devices:; 2020-04-24 15:59:50.378283: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-04-24 15:59:50.380979: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I0424 15:59:50.447775 139872277903104 modeling.py:563] Initializing model with random parameters; W0424 15:59:50.449538 139872277903104 estimator.py:1821] Using temporary folder as model directory: /tmp/tmp3bl4tsmc; I0424 15:59:50.450443 139872277903104 estimator.py:212] Using config: {'_model_dir': '/tmp/tmp3bl4tsmc', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': , '_keep_checkpoint_max': 100000, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f3659263518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '',",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/304:1392,Tune,Tune,1392,,https://github.com/google/deepvariant/issues/304,2,"['Tune', 'perform']","['Tune', 'performance']"
Performance,"or updating:; Use standard file APIs to check for files with this prefix.; I0208 09:29:54.405941 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:55.469674 139859027293952 session_manager.py:491] Running local_init_op.; I0208 09:29:55.510524 139859027293952 session_manager.py:493] Done running local_init_op.; I0208 09:29:55.864006 139859027293952 modeling.py:410] Reloading EMA...; I0208 09:29:55.864634 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]; ```; top. Looks like there is a lot of under utilized compute resources; ```; top - 00:16:22 up 1 day, 23:24, 1 user, load average: 16.03, 16.02, 16.00; Tasks: 621 total, 1 running, 620 sleeping, 0 stopped, 0 zombie; %Cpu(s): 25.0 us, 0.0 sy, 0.0 ni, 75.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st; KiB Mem : 52262400+total, 48358937+free, 8878616 used, 30156016 buff/cache; KiB Swap: 0 total, 0 free, 0 used. 51216832+avail Mem . PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND ; 23437 root 20 0 27.319g 3.709g 136440 S 800.0 0.7 18646:29 python ; 23944 root 20 0 27.259g 3.687g 137396 S 799.7 0.7 18613:17 python ; 1 root 20 0 119604 5788 4112 S 0.0 0.0 0:03.37 systemd ; 2 root 20 0 0 0 0 S 0.0 0.0 0:00.01 kthreadd ; 3 root 20 0 0 0 0 S 0.0 0.0 0:00.06 ksoftirqd/0 ; 4 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0 ; ```; search for recently modified files; ```; ubuntu@ip-172-31-21-181:/deepTmp$ sudo find . -type f -printf '%T@ %p\n' | sort -n | tail -1 | cut -f2- -d"" ""; ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; ubuntu@ip-172-31-21-181:/deepTmp$ ls -l ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; -rw-r--r-- 1 root 0 Feb 8 09:29 ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; ubuntu@ip-172-31-21-181:/deepTmp$ date; Mon Feb 10 00:22:46 UTC 2020; ubuntu@ip-172-31-21-181:/deepTmp$ ; ```. I also ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/269:4067,cache,cache,4067,,https://github.com/google/deepvariant/issues/269,1,['cache'],['cache']
Performance,"orflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format.; WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets; WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object return",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:1487,load,loaded,1487,,https://github.com/google/deepvariant/issues/722,1,['load'],['loaded']
Performance,ormance hint: _common.pyx:299:19: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:322:50: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:426:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:465:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:509:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:592:36: Exception check after calling 'f0' will always require the GIL to be acquired. Declare 'f0' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:596:36: Exception check after calling 'f1' will always require the GIL to be acquired. Declare 'f1' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:600:36: Exception check after calling 'f2' will always require the GIL to be acquired. Declare 'f2' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:604:36: Exception check ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:3759,perform,performance,3759,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance,"ot/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:; #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.1 Repository rule _tf_http_archive defined at:; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.2 (21:51:08) Loading: 0 packages loaded; #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s; #16 1497.3 (21:51:09) INFO: 0 processes.; #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded); #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:8246,cache,cache,8246,,https://github.com/google/deepvariant/issues/608,1,['cache'],['cache']
Performance,performance re:openvino,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/408:0,perform,performance,0,,https://github.com/google/deepvariant/issues/408,1,['perform'],['performance']
Performance,plicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true; #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30; #16 1490.1 (21:51:01) Loading:; #16 1490.1 (21:51:01) Loading: 0 packages loaded; #16 1491.1 (21:51:02) Loading: 0 packages loaded; #16 1492.2 (21:51:03) Loading: 0 packages loaded; #16 1493.2 (21:51:04) Loading: 0 packages loaded; #16 1494.2 (21:51:05) Loading: 0 packages loaded; #16 1495.2 (21:51:06) Loading: 0 packages loaded; #16 1496.2 (21:51:07) Loading: 0 packages loaded; #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:; #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.0 Repository rule _tf_http_archive defined at:; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.0 (21:51:08) WARNING: Download from http://mirror.t,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:5012,Load,Loading,5012,,https://github.com/google/deepvariant/issues/608,2,"['Load', 'load']","['Loading', 'loaded']"
Performance,"pr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o; neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs; I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']; I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Pr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:8415,optimiz,optimized,8415,,https://github.com/google/deepvariant/issues/653,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,pt/tensorflow/.bazelrc: --define framework_shared_object=false; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true; #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30; #16 1490.1 (21:51:01) Loading:; #16 1490.1 (21:51:01) Loading: 0 packages loaded; #16 1491.1 (21:51:02) Loading: 0 packages loaded; #16 1492.2 (21:51:03) Loading: 0 packages loaded; #16 1493.2 (21:51:04) Loading: 0 packages loaded; #16 1494.2 (21:51:05) Loading: 0 packages loaded; #16 1495.2 (21:51:06) Loading: 0 packages loaded; #16 1496.2 (21:51:07) Loading: 0 packages loaded; #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:; #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.0 Repository rule _tf_http_archive defined at:; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/thi,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:4912,Load,Loading,4912,,https://github.com/google/deepvariant/issues/608,2,"['Load', 'load']","['Loading', 'loaded']"
Performance,"py:257] Task 6/32: Preparing inputs; [W::hts_idx_load3] The index file is older than the data file: HG003_PacBio_GRCh37.bam.bai; I0413 03:58:44.217749 140138481125184 genomics_reader.py:222] Reading HG003_PacBio_GRCh37.bam with NativeSamReader; I0413 03:58:44.238726 140138481125184 make_examples_core.py:257] Task 6/32: Common contigs are ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', 'X', 'Y', 'MT']; [W::hts_idx_load3] The index file is older than the data file: HG003_PacBio_GRCh37.bam.bai; I0413 03:58:44.459303 140102426851136 genomics_reader.py:222] Reading HG003_PacBio_GRCh37.bam with NativeSamReader; I0413 03:58:44.479173 140102426851136 make_examples_core.py:257] Task 8/32: Preparing inputs; 2023-04-13 03:58:42.980890: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; [W::hts_idx_load3] The index file is older than the data file: HG003_PacBio_GRCh37.bam.bai; I0413 03:58:43.703165 140193998387008 genomics_reader.py:222] Reading HG003_PacBio_GRCh37.bam with NativeSamReader; I0413 03:58:43.723608 140193998387008 make_examples_core.py:257] Task 1/32: Preparing inputs; [W::hts_idx_load3] The index file is older than the data file: HG003_PacBio_GRCh37.bam.bai; I0413 03:58:43.837812 140193998387008 genomics_reader.py:222] Reading HG003_PacBio_GRCh37.bam with NativeSamReader; I0413 03:58:43.859502 140193998387008 make_examples_core.py:257] Task 1/32: Common contigs are ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', 'X', 'Y', 'MT']; I0413 03:58:44.071241 140193998387008 make_examples_core.py:257] Task 1/32: Starting from v0.9.0, --use_ref_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:15241,optimiz,optimized,15241,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.5519920587539673, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1_weighted=1.0, train/false_negatives=0.0, train/false_positives=0.0, train/learning_rate=9.999999747378752e-05, train/loss=0.551992654800415, train/precisio",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12779,tune,tune,12779,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"r TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants; output_queue = multiprocessing.Queue(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__; self._rlock = ctx.Lock(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock; return Lock(ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__; SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__; sl = self._semlock = _multiprocessing.SemLock(; FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s; user 0m6.224s; sys 0m3.683s. ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```; gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733:2892,queue,queues,2892,,https://github.com/google/deepvariant/issues/733,1,['queue'],['queues']
Performance,"r,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}; ##bcftools_viewVersion=1.10.2+htslib-1.10.2; ##bcftools_viewCommand=view Case1.glnexus.merged.bcf; Date=Tue Feb 15 12:15:20 2022; ```. ### Variant line; ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband; X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:..; ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:; Pipeline; ```; # Load singularity; module load singularity; BIN_VERSION=""1.1.0"". # Load env for bcftools; ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/; source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh; conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped; export SINGULARITY_CACHEDIR=$PWD; singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads; NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered); cd $SLURM_SUBMIT_DIR; WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space; mkdir -p $WORKING_DIR; cd $WORKING_DIR. #### GRCh38 #### ; echo ""GRCh38 genome""; GENOME=GRCh38; FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/; FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS; BAM_DIR=$WORKING_DIR; Case_ID=Case1; FAMILY_ID=$Case_ID; PROBAND_ID=${Case_ID}_proband; MOTHER_ID=${Case_ID}_mother; FATHER_ID=${Case_ID}_father. PROBAND_BAM=${PROBAND_ID}.sorted.bam; FATHER_BAM=${FATHER_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518:6920,Load,Load,6920,,https://github.com/google/deepvariant/issues/518,1,['Load'],['Load']
Performance,"r.gpu.output.g.vcf.gz \; --num_shards=$(nproc) \; --customized_model=input/weights-51-0.995354.ckpt; INFO: /usr/local/etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-02-17 23:31:25.687399: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-02-17 23:31:39.809521: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:31:39.810043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-02-17 23:31:59.620996: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; I0217 23:31:59.623967 140288433825600 run_deepvariant.py:519] Re-using the directory for intermediate res",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:3041,optimiz,optimized,3041,,https://github.com/google/deepvariant/issues/774,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"r.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:; #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.1 Repository rule _tf_http_archive defined at:; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.2 (21:51:08) Loading: 0 packages loaded; #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.3 (21:51:09) INFO: Elapsed time:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:8041,cache,cache,8041,,https://github.com/google/deepvariant/issues/608,1,['cache'],['cache']
Performance,"rain/recall=1.0, train/recall_het=0.0, train/recall_homalt=0.0, train/recall_homref=1.0, train/true_negatives=12800.0, train/true_positives=6400.0; I0829 07:28:44.566404 140318776715072 local.py:41] Setting work unit notes: 0.3 steps/s, 61.4% (13948/22724), ETA: 8h42m; I0829 07:28:44.568708 140305134778112 logging_writer.py:48] [13948] steps_per_sec=0.280075; I0829 07:28:44.568793 140305134778112 logging_writer.py:48] [13948] uptime=74874.6; I0829 07:31:25.151819 140318776715072 train.py:361] Running tune at step=13993 epoch=0; I0829 07:31:25.152109 140318776715072 train.py:366] Tune step 0 / 3162 (0.0%); I0829 07:33:17.573163 140318776715072 train.py:366] Tune step 100 / 3162 (0.0%); I0829 07:35:10.013494 140318776715072 train.py:366] Tune step 200 / 3162 (10.0%); I0829 07:37:02.497336 140318776715072 train.py:366] Tune step 300 / 3162 (10.0%); I0829 07:38:54.834164 140318776715072 train.py:366] Tune step 400 / 3162 (10.0%); I0829 07:40:47.319165 140318776715072 train.py:366] Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:9896,Tune,Tune,9896,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"reads /home/paul/exome-case-study/input/data/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam --examples /home/paul/exome-case-study/output/HG002.examples.tfrecord@1.gz --regions /home/paul/exome-case-study/input/data/refseq.coding_exons.b37.extended50.bed --task 0; Illegal instruction (core dumped); $; $ mkdir make-examples && cd make-examples; $ cd unzip ~/exome-case-study/input/bin/make_examples.zip; $ cd runfiles/genomics; $; $ PYTHONPATH=. /usr/bin/python deepvariant/make_examples.py --mode calling --ref /home/paul/exome-case-study/input/data/hs37d5.fa.gz --reads /home/paul/exome-case-study/input/data/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam --examples /home/paul/exome-case-study/output/HG002.examples.tfrecord@1.gz --regions /home/paul/exome-case-study/input/data/refseq.coding_exons.b37.extended50.bed --task 0; Illegal instruction (core dumped); $; ```; After digging a bit deeper, I noticed that loading the `pileup_image_native` module was causing this issue. I was curious and looked at the assembly instructions:. ```; $ gdb -ex r --args python -c ""from deepvariant.python import pileup_image_native""; Program received signal SIGILL, Illegal instruction.; 0x00007ffff5d308b4 in google::protobuf::DescriptorPool::Tables::Tables() (); from /home/paul/make-examples/runfiles/genomics/deepvariant/python/../../_solib_k8/libexternal_Sprotobuf_Uarchive_Slibprotobuf.so; (gdb) disassemble $pc,$pc+32; Dump of assembler code from 0x7ffff5d308b4 to 0x7ffff5d308d4:; => 0x00007ffff5d308b4 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+676>: vpxor %xmm0,%xmm0,%xmm0; 0x00007ffff5d308b8 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+680>: lea 0x1b0(%rbx),%rax; 0x00007ffff5d308bf <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+687>: movl $0x0,0x1b0(%rbx); 0x00007ffff5d308c9 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+697>: movq $0x0,0x1b8(%rbx); End of assembler dump.; (gdb);",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21:2069,load,loading,2069,,https://github.com/google/deepvariant/issues/21,1,['load'],['loading']
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.431128: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.649384: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.283502: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.696454: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.910042: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.104699: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:3083,optimiz,optimized,3083,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.520424: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.431128: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.649384: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.283502: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.696454: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.910042: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:2720,optimiz,optimized,2720,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.649384: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.283502: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.696454: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.910042: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.104699: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.553518: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:3446,optimiz,optimized,3446,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.696454: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.910042: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.104699: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.553518: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:38.222775: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:38.003846: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:4172,optimiz,optimized,4172,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.104699: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.553518: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:38.222775: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:38.003846: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:38.914371: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.018427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:4898,optimiz,optimized,4898,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.283502: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.696454: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.910042: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.104699: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.553518: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:38.222775: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:3809,optimiz,optimized,3809,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.553518: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:38.222775: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:38.003846: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:38.914371: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.018427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.422219: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:5261,optimiz,optimized,5261,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.910042: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.104699: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:37.553518: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:38.222775: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:38.003846: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:38.914371: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:4535,optimiz,optimized,4535,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:38.003846: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:38.914371: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.018427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.422219: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.063431: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.062198: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:5987,optimiz,optimized,5987,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:38.222775: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:38.003846: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:38.914371: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.018427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.422219: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.063431: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:5624,optimiz,optimized,5624,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:38.914371: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.018427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.422219: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.063431: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.062198: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.979949: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:6350,optimiz,optimized,6350,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.018427: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.422219: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.063431: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.062198: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.979949: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.478141: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:6713,optimiz,optimized,6713,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.062198: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.979949: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.478141: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.621575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.375862: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.851554: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:7802,optimiz,optimized,7802,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.422219: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.063431: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.062198: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.979949: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.478141: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.621575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:7076,optimiz,optimized,7076,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.979949: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.478141: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.621575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.375862: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.851554: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:41.520879: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:8165,optimiz,optimized,8165,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.063431: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.062198: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:39.979949: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.478141: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.621575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.375862: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:7439,optimiz,optimized,7439,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.375862: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.851554: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:41.520879: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.835700: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:41.522882: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.200511: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:9254,optimiz,optimized,9254,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.478141: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.621575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.375862: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.851554: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:41.520879: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.835700: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:8528,optimiz,optimized,8528,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.621575: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.375862: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.851554: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:41.520879: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.835700: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:41.522882: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:8891,optimiz,optimized,8891,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.835700: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:41.522882: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.200511: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:41.688520: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; [W::hts_idx_load3] The index file is older than the data file: HG003_PacBio_GRCh37.bam.bai; I0413 03:58:42.598038 140301397178176 genomics_reader.py:222] Reading HG003_PacBio_GRCh37.bam with NativeSamReader; I0413 03:58:42.617601 140301397178176 make_examples_core.py:257] Task 0/32: Preparing inputs; [W::hts_idx_load3] The index file is older than the data file: HG003_PacBio_GRCh37.bam.bai; I0413 03:58:42.913093 140301397178176 genomics_reader.py:222] Reading HG003_PacBio_GRCh37.bam wit",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:10343,optimiz,optimized,10343,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.851554: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:41.520879: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.835700: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:41.522882: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.200511: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:41.688520: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:9617,optimiz,optimized,9617,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:41.520879: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:40.835700: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:41.522882: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.200511: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:41.688520: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; [W::hts_idx_load3] The index file is older than the data file: HG003_PacBio_GRCh37.bam.bai; I0413 03:58:42.598038 14030139717817",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:9980,optimiz,optimized,9980,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:41.522882: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.200511: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:41.688520: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; [W::hts_idx_load3] The index file is older than the data file: HG003_PacBio_GRCh37.bam.bai; I0413 03:58:42.598038 140301397178176 genomics_reader.py:222] Reading HG003_PacBio_GRCh37.bam with NativeSamReader; I0413 03:58:42.617601 140301397178176 make_examples_core.py:257] Task 0/32: Preparing inputs; [W::hts_idx_load3] The index file is older than the data file: HG003_PacBio_GRCh37.bam.bai; I0413 03:58:42.913093 140301397178176 genomics_reader.py:222] Reading HG003_PacBio_GRCh37.bam with NativeSamReader; I0413 03:58:42.934488 140301397178176 make_examples_core.py:257] Task 0/32: Common contigs are ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', 'X', 'Y', 'MT']; I0413 03:58:43.149506 140301397178176 make_examples_core.py:257] Task 0/32: Starting from v0.9.0, --use_ref_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:10706,optimiz,optimized,10706,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.565788: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.798464: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.381208: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; [W::hts_idx_load3] The index file is older than the data file: HG003_PacBio_GRCh37.bam.bai; I0413 03:58:44.143414 140138481125184 genomics_reader.py:222] Reading HG003_PacBio_GRCh37.bam with NativeSamReader; I0413 03:58:44.163062 140138481125184 make_examples_core.py:257] Task 6/32: Preparing inputs; [W::hts_idx_load3] The index file is older than the data file: HG003_PacBio_GRCh37.bam.bai; I0413 03:58:44.217749 140138481125184 genomics_reader.py:222] Reading HG003_PacBio_GRCh37.bam with NativeSamReader; I0413 03:58:44.238726 140138481125184 make_examples_core.py:257] Task 6/32: Common contigs are ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', 'X', 'Y', 'MT']; [W::hts_idx_load3] The index file is older than the data file: HG003_PacBio_GRCh37.bam.bai; I0413 03:58:44.4",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:13830,optimiz,optimized,13830,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.980837: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.565788: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.798464: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:42.381208: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; [W::hts_idx_load3] The index file is older than the data file: HG003_PacBio_GRCh37.bam.bai; I0413 03:58:44.143414 140138481125184 genomics_reader.py:222] Reading HG003_PacBio_GRCh37.bam with NativeSamReader; I0413 03:58:44.163062 140138481125184 make_examples_core.py:257] Task 6/32: Preparing inputs; [W::hts_idx_load3] The index file is older than the data file: HG003_PacBio_GRCh37.bam.bai; I0413 03:58:44.217749 140138481125184 genomics_reader.py:222] Reading HG003_PacBio_GRCh37.bam wit",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:13467,optimiz,optimized,13467,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"reference genome, anything special that is unlike the case studies?) Pacbio Revel fresh data. . **Steps to reproduce:**; - Command: docker run --gpus 1 google/deepvariant:1.5.0-gpu or docker run --gpus 1 google/deepvariant:1.6.1-gpu; - Error trace: (if applicable); ...; CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-03 17:21:57.549571: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-07-03 17:21:57.644332: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2024-07-03 17:21:58.247052: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-07-03 17:21:58.247080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libra",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:1473,optimiz,optimized,1473,,https://github.com/google/deepvariant/issues/844,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prog.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by ',MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:13374,cache,cache,13374,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:13657,cache,cache,13657,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,renced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_array.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.h' contains an error and its package is in error and reference,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:17671,cache,cache,17671,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,renced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/utf.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/deepvariant/deepvariant/testing/BUILD:19:1: Target '@com_googlesource_code_re2//:re2' contains an error and its package is in error and referenced by '//deepvariant/testing:gunit_extras'; (09:27:18) ERROR: Analysis of target '//,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:19672,cache,cache,19672,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,"rently loading: deepvariant/core/genomics ... (3 packages); (09:27:08) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:09) Analyzing: 242 targets (15 packages loaded); (09:27:11) Analyzing: 242 targets (16 packages loaded); (09:27:12) Analyzing: 242 targets (18 packages loaded); (09:27:14) Analyzing: 242 targets (31 packages loaded); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.; (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:100:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:102:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:104:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:106:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:108:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:110:1: name 're2_test' is not defined ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:4475,cache,cache,4475,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,"risk having version incompatibilities between our C++ code and the Python; # code we use at runtime.; if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; export DV_CPP_TENSORFLOW_TAG=""master""; else; export DV_CPP_TENSORFLOW_TAG=""r1.12""; fi; export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing; # value in the environment (allowing command line control of the build),; # defaulting to 0 (CPU only build).; export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file; # compiled with MKL support for corei7 or better chipsets, which; # significantly speeds up execution when running on modern CPUs. The default; # TensorFlow wheel files don't contain these instructions (and thereby run on a; # broader set of CPUs). Using this optimized wheel reduces the runtime of; # DeepVariant's call_variants step by >3x. This is called the GCP (Google Cloud; # Platform) optimized wheel because all GCP instances have at least Sandy Bridge; # or better chipsets, so this wheel should run anywhere on GCP.; export DV_USE_GCP_OPTIMIZED_TF_WHL=""${DV_USE_GCP_OPTIMIZED_TF_WHL:-1}""; export GCP_OPTIMIZED_TF_WHL_FILENAME=""tensorflow-${DV_GCP_OPTIMIZED_TF_WHL_VERSION}.deepvariant_gcp-cp27-none-linux_x86_64.whl""; export GCP_OPTIMIZED_TF_WHL_PATH=""${DV_PACKAGE_BUCKET_PATH}/tensorflow""; export GCP_OPTIMIZED_TF_WHL_CURL_PATH=""${DV_PACKAGE_CURL_PATH}/tensorflow"". # Set this to 1 to make our prereq scripts install the CUDA libraries.; # If you already have CUDA installed, such as on a properly provisioned; # Docker image, it shouldn't be necessary.; export DV_INSTALL_GPU_DRIVERS=""${DV_INSTALL_GPU_DRIVERS:-0}"". export PYTHON_BIN_PATH=$(which python); export USE_DEFAULT_PYTHON_LIB_PATH=1; export DV_COPT_FLAGS=""--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-st",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:5676,optimiz,optimized,5676,,https://github.com/google/deepvariant/issues/145,1,['optimiz'],['optimized']
Performance,"rt DV_TF_NIGHTLY_BUILD=0; ++ DV_TF_NIGHTLY_BUILD=0; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ DV_COPT_FLAGS='--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ export DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; ++ DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; ..................; (09:27:04) INFO: Current date is 2017-12-21; (09:27:04) Loading: ; (09:27:04) Loading: 0 packages loaded; (09:27:05) Loading: 0 packages loaded; (09:27:06) Loading: 7 packages loaded; currently loading: deepvariant/core/genomics ... (6 packages); (09:27:07) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:08) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:09) Analyzing: 242 targets (15 packages loaded); (09:27:11) Analyzing: 242 targets (16 packages loaded); (09:27:12) Analyzing: 242 targets (18 packages loaded); (09:27:14) Analyzing: 242 targets (31 packages loaded); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=false to temporarily disable this check.; (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:98:1: name 're2_test' is not defined (did you mean 'ios_test'?); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_r",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:3539,Load,Loading,3539,,https://github.com/google/deepvariant/issues/19,3,"['Load', 'load']","['Loading', 'loaded', 'loading']"
Performance,"s same source-code is placed so that it is seen by the build-prereq.sh script. I set the `export DV_USE_PREINSTALLED_TF=1`. In settings.sh, I changed DV_BAZEL_VERSION to DV_BAZEL_VERSION=""0.15.0-"" (to match the bazel version above). I also removed the corei7 option in DV_COPT_FLAGS. . In build-prereq.sh, I hard-coded the following in: `DV_PLATFORM=""ubuntu-16""`, since `lsb_release` didn't match the case statement conditions there. The following is the result `lsb_release`.; root@1f07cee05809:~/deepvariant# lsb_release; LSB Version: core-9.20160110ubuntu0.2-noarch:core-9.20160110ubuntu0.2-ppc64el:security-9.20160110ubuntu0.2-noarch:security-9.20160110ubuntu0.2-ppc64el. After these changes, build-prereq.sh runs fine. However, build_and_test.sh fails with the following error:; (03:21:40) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:424:1: ClifProtoLibraryGeneration third_party/nucleus/protos/reads_pyclif.h failed (Exit 2): proto failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/genfiles/third_party/nucleus/protos/reads_pyclif.cc -h bazel-out/ppc-opt/genfiles/third_party/nucleus/protos/reads_pyclif.h '--strip_dir=bazel; -out/ppc-opt/genfiles' '--source_dir='\''.'\''' third_party/nucleus/protos/reads.proto); bazel-out/host/bin/external/clif/proto: 3: bazel-out/host/bin/external/clif/proto: __requires__: not found; bazel-out/host/bin/external/clif/proto: 4: bazel-out/host/bin/external/clif/proto: import: not found; bazel-out/host/bin/external/clif/proto: 5: bazel-out/host/bin/external/clif/proto: import: not found; bazel-out/host/bin/external/clif/proto: 6: bazel-out/host/bin/external/clif/proto: from: not found; bazel-out/host/bin/external/clif/proto: 9: bazel-out/host/bin/external/clif/proto: Syntax error: ""("" unexpected (expecting ""then""). It would be great if I can get some help on this. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/122:1312,cache,cache,1312,,https://github.com/google/deepvariant/issues/122,1,['cache'],['cache']
Performance,se exceptions.; performance hint: _common.pyx:322:50: Exception check after calling 'random_func' will always require the GIL to be acquired. Declare 'random_func' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:426:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:465:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:509:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:592:36: Exception check after calling 'f0' will always require the GIL to be acquired. Declare 'f0' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:596:36: Exception check after calling 'f1' will always require the GIL to be acquired. Declare 'f1' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:600:36: Exception check after calling 'f2' will always require the GIL to be acquired. Declare 'f2' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:604:36: Exception check after calling 'f3' will always require the GIL to be acquired. Declare 'f3' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:638:31: Exception chec,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:3997,perform,performance,3997,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance,"sets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-08-28 10:40:42.588215: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; I0828 10:40:42.589054 140318776715072 train.py:92] Running with debug=False; I0828 10:40:42.589343 140",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:2455,tune,tune,2455,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"sr/lib/locale/ --bind $ccsbam:$ccsbam --bind $ccsbam.bai:$ccsbam.bai --bind $fasta:$fasta --bind $fasta.fai:$fasta.fai --bind /hwfssz5/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/translatome/HCT116/PacBio/analysis/deepvariant.v3/T202302180201:/output /hwfssz1/ST_SUPERCELLS/P21Z10200N0125/zhanghaibo/software/deepvariant/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=$fasta --reads=$ccsbam --output_vcf=/output/T202302180201.deepvariant.vcf.gz --output_gvcf=/output/T202302180201.deepvariant.g.vcf.gz --num_shards=20 --intermediate_results_dir=/tmp. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examples.tfrecord@20.gz"" --checkpoint ""/opt/models/pacbio/model.ckpt"". 2023-07-15 14:06:58.063861: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0715 14:07:10.199614 47821886322496 call_variants.py:317] From /tmp/make_examples.tfrecord-00000-of-00020.gz.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; I0715 14:07:10.205330 47821886322496 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; 2023-07-15 14:07:10.211204: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical oper; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-07-15 14:07:10.223654: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread poo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:1182,optimiz,optimized,1182,,https://github.com/google/deepvariant/issues/679,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"ssw_init; | ; --0.92%--qP_byte. 28.27% , 0.04% ,python ,libssw.so ,[.] ssw_align; | ; --28.23%--ssw_align; | ; |--14.88%--sw_sse2_word; | ; |--8.45%--sw_sse2_byte; | ; |--2.89%--banded_sw; | ; --1.19%--__memcpy_sse2_unaligned. 14.88% , 14.86% ,python ,libssw.so ,[.] sw_sse2_word; | ; --14.86%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --14.86%--ssw_align; sw_sse2_word. 8.45% , 8.43% ,python ,libssw.so ,[.] sw_sse2_byte; | ; --8.43%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --8.43%--ssw_align; sw_sse2_byte. 4.72% , 0.00% ,python ,[unknown] ,[.] 0x00000000009063e0; |; ---0x9063e0; | ; --3.94%--PyEval_EvalFrameEx; | ; --3.57%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | ; --3.32%--learning::genomics::deepvariant::DeBruijnGraph::Build; | ; --3.02%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | ; --2.63%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | ; --1.89%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | ; --1.60%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | ; --0.56%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. To do this properly would require that the tests be performed on different datasets, and different CPUs on the same Cloud environment - with different distributed scenarios - which would be cost-prohibitive for me. Hope it helps and have a great weekend!; Paul",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:12317,perform,performed,12317,,https://github.com/google/deepvariant/issues/50,1,['perform'],['performed']
Performance,"start-output/examples.tfrecord.gz; 2018-12-20 07:17:31.684869: I third_party/nucleus/io/sam_reader.cc:561] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2018-12-20 07:17:31.688126: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.688252 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.884236 140029649073920 make_examples.py:1119] Task 0: 6 candidates (6 examples) [0.20s elapsed]; I1220 07:17:33.209975 140029649073920 make_examples.py:1134] Writing MakeExamplesRunInfo to /home/chungtsai_su/quickstart-output/examples.tfrecord.gz.run_info.pbtxt; I1220 07:17:33.241107 140029649073920 make_examples.py:1137] Found 76 candidate variants; I1220 07:17:33.241497 140029649073920 make_examples.py:1138] Created 82 examples; ```. Also, the problem can be detected in test case. ```; //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s; //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s; //deepvariant/vendor:timer_test PASSED in 0.8s; //deepvariant:make_examples_test FAILED in 2 out of 2 in 1.7s; Stats over 2 runs: max = 1.7s, min = 1.6s, avg = 1.7s, dev = 0.1s; /home/chungtsai_su/.cache/bazel/_bazel_chungtsai_su/959496e1d4e585c03b8886e389170de9/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /home/chungtsai_su/.cache/bazel/_bazel_chungtsai_su/959496e1d4e585c03b8886e389170de9/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test PASSED in 49.7s; Stats over 10 runs: max = 49.7s, min = 2.6s, avg = 9.1s, dev = 13.6s; //deepvariant:model_train_test PASSED in 127.0s; Stats over 10 runs: max = 127.0s, min = 2.7s, avg = 42.5s, dev = 47.3s. Executed 38 out of 38 tests: 37 tests pass and 1 fails locally.; (06:34:35) INFO: Build completed, 1 test FAILED, 2471 total actions; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131:5712,cache,cache,5712,,https://github.com/google/deepvariant/issues/131,2,['cache'],['cache']
Performance,"stats are reported for homref though. I have now tried running the training several times with different hyperparameters but so far still no change at the het or homalt eval stats. . My first, very simple question is thus, are these eval stats truly 0 (i.e. the model is very bad) or is 0.0 some starting value and there are not enough data to calculate them initially? I am warmstarting from the 1.6.1 wgs model so I cant imagine the model is really that bad at calling variants initially, even if in a fish. . **Setup**; Running on a university computing cluster (https://hpc-unibe-ch.github.io/) ; OS: Rocky 9.3 Blue Onyx; GPU: rtx4090 ; Installation: Running from Docker image via singularity; DV version: 1.6.1. **Data**; I am training on examples from 5 individuals, data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had hig",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:1287,tune,tune,1287,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:11302,Tune,Tune,11302,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_w",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:11385,Tune,Tune,11385,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:11468,Tune,Tune,11468,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/prec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:11551,Tune,Tune,11551,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:11634,Tune,Tune,11634,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:11717,Tune,Tune,11717,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:11800,Tune,Tune,11800,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:11883,Tune,Tune,11883,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:11966,Tune,Tune,11966,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12049,Tune,Tune,12049,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,t/tensorflow/.bazelrc: --copt=-w --host_copt=-w --define=PREFIX=/usr --define=LIBDIR=$(PREFIX)/lib --define=INCLUDEDIR=$(PREFIX)/include --define=PROTOBUF_INCLUDE_PATH=$(PREFIX)/include --cxxopt=-std=c++14 --host_cxxopt=-std=c++14 --config=dynamic_kernels --distinct_host_configuration=false --experimental_guard_against_concurrent_changes; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:dynamic_kernels in file /opt/tensorflow/.bazelrc: --define=dynamic_loaded_kernels=true --copt=-DAUTOLOAD_DYNAMIC_KERNELS; #16 1489.8 (21:51:01) INFO: Found applicable config definition build:linux in file /opt/deepvariant/.bazelrc: --distinct_host_configuration=true; #16 1490.1 (21:51:01) INFO: Current date is 2023-01-30; #16 1490.1 (21:51:01) Loading:; #16 1490.1 (21:51:01) Loading: 0 packages loaded; #16 1491.1 (21:51:02) Loading: 0 packages loaded; #16 1492.2 (21:51:03) Loading: 0 packages loaded; #16 1493.2 (21:51:04) Loading: 0 packages loaded; #16 1494.2 (21:51:05) Loading: 0 packages loaded; #16 1495.2 (21:51:06) Loading: 0 packages loaded; #16 1496.2 (21:51:07) Loading: 0 packages loaded; #16 1497.0 (21:51:08) INFO: Repository tf_runtime instantiated at:; #16 1497.0 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:28:15: in workspace; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/tf_runtime/workspace.bzl:12:20: in repo; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.0 Repository rule _tf_http_archive defined at:; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archiv,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:5062,Load,Loading,5062,,https://github.com/google/deepvariant/issues/608,2,"['Load', 'load']","['Loading', 'loaded']"
Performance,"t_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log; -------; STARTING DEEPVARIANT; I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/; I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****; ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfrecord@64.gz"" --noadd_hp_channel --alt_aligned_pileup ""none"" --gvcf ""/cromwell_root/pepper_output/dv_intermediate_outputs/gvcf.tfrecord@64.gz"" --min_base_quality ""1"" --min_mapping_quality ""1"" --parse_sam_aux_fields --proposed_variants ""/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --norealign_reads --sample_name ""6061-SL-0029"" --sort_by_haplotypes --variant_cal",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:22972,load,load,22972,,https://github.com/google/deepvariant/issues/491,1,['load'],['load']
Performance,"tart-output"". singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ \; /fh/fast/furlan_s/grp/sifs/deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz. - Error trace: (if applicable) SEE BELOW. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. YES THIS IS WITH THE QUICK START EXAMPLE. **Any additional context:**. Message:. 2023-05-02 14:40:43.757041: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0502 14:40:56.961649 140501830911808 run_deepvariant.py:364] Re-using the directory for intermediate results in /fh/scratch/delete90/furlan_s/targ_reseq/230117_Sami/AML_1101_merge/tmp_dir/tmpzz53zv8p. ***** Intermediate results will be written to /fh/scratch/delete90/furlan_s/targ_reseq/230117_Sami/AML_1101_merge/tmp_dir/tmpzz53zv8p in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/fh/scratch/delete90/furlan_s/targ_reseq/230117_Sami/AML_1101_merge/tmp_dir/tmpzz53zv8p/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/fh/scratch/delete90/furlan_s/targ_reseq/230117_Sami/AML_1101_merge/tmp_dir/tmpzz53zv8p/gvcf.tfrecord@1.gz"" --regions ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640:1966,optimiz,optimized,1966,,https://github.com/google/deepvariant/issues/640,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,ter.py:48] [13948] uptime=74874.6; I0829 07:31:25.151819 140318776715072 train.py:361] Running tune at step=13993 epoch=0; I0829 07:31:25.152109 140318776715072 train.py:366] Tune step 0 / 3162 (0.0%); I0829 07:33:17.573163 140318776715072 train.py:366] Tune step 100 / 3162 (0.0%); I0829 07:35:10.013494 140318776715072 train.py:366] Tune step 200 / 3162 (10.0%); I0829 07:37:02.497336 140318776715072 train.py:366] Tune step 300 / 3162 (10.0%); I0829 07:38:54.834164 140318776715072 train.py:366] Tune step 400 / 3162 (10.0%); I0829 07:40:47.319165 140318776715072 train.py:366] Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 220,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:10306,Tune,Tune,10306,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PYTHON_BIN_PATH=/opt/at11.0/bin/python \; PYTHON_LIB_PATH=/opt/at11.0/lib64/python3.6/site-packages \; TF_CONFIGURE_IOS=0 \; TF_CUDA_COMPUTE_CAPABILITIES=3.7,6.0,7.0 \; TF_CUDA_VERSION=10.0 \; TF_CUDNN_VERSION=7 \; TF_NEED_CUDA=1 \; /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/external/org_tensorflow/tensorflow/tools/git/gen_git_source --generate external/local_config_git/gen/spec.json external/local_config_git/gen/head external/local_config_git/gen/branch_ref ""bazel-out/ppc-opt/bin/external/org_tensorflow/tensorflow/core/util/version_info.cc"" --git_tag_override=${GIT_TAG_OVERRIDE:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/356:1639,cache,cache,1639,,https://github.com/google/deepvariant/issues/356,1,['cache'],['cache']
Performance,"time/gpu/gpu_device.cc:1030] Found device 0 with properties:; name: Tesla P100-PCIE-12GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285; pciBusID: 0000:3b:00.0; totalMemory: 11.91GiB freeMemory: 11.62GiB; 2018-05-02 10:58:57.263682: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla P100-PCIE-12GB, pci bus id: 0000:3b:00.0, compute capability: 6.0); INFO:tensorflow:Restoring parameters from /tmp/deepvariant/model.ckpt-0; I0502 10:58:57.455770 139632719935232 tf_logging.py:82] Restoring parameters from /tmp/deepvariant/model.ckpt-0; INFO:tensorflow:Starting Session.; I0502 10:59:09.842276 139632719935232 tf_logging.py:82] Starting Session.; INFO:tensorflow:Saving checkpoint to path /tmp/deepvariant/model.ckpt; I0502 10:59:10.099534 139621333726976 tf_logging.py:82] Saving checkpoint to path /tmp/deepvariant/model.ckpt; INFO:tensorflow:Starting Queues.; I0502 10:59:10.102293 139632719935232 tf_logging.py:82] Starting Queues.; INFO:tensorflow:global_step/sec: 0; I0502 10:59:13.668776 139621325334272 tf_logging.py:121] global_step/sec: 0; INFO:tensorflow:Recording summary at step 0.; I0502 10:59:14.875045 139621316941568 tf_logging.py:82] Recording summary at step 0.; INFO:tensorflow:global step 1: loss = 0.2608 (4.963 sec/step); I0502 10:59:15.326091 139632719935232 tf_logging.py:82] global step 1: loss = 0.2608 (4.963 sec/step); 2018-05-02 10:59:15.584978: E tensorflow/core/kernels/check_numerics_op.cc:157] abnormal_detected_host @0x104ef6dce00 = {1, 0} LossTensor is inf or nan; 2018-05-02 10:59:15.615399: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: LossTensor is inf or nan : Tensor had NaN values; [[Node: train_op/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=""LossTensor is inf or nan"", _device=""/job:localhost/replica:0/task:0/device:GPU:0""](control_dependency_4)]]; INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.InvalidAr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/69:2742,Queue,Queues,2742,,https://github.com/google/deepvariant/issues/69,1,['Queue'],['Queues']
Performance,"tlas ; #SBATCH --time=5-48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=gpu-a100 # standard node(s); #SBATCH --ntasks=1; #SBATCH --job-name=""deepvariant_modeltraining""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. # LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export APPTAINER_CACHEDIR=$TMPDIR ; export APPTAINER_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/training_set_channelsize_F1F1shuffle.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/validation_set_channelsize_F1F2shuffled.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.02 \; --config.num_validation_examples=0 \; --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/modeltrainout/fullindividualmodel"" \; --strategy=mirrored \; --config.batch_size=32`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/840:2680,load,load,2680,,https://github.com/google/deepvariant/issues/840,1,['load'],['load']
Performance,"train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.1250",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12281,tune,tune,12281,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"train/precision_het=0.0, train/precision_homalt=0.0, train/precision_homref=1.0, train/recall=1.0, train/recall_het=0.0, train/recall_homalt=0.0, train/recall_homref=1.0, train/true_negatives=12800.0, train/true_positives=6400.0; I0829 07:28:44.566404 140318776715072 local.py:41] Setting work unit notes: 0.3 steps/s, 61.4% (13948/22724), ETA: 8h42m; I0829 07:28:44.568708 140305134778112 logging_writer.py:48] [13948] steps_per_sec=0.280075; I0829 07:28:44.568793 140305134778112 logging_writer.py:48] [13948] uptime=74874.6; I0829 07:31:25.151819 140318776715072 train.py:361] Running tune at step=13993 epoch=0; I0829 07:31:25.152109 140318776715072 train.py:366] Tune step 0 / 3162 (0.0%); I0829 07:33:17.573163 140318776715072 train.py:366] Tune step 100 / 3162 (0.0%); I0829 07:35:10.013494 140318776715072 train.py:366] Tune step 200 / 3162 (10.0%); I0829 07:37:02.497336 140318776715072 train.py:366] Tune step 300 / 3162 (10.0%); I0829 07:38:54.834164 140318776715072 train.py:366] Tune step 400 / 3162 (10.0%); I0829 07:40:47.319165 140318776715072 train.py:366] Tune step 500 / 3162 (20.0%); I0829 07:42:39.802007 140318776715072 train.py:366] Tune step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 31",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:9814,Tune,Tune,9814,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,"ts.py:1211] Using sample name from call_variants output. Sample name: default; I0105 16:01:06.676597 140416700553024 postprocess_variants.py:1313] CVO sorting took 0.006136405467987061 minutes; I0105 16:01:06.677379 140416700553024 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0105 16:01:06.677495 140416700553024 postprocess_variants.py:1318] Using 2 CPUs for parallelization of variant transformation.; I0105 16:01:06.808352 140416700553024 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0105 16:01:08.209710 140416700553024 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.01743464469909668 minutes; I0105 16:01:10.258949 140416700553024 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.03414338032404582 minutes. real 0m21.740s; user 0m13.473s; sys 0m2.305s. ***** Running the command:*****; time /opt/deepvariant/bin/vcf_stats_report --input_vcf ""./outputgpu/output.vcf.gz"" --outfile_base ""./outputgpu/output"". 2024-01-05 16:01:21.188421: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 16:01:21.188700: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 16:01:28.513759: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 16:01:28.547411 140591583876928 genomics_reader.py:222] Reading ./outputgpu/output.vcf.gz with NativeVcfReader. real 0m18.513s; user 0m11.281s; sys 0m1.577s. `",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:17561,load,load,17561,,https://github.com/google/deepvariant/issues/761,1,['load'],['load']
Performance,"u-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/c_elegans.PRJEB28388.WS274.genomic.fa \; --reads=""${INPUT_DIR}""/maddog_bam_trim_bwaMEM_sort_dedupped.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. The error looks like:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; time=""2020-03-31T17:40:13-07:00"" level=warning msg=""\""/run/user/3019658\"" directory set by $XDG_RUNTIME_DIR does not exist. Either create the directory or unset $XDG_RUNTIME_DIR.: stat /run/user/30196",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:1804,load,load,1804,,https://github.com/google/deepvariant/issues/292,1,['load'],['load']
Performance,"uestion is thus, are these eval stats truly 0 (i.e. the model is very bad) or is 0.0 some starting value and there are not enough data to calculate them initially? I am warmstarting from the 1.6.1 wgs model so I cant imagine the model is really that bad at calling variants initially, even if in a fish. . **Setup**; Running on a university computing cluster (https://hpc-unibe-ch.github.io/) ; OS: Rocky 9.3 Blue Onyx; GPU: rtx4090 ; Installation: Running from Docker image via singularity; DV version: 1.6.1. **Data**; I am training on examples from 5 individuals, data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. *",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:1487,Perform,Performed,1487,,https://github.com/google/deepvariant/issues/876,1,['Perform'],['Performed']
Performance,"ults in intermediate_results_dir; ***** Intermediate results will be written to intermediate_results_dir in docker. ****; ***** Running the command:*****; time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/sfs-GCS/ann-BIstorage/DB/data/sentieon/hs37d5/hs37d5.fasta"" --reads ""HG003_PacBio_GRCh37.bam"" --examples ""intermediate_results_dir/make_examples.tfrecord@32.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""intermediate_results_dir/gvcf.tfrecord@32.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}; ; 2023-04-13 03:58:35.887616: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.520424: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.431128: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.649384: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:1631,optimiz,optimized,1631,,https://github.com/google/deepvariant/issues/631,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,unction to raise exceptions.; performance hint: _common.pyx:509:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:592:36: Exception check after calling 'f0' will always require the GIL to be acquired. Declare 'f0' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:596:36: Exception check after calling 'f1' will always require the GIL to be acquired. Declare 'f1' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:600:36: Exception check after calling 'f2' will always require the GIL to be acquired. Declare 'f2' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:604:36: Exception check after calling 'f3' will always require the GIL to be acquired. Declare 'f3' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:638:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:675:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:712:63: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:754:31: Exception check afte,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:4717,perform,performance,4717,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance,unction to raise exceptions.; performance hint: _common.pyx:596:36: Exception check after calling 'f1' will always require the GIL to be acquired. Declare 'f1' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:600:36: Exception check after calling 'f2' will always require the GIL to be acquired. Declare 'f2' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:604:36: Exception check after calling 'f3' will always require the GIL to be acquired. Declare 'f3' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:638:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:675:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:712:63: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:754:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:785:31: Exception check after calling 'f' will always require the GIL to be acquired. Declare 'f' as 'noexcept' if you control the definition and you're sure you don't want the function to raise exceptions.; performance hint: _common.pyx:903:40: Exception check after ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:5195,perform,performance,5195,,https://github.com/google/deepvariant/issues/859,1,['perform'],['performance']
Performance,"une step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.5519920587539673, train/f1_het=0.0, train/f1_homalt=0.0, t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12551,tune,tune,12551,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,"une step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 3000 / 3162 (90.0%); I0829 08:29:29.592094 140318776715072 train.py:366] Tune step 3100 / 3162 (100.0%); I0829 08:30:42.583051 140305134778112 logging_writer.py:48] [13993] tune/categorical_accuracy=0.9916982650756836, tune/categorical_crossentropy=0.560210645198822, tune/f1_het=0.0, tune/f1_homalt=0.0, tune/f1_homref=0.9958318471908569, tune/f1_macro=0.33194395899772644, tune/f1_micro=0.9916982650756836, tune/f1_weighted=0.9958318471908569, tune/false_negatives_1=1777.0, tune/false_positives_1=1544.0, tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.5519920587539673, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:12629,tune,tune,12629,,https://github.com/google/deepvariant/issues/876,1,['tune'],['tune']
Performance,une step 600 / 3162 (20.0%); I0829 07:44:32.297624 140318776715072 train.py:366] Tune step 700 / 3162 (20.0%); I0829 07:46:24.658610 140318776715072 train.py:366] Tune step 800 / 3162 (30.0%); I0829 07:48:17.176530 140318776715072 train.py:366] Tune step 900 / 3162 (30.0%); I0829 07:50:09.700463 140318776715072 train.py:366] Tune step 1000 / 3162 (30.0%); I0829 07:52:02.226121 140318776715072 train.py:366] Tune step 1100 / 3162 (30.0%); I0829 07:53:54.613348 140318776715072 train.py:366] Tune step 1200 / 3162 (40.0%); I0829 07:55:47.134974 140318776715072 train.py:366] Tune step 1300 / 3162 (40.0%); I0829 07:57:39.682815 140318776715072 train.py:366] Tune step 1400 / 3162 (40.0%); I0829 07:59:32.215537 140318776715072 train.py:366] Tune step 1500 / 3162 (50.0%); I0829 08:01:24.651632 140318776715072 train.py:366] Tune step 1600 / 3162 (50.0%); I0829 08:03:17.188146 140318776715072 train.py:366] Tune step 1700 / 3162 (50.0%); I0829 08:05:09.741266 140318776715072 train.py:366] Tune step 1800 / 3162 (60.0%); I0829 08:07:02.262498 140318776715072 train.py:366] Tune step 1900 / 3162 (60.0%); I0829 08:08:54.673932 140318776715072 train.py:366] Tune step 2000 / 3162 (60.0%); I0829 08:10:47.221370 140318776715072 train.py:366] Tune step 2100 / 3162 (70.0%); I0829 08:12:39.774174 140318776715072 train.py:366] Tune step 2200 / 3162 (70.0%); I0829 08:14:32.322385 140318776715072 train.py:366] Tune step 2300 / 3162 (70.0%); I0829 08:16:24.722720 140318776715072 train.py:366] Tune step 2400 / 3162 (80.0%); I0829 08:18:17.252759 140318776715072 train.py:366] Tune step 2500 / 3162 (80.0%); I0829 08:20:09.823046 140318776715072 train.py:366] Tune step 2600 / 3162 (80.0%); I0829 08:22:02.367495 140318776715072 train.py:366] Tune step 2700 / 3162 (90.0%); I0829 08:23:54.783612 140318776715072 train.py:366] Tune step 2800 / 3162 (90.0%); I0829 08:25:47.336242 140318776715072 train.py:366] Tune step 2900 / 3162 (90.0%); I0829 08:27:39.775715 140318776715072 train.py:366] Tune step 300,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:10970,Tune,Tune,10970,,https://github.com/google/deepvariant/issues/876,1,['Tune'],['Tune']
Performance,urce_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:15093,cache,cache,15093,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,"v)); File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main; make_examples_core.make_examples_runner(options); File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner; regions, calling_regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options; ref_contigs = fasta.IndexedFastaReader(; File ""/tmp/Bazel.runfiles_luflf_op/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__; self._reader = reference.IndexedFastaReader.from_file(; ****************; File ""/tmp/Bazel.runfiles_8r9pgqu5/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__; self._reader = reference.IndexedFastaReader.from_file(; ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; I0522 08:40:37.129531 140035375298368 make_examples_core.py:257] Task 7/32: Preparing inputs; [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main; make_examples_core.make_examples_runner(options); File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2114, in make_examples_runner; regions, c",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:5055,load,load,5055,,https://github.com/google/deepvariant/issues/653,1,['load'],['load']
Performance,"vcf ""/tmp/tmpl3fvinw4/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 1. ```. This is what my input directory looks like:. ```; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; ```. I noticed there are a few more input files in the sample example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.impute.vcf.gz; module loa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:8120,load,load,8120,,https://github.com/google/deepvariant/issues/292,1,['load'],['load']
Performance,"ve_impl; #16 1497.0 ctx.download_and_extract(; #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:; #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.1 Repository rule _tf_http_archive defined at:; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.2 (21:51:08) Loading: 0 packages loaded; #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:7895,cache,cache,7895,,https://github.com/google/deepvariant/issues/608,1,['cache'],['cache']
Performance,"ver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def; producer_op_list=producer_op_list); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 501, in _import_graph_def_internal; graph._c_graph, serialized, options) # pylint: disable=protected-access; tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed`. **Does the quick start test work on your system?** ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Quick start works on my system -- I can perform make_examples, call_variants and post_processing. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`). Code snippet :; `import tensorflow as tf. meta_path = '/opt/models/wgs/model.ckpt.meta'. ckpt_folder = '/opt/models/wgs'. with tf.compat.v1.Session() as sess:. saver = tf.compat.v1.train.import_meta_graph(meta_path). print(""\n**Import Sucessful\n**""). saver.restore(sess,tf.compat.v1.train.latest_checkpoint(ckpt_folder)); `",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/339:2595,load,loading,2595,,https://github.com/google/deepvariant/issues/339,2,"['load', 'perform']","['loading', 'perform']"
Performance,"versions of the numpy library, but the issue persisted. ./build-prereq.sh; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Install the runtime packages' starting; ========== This script is only maintained for Ubuntu 20.04.; ========== Load config settings.; ========== [Fri 02 Aug 2024 02:19:28 PM CST] Stage 'Misc setup' starting; ========== [Fri 02 Aug 2024 02:20:04 PM CST] Stage 'Update package list' starting; ========== [Fri 02 Aug 2024 02:20:06 PM CST] Stage 'run-prereq.sh: Install development packages' starting; Calling wait_for_dpkg_lock.; ========== [Fri 02 Aug 2024 02:20:10 PM CST] Stage 'Install python3 packaging infrastructure' starting; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 100 2213k 100 2213k 0 0 1634k 0 0:00:01 0:00:01 --:--:-- 1634k; Collecting pip; Using cached pip-24.2-py3-none-any.whl.metadata (3.6 kB); Using cached pip-24.2-py3-none-any.whl (1.8 MB); Installing collected packages: pip; WARNING: The scripts pip, pip3 and pip3.10 are installed in '/root/.local/bin' which is not on PATH.; Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.; Successfully installed pip-24.2; WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning. [notice] A new release of pip is available: 24.0 -> 24.2; [notice] To update, run: pip install --upgrade pip; Python 3.10.14; pip 24.0 from /usr/local/lib/python3.10/site-packages/pip (python 3.10); ========== [Fri 02 Aug 2024 02:20:22 PM CST] Stage 'Install python3 packages' starting; error: subprocess-exit",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:1181,cache,cached,1181,,https://github.com/google/deepvariant/issues/859,1,['cache'],['cached']
Performance,"wing error in the call variants step.; ```bash; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@96.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/output/intermediate_results_dir"" --tpu_name ""variantcaller-node1"" --tpu_zone ""europe-west4-a"" --use_tpu. I0524 21:18:26.485428 140032543119168 transport.py:157] Attempting refresh to obtain initial access_token; I0524 21:18:26.576728 140032543119168 call_variants.py:336] Shape of input examples: [100, 221, 6]; I0524 21:18:26.579230 140032543119168 call_variants.py:361] /opt/models/wgs/model.ckpt.input_shape has the correct shape: [100, 221, 6].; 2022-05-24 21:18:26.581705: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2022-05-24 21:18:26.586196: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2022-05-24 21:18:26.587127: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2000160000 Hz; WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp_f348kd0; W0524 21:18:26.619681 140032543119168 estimator.py:1846] Using temporary folder as model directory: /tmp/tmp_f348kd0; INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmp_f348kd0', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true; graph_options {; rewrite_options {; meta_optimizer_iterations: ONE; }; }; , '_keep_checkpoint_max': 100000, '_ke",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:1571,optimiz,optimized,1571,,https://github.com/google/deepvariant/issues/537,2,"['optimiz', 'perform']","['optimized', 'performance-critical']"
Performance,"xamples.tfrecord@1.gz"" --gvcf ""/tmp/tmpl3fvinw4/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 1. ```. This is what my input directory looks like:. ```; c_elegans.PRJEB28388.WS274.genomic.fa; c_elegans.PRJEB28388.WS274.genomic.fa.fai; maddog_bam_trim_bwaMEM_sort_dedupped.bam; maddog_bam_trim_bwaMEM_sort_dedupped.bam.bai; ```. I noticed there are a few more input files in the sample example `quickstart-input`; is it possible the error is caused by that? . ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ## Trying to fill in the missing input files. I used `bgzip` to convert to gzip and `faidx` to get the `.fai`/`.gzi` files:. ```; module load nixpkgs/16.09; module load gcc/7.3.0; module load samtools/1.9; bgzip c_elegans.PRJEB28388.WS274.genomic.fa; samtools faidx c_elegans.PRJEB28388.WS274.genomic.fa.gz; ```. Next I download the `.gff3` annotation from and converted it to `.bed` format:. ```; module load nixpkgs/16.09; module load gcc/6.4.0; module load bedops/2.4.35. wget ftp://ftp.wormbase.org/pub/wormbase/releases/WS274/species/c_elegans/PRJEB28388/c_elegans.PRJEB28388.WS274.annotations.gff3.gz; bgzip -d c_elegans.PRJEB28388.WS274.annotations.gff3.gz; gff2bed < c_elegans.PRJEB28388.WS274.annotations.gff3 > c_elegans.PRJEB28388.WS274.annotations.bed; rm c_elegans.PRJEB28388.WS274.annotations.gff3; ```. The `.vcf.gz` file I download from [CeNDR](https://www.elegansvariation.org/data/release/latest) (comparable to the [DGV database in humans](http://dgv.tcag.ca/dgv/app/home)) then generate its index file `vcf.gz.tbi`:. ```; wget https://storage.googleapis.com/elegansvariation.org/releases/20180527/variation/WI.20180527.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:8093,load,load,8093,,https://github.com/google/deepvariant/issues/292,1,['load'],['load']
Performance,"xh in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/[tmp/tmp40dn43xh/make_examples.tfrecord@16.gz](mailto:tmp/tmp40dn43xh/make_examples.tfrecord@16.gz)"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. Error in tempfile() using template /local_scratch/pbs.4762337.pbs02/parXXXXX.par: Parent directory (/local_scratch/pbs.4762337.pbs02/) does not exist at /usr/bin/parallel line 3889.; ```. I can set `export TMPDIR = "".""` and this bypasses this error only to receive a different error stating that it cannot find any of the files that are downloaded in the previous steps of the tutorial. . **Error 2**; ```; INFO: Using cached SIF image; I0404 16:29:50.730109 22987118802752 run_deepvariant.py:345] Re-using the directory for intermediate results in ./tmpkj84jstw. ***** Intermediate results will be written to ./tmpkj84jstw in docker. ****. ***** Running the command:*****; time seq 0 15 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""./tmpkj84jstw/make_examples.tfrecord@16.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --pileup_image_width ""199"" --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {}. [E::hts_open_format] Failed to open file ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" : No such file or directory; Traceback (most recent call last):; File ""./Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>; app.run(m",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/533:2116,cache,cached,2116,,https://github.com/google/deepvariant/issues/533,1,['cache'],['cached']
Performance,y '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/compile.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/dfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/mimics_pcre.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/nfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/onepass.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/parse.cc' contains an error and its package is in error and referenced by '@com,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:9663,cache,cache,9663,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,y '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/dfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/mimics_pcre.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/nfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/onepass.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/parse.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/perl_groups.cc' contains an error and its package is in error and referenced by ',MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:9952,cache,cache,9952,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,y '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/onepass.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/parse.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/perl_groups.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/prefilter_tree.h' contains an error and its package is in error and ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:11090,cache,cache,11090,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,y '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_array.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/utf.h' contains an error and its package is in error and referenced by '@c,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:17961,cache,cache,17961,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,y '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/sparse_set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/strutil.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/utf.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_g,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:18820,cache,cache,18820,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Performance,yzing: 242 targets (45 packages loaded); (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitmap256.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/bitstate.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/compile.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/dfa.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/mimics_pcre.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/nfa.cc' contains an error and its package is in error and referenced b,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:9092,cache,cache,9092,,https://github.com/google/deepvariant/issues/19,1,['cache'],['cache']
Safety," ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:14455,predict,predict,14455,,https://github.com/google/deepvariant/issues/537,2,['predict'],['predict']
Safety," in run; outputs = _WrappedSession.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run; result = self._run(None, fetches, feed_dict, options_ptr,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run; results = self._do_run(handle, final_targets, final_fetches,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1371, in _do_run; return self._do_call(_run_fn, feeds, fetches, targets, options,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call; raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter; tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict; features, input_hooks = self._get_features_from_input_fn(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_f",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:8186,Detect,Detected,8186,,https://github.com/google/deepvariant/issues/679,1,['Detect'],['Detected']
Safety," install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469:3880,avoid,avoid,3880,,https://github.com/google/deepvariant/issues/469,1,['avoid'],['avoid']
Safety," op, message); tensorflow.python.framework.errors_impl.InvalidArgumentError: Assign requires shapes of both tensors to match. lhs shape= [3,3,6,32] rhs shape= [3,3,7,32]; 	 [[Node: save_1/Assign_3 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV3/Conv2d_1a_3x3/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV3/Conv2d_1a_3x3/weights, save_1/RestoreV2:3)]]. Caused by op u'save_1/Assign_3', defined at:; File ""/tmp/Bazel.runfiles_qGBYAy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 399, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_qGBYAy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_qGBYAy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 348, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 549, in predict; hooks=all_hooks) as mon_sess:; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 826, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 549, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1012, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1017, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 706, in create_session; self.tf_sess = self._sessio",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/117:6217,predict,prediction,6217,,https://github.com/google/deepvariant/issues/117,2,['predict'],"['prediction', 'predictions']"
Safety,""", line 2045, in __init__; self._traceback = tf_stack.extract_stack_for_node(self._c_op). During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:18715,predict,prediction,18715,,https://github.com/google/deepvariant/issues/537,2,['predict'],"['prediction', 'predictions']"
Safety,"(versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so I’m not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didn’t match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and I’m not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)?. Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that?. **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:3468,avoid,avoids,3468,,https://github.com/google/deepvariant/issues/171,1,['avoid'],['avoids']
Safety,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; can deepvariant detect multiallelic positions, for example, Ref is A, and Alt is C, G. And the GT is denoted as 1/2",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/480:731,detect,detect,731,,https://github.com/google/deepvariant/issues/480,1,['detect'],['detect']
Safety,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: yes . **Describe the issue:**; I am running deep-variant trough a docker installation of the pepper-margin-deepvariant pipeline `kishwars/pepper_deepvariant:r0.8-gpu` on data aligned with minimap2 and data aligned with lra. It is working fine with the minimap2 aligned data, but deepvariant does not produce a final VCF with lra aligned data. . It seems that deep-variant cannot read the base quality score during SNP calling:. ```; 2022-05-26 00:08:16.416812: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores 2e95d959-f3f1-403f-acff-a2bf4f2c12fe: Not found: Could not read base quality scores; 2022-05-26 00:08:16.450548: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (81 vs. 0); Fatal Python error: Aborted; ```; and the job eventually fails:. ```; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7; ```. I checked the lra bam with samtools view and the base quality scores are there.; I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**; - Operating system: Ubuntu 20.04.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/539:870,Abort,Aborted,870,,https://github.com/google/deepvariant/issues/539,1,['Abort'],['Aborted']
Safety,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.); Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**; - Operating system: MacOs (Mac mini/ m1 chip); - DeepVariant version:1.4.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); The test data from GitHub; **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/545:439,Abort,Aborted,439,,https://github.com/google/deepvariant/issues/545,1,['Abort'],['Aborted']
Safety,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:; yes，i have checked this FAQ document. ; **Describe the issue:**; I used Deeprio v1.4.0 version to perform family analysis on three samples, HG002, HG003, and HG004, and evaluated the accuracy of mutation detection using the GIAB database (NISTv4.2.1). I found that the evaluation results through GIAB were particularly unsatisfactory, but I used the same data and Deepvariant v1.5.0 for single sample analysis, and the evaluation results were very ideal, I don't quite understand why the analysis results of a single sample perform so well at the evaluation level compared to the results of family analysis, and why the results of family analysis are relatively poor. The following is my family analysis and analysis code for individual samples, as well as the evaluation results of the GIAB database, for developers to review:; Data comparison to reference genome:; ```; echo HG002.merged.fastq.gz > HG002.fofn ; pbmm2 align \; --preset HIFI \; genome/hg38.fa.mmi \; HG002.fofn \; --sample HG002 \; -j 10 \; HG002.aligned.tmp.bam ; samtools sort -@ 10 HG002.aligned.tmp.bam -O BAM -o HG002.aligned.tmp.sort.bam ; samtools index -@ 10 HG002.aligned.tmp.sort.bam ; chromosomes=(chr1 chr2 chr3 chr4 chr5 chr6 chr7 chr8 chr9 chr10 chr11 chr12 chr13 chr14 chr15 chr16 chr17 chr18 chr19 chr20 chr21 chr22 chrX chrY chrM) ; for chromosome in ""${chromosomes[@]}""; \; do \; samtools view -@ 2 -b -h HG002.aligned.tmp.sort.bam ""$chromosome"" --output HG002.aligned.$chromosome.tmp.bam & ; done ; wait ; samtools merge HG002.aligned.bam HG002.aligned.chr*.tmp.bam ; samtools sort -@ 10 HG002.aligned.bam -O BAM -o HG002.sort.bam ; samtools index -@ 10 HG002.sort.bam ; ```; Family analysis code:; ```; rm -rf chr20_GLnexus.DB tmp_ramdom_TrioDemo_chr20 ; samtools view --write-index --threads 10 -h -b -S HG002.sort.bam chr20 -O BAM -o HG002.chr20.sort.bam ; samtools view --write-index --threads 10 -h -b -S HG003.sort.b",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689:296,detect,detection,296,,https://github.com/google/deepvariant/issues/689,1,['detect'],['detection']
Safety,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**; When variant is not detected, the program will freeze in the last step；. **Setup**; - Operating system:Centos7.6; - DeepVariant version: 1.6 ; - Installation method (Docker, built from source, etc.): singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO-SMART；A reference sequence for a normal person；. **Steps to reproduce:**; - Command: /bin/singularity run -B /work/:/work/ /work/deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=human_geneA_reference.fa --reads=reference.align.bam --output_vcf=out.vcf --output_gvcf=out.gvcf --num_shards=32; - Error trace: Last line： I0119 11:43:53.450599 47012502976320 call_variants.py:623] Complete: call_variants（Stuck at this step）. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. [deepvariant_1.6.pdf](https://github.com/google/deepvariant/files/13986125/deepvariant_1.6.pdf)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/764:137,detect,detected,137,,https://github.com/google/deepvariant/issues/764,1,['detect'],['detected']
Safety,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:; YES, RNA and STAR are not covered not covered. **Describe the issue:**; 2023-12-14 03:00:18.822708: F deepvariant/allelecounter.cc:204] Check failed: offset + len <= read.aligned_quality_size() (8 vs. 0); Fatal Python error: Aborted. Current thread 0x00007f351d854740 (most recent call first):; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 72 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 806 in realign_reads; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1881 in realign_reads; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1908 in <listcomp>; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1907 in realign_reads_per_sample_multisample; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1709 in process; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_imzp9_kb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref dnaref/genome.fa --reads SAMN029",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/752:317,Abort,Aborted,317,,https://github.com/google/deepvariant/issues/752,1,['Abort'],['Aborted']
Safety,"--------|------|---------|------|--------|------|------|----------|----------|----------|----------|-------------|-------------|-------------|-------------|; | HG002 | SNP | PASS | 3365127 | 3361925 | 3202 | 4312006 | 7891 | 942045 | 1197 | 719 | 0.999048 | 0.997658 | 0.21847 | 0.998353 | 2.100128487 | 1.823839956 | 1.581195853 | 1.50423718 |; | HG003 | INDEL | PASS | 504501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |; | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |; | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**; - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0; - DeepVariant version:deeptrio-1.4.0; - Installation method (Docker, built from source, etc.):Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); HiFi data,those data download links follows:; * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/; * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb; * HG004:https://s3-us-west-2.amazonaws.com/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689:6341,detect,detection,6341,,https://github.com/google/deepvariant/issues/689,1,['detect'],['detection']
Safety,"-18 00:34:58.566676: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory.; I0218 00:35:01.595164 140119155529536 call_variants.py:583] Predicted 1024 examples in 1 batches [0.637 sec per 100].; 2024-02-18 00:35:02.648043: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory.; 2024-02-18 00:35:03.234445: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory.; 2024-02-18 00:35:07.222464: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory.; I0218 00:38:56.687749 140119155529536 call_variants.py:583] Predicted 52224 examples in 51 batches [0.463 sec per 100].; I0218 00:42:59.116032 140119155529536 call_variants.py:583] Predicted 103424 examples in 101 batches [0.468 sec per 100].; I0218 00:46:58.822113 140119155529536 call_variants.py:583] Predicted 154624 examples in 151 batches [0.468 sec per 100].; I0218 00:47:39.156648 140119155529536 call_variants.py:623] Complete: call_variants. real	13m21.231s; user	118m36.634s; sys	25m56.983s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpd74of138/gvcf.tfrecord@16.gz"". 2024-02-18 00:47:52.195457: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-18 00:47:52.196245: W tensorflow/compiler/tf2tensorrt/utils/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:17259,Predict,Predicted,17259,,https://github.com/google/deepvariant/issues/774,1,['Predict'],['Predicted']
Safety,"-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>; tf.app.run(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants; prediction = next(predictions); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict; hooks=all_hooks) as mon_sess:; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session; return self._sess_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:5244,predict,predict,5244,,https://github.com/google/deepvariant/issues/166,1,['predict'],['predict']
Safety,". In this case, I assume the position has no reads in the data and so I'm expecting a missing genotype (`./.`) not a hom ref. This error can really mess up segregation analysis.; Trying to understand what is going on, I've looked at the single g.vcf generated by deepvariant for some of this position and I've noticed that the errors seem related to variants output as a hom ref block with MIN_DP zero in the g.vcf file. See the following example:. **multi-sample VCF:**; ```; chr1 72787 chr1_72787_C_T C T 18 . AF=0.5;AQ=18 GT:DP:AD:GQ:PL:RNC 0/0:0:0,0:1:0,3,29:.. 1/1:2:0,2:6:18,9,0:..; ```; **g.vcf sample1:**; ```; chr1 72121 . A <*> 0 . END=73000 GT:GQ:MIN_DP:PL 0/0:1:0:0,3,29; ```; **g.vcf sample2:**; ```; chr1 72787 . C T,<*> 18.8 PASS . GT:GQ:DP:AD:VAF:PL 1/1:10:2:0,2,0:1,0:18,9,0,990,990,990; ```. Instead, if the g.vcf line is like the one below the genotype is correctly reported as missing in the multi-sample VCF.; ```; chr1 20595 . A <*> 0 . END=20595 GT:GQ:MIN_DP:PL ./.:0:4:17,0,77; ```. So a couple of questions:; 1. Is this behavior expected for deepvariant or is it a kind of bug?; 2. How to interpret a g.vcf block like the one above with MIN_DP zero? Does it mean that some of the positions in this block have actually non-zero coverage or all of them have zero? In the first case, I suggest splitting the block when a position has zero coverage; in the second case, it is probably better to output the block with `./.` genotype.; 3. How can I interpret `0/0` genotypes with zero DP in the multi-sample VCF? Currently, I assume that the DP notation is correct and thus convert all these ones to missing genotypes. Or is it possible that some of them actually have reads covering the position? This latter case would be particularly problematic since I would not be able to safely distinguish between an actual home ref and a missing call due to zero coverage.; 4. Is there any setting I can change in GLnexus to have positions with DP zero outputted as missing?. Thanks a lot!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/346:2247,safe,safely,2247,,https://github.com/google/deepvariant/issues/346,1,['safe'],['safely']
Safety,".428281: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OPT_BLOCK_SIZE to 134217728; I0208 03:49:01.743115 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam with NativeSamReader; I0208 03:49:01.755232 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam with NativeSamReader; I0208 03:49:02.136116 140440947410688 make_examples.py:1363] Task 63: 0 candidates (0 examples) [1.19s elapsed]; I0208 06:50:01.437930 140440947410688 make_examples.py:1363] Task 63: 101 candidates (101 examples) [10859.30s elapsed]; I0208 07:30:38.055526 140440947410688 make_examples.py:1380] Found 176 candidate variants; I0208 07:30:38.056374 140440947410688 make_examples.py:1381] Created 178 examples. real	346m1.860s; user	7558m17.436s; sys	11m12.192s; ```. looks like it starts making predictions; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@64.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". ```. the tail of my noup.out has not changed in over a day; ```; packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.; Instructions for updating:; Use standard file APIs to check for files with this prefix.; I0208 09:29:54.405941 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:55.469674 139859027293952 session_manager.py:491] Running local_init_op.; I0208 09:29:55.510524 139859027293952 session_manager.py:493] Done running local_init_op.; I0208 09:29:55.864006 139859027293952 modeling.py:410] Reloading EMA...; I0208 09:29:55.864634 139859027293952 saver.py:1270] Restoring parameters from /opt/models/w",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/269:2513,predict,predictions,2513,,https://github.com/google/deepvariant/issues/269,1,['predict'],['predictions']
Safety,".runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict; features, input_hooks = self._get_features_from_input_fn(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn; result, _, hooks = estimator_util.parse_input_fn_result(result); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result; result = iterator.get_next(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 444, in get_next; flat_ret = gen_dataset_ops.iterator_get_next(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2865, in iterator_get_next; _, _, _op, _outputs = _op_def_library._apply_op_helper(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper; op = g._create_op_internal(op_type_name, inputs, dtypes=None,; File ""/usr/local/lib/python3.8/dist-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:15826,predict,predict,15826,,https://github.com/google/deepvariant/issues/564,1,['predict'],['predict']
Safety,/deepvariant/bin/deeptrio/run_deeptrio \; --model_type PACBIO \; --ref=genome/hg38.fa \; --reads_child HG002.chr20.sort.bam \; --reads_parent1 HG003.chr20.sort.bam \; --reads_parent2 HG004.chr20.sort.bam \; --output_vcf_child HG002.chr20.output.vcf.gz \; --output_vcf_parent1 HG003.chr20.output.vcf.gz \; --output_vcf_parent2 HG004.chr20.output.vcf.gz \; --sample_name_child 'HG002' \; --sample_name_parent1 'HG003' \; --sample_name_parent2 'HG004' \; --num_shards 10 \; --intermediate_results_dir tmp_ramdom_TrioDemo_chr20 \; --output_gvcf_child HG002.chr20.g.vcf.gz \; --output_gvcf_parent1 HG003.chr20.g.vcf.gz \; --output_gvcf_parent2 HG004.chr20.g.vcf.gz ; glnexus_cli_v1.4.1 \; --config DeepVariant_unfiltered \; --squeeze \; --dir chr20_GLnexus.DB \; HG002.chr20.g.vcf.gz HG003.chr20.g.vcf.gz HG004.chr20.g.vcf.gz \; --threads 10 | \; /opt/conda/envs/bio/bin/bcftools view \; --threads 10 -O z \; -o TrioDemo_chr20.trio_merged.vcf.gz - ; ```; The following is the code for single sample mutation detection:; ```; samtools view --threads 10 -h -b -S HG002.sort.bam chr20 > HG002/HG002.chr20.sort.bam ; samtools index -@ 10 HG002/HG002.chr20.sort.bam ; singularity run /share:/share Singularity/google-deepvariant.1.5.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref genome/hg38.fa \; --intermediate_results_dir HG002/tmp_ramdom_HG002_chr20 \; --reads HG002/HG002.chr20.sort.bam \; --output_vcf HG002/HG002.chr20.vcf.gz \; --num_shards 10 ; rm -fr HG002/tmp_ramdom_HG002_chr20; ```. The evaluation results of the trio family sample are as follows:. | HG002 | INDEL | PASS | 525469 | 190669 | 334800 | 361066 | 7821 | 151401 | 4826 | 2317 | 0.362855 | 0.962698 | 0.419317 | 0.527055 | | | 1.528275734 | 1.988539738 |; |-------|-------|------|---------|---------|---------|---------|-------|--------|-------|------|----------|----------|----------|----------|-------------|------------|-------------|-------------|; | HG002 | SNP | PASS | 3365127 | 1236543 | 2128584 |,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689:3253,detect,detection,3253,,https://github.com/google/deepvariant/issues/689,1,['detect'],['detection']
Safety,"/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 640, in predict; preds_evaluated = mon_sess.run(predictions); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 754, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1259, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1360, in run; raise six.reraise(*original_exc_info); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/six_archive/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1345, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1418, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1176, in run; return self._sess.run(*args, *",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:13974,predict,predictions,13974,,https://github.com/google/deepvariant/issues/358,1,['predict'],['predictions']
Safety,"/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_ses",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:19463,predict,predict,19463,,https://github.com/google/deepvariant/issues/537,1,['predict'],['predict']
Safety,"000123136 tf_logging.py:115] Done calling model_fn.; I1108 10:29:06.500680 140295000123136 tf_logging.py:115] Graph was finalized.; I1108 10:29:06.501178 140295000123136 tf_logging.py:115] Restoring parameters from /gpfs/projects/bioinfo/najeeb/playGround/deepVariants/models/model.ckpt; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_qGBYAy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 399, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_qGBYAy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_qGBYAy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 348, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 549, in predict; hooks=all_hooks) as mon_sess:; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 826, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 549, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1012, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1017, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 706, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 477, in create_se",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/117:3174,predict,predict,3174,,https://github.com/google/deepvariant/issues/117,1,['predict'],['predict']
Safety,"07:17:31.684022 140029649073920 make_examples.py:1086] Writing examples to /home/chungtsai_su/quickstart-output/examples.tfrecord.gz; 2018-12-20 07:17:31.684869: I third_party/nucleus/io/sam_reader.cc:561] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2018-12-20 07:17:31.688126: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.688252 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.884236 140029649073920 make_examples.py:1119] Task 0: 6 candidates (6 examples) [0.20s elapsed]; I1220 07:17:33.209975 140029649073920 make_examples.py:1134] Writing MakeExamplesRunInfo to /home/chungtsai_su/quickstart-output/examples.tfrecord.gz.run_info.pbtxt; I1220 07:17:33.241107 140029649073920 make_examples.py:1137] Found 76 candidate variants; I1220 07:17:33.241497 140029649073920 make_examples.py:1138] Created 82 examples; ```. Also, the problem can be detected in test case. ```; //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s; //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s; //deepvariant/vendor:timer_test PASSED in 0.8s; //deepvariant:make_examples_test FAILED in 2 out of 2 in 1.7s; Stats over 2 runs: max = 1.7s, min = 1.6s, avg = 1.7s, dev = 0.1s; /home/chungtsai_su/.cache/bazel/_bazel_chungtsai_su/959496e1d4e585c03b8886e389170de9/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /home/chungtsai_su/.cache/bazel/_bazel_chungtsai_su/959496e1d4e585c03b8886e389170de9/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test PASSED in 49.7s; Stats over 10 runs: max = 49.7s, min = 2.6s, avg = 9.1s, dev = 13.6s; //deepvariant:model_train_test PASSED in 127.0s; Stats over 10 runs: max = 127.0s, min = 2.7s, avg = 42.5s, dev = 47.3s. Executed 38 out of 38 tests: 37 tests ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131:5364,detect,detected,5364,,https://github.com/google/deepvariant/issues/131,1,['detect'],['detected']
Safety,0908051506_s1_p0/18204/0_11826: NOT_FOUND: Could not read base quality scores; 2023-04-13 03:58:45.083564: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores m141207_021027_42177R_c100762112550000001823161707071506_s1_p0/128181/16776_21715: NOT_FOUND: Could not read base quality scores; 2023-04-13 03:58:45.083624: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores m150304_023026_42163R_c100791492550000001823175409091556_s1_p0/94463/13434_17465: NOT_FOUND: Could not read base quality scores; 2023-04-13 03:58:45.083666: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores m150205_114659_42163R_c100780092550000001823165208251532_s1_p0/41607/2555_6463: NOT_FOUND: Could not read base quality scores; 2023-04-13 03:58:45.083710: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores m150209_042724_42177R_c100779832550000001823165208251590_s1_p0/129762/7102_9303: NOT_FOUND: Could not read base quality scores; 2023-04-13 03:58:45.083973: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores m150309_153203_42156_c100797772550000001823175109091511_s1_p0/155759/0_8477: NOT_FOUND: Could not read base quality scores; 2023-04-13 03:58:45.084070: W third_party/nucleus/io/sam_reader.cc:599] Could not read base quality scores m150304_151228_42163R_c100797832550000001823175109091520_s1_p0/45204/2004_7491: NOT_FOUND: Could not read base quality scores; 2023-04-13 03:58:45.111139: F deepvariant/allelecounter.cc:198] Check failed: offset + len <= read.aligned_quality_size() (1261 vs. 0); Fatal Python error: Aborted. cmd:; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref hs37d5.fasta \; --reads HG003_PacBio_GRCh37.bam \; --output_vcf HG003_PacBio.depv.vcf.gz \; --output_gvcf HG003_PacBio.depv.g.vcf.gz \; --num_shards 32 \; --intermediate_results_dir intermediate_results_dir; ; What can i do to fix it?; Looking forward to your reply. Thanks.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:39586,Abort,Aborted,39586,,https://github.com/google/deepvariant/issues/631,1,['Abort'],['Aborted']
Safety,"1 14:31:57] INFO: THREAD 25 FINISHED SUCCESSFULLY.; [11-03-2021 14:31:58] INFO: THREAD 0 FINISHED SUCCESSFULLY.; [11-03-2021 14:32:00] INFO: THREAD 52 FINISHED SUCCESSFULLY.; [11-03-2021 14:32:00] INFO: THREAD 32 FINISHED SUCCESSFULLY.; [11-03-2021 14:32:01] INFO: THREAD 23 FINISHED SUCCESSFULLY.; [11-03-2021 14:32:03] INFO: THREAD 30 FINISHED SUCCESSFULLY.; [11-03-2021 14:32:03] INFO: THREAD 43 FINISHED SUCCESSFULLY.; [11-03-2021 14:32:03] INFO: THREAD 11 FINISHED SUCCESSFULLY.; [11-03-2021 14:32:04] INFO: THREAD 1 FINISHED SUCCESSFULLY.; [11-03-2021 14:32:07] INFO: THREAD 13 FINISHED SUCCESSFULLY.; [11-03-2021 14:32:07] INFO: THREAD 59 FINISHED SUCCESSFULLY.; [11-03-2021 14:32:12] INFO: THREAD 57 FINISHED SUCCESSFULLY.; [11-03-2021 14:32:20] INFO: THREAD 47 FINISHED SUCCESSFULLY.; [11-03-2021 14:32:23] INFO: THREAD 27 FINISHED SUCCESSFULLY.; [11-03-2021 14:32:23] INFO: THREAD 29 FINISHED SUCCESSFULLY.; [11-03-2021 14:32:23] INFO: FINISHED PREDICTION; [11-03-2021 14:32:23] INFO: ELAPSED TIME: 14 Min 14 Sec; [11-03-2021 14:32:23] INFO: PREDICTION FINISHED SUCCESSFULLY. ; [11-03-2021 14:32:23] TOTAL ELAPSED TIME FOR INFERENCE: 14 Min 14 Sec; [11-03-2021 14:32:23] STEP 3.1: CALLING VARIANTS; [11-03-2021 14:32:23] INFO: OUTPUT: /cromwell_root/pepper_output/pepper_hp/; [11-03-2021 14:32:23] INFO: PROCESSING HAPLOTAG: 0; [11-03-2021 14:32:24] INFO: PROCESSING CONTIG: chr10; [11-03-2021 14:39:30] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 378085 TOTAL TIME SPENT: 7 Min 6 Sec; [11-03-2021 14:39:37] INFO: PROCESSING CONTIG: chr14; [11-03-2021 14:39:48] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 2550 TOTAL TIME SPENT: 0 Min 11 Sec; [11-03-2021 14:39:49] TOTAL ELAPSED TIME FOR VARIANT CALLING: 26 Min 43 Sec. real	26m44.555s; user	1220m53.668s; sys	15m35.409s; [11-03-2021 14:39:49] INFO: [6/9] RUNNING THE FOLLOWING COMMAND; -------; mv /cromwell_root/pepper_output/pepper_hp/*.vcf /cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf; ; bgzip /cromwell_roo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:20159,PREDICT,PREDICTION,20159,,https://github.com/google/deepvariant/issues/491,2,['PREDICT'],['PREDICTION']
Safety,"1032021_134041/; [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP.; [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64; [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1; [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX; [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35.; [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35.; [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35.; [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35.; [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35.; [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35.; [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35.; [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY.; [11-03-2021 13:50:44] INFO: FINISHED PREDICTION; [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec; [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. ; [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec; [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES; [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/; [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10; [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013.; [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14; [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092.; [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s; user	579m29.953s; sys	11m32.825s; [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND; -------; mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; ; bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; ; tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; ; rm -rf /cromwell_root/pepper_output/pepper_snp/; ; echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; ; zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq; -------; CONTIGS FOUND IN PEPPER SNP VCF:; chr10; chr14; [11-03-",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:4824,PREDICT,PREDICTION,4824,,https://github.com/google/deepvariant/issues/491,1,['PREDICT'],['PREDICTION']
Safety,"15:53:39.096611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:39.226747: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:39.226871: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:49.941043: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:53:49.987410 140173517489984 genomics_reader.py:222] Reading /public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam with NativeSamReader; W0105 15:53:49.988560 140173517489984 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I0105 15:53:50.021419 140173517489984 make_examples_core.py:301] Task 0/2: Preparing inputs; I0105 15:53:50.036767 140173517489984 genomics_reader.py:222] Reading /public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam with NativeSamReader; I0105 15:53:50.054040 140173517489984 make_examples_core.py:301] Task 0/2: Common contigs are ['NC_037590.1', 'NC_037591.1', 'NC_037592.1', 'NC_037593.1', 'NC_037594.1', 'NC_037595.1', 'NC_037596.1', 'NC_037597.1', 'NC_037598.1', 'NC_037599.1', 'NC_037600.1', 'NC_037601.1', 'NC_037602.1', 'NC_037603.1', 'NW",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:5936,detect,detected,5936,,https://github.com/google/deepvariant/issues/761,1,['detect'],['detected']
Safety,"1: W tensorflow/core/framework/op_kernel.cc:1273] OP_REQUIRES failed at save_restore_tensor.cc:172 : Data loss: Unable to open table file /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>; tf.app.run(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants; prediction = next(predictions); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict; hooks=all_hooks) as mon_sess:; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session; return self._sess_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session; self.tf",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:5109,predict,prediction,5109,,https://github.com/google/deepvariant/issues/166,2,['predict'],"['prediction', 'predictions']"
Safety,"1] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/; [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES; [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']; [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895; [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS; [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]; ...; [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]; [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]; [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFULLY.; [11-03-2021 13:44:25] FINISHED IMAGE GENERATION; [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec; [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE; [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/; [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP.; [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64; [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1; [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX; [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35.; [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35.; [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35.; [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35.; [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35.; [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35.; [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35.; [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY.; [11-03-2021 13:50:44] INFO: FINISHED PREDICTION; [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec; [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. ; [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:3774,PREDICT,PREDICTION,3774,,https://github.com/google/deepvariant/issues/491,1,['PREDICT'],['PREDICTION']
Safety,"2422_S5_L005_R1_001_output.tfrecord.gz; I1108 10:29:03.210211 140295000123136 tf_logging.py:115] Calling model_fn.; I1108 10:29:05.463484 140295000123136 tf_logging.py:115] Done calling model_fn.; I1108 10:29:06.500680 140295000123136 tf_logging.py:115] Graph was finalized.; I1108 10:29:06.501178 140295000123136 tf_logging.py:115] Restoring parameters from /gpfs/projects/bioinfo/najeeb/playGround/deepVariants/models/model.ckpt; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_qGBYAy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 399, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_qGBYAy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_qGBYAy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 348, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 549, in predict; hooks=all_hooks) as mon_sess:; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 826, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 549, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1012, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1017, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 706, in create_session; self.tf_sess = self._sessio",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/117:3041,predict,prediction,3041,,https://github.com/google/deepvariant/issues/117,2,['predict'],"['prediction', 'predictions']"
Safety,"299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in c",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:19293,predict,predict,19293,,https://github.com/google/deepvariant/issues/537,1,['predict'],['predict']
Safety,"3 12:26:09.056452: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/25963197: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.056463: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/25963211: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.056474: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/25963212: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.056489: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/9461690: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.056506: W third_party/nucleus/io/sam_reader.cc:612] Could not read base quality scores molecule/8764719: NOT_FOUND: Could not read base quality scores; 2024-09-03 12:26:09.086379: F deepvariant/allelecounter.cc:204] Check failed: offset + len <= read.aligned_quality_size() (496 vs. 0); Fatal Python error: Aborted. Current thread 0x00007b82cc097740 (most recent call first):; File ""/tmp/Bazel.runfiles_vzux0__g/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2040 in candidates_in_region; File ""/tmp/Bazel.runfiles_vzux0__g/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process; File ""/tmp/Bazel.runfiles_vzux0__g/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner; File ""/tmp/Bazel.runfiles_vzux0__g/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main; File ""/tmp/Bazel.runfiles_vzux0__g/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_vzux0__g/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_vzux0__g/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/GRCh38.p13.genome.fa -",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/877:5133,Abort,Aborted,5133,,https://github.com/google/deepvariant/issues/877,1,['Abort'],['Aborted']
Safety,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]; I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]; I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]; I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]; I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]; I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]; I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]; I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]; I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]; I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]; I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]; # the program died here; ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks!; Steve",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:36228,predict,predicting,36228,,https://github.com/google/deepvariant/issues/491,1,['predict'],['predicting']
Safety,"3, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:14575,predict,predict,14575,,https://github.com/google/deepvariant/issues/537,2,['predict'],['predict']
Safety,"32] rhs shape= [3,3,7,32]; 	 [[Node: save_1/Assign_3 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV3/Conv2d_1a_3x3/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV3/Conv2d_1a_3x3/weights, save_1/RestoreV2:3)]]. Caused by op u'save_1/Assign_3', defined at:; File ""/tmp/Bazel.runfiles_qGBYAy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 399, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_qGBYAy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 389, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_qGBYAy/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 348, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/estimator/estimator.py"", line 549, in predict; hooks=all_hooks) as mon_sess:; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 826, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 549, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1012, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1017, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 706, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 468, in create_se",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/117:6350,predict,predict,6350,,https://github.com/google/deepvariant/issues/117,1,['predict'],['predict']
Safety,"3517489984 genomics_reader.py:222] Reading /public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam with NativeSamReader; I0105 15:53:50.054040 140173517489984 make_examples_core.py:301] Task 0/2: Common contigs are ['NC_037590.1', 'NC_037591.1', 'NC_037592.1', 'NC_037593.1', 'NC_037594.1', 'NC_037595.1', 'NC_037596.1', 'NC_037597.1', 'NC_037598.1', 'NC_037599.1', 'NC_037600.1', 'NC_037601.1', 'NC_037602.1', 'NC_037603.1', 'NW_020229205.1', 'NW_020229206.1', 'NW_020229207.1', 'NW_020229208.1', 'NW_020229209.1', 'NW_020229210.1', 'NW_020229211.1', 'NW_020229212.1', 'NW_020229213.1', 'NC_024586.1']; I0105 15:53:50.067565 140173517489984 make_examples_core.py:301] Task 0/2: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2024-01-05 15:53:49.942446: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:53:49.983960 140329169033024 genomics_reader.py:222] Reading /public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam with NativeSamReader; W0105 15:53:49.992453 140329169033024 make_examples_core.py:344] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I0105 15:53:50.050559 140329169033024 make_examples_core.py:301] Task 1/2: Preparing inputs; I0105 15:53:50.080640 140329169033024 genomics_reader.py:222] Reading /public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam with NativeSamReader; I0105 15:53:50.128940 140329169033024 make_examples_core.py:301] Task 1/2: Common contigs are ['NC_037590.1', 'NC_037591.1', 'NC_037592.1', 'NC_037593.1', 'NC_037594.1', 'NC_037595.1', 'NC_037596.1', 'NC_037597.1', 'NC_037598.1', 'NC_037599.1', 'NC_037600.1', 'NC_037601.1', 'NC_037602.1', 'NC_037603.1', 'NW",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:7491,detect,detected,7491,,https://github.com/google/deepvariant/issues/761,1,['detect'],['detected']
Safety,"3:44:25] FINISHED IMAGE GENERATION; [11-03-2021 13:44:25] TOTAL ELAPSED TIME FOR IMAGE GENERATION: 3 Min 44 Sec; [11-03-2021 13:44:25] STEP 2: RUNNING INFERENCE; [11-03-2021 13:44:25] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/predictions_11032021_134041/; [11-03-2021 13:44:25] INFO: DISTRIBUTED CPU SETUP.; [11-03-2021 13:44:25] INFO: TOTAL CALLERS: 64; [11-03-2021 13:44:25] INFO: THREADS PER CALLER: 1; [11-03-2021 13:44:25] INFO: MODEL LOADING TO ONNX; [11-03-2021 13:45:22] INFO: BATCHES PROCESSED 5/35.; [11-03-2021 13:46:21] INFO: BATCHES PROCESSED 10/35.; [11-03-2021 13:47:17] INFO: BATCHES PROCESSED 15/35.; [11-03-2021 13:48:11] INFO: BATCHES PROCESSED 20/35.; [11-03-2021 13:49:06] INFO: BATCHES PROCESSED 25/35.; [11-03-2021 13:49:59] INFO: BATCHES PROCESSED 30/35.; [11-03-2021 13:50:39] INFO: BATCHES PROCESSED 35/35.; [11-03-2021 13:50:39] INFO: THREAD 0 FINISHED SUCCESSFULLY.; [11-03-2021 13:50:44] INFO: FINISHED PREDICTION; [11-03-2021 13:50:44] INFO: ELAPSED TIME: 6 Min 18 Sec; [11-03-2021 13:50:44] INFO: PREDICTION FINISHED SUCCESSFULLY. ; [11-03-2021 13:50:44] TOTAL ELAPSED TIME FOR INFERENCE: 6 Min 18 Sec; [11-03-2021 13:50:44] STEP 3: RUNNING FIND CANDIDATES; [11-03-2021 13:50:44] INFO: PREDICTION OUTPUT: /cromwell_root/pepper_output/pepper_snp/; [11-03-2021 13:50:44] INFO: PROCESSING CONTIG: chr10; [11-03-2021 13:53:46] INFO: FINISHED PROCESSING chr10, TOTAL CANDIDATES FOUND: 345013.; [11-03-2021 13:53:53] INFO: PROCESSING CONTIG: chr14; [11-03-2021 13:54:02] INFO: FINISHED PROCESSING chr14, TOTAL CANDIDATES FOUND: 3092.; [11-03-2021 13:54:02] TOTAL ELAPSED TIME FOR VARIANT CALLING: 13 Min 21 Sec. real	13m23.051s; user	579m29.953s; sys	11m32.825s; [11-03-2021 13:54:03] INFO: [3/9] RUNNING THE FOLLOWING COMMAND; -------; mv /cromwell_root/pepper_output/pepper_snp/*.vcf /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; ; bgzip /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf; ; tabix -p vcf /cromwell_root/pepper_output/PEPPER_SNP_O",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:4539,PREDICT,PREDICTION,4539,,https://github.com/google/deepvariant/issues/491,2,['PREDICT'],['PREDICTION']
Safety,"52.923618 140119155529536 keras_modeling.py:337] inceptionv3: No initial checkpoint specified.; 2024-02-18 00:34:57.911320: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory.; 2024-02-18 00:34:58.566676: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory.; I0218 00:35:01.595164 140119155529536 call_variants.py:583] Predicted 1024 examples in 1 batches [0.637 sec per 100].; 2024-02-18 00:35:02.648043: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory.; 2024-02-18 00:35:03.234445: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory.; 2024-02-18 00:35:07.222464: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory.; I0218 00:38:56.687749 140119155529536 call_variants.py:583] Predicted 52224 examples in 51 batches [0.463 sec per 100].; I0218 00:42:59.116032 140119155529536 call_variants.py:583] Predicted 103424 examples in 101 batches [0.468 sec per 100].; I0218 00:46:58.822113 140119155529536 call_variants.py:583] Predicted 154624 examples in 151 batches [0.468 sec per 100].; I0218 00:47:39.156648 140119155529536 call_variants.py:623] Complete: call_variants. real	13m21.231s; user	118m36.634s; sys	25m56.983s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpd74of138/gvcf.tfrecord@16.gz"". 2024-02-18 00:47:52.195457: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:17015,Predict,Predicted,17015,,https://github.com/google/deepvariant/issues/774,1,['Predict'],['Predicted']
Safety,"6737.31s elapsed]; I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526.; I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]; I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234.; I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]; I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s; user	632m52.969s; sys	6m20.594s; INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:; ```; Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.; ```; This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/614:12986,Detect,Detected,12986,,https://github.com/google/deepvariant/issues/614,1,['Detect'],['Detected']
Safety,"9ec44bd Replace C++ `#import <...>` with `#include <...>`; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""); -- Checking for module 'protobuf'; -- No package 'protobuf' found; CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):; A required package was not found; Call Stack (most recent call first):; /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal); clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules); clif/CMakeLists.txt:22 (include); ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:4104,Detect,Detecting,4104,,https://github.com/google/deepvariant/issues/739,8,['Detect'],['Detecting']
Safety,": W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory.; 2024-02-18 00:34:58.566676: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory.; I0218 00:35:01.595164 140119155529536 call_variants.py:583] Predicted 1024 examples in 1 batches [0.637 sec per 100].; 2024-02-18 00:35:02.648043: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory.; 2024-02-18 00:35:03.234445: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory.; 2024-02-18 00:35:07.222464: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory.; I0218 00:38:56.687749 140119155529536 call_variants.py:583] Predicted 52224 examples in 51 batches [0.463 sec per 100].; I0218 00:42:59.116032 140119155529536 call_variants.py:583] Predicted 103424 examples in 101 batches [0.468 sec per 100].; I0218 00:46:58.822113 140119155529536 call_variants.py:583] Predicted 154624 examples in 151 batches [0.468 sec per 100].; I0218 00:47:39.156648 140119155529536 call_variants.py:623] Complete: call_variants. real	13m21.231s; user	118m36.634s; sys	25m56.983s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --infile ""/tmp/tmpd74of138/call_variants_output.tfrecord.gz"" --outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz"" --cpus ""16"" --gvcf_outfile ""output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmpd74of138/gvcf.tfrecord@16.gz"". 2024-02-18 00:47:52.195457: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvid",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:17136,Predict,Predicted,17136,,https://github.com/google/deepvariant/issues/774,1,['Predict'],['Predicted']
Safety,A timeout error occurs,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/831:2,timeout,timeout,2,,https://github.com/google/deepvariant/issues/831,1,['timeout'],['timeout']
Safety,"ARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:55:31.140953: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-01-05 15:55:38.664328: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:55:38.709242 140372734228288 call_variants.py:471] Total 1 writing processes started.; I0105 15:55:38.765925 140372734228288 dv_utils.py:365] From /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord-00000-of-00002.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0105 15:55:38.766286 140372734228288 call_variants.py:506] Shape of input examples: [100, 221, 7]; I0105 15:55:38.768594 140372734228288 call_variants.py:510] Use saved model: True; I0105 15:56:02.220975 140372734228288 dv_utils.py:365] From /opt/models/wgs/example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0105 15:56:02.221645 140372734228288 dv_utils.py:365] From /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord-00000-of-00002.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:13827,detect,detected,13827,,https://github.com/google/deepvariant/issues/761,1,['detect'],['detected']
Safety,Add missing import to avoid build failure,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/68:22,avoid,avoid,22,,https://github.com/google/deepvariant/pull/68,1,['avoid'],['avoid']
Safety,CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/820:48,detect,detected,48,,https://github.com/google/deepvariant/issues/820,1,['detect'],['detected']
Safety,"Dear DeepVariant team,; congrats for this excellent software. Are there any plans to train/offer DeepVariant (or a fork) for haploid (bacterial) genomes? . As the diploid results are fantastic, I think this might be a very helpful and valuable contribution to the microbial bioinformatics community as well!. In particular, for SNP detection in bacterial pathogens this might be very interesting!. Best regards!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/183:332,detect,detection,332,,https://github.com/google/deepvariant/issues/183,1,['detect'],['detection']
Safety,"Dear all,. I was trying to run deepvariant from singularity but I encountered this error:. time /opt/deepvariant/bin/call_variants --outfile ""deepvariant/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""deepvariant/interm; ediate_results_dir/make_examples.tfrecord@12.gz"" --checkpoint ""/opt/models/wes/model.ckpt"". /mnt/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this ; version of SciPy (detected version 1.24.2; warnings.warn(f""A NumPy version >={np_minversion} and <{np_maxversion}""; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 45, in <module>; from deepvariant import modeling; File ""/tmp/Bazel.runfiles_nnuiry6u/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 56, in <module>; from tensorflow.python.tpu import tpu_config ; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/tpu/tpu_config.py"", line 18, in <module>; from tensorflow_estimator.python.estimator.tpu.tpu_config import *; File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/__init__.py"", line 8, in <module>; from tensorflow_estimator._api.v1 import estimator; File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/__init__.py"", line 8, in <module>; from tensorflow_estimator._api.v1.estimator import experimental; File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/_api/v1/estimator/experimental/__init__.py"", line 8, in <module>; from tensorflow_estimator.python.estimator.canned.dnn import dnn_logit_fn_builder; File ""/mnt.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/canned/dnn.py"", line 26, in <module>; from tensorflow_estimator.python.estimator import estimator; File ""/mnt/.local/lib/python3.8/site-packages/tensorflow_estimator/python/estimator/estimator.py"", line 40, in <module>; from tensorflow.python.saved_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/673:491,detect,detected,491,,https://github.com/google/deepvariant/issues/673,1,['detect'],['detected']
Safety,"Dear all,. I would like to share an observation with you.; Deepvariant reported a lot of variants in a region where no reads are mapped.; It is the consequence of Deepvariant local realignment as expected.; But the realigned reads do not seem to belong to this region : the orignal alignment shows less variants than the local realignment.; The genome contains a big region repeated several times (see on the screenshots below). The repeats could be the reason why some reads are moved to the left. The given example is for X chromosome but another region in chromosome 1 (_FLG2_ gene region) showed a lot of variants. Why the reads are moved to the left?; Are the reads moved to the most possible left position?. Original alignment and detected variants (bwa-mem); ![image](https://github.com/google/deepvariant/assets/30355684/7f6ac2c3-59b9-4754-9e15-b70ed8b5461c). Local realignment made by DeepVariant (X_140993145_140994144); ![image](https://github.com/google/deepvariant/assets/30355684/4c6fdc69-f881-44aa-8935-8c627a591f6f). Example of reads that were moved:; Original alignment; NB501857:464:HH7FWBGXV:2:23210:26812:14806 99 X 140993994 50 79M = 140994064 149 CCAGATTCCTGTGAGCCGCTCCTTCTCCTCCACTTTAGTGAGTCTTTTCCAGAGTTCCCCTGAGAGAACTCAGAGTACT AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEE<EEEEEEEEEEEEAEEEEE XA:Z:X,+140993784,79M,2; PG:Z:MarkDuplicates AS:i:74 XS:i:69 MD:Z:17C61 NM:i:1 RG:Z:DM_23_2198; NB501857:464:HH7FWBGXV:2:23210:26812:14806 147 X 140994064 57 79M = 140993994 -149 CAGAGTACTTTTGAGGGTTTTCCCCAGTCTCCTCTCCAGATTCCTGTGAGCTCCTCCTCCTCCTCCACTTTATTGAGTC AAEEEEEEEEEEEEEE<66EEEEEEEEEE/EAEEEEEEEE6EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAAAAA XA:Z:X,-140994589,50M3D29M,4; PG:Z:MarkDuplicates AS:i:79 XS:i:67 MD:Z:79 NM:i:0 RG:Z:DM_23_2198; Local realignment; X:140993145-140994144/ X_140993145_140994144realigned_reads.bam X_140993145_140994144realigned_reads.bam.bai; frmascla@frt:DeepV-TEST$ samtools view Local/X_140993145_140994144realigned_reads.bam | grep NB501857:464:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/763:737,detect,detected,737,,https://github.com/google/deepvariant/issues/763,1,['detect'],['detected']
Safety,"Dear deepvariant team,. we are currently playing a bit around with the GPU docker version of deepvariant 0.10.0. I've noticed the following errors in the ""call_variants"" step and I'm not sure if the GPU is used correctly in this step (""cannot find working devices"" ???) or if this is just a warning because the calling went through without any abort:; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmp7l6e69ft/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmp7l6e69ft/make_examples.tfrecord@25.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"". I0703 17:18:45.593843 140322304501504 call_variants.py:316] Set KMP_BLOCKTIME to 0; 2020-07-03 17:18:45.625121: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA; 2020-07-03 17:18:45.661204: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2800000000 Hz; 2020-07-03 17:18:45.667182: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4f919f0 executing computations on platform Host. Devices:; 2020-07-03 17:18:45.667206: I tensorflow/compiler/xla/service/service.cc:175] StreamExecutor device (0): Host, Default Version; 2020-07-03 17:18:45.674475: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1; 2020-07-03 17:18:45.680312: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; 2020-07-03 17:18:45.680339: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 7c1895dbad7c; 2020-07-03 17:18:45.680346: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 7c1895dbad7c; 2020-07-03 17:18:45.680397: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 410.129.0; 2020-07-03",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/321:344,abort,abort,344,,https://github.com/google/deepvariant/issues/321,1,['abort'],['abort']
Safety,"During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict; preds_evaluated = mon_sess.run(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 786, in run; return self._sess.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1315, in run; return self._sess.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1420, in run; raise six.reraise(*original_exc_info); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1405, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1473, in run; out",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:10954,predict,prediction,10954,,https://github.com/google/deepvariant/issues/564,2,['predict'],"['prediction', 'predictions']"
Safety,Empirical formula predicting disk space usage?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:18,predict,predicting,18,,https://github.com/google/deepvariant/issues/491,1,['predict'],['predicting']
Safety,"Error in trying RNA-Seq case study , it aborted",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581:40,abort,aborted,40,,https://github.com/google/deepvariant/issues/581,1,['abort'],['aborted']
Safety,Fatal Python error: Aborted,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/877:20,Abort,Aborted,20,,https://github.com/google/deepvariant/issues/877,1,['Abort'],['Aborted']
Safety,Feedback on poor results in family variation detection and GIAB dataset evaluation,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689:45,detect,detection,45,,https://github.com/google/deepvariant/issues/689,1,['detect'],['detection']
Safety,"Hello! . not an issue, a simple suggestion: several persons I know are put off by DeepVariant because they can't get a gVCF with a line per base, basically they would like to deactivate the formation of blocks. Personally, blocks have never worried me and I guess computationally it has its advantages. . But just to tlet you know as a ""user experience"". Cheers and stay safe. DeepVariant is a very awesome tool.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/398:371,safe,safe,371,,https://github.com/google/deepvariant/issues/398,1,['safe'],['safe']
Safety,"Hello, . I've been attempting to use the customized_classes_labeler to train a DeepVariant model. Specifically, I've been trying use the ""callsets"" field from the INFO field of a Genome In A Bottle VCF file. I've been working with NA12878, VCF/BED files available here: ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/latest/GRCh38/. At first, I could not make training examples using this as that field is an integer, but by making a copy of the VCF where I changed that field to be a string, I was able to make examples (using the `--labeler algorithm`, `--customized_classes_labeler_info_field_name`, and `--customized_classes_labeler_classes_list` options) and train the model. However, when I use the best model from training to predict variants, this class label information is not included in the VCF file. Am I misinterpreting how to use this customized class labeling? Any suggestions on how to incorporate this field into training and variant prediction? Thank you for your time!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/454:750,predict,predict,750,,https://github.com/google/deepvariant/issues/454,2,['predict'],"['predict', 'prediction']"
Safety,"Hello, a following-up thread of a previous issue. I don't know if you get notified of a reply in a closed issue so I am opening a new one. ; I have tried your flag but it doesn't work. . Thank you. ```. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 442, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 432, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_QDZzEL/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 627, in predict; hooks=all_hooks) as mon_sess:; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 934, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 648, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1122, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1127, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 805, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 571, in create_session; init_fn=self._scaffold.init_fn); File ""/usr/local/lib/python2.7/dist",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/273:757,predict,prediction,757,,https://github.com/google/deepvariant/issues/273,3,['predict'],"['predict', 'prediction', 'predictions']"
Safety,"Hello,. I want to suggest here to left-aligne indel in the DeepVariant output VCF / gVCF to avoid the issue described below. I'm running DeepVariant v1.1.0 on a set of samples sequenced with Illumina 2x150 paired-end reads. My workflow right now includes calling variant using DV and then merge individual gVCFs using GLnexus as described in your best-practices for multi-sample VCF. Inspecting the resulting cohort VCF I've noticed that the representation of indels in repetitive / homopolymer regions is not normalized to the leftmost position and this generates odd situations downstream. Essentially, the multi-sample VCF, would contain 2 different variants that, when left-aligned downstream using for example bcftools norm, become the same locus generating duplicated vars with different genotypes. I didn't notice this issue with recent versions of GATK so I suppose they left-align indels in the output VCF. See the example below:. These are 2 indel variants in my multi-sample VCF:; ```; chr3 105259621 chr3_105259621_T_TTA T TTA; chr3 105259623 chr3_105259623_A_ATA A ATA; ```; As you can see in the screenshot, the actual locus is a repetitive region with TA repeats, so the exact location of a TA insertion in the stretch can not be known.; ![image](https://user-images.githubusercontent.com/51458073/134807481-94191333-32bc-4249-ac11-b265711e435e.png). When I apply bcftools norm, it changes the second one to the leftmost position, making it identical to the first one (which is the expected behavior). So in the end I have 2 duplicated vars in my VCF, each with different genotypes: ; ```; chr3 105259621 chr3_105259621_T_TTA T TTA; chr3 105259621 chr3_105259623_A_ATA T TTA; ```; This situation creates troubles for downstream analysis and segregation, even if probably most of these variants can be discarded since they are likely artifacts.; The problem does not affect many single allele variants (just 51 out of 24054518 in my dataset), but it affects lot of the multi-allelic ones",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/487:92,avoid,avoid,92,,https://github.com/google/deepvariant/issues/487,1,['avoid'],['avoid']
Safety,"Hello,. This may be a naive question but I have some variants which were detected as PASS by DeepVariant - however I cannot see this variants within the bam files?. Any ideas? I likely will have to filter these variants out anyway but wanted to double check if there could be reasons causing this. . For reference: HG38; Chr15:41570158 T>C; The depth is 14,9 from the VCF. This is the region in the bam:; ![Image 02-08-2023 at 16 38](https://github.com/google/deepvariant/assets/110385188/7df0492e-ec54-4330-ada0-cf758de1c75d). I know this is probably an IGV question but thought I would just ask incase!. Thanks!!; Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691:73,detect,detected,73,,https://github.com/google/deepvariant/issues/691,1,['detect'],['detected']
Safety,"Hello. I am working on a clonal organism but we believe more and more it might not be so clonal. We have HiFi data and I want to use them to detect MAF. Is it possible with DeepVariant to detect that? Do you think the models are capable of that? I have asked the question over there to Clair3 people, they said the model was not conceived with this in mind so it might or might not work. Your opinion? . Thank you. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/682:141,detect,detect,141,,https://github.com/google/deepvariant/issues/682,2,['detect'],['detect']
Safety,"Hi , when i run call_variant , it arises this warn which means can't use the gpu,but i can make sure that the tensorflow can use the gpu.There are the screen shots of the warn and the existence of the gpu. - the gpu existence; ![image](https://github.com/google/deepvariant/assets/71956115/367b1a98-123c-48fb-b170-3f8e4aae7d30). ```python; tensorflow.test.is_gpu_available(); WARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.config.list_physical_devices('GPU')` instead.; 2024-05-12 21:36:00.744470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /device:GPU:0 with 15089 MB memory: -> device: 0, name: Vega 20, pci bus id: 0000:26:00.0; True; ```. - the warn ; ![image](https://github.com/google/deepvariant/assets/71956115/246d5cfd-a9b3-4ac5-aea7-bf4c89401c76). ```shell; warnings.warn(; 2024-05-12 21:43:29.067332: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ```. - Operating system: Linux ; - DeepVariant version: 1.6.1-gpu; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/820:1121,detect,detected,1121,,https://github.com/google/deepvariant/issues/820,1,['detect'],['detected']
Safety,"Hi . I am using the quick start docker image. I think all the examples have been created at this point. When it first started creating examples top showed that all 64 of my cpu's where at 100% utilization, and there was still lots of available memory. I have not seen any new log files in over a day. I have check top several times over the last 2 days. It only shows 2 python processes and each of them is at 800% utilization. In my experience training models takes a long time, however making predictions is quick. Should I kill my job and try and start over again? I ran into a problem like this before on a much smaller machine. After 11 days I killed the jobs. I do not know much about docker. I looked in /var/lib/docker/containers. I did not find anything that looked a like a log file. any debugging tips would be appreciated. Andy. config ; ```; google/deepvariant:0.9.0; --model_type=WES; --regions=/input/agilent_sureselect_human_all_exon_v5_b37_targets.bed; --num_shards=64; ```; Looks like make_example completed; ```; I0208 03:49:00.939260 140440947410688 make_examples.py:1330] Writing examples to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00063-of-00064.gz; I0208 03:49:00.940793 140440947410688 make_examples.py:1334] Writing gvcf records to /tmp/deepvariant_tmp_output/gvcf.tfrecord-00063-of-00064.gz; I0208 03:49:01.427521 140440947410688 make_examples.py:905] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2020-02-08 03:49:01.428281: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OPT_BLOCK_SIZE to 134217728; I0208 03:49:01.743115 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam with NativeSamReader; I0208 03:49:01.755232 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam wit",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/269:495,predict,predictions,495,,https://github.com/google/deepvariant/issues/269,1,['predict'],['predictions']
Safety,"Hi All,. When using the DeepVariant model type ONT_R104, does it also call structural variants (SVs), specifically ones between 0 and 30bp in length? We observe tools such as Sniffles and CuteSV typically identify SVs with a minimum length of 30bp, while Clair3 can detect SVs up to 50bp. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/874:266,detect,detect,266,,https://github.com/google/deepvariant/issues/874,1,['detect'],['detect']
Safety,"Hi deepvariant developer,; Is DeepVariant suitable for performing variation detection analysis, particularly **calling SNP and indel information**, using **bam formate** alignment results obtained from **vg software pan-genome analysis** and NGS resequencing data as input files?. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/667:76,detect,detection,76,,https://github.com/google/deepvariant/issues/667,1,['detect'],['detection']
Safety,"Hi everyone,. I wanted to ask if I could use DeepVariant with **Tumor Illumina samples** to accurately detect **Germline** variants? Can it accurately distinguish them from the Somatic ones? I am interested in Germline variants only. Thank you for your time,; Konstantinos Kyriakidis",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/466:103,detect,detect,103,,https://github.com/google/deepvariant/issues/466,1,['detect'],['detect']
Safety,"Hi folks,. Our users are running deepvariant on our HPC and resource requests are proving quite tricky for them as different stages of the pipeline seem to have different resource needs. This leaves execution nodes essentially idle for much of the time of the jobs' running. In the two days runtime you can see below that a user who has asked for 48 cpus doesn't use most of the cores for most of the time:. ![Image](https://github.com/user-attachments/assets/c631a80d-a7b3-435a-ac50-97a1743f7638). ```; singularity run deepvariant_1.6.1.sif /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref /path/to/their/reference.fasta \; --reads /path/to/their/input.bam \; --sample_name unique_name \; --output_vcf /path/to/their/unique_name.vcf.gz \; --output_gvcf /path/to/their/unique_name.g.vcf.gz \; --num_shards 48; ```; Is there a way that each stage of the pipeline with discreet resource requests so that the bits that can be cleanly parallelized can go in one job submission and the stages that don't can be in separate jobs/commands, to avoid having requested and reserved resources being idle?. Cheers",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/905:1058,avoid,avoid,1058,,https://github.com/google/deepvariant/issues/905,1,['avoid'],['avoid']
Safety,"Hi there!. How is the QUAL score calculated when using DeepVariant? E.g., with GATK, it's the phred-scaled posterior probability of a variant being homozygous ref. I've been comparing known true positives detected by both HaplotypeCaller and DeepVariant, and found that DV's QUAL score is approximately correlated with HC's QualByDepth, so I'm wondering whether the QUAL score is already normalized by depth or is calculated in some other way. Thanks!; Bari",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/206:205,detect,detected,205,,https://github.com/google/deepvariant/issues/206,1,['detect'],['detected']
Safety,"Hi!; I am new in variant detection, especially detection in deep learning. I want to implement the process from bam file to output file for deepening the knowledge . but it's difficlut to find the definite network structure in the code, so, is there a cnn structure like the function,""model.summary()"", result in tensorflow2? That will help me a lot!; thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/328:25,detect,detection,25,,https://github.com/google/deepvariant/issues/328,2,['detect'],['detection']
Safety,"Hi, . I am trying to run the following:. #!/bin/bash; #set -euo pipefail <- it fails if I use this!; # Set common settings.; PROJECT_ID=ms-deepvariant; OUTPUT_BUCKET=gs://ms_bam/recover; STAGING_FOLDER_NAME=recover_tmp; OUTPUT_FILE_NAME=recover.gvcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_I",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/214:178,recover,recover,178,,https://github.com/google/deepvariant/issues/214,2,['recover'],['recover']
Safety,"Hi, I am trying to build DeepVariant from source, and I encounter the following issue in build_and_test. I have bazel 0.26.1 compiled from source as well. ```; (16:39:00) ERROR: Analysis of target '//deepvariant:make_examples_utils_test' failed; build aborted: . /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/bazel_tools/tools/jdk/BUILD:487:14: Configurable attribute ""actual"" doesn't match this configuration: Could not find a JDK for host execution environment, please explicitly provide one using `--host_javabase.`; ```; I tried passing the argument ""--host_javabase=@local_jdk//:jdk"" to bazel to no avail. Java:; ```; # java -version; openjdk version ""1.8.0_262""; OpenJDK Runtime Environment (build 1.8.0_262-b10); OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode); ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/355:252,abort,aborted,252,,https://github.com/google/deepvariant/issues/355,1,['abort'],['aborted']
Safety,"Hi, I had a quick question involving [training](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) DeepVariant from an existing one. On the training data [page](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md), the footnotes say that in v1.1 you exclude HG003 and chr20-22 from the training data. . Is this true for all versions since then? Does that mean if I wanted to train a new model from an existing one, it's safe to train on chr1-19 and then validate on chr20-22 on NIST data without the model ever seeing the variants there? Is it safe to use HG003 everywhere even though they are a direct relative of HG002 (whose data is used extensively in the training)? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/788:496,safe,safe,496,,https://github.com/google/deepvariant/issues/788,2,['safe'],['safe']
Safety,"Hi, The attached variant was missed. It was detected in 39 / 39 reads (100%) at the locus, and all reads have high MAPQ. Very strange, but warrants an explanation. It was not even a RefCall variant. Appreciate any advice.; <img width=""943"" alt=""Screen Shot 2020-11-24 at 4 31 05 PM"" src=""https://user-images.githubusercontent.com/47928628/100153865-bd2e2700-2e72-11eb-9af1-c182b3fd57ee.png"">",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/388:44,detect,detected,44,,https://github.com/google/deepvariant/issues/388,1,['detect'],['detected']
Safety,"Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue?. **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command?. This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant; REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \; -v /mnt/efs-genome:/mnt/efs-genome \; gcr.io/deepvariant-docker/deepvariant \; /opt/deepvariant/bin/postprocess_variants \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${FINAL_OUTPUT_VCF}""; ```. Thank you very much for your assistance!. Sincerely,; Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167:875,predict,prediction,875,,https://github.com/google/deepvariant/issues/167,1,['predict'],['prediction']
Safety,"Hi,. Here is a set of additional parameter I used in 'make_examples' step to create examples for retraining DeepVariant:; --min_base_quality 5 \; --min_mapping_quality 1 \; --vsc_min_fraction_snps 0.02 \; --p_error 0.1 \. After retraining DeepVariant and get the model with best score, I compared the vcf outputs using default DeepVariant and retrained DeepVariant model on HG003 sample at chromosome20. I notice that there are many variants that two models classify differently; Here is an example of the difference:. default model:; chr20 61083 . C T 33.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:33:40:19,21:0.525:33,0,42; chr20 11479054 . A G 56.6 PASS . GT:GQ:DP:AD:VAF:PL 1/1:53:29:0,29:1:56,55,0; chr20 29356747 . A G 43.4 PASS . GT:GQ:DP:AD:VAF:PL 1/1:29:26:0,26:1:43,29,0; chr20	54889360	.	G	A	56.8	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:54:39:0,39:1:56,57,0. trained model:; chr20 61083 . C T 24.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:41:19,21:0.512195:24,0,18; chr20 11479054 . A G 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:36:29:0,29:1:0,47,36; chr20 29356747 . A G 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:33:28:0,28:1:0,47,33; chr20	54889360	.	G	A	0.5	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:10:39:0,39:1:0,29,9. Is there an output from DeepVariant (e.g intermediate files) that help me to understand how DV make decision on classifying the variant? My goal is to train DeepVariant so that it can keep those 'PASS' variants as in the default model and detect more variants in the dataset. Please advise how I can do that. Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/894:1422,detect,detect,1422,,https://github.com/google/deepvariant/issues/894,1,['detect'],['detect']
Safety,"Hi,. I am running the shuffle script (https://raw.githubusercontent.com/google/deepvariant/r1.0/tools/shuffle_tfrecords_beam.py) on some training data. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as [well](https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi)). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/364:696,sanity check,sanity check,696,,https://github.com/google/deepvariant/issues/364,1,['sanity check'],['sanity check']
Safety,"Hi,. I used DeepVariant+GLnexus to detect the mutations.; In the GT area of the VCF file, there were descriptions such as "" . /0"" , "" . /1"" or "" . /2"". I've never seen a pattern like "" . /0"" or "" . /1"" when using gatk's haplotypecaller.; Basically, I recognized "" . "" as a missing value. What do the missing values and the heterogeneity of ALT mean?; Does this mean that it is different from the heterozygous mutation of ""REF/ALT"", which is different from "" 0/0"" or "" 0/1""??. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/302:35,detect,detect,35,,https://github.com/google/deepvariant/issues/302,1,['detect'],['detect']
Safety,"Hi,. I'm running DeepVariant via NVIDA docker image on Horizon sample (Detail of the sample dataset is here: https://horizondiscovery.com/en/reference-standards/products/truq-1-5-tier-reference-standard) . The result is not good as only 5 variants out of 13 expected variants are found, and among 5 found variants, only 1 variant has PASS filter and the other 4 variants have REFCALL filter. . I read the blog post of pileup image of Deepvaraint. Is there a way for me to see the pileup image of variants in my results. I want to know why the other variants are not detected event though I relaxed some of the parameters: vsc_min_fraction_snps = 0.03 instead of 0.12 as default, --min_base_quality=5 instead of 10 as default. Also, I want to see why the variants has REFCALL filter instead of expected PASS filter. . Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/690:566,detect,detected,566,,https://github.com/google/deepvariant/issues/690,1,['detect'],['detected']
Safety,"I am following the instructions under:; https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. 1. start GCE image : Ubuntu 16.04 with 100GB. git clone https://github.com/google/deepvariant; cd deepvariant; ./build-prereq.sh; ./build_and_test.sh; ```; ...; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ DV_COPT_FLAGS='--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ export DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; ++ DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; (17:54:59) INFO: Current date is 2017-12-22; (17:55:18) ERROR: /home/<mypath>/0fcc5a420905d68918d80793ee59fab4/external/com_goo; glesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start; with either '//', ':', or '@'. Us; e --incompatible_load_argument_is_label=false to temporarily disable this check. ... (17:55:26) ERROR: Analysis of target '//deepvariant/testing:gunit_extras' failed; build aborted: Loading failed; (17:55:26) INFO: Elapsed time: 27.289s; (17:55:26) FAILED: Build did NOT complete successfully (50 packages loaded); (17:55:26) ERROR: Couldn't start the build. Unable to run tests; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/20:1347,abort,aborted,1347,,https://github.com/google/deepvariant/issues/20,1,['abort'],['aborted']
Safety,"I am running on BAM generated with BWA. The fastq that was used had one reads file with less sequences. Docker latest image. . /opt/deepvariant/bin/run_deepvariant --model_type WGS --num_shards 5 --output_vcf 19CT030668_deepvariant.vcf --reads 19CT030668.bam --ref ../human_g1k_v37.fasta --sample_name 19CT030668. The processe starts but when it gets to end of the BAM it exits. . 2020-12-11 15:45:32.928477: W third_party/nucleus/io/sam_reader.cc:532] Could not read base quality scores A00215:130:HK3H2DSXX:2:2549:4869:24768: Not found: Could not read base quality scores; 2020-12-11 15:45:32.931412: F deepvariant/allelecounter.cc:103] Check failed: offset + len <= read.aligned_quality_size() (1 vs. 0); Fatal Python error: Aborted. Current thread 0x00007f38280a8700 (most recent call first):; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 76 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 237 in select_windows; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 574 in realign_reads; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1129 in region_reads; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1055 in process; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1377 in make_examples_runner; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1500 in main; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/absl_py/absl/app.py"", line 251 in _run_main; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/tmp/Bazel.runfiles_pfk22dn_/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1510 in <module>; p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/396:728,Abort,Aborted,728,,https://github.com/google/deepvariant/issues/396,1,['Abort'],['Aborted']
Safety,"I am trying to install deepvariant within a singularity container using recipe build. I have successfully complied google-sdk within the container, when it comes to the point where its runs ""./build-prereq.sh"" script, it terminates with this error: . **Setting up unzip (6.0-9ubuntu1) ...; Setting up zip (3.0-8) ...; Setting up zlib1g-dev:amd64 (1:1.2.8.dfsg-1ubuntu1) ...; Processing triggers for libc-bin (2.19-0ubuntu6) ...; Processing triggers for sgml-base (1.26+nmu4ubuntu1) ...; ========== [Tue Apr 17 00:09:23 UTC 2018] Stage 'Install python packaging infrastructure' starting; Reading package lists... Done; Building dependency tree ; Reading state information... Done; E: Unable to locate package python-wheel; ABORT: Aborting with RETVAL=255; Cleaning up...**. I am using singularity within in vagrant on my mac and here is my recipe file:. **Bootstrap: shub; From: singularityhub/ubuntu. %runscript; exec echo ""The runscript is the containers default runtime command!"". %files; # /home/vanessa/Desktop/hello-kitty.txt # copied to root of container; # /home/vanessa/Desktop/party_dinosaur.gif /opt/the-party-dino.gif #. %environment. export CLOUD_SDK_REPO=""cloud-sdk-$(lsb_release -c -s)"". %labels; AUTHOR mnoon@email.arizona.edu. %post; apt-get update && apt-get -y install python2.7 git wget curl ; mkdir /data; echo ""The post section is where you can install, and configure your container."". echo ""deb http://packages.cloud.google.com/apt $CLOUD_SDK_REPO main"" > /etc/apt/sources.list.d/google-cloud-sdk.list; curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -. apt-get update && apt-get install -y google-cloud-sdk; ##download deepVariant scripts and run them; git clone https://github.com/google/deepvariant.git; cd deepvariant; ; ./build-prereq.sh. ./build_and_test.sh. ./run-prereq.sh**. I have no idea whats causing this error. any help will be appreciated. -M",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/66:722,ABORT,ABORT,722,,https://github.com/google/deepvariant/issues/66,2,"['ABORT', 'Abort']","['ABORT', 'Aborting']"
Safety,"I have encountered the following error in several PacBio HiFi samples while running the docker image of deepvariant 1.4.0:. > F deepvariant/allelecounter.cc:872] Check failed: left_padding + right_padding < counts_.size() (5000 vs. 4022); Fatal Python error: Aborted. Deepvariant was run while enabling read normalization:. docker run -v ""input_path"":/input -v ""output_path"":/output google/deepvariant:1.4.0 /opt/deepvariant/bin /run_deepvariant --model_type=PACBIO --make_examples_extra_args=""normalize_reads=true"" --ref=/input/reference.fa --reads=/input/sample.bam --output_vcf=/output/sample.vcf --output_gvcf=/output/sample.gvcf --num_shards=16 --logging_dir=/output/logs. I know it is discouraged to enable read normalization due to potential excessive computational times, but I need it to make sure that I am capturing INDELs on the same conditions as my Illumina left-aligned samples. Any ideas on how to solve this issue?; Thank you,; Eugenio",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/762:259,Abort,Aborted,259,,https://github.com/google/deepvariant/issues/762,1,['Abort'],['Aborted']
Safety,"I rebuilt docker images from instruction on https://github.com/google/deepvariant/issues/99#issuecomment-428366972. ```; gcloud builds submit \; --project ""${PROJECT_ID}"" \; --config cloudbuild.yaml \; --substitutions TAG_NAME=""${VERSION_NUMBER}"" \; --timeout 2h .; ```. I see three images on GCP Container Registry:. 1. **deepvariant**; 1. **deepvariant_gpu**; 1. **deepvariant_runner**. After finishing make_examples, now I am running calll_variant, but seems my rebuilt deepvariant_gpu image doesn't have CUDA installed, seeing such error. ```; ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory; ```. The command I used:; ```; ( time nvidia-docker run -v /home/${USER}:/home/${USER} gcr.io/my_project/deepvariant_gpu:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) | tee ""${LOG_DIR}/call_variants.log"" 2>&1; ```. I confirmed this is NOT an issue with gcr.io/deepvariant-docker/deepvariant_gpu, which means that it's just my rebuilt image missing CUDA driver. How should I modify the build command to build an image with CUDA driver, please?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/102:252,timeout,timeout,252,,https://github.com/google/deepvariant/issues/102,1,['timeout'],['timeout']
Safety,"I'm trying to fine tune the original DeepVariant model with some extra data.; However, during the fine tuning process, the model suddenly losses all its predictive power in the first 10000 or 20000 steps. The call-variants output of these models are like all sites have homo-alt variants with a same qual value, 6.8 for example.; The sudden change in the model happens in the first step of fine tuning, as the saved model.ckpt-0 in the begining already gives the above output.; I find this result quite confusing, as the loss should increase dramatically. Is that a normal output of the fine tuning process?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185:153,predict,predictive,153,,https://github.com/google/deepvariant/issues/185,1,['predict'],['predictive']
Safety,Model losses all predictive power in the first steps during fine tuning,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/185:17,predict,predictive,17,,https://github.com/google/deepvariant/issues/185,1,['predict'],['predictive']
Safety,"My environment is Ubuntu20.04, python3.8. I look up for it, and find this problem happened at installing clif library, but the former sentences are successful, the protobuf3.13.0 have installed, I wonder how it happen and what can i do?. ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHON_EXECUTABLE=/usr/local/bin/python3 /root/clif; -- The C compiler identification is GNU 9.4.0; -- The CXX compiler identification is GNU 9.4.0; -- Check for working C compiler: /usr/bin/cc; -- Check for working C compiler: /usr/bin/cc -- works; -- Detecting C compiler ABI info; -- Detecting C compiler ABI info - done; -- Detecting C compile features; -- Detecting C compile features - done; -- Check for working CXX compiler: /usr/bin/c++; -- Check for working CXX compiler: /usr/bin/c++ -- works; -- Detecting CXX compiler ABI info; -- Detecting CXX compiler ABI info - done; -- Detecting CXX compile features; -- Detecting CXX compile features - done; -- Found PkgConfig: /usr/bin/pkg-config (found version ""0.29.1""); -- Checking for module 'protobuf'; -- No package 'protobuf' found; CMake Error at /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:463 (message):; A required package was not found; Call Stack (most recent call first):; /usr/share/cmake-3.16/Modules/FindPkgConfig.cmake:643 (_pkg_check_modules_internal); clif/cmake/modules/CLIFUtils.cmake:31 (pkg_check_modules); clif/CMakeLists.txt:22 (include)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/737:732,Detect,Detecting,732,,https://github.com/google/deepvariant/issues/737,8,['Detect'],['Detecting']
Safety,"ONEDNN_OPTS=0`.; 2024-07-03 17:21:58.247052: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-07-03 17:21:58.247080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; Runs all 3 steps to go from input DNA reads to output VCF/gVCF files.; (... then -- the list of all options follows). If run on a proper BAM file with all options provided, all TF-TRT warning messages are periodically repeated as well as ; CUDA Version 11.3.1; 2024-07-02 22:47:07.493311: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-02 22:47:12.386498: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ...; and processing is performed on CPUs. . Also, these details are put in the log hudreds of times:; 2024-07-03 18:27:31.862526: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-03 18:27:31.862557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 8308a7bb3067; 2024-07-03 18:27:31.862563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 8308a7bb3067; 2024-07-03 18:27:31.862607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 555.42.6; 2024-07-03 18:27:31.862621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagno",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:3080,detect,detected,3080,,https://github.com/google/deepvariant/issues/844,1,['detect'],['detected']
Safety,"PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts.; export DEEPVARIANT_BUCKET=""gs://deepvariant""; export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages""; export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a; # named release version. Set it to an already existing value in the environment; # (allowing command line control of the build), defaulting to 0 (release build).; # Note that setting this to 1 implies that the C++ code in DeepVariant will be; # build using the master branch and not the pinned version to avoid; # incompatibilities between TensorFlow C++ used to build DeepVariant and the; # tf-nightly wheel.; export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not; # the same as the python version of TensorFlow we use, but should be similar or; # we risk having version incompatibilities between our C++ code and the Python; # code we use at runtime.; if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; export DV_CPP_TENSORFLOW_TAG=""master""; else; export DV_CPP_TENSORFLOW_TAG=""r1.12""; fi; export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing; # value in the environment (allowing command line control of the build),; # defaulting to 0 (CPU only build).; export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is set to 1, DeepVariant will use a TensorFlow wheel file; # compiled with MKL support for corei7 or better chipsets, which; # significantly speeds up execution when running on modern CPUs. The default; # TensorFlow wheel files don't contain these instructions (and thereby run on a; # broader set of CPUs). Using this optimized wheel reduces the runtime",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:4711,risk,risk,4711,,https://github.com/google/deepvariant/issues/145,1,['risk'],['risk']
Safety,"Running make_examples locally from the Docker container. It runs without errors on the NA12878_S1.chr20.10_10p1mb.bam dataset, but fails with my BAM file. See the stack trace below. My BAM file is too large to attach here. What do you recommend? Also what are the allowed values for --logging_level? Please advise. ```; # ./opt/deepvariant/bin/make_examples --logging_level DEBUG --mode calling --ref /dv2/reference/CFSAN000189.fasta --reads /dv2/samples/CFSAN000211/reads.sorted.bam --examples output.examples.tfrecord; WARNING: Logging before flag parsing goes to stderr.; I1228 21:10:23.407845 140668049200896 client.py:1004] Timeout attempting to reach GCE metadata service.; W1228 21:10:23.408325 140668049200896 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 969, in main; options = default_options(add_flags=True, flags=FLAGS); File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 207, in default_options; sample_name = extract_sample_name_from_reads(flags.reads); File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 406, in extract_sample_name_from_reads; raise ValueError('Expected a single sample, found {}'.format(samples)); ValueError: Expected a single sample, found set([]). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/28:629,Timeout,Timeout,629,,https://github.com/google/deepvariant/issues/28,1,['Timeout'],['Timeout']
Safety,"T libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; Runs all 3 steps to go from input DNA reads to output VCF/gVCF files.; (... then -- the list of all options follows). If run on a proper BAM file with all options provided, all TF-TRT warning messages are periodically repeated as well as ; CUDA Version 11.3.1; 2024-07-02 22:47:07.493311: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-02 22:47:12.386498: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ...; and processing is performed on CPUs. . Also, these details are put in the log hudreds of times:; 2024-07-03 18:27:31.862526: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-03 18:27:31.862557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 8308a7bb3067; 2024-07-03 18:27:31.862563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 8308a7bb3067; 2024-07-03 18:27:31.862607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 555.42.6; 2024-07-03 18:27:31.862621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 555.42.6; 2024-07-03 18:27:31.862626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 555.42.6; (then repeated with every call). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:3534,detect,detected,3534,,https://github.com/google/deepvariant/issues/844,1,['detect'],['detected']
Safety,"This is from DeepVariant's documentation. ""For somatic data or any other samples where the genotypes go beyond two copies of DNA, DeepVariant will not work out of the box because the only genotypes supported are hom-alt, het, and hom-ref."". Does the same apply to the RNA-seq model of DeepVariant? If a mutation has a low VAF is it predicted as hom-ref?. Because in RNA-seq, low VAF doesn't necessarily mean the variant is hom-ref in the genome. . ### In RNA-seq:; - **Expression Levels**: RNA-seq measures gene expression, so the VAF of a mutation depends on the expression of the mutant and wild-type alleles. If a gene is highly expressed in some cells and not others, or if only one allele is expressed (allele-specific expression), the VAF may be skewed.; - **Variable Expression**: The VAF in RNA-seq data can be influenced by tissue-specific expression, transcriptional noise, or RNA degradation, making it less consistent compared to DNA-seq.; - **Allelic Imbalance**: In RNA-seq, you might observe allelic imbalance due to factors like imprinting or preferential expression of one allele, further complicating the interpretation of VAF.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/864:332,predict,predicted,332,,https://github.com/google/deepvariant/issues/864,1,['predict'],['predicted']
Safety,"Total params: 21,810,083; Trainable params: 21,775,651; Non-trainable params: 34,432; __________________________________________________________________________________________________; /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 7 input channels.; input_shape = imagenet_utils.obtain_input_shape(; I0218 00:34:52.923406 140119155529536 keras_modeling.py:325] Number of l2 regularizers: 95.; I0218 00:34:52.923618 140119155529536 keras_modeling.py:337] inceptionv3: No initial checkpoint specified.; 2024-02-18 00:34:57.911320: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory.; 2024-02-18 00:34:58.566676: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory.; I0218 00:35:01.595164 140119155529536 call_variants.py:583] Predicted 1024 examples in 1 batches [0.637 sec per 100].; 2024-02-18 00:35:02.648043: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory.; 2024-02-18 00:35:03.234445: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 842268672 exceeds 10% of free system memory.; 2024-02-18 00:35:07.222464: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 1330642944 exceeds 10% of free system memory.; I0218 00:38:56.687749 140119155529536 call_variants.py:583] Predicted 52224 examples in 51 batches [0.463 sec per 100].; I0218 00:42:59.116032 140119155529536 call_variants.py:583] Predicted 103424 examples in 101 batches [0.468 sec per 100].; I0218 00:46:58.822113 140119155529536 call_variants.py:583] Predicted 154624 examples in 151 batches [0.468 sec per 100].; I0218 00:47:39.156648 140119155529536 call_variants.py:623] Complete: call_variants. real	13m21.231s; user	118m36.634s; sys	25m56.983s. ***** Running the command:***",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:16471,Predict,Predicted,16471,,https://github.com/google/deepvariant/issues/774,1,['Predict'],['Predicted']
Safety,"UT_DIR=""${PWD}/data""; OUTPUT_DIR=""${PWD}/output""; LOGDIR=""${PWD}/log""; N_SHARDS=$( /bin/ls output/ | wc -l ); ; sudo docker run --gpus 1 \; -v ${HOME}:${HOME} \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""gs://deepvariant/models/DeepVariant/1.0.0/DeepVariant-inception_v3-1.0.0+data-pacbio_standard/model.ckpt""; ```. the following error occurs:. ```; I1203 17:49:21.931325 140389904897792 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-12-03 17:49:32.284722: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/392:1097,Abort,Aborted,1097,,https://github.com/google/deepvariant/issues/392,1,['Abort'],['Aborted']
Safety,ValueError: `call_variants_outputs` did not pass sanity check.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/517:49,sanity check,sanity check,49,,https://github.com/google/deepvariant/issues/517,1,['sanity check'],['sanity check']
Safety,Variant detected in VCF but not in bam,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/691:8,detect,detected,8,,https://github.com/google/deepvariant/issues/691,1,['detect'],['detected']
Safety,"When I run build_and_test.sh, I get the following isssues; ```. Extracting Bazel installation...; .............................; (12:58:42) INFO: Current date is 2018-03-20; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:03) ERROR: Analysis of target '//deepvariant:binaries' failed; build aborted: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream; (13:01:03) INFO: Elapsed time: 146.946s; (13:01:03) FAILED: Build did NOT complete successfully (60 packages loaded); Fetching https://mirror.bazel.build/ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz; 26,415b 43s; Fetching https://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz; 32,588b 42s; Fetching https://mirror.bazel.build/www.kurims.kyoto-u.ac.jp/~ooura/fft.tgz; 20,092b 40s; (13:01:03) ERROR: Couldn't start the build. Unable to run tests. ```. Please Help",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/59:817,abort,aborted,817,,https://github.com/google/deepvariant/issues/59,1,['abort'],['aborted']
Safety,"When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:; `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.; /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**; - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa); - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: docker run google/deepvariant:1.4.0; - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.; /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; CPU information from /proc/cpuinfo; product: Common KVM processor; vendor: Intel Corp.; physical id: 2; bus info: cpu@1; width: 64 bits; capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552:251,Abort,Aborted,251,,https://github.com/google/deepvariant/issues/552,2,['Abort'],['Aborted']
Safety,"]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:14406,predict,predict,14406,,https://github.com/google/deepvariant/issues/537,2,['predict'],['predict']
Safety,"_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --reads \""/input-gcsfused-{}/${BAM}\"" --ref \""${INPUT_REF}\"" --task {} --gvcf \""${GVCF}\""/gvcf_output.tfrecord@\""${SHARDS}\"".gz\"""": exit status 127: bash: gcsfuse: command not found. Is it possible to identify the problem/typo?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/214:1762,timeout,timeout,1762,,https://github.com/google/deepvariant/issues/214,2,['timeout'],['timeout']
Safety,"al 1 writing processes started.; I0105 15:55:38.765925 140372734228288 dv_utils.py:365] From /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord-00000-of-00002.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0105 15:55:38.766286 140372734228288 call_variants.py:506] Shape of input examples: [100, 221, 7]; I0105 15:55:38.768594 140372734228288 call_variants.py:510] Use saved model: True; I0105 15:56:02.220975 140372734228288 dv_utils.py:365] From /opt/models/wgs/example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0105 15:56:02.221645 140372734228288 dv_utils.py:365] From /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord-00000-of-00002.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0105 15:56:51.296850 140372734228288 call_variants.py:583] Predicted 1024 examples in 1 batches [4.670 sec per 100].; I0105 16:00:45.139408 140372734228288 call_variants.py:623] Complete: call_variants. real 5m27.431s; user 6m58.490s; sys 0m19.033s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/refgenome/GCF_003073045.1_ASM307304v1_genomic.fna"" --infile ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/call_variants_output.tfrecord.gz"" --outfile ""./outputgpu/output.vcf.gz"" --cpus ""2"" --gvcf_outfile ""./outputgpu/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/gvcf.tfrecord@2.gz"". 2024-01-05 16:00:59.661436: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 16",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:14884,Predict,Predicted,14884,,https://github.com/google/deepvariant/issues/761,1,['Predict'],['Predicted']
Safety,"all last):; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768, in main; call_variants(; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 598, in call_variants; model_example_shape = dv_utils.get_shape_and_channels_from_json(; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/dv_utils.py"", line 367, in get_shape_and_channels_from_json; example_info = json.load(f); File ""/usr/lib/python3.8/json/__init__.py"", line 293, in load; return loads(fp.read(),; File ""/usr/lib/python3.8/json/__init__.py"", line 357, in loads; return _default_decoder.decode(s); File ""/usr/lib/python3.8/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/usr/lib/python3.8/json/decoder.py"", line 355, in raw_decode; raise JSONDecodeError(""Expecting value"", s, err.value) from None; json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0). Process ForkProcess-1:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 454, in post_processing; item = output_queue.get(timeout=180); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 108, in get; raise Empty; _queue.Empty. real 3m2.335s; user 0m7.450s; sys 0m4.274s`. **Does the quick start test work on your system?**; Yes",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:7516,timeout,timeout,7516,,https://github.com/google/deepvariant/issues/869,1,['timeout'],['timeout']
Safety,"ast):; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict; preds_evaluated = mon_sess.run(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run; return self._sess.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run; return self._sess.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run; outputs = _WrappedSession.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run; result = self._run(None, fetches, feed_dict, options_ptr,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1191, in _run; results = self",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:6682,predict,predictions,6682,,https://github.com/google/deepvariant/issues/679,1,['predict'],['predictions']
Safety,"c,/public3:/public3,/public2:/public2,/fast3:/fast3 \; /public/software/deepvariants/1.6.0/gpuver/deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$REF \; --reads=""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam"" \; --regions ""NC_037590.1:200,000-950,000"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --num_shards=2`. Error messages:; `==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available.; Use the NVIDIA Container Toolkit to start this container with GPU support; see; https://docs.nvidia.com/datacenter/cloud-native/ . 2024-01-05 15:52:56.748367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-01-05 15:52:57.864310: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2024-01-05 15:53:10.688853: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfe",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:1540,detect,detected,1540,,https://github.com/google/deepvariant/issues/761,1,['detect'],['detected']
Safety,"call_variants_output to variants.; I0217 17:38:26.135714 139808123168576 postprocess_variants.py:1318] Using 60 CPUs for parallelization of variant transformation.; I0217 17:39:46.024054 139808123168576 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: SRR1572254; I0217 17:39:46.024466 139808123168576 postprocess_variants.py:1216] --sample_name is set but was not used.; I0217 17:53:05.493927 139808123168576 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 7.4284539779027305 minutes; [E::hts_open_format] Failed to open file ""/public1/home/yinhang/projects/two_genomes/04_T2T/11_snp/03_vcf/SRR1572254/SRR1572254.vcf.gz"" : No such file or directory; 2024-02-17 17:53:05.496309: E third_party/nucleus/io/merge_variants.cc:115] opening writer failedCould not open variants_path: /public1/home/yinhang/projects/two_genomes/04_T2T/11_snp/03_vcf/SRR1572254/SRR1572254.vcf.gz; 2024-02-17 17:53:05.496469: F ./third_party/nucleus/core/statusor.h:230] Non-OK-status: status_ status: UNKNOWN: Could not open variants_path: /public1/home/yinhang/projects/two_genomes/04_T2T/11_snp/03_vcf/SRR1572254/SRR1572254.vcf.gz; Fatal Python error: Aborted. Current thread 0x00007f279d84a740 (most recent call first):; File ""/tmp/Bazel.runfiles_vibz8587/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1391 in main; File ""/tmp/Bazel.runfiles_vibz8587/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_vibz8587/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_vibz8587/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419 in <module>. real	27m34.677s; user	18m45.707s; sys	4m47.747s; **Any additional context:**; ![image](https://github.com/google/deepvariant/assets/108465040/18d9cc45-ef3a-4f05-8a85-19eefac74034); call_variants may be have done ,but postprocess_variants Failed to open file ""file.vcf.gz"" : No such file or directory",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/773:3718,Abort,Aborted,3718,,https://github.com/google/deepvariant/issues/773,1,['Abort'],['Aborted']
Safety,com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/deepvariant/deepvariant/testing/BUILD:19:1: Target '@com_googlesource_code_re2//:re2' contains an error and its package is in error and referenced by '//deepvariant/testing:gunit_extras'; (09:27:18) ERROR: Analysis of target '//deepvariant/testing:gunit_extras_test' failed; build aborted: Loading failed; (09:27:18) INFO: Elapsed time: 14.618s; (09:27:18) FAILED: Build did NOT complete successfully (48 packages loaded); (09:27:18) ERROR: Couldn't start the build. Unable to run tests; ```; Could anyone shed some light on this issue? Interestingly this was working a few days ago but possibly on a different host. Could it be hardware dependent?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:20800,abort,aborted,20800,,https://github.com/google/deepvariant/issues/19,1,['abort'],['aborted']
Safety,"d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 622, in predict; features, None, ModeKeys.PREDICT, self.config); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1148, in _call_model_fn; model_fn_results = self._model_fn(features=features, **kwargs); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 914, in model_fn; is_training=mode == tf.estimator.ModeKeys.TRAIN); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 744, in create; return self._create(images, num_classes, is_training); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 1122, in _create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 587, in inception_v3; depth_multiplier=depth_multiplier); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 117, in inception_v3_base; net = layers.conv2d(inputs, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:17411,PREDICT,PREDICT,17411,,https://github.com/google/deepvariant/issues/358,1,['PREDICT'],['PREDICT']
Safety,"didates in chr1:10001-11000 [0.01s elapsed]; I0201 17:01:49.757262 140183113676544 make_examples.py:782] Found 0 candidates in chr1:14001-15000 [0.01s elapsed]; I0201 17:01:49.769794 140183113676544 make_examples.py:782] Found 0 candidates in chr1:18001-19000 [0.01s elapsed]`. However, when I go up in number of shards and cores, even to just 8, let alone the recommended 64, I don't get past this point:. `Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrUn_GL456370', u'chrUn_GL456379', u'chrUn_GL456366', u'chrUn_GL456368', u'chrUn_JH584304']`. Then it just seems to lag, for up to 24 hours before I aborted the job because of cost concerns. Any ideas of what might be happening under the hood such that 4 cores and shards seems to run immediately with no problem, but things just freeze and don't advance with more? 4 would be far too slow and expensive unfortunately for WGS, but I'm not sure how to get past this. Thanks in advance for any help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/150:5688,abort,aborted,5688,,https://github.com/google/deepvariant/issues/150,1,['abort'],['aborted']
Safety,"eepVariant.; DV_BAZEL_VERSION=""0.15.0"". # We need to make sure that $HOME/bin is first in the binary search path so that; # `bazel` will find the latest version of bazel installed in the user's home; # directory. This is set in setting.sh as all DeepVariant scripts source; # settings.sh and assume that `bazel` will find the right version.; export PATH=""$HOME/bin:$PATH"". # Path to the public bucket containing DeepVariant-related artifacts.; export DEEPVARIANT_BUCKET=""gs://deepvariant""; export DV_PACKAGE_BUCKET_PATH=""${DEEPVARIANT_BUCKET}/packages""; export DV_PACKAGE_CURL_PATH=""https://storage.googleapis.com/deepvariant/packages"". # Set this to 1 to use the nightly (latest) build of TensorFlow instead of a; # named release version. Set it to an already existing value in the environment; # (allowing command line control of the build), defaulting to 0 (release build).; # Note that setting this to 1 implies that the C++ code in DeepVariant will be; # build using the master branch and not the pinned version to avoid; # incompatibilities between TensorFlow C++ used to build DeepVariant and the; # tf-nightly wheel.; export DV_TF_NIGHTLY_BUILD=""${DV_TF_NIGHTLY_BUILD:-1}"". # The branch/tag we checkout to build our C++ dependencies against. This is not; # the same as the python version of TensorFlow we use, but should be similar or; # we risk having version incompatibilities between our C++ code and the Python; # code we use at runtime.; if [[ ""${DV_TF_NIGHTLY_BUILD}"" = ""1"" ]]; then; export DV_CPP_TENSORFLOW_TAG=""master""; else; export DV_CPP_TENSORFLOW_TAG=""r1.12""; fi; export DV_GCP_OPTIMIZED_TF_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=""1.12.0""; export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=""1.12.0"". # Set this to 1 to use DeepVariant with GPUs. Set it to an already existing; # value in the environment (allowing command line control of the build),; # defaulting to 0 (CPU only build).; export DV_GPU_BUILD=""${DV_GPU_BUILD:-1}"". # If this variable is se",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:4382,avoid,avoid,4382,,https://github.com/google/deepvariant/issues/145,1,['avoid'],['avoid']
Safety,"els/wgs/model.ckpt'); [[node save_1/RestoreV2 (defined at usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py:623) ]]. Original stack trace for 'save_1/RestoreV2':; File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:14255,predict,prediction,14255,,https://github.com/google/deepvariant/issues/537,4,['predict'],"['prediction', 'predictions']"
Safety,"exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 493, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/loc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:18867,predict,predict,18867,,https://github.com/google/deepvariant/issues/537,1,['predict'],['predict']
Safety,"exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict; preds_evaluated = mon_sess.run(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run; return self._sess.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run; return self._sess.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run; outputs = _WrappedSession.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 968, in run; result = self._run(None, fetches, feed_dict, options_ptr,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/cli",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:6642,predict,predict,6642,,https://github.com/google/deepvariant/issues/679,1,['predict'],['predict']
Safety,"f/home/cuirf/.tmp/tmp3vf8mpw9/call_variants_output.tfrecord.gz"" --outfile ""./outputgpu/output.vcf.gz"" --cpus ""2"" --gvcf_outfile ""./outputgpu/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/gvcf.tfrecord@2.gz"". 2024-01-05 16:00:59.661436: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 16:00:59.661893: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 16:01:06.236791: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 16:01:06.304423 140416700553024 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0105 16:01:06.676597 140416700553024 postprocess_variants.py:1313] CVO sorting took 0.006136405467987061 minutes; I0105 16:01:06.677379 140416700553024 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0105 16:01:06.677495 140416700553024 postprocess_variants.py:1318] Using 2 CPUs for parallelization of variant transformation.; I0105 16:01:06.808352 140416700553024 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0105 16:01:08.209710 140416700553024 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.01743464469909668 minutes; I0105 16:01:10.258949 140416700553024 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.03414338032404582 minutes. real 0m21.740s; user 0m13.473s; sys 0m2.305s. ***",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:16323,detect,detected,16323,,https://github.com/google/deepvariant/issues/761,1,['detect'],['detected']
Safety,"flow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2024-01-05 15:53:10.688853: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 15:53:10.692890: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 15:53:26.990784: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 15:53:27.004992 140619855705920 run_deepvariant.py:519] Re-using the directory for intermediate results in /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9. ***** Intermediate results will be written to /public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9 in docker. ****; ***** Running the command:*****; time seq 0 1 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/refgenome/GCF_003073045.1_ASM307304v1_genomic.fna"" --reads ""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam"" --examples ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/make_examples.tfrecord@2.gz"" --channels ""insert_size"" --gvcf ""/public3/group_crf/home/cuirf/.tmp/tmp3vf8mpw9/gvcf.tfrecord@2.gz"" --regions ""NC_037590.1:200,000-950,000"" --task {}. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL =",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:3149,detect,detected,3149,,https://github.com/google/deepvariant/issues/761,1,['detect'],['detected']
Safety,"hello,; I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening?; ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b); ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf); ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed); Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/655:415,avoid,avoid,415,,https://github.com/google/deepvariant/issues/655,1,['avoid'],['avoid']
Safety,"il.py:58). Operation defined at: (most recent call last); >>> File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; >>> tf.compat.v1.app.run(); >>> ; >>> File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 300, in run; >>> _run_main(main, args); >>> ; >>> File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 251, in _run_main; >>> sys.exit(main(argv)); >>> ; >>> File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; >>> call_variants(; >>> ; >>> File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants; >>> prediction = next(predictions); >>> ; >>> File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict; >>> features, input_hooks = self._get_features_from_input_fn(; >>> ; >>> File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn; >>> result, _, hooks = estimator_util.parse_input_fn_result(result); >>> ; >>> File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result; >>> result = iterator.get_next(); >>> . Original stack trace for 'IteratorGetNext':; File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/pbs.1173981.omics/Bazel.runfil",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:14287,predict,predict,14287,,https://github.com/google/deepvariant/issues/564,1,['predict'],['predict']
Safety,"ine 60, in parse_input_fn_result; result = iterator.get_next(); Node: 'IteratorGetNext'; corrupted record at 484865306; [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict; features, input_hooks = self._get_features_from_input_fn(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn; result, _, hooks = estimator_util.parse_input_fn_result(result); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result; result = iterator.get_next(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next; flat_ret = gen_dataset_ops.iterator_get_next(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next; _, _, _op, _outputs = _op_def_library._apply_op_helper(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:10378,predict,prediction,10378,,https://github.com/google/deepvariant/issues/679,2,['predict'],"['prediction', 'predictions']"
Safety,"k.errors_impl.DataLossError: corrupted record at 484865306; [[{{node IteratorGetNext}}]]. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict; preds_evaluated = mon_sess.run(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 778, in run; return self._sess.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1307, in run; return self._sess.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1397, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1464, in run; outputs = _WrappedSession.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1228, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:6499,predict,prediction,6499,,https://github.com/google/deepvariant/issues/679,2,['predict'],"['prediction', 'predictions']"
Safety,"l.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict; preds_evaluated = mon_sess.run(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 786, in run; return self._sess.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1315, in run; return self._sess.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1420, in run; raise six.reraise(*original_exc_info); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1405, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1473, in run; outputs = _WrappedSession.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:11097,predict,predict,11097,,https://github.com/google/deepvariant/issues/564,1,['predict'],['predict']
Safety,"licable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in chec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/471:1825,Abort,Aborted,1825,,https://github.com/google/deepvariant/issues/471,1,['Abort'],['Aborted']
Safety,"mics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; I1029 09:19:03.708104 139762738964288 make_examples_core.py:243] Task 2/8: Writing examples to output/intermediate_results_dir/make_examples.tfrecord-00002-of-00008.gz; I1029 09:19:03.708200 139762738964288 make_examples_core.py:243] Task 2/8: Overhead for preparing inputs: 2 seconds; I1029 09:19:04.060117 140039545739072 genomics_reader.py:222] Reading data/hg005_gm26107.mrna.grch38.bam with NativeSamReader; I1029 09:19:04.129533 140039545739072 make_examples_core.py:243] Task 6/8: Writing examples to output/intermediate_results_dir/make_examples.tfrecord-00006-of-00008.gz; I1029 09:19:04.129874 140039545739072 make_examples_core.py:243] Task 6/8: Overhead for preparing inputs: 2 seconds; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.525670: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 278310 end: 278449; Fatal Python error: Aborted. Current thread 0x00007f543a64b740 (most recent call first):; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 67 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 675 in realign_reads; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581:13947,Abort,Aborted,13947,,https://github.com/google/deepvariant/issues/581,1,['Abort'],['Aborted']
Safety,multiallele detection,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/480:12,detect,detection,12,,https://github.com/google/deepvariant/issues/480,1,['detect'],['detection']
Safety,"nd Keras-NLP). Then when finishing, I got this error:. Saving model using saved_model format.; WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets; WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter; W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be f",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:2175,Detect,Detecting,2175,,https://github.com/google/deepvariant/issues/722,1,['Detect'],['Detecting']
Safety,"nd from call_variants_outputs for variant reference_bases: ""C""; alternate_bases: ""A""; calls {; info {; key: ""AD""; value {; values {; int_value: 17; }; values {; int_value: 4; }; }; }; info {; key: ""DP""; value {; values {; int_value: 21; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 0.190476190476; }; }; }; genotype: -1; genotype: -1; call_set_name: ""XY406-1""; }; end: 10147; reference_name: ""1""; start: 10146; is [[0], [0], [0]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 874, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 851, in main; header=header); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 595, in write_variants_to_vcf; for variant in variant_generator:; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 110, in _group_overlapping_variants; for variant in sorted_variants:; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 631, in _transform_call_variants_output_to_variants; outputs, multi_allelic_qual_filter); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 559, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. **Does the quick start test work on your system?**; YES",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/485:3048,sanity check,sanity check,3048,,https://github.com/google/deepvariant/issues/485,2,['sanity check'],['sanity check']
Safety,"ne 209, in _tpu_service; raise RuntimeError('Missing runtime dependency on the Google API client. '; RuntimeError: Missing runtime dependency on the Google API client. Run `pip install cloud-tpu-client` to fix. ```; However, cloud-tpu-client is not actually the problem. The issue is that `google.api_core.client_options` is not found when being imported from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469:3646,avoid,avoid,3646,,https://github.com/google/deepvariant/issues/469,1,['avoid'],['avoid']
Safety,"nment; ENV PATH=""/opt/miniconda/bin:${PATH}"". RUN conda config --add channels defaults && \; conda config --add channels bioconda && \; conda config --add channels conda-forge && \; conda create -n bio bioconda::bcftools bioconda::samtools -y && \; conda clean -a. # Clone DeepVariant and build; FROM base AS builder. # Clone the DeepVariant repository; RUN git clone https://github.com/google/deepvariant.git /opt/deepvariant && \; cd /opt/deepvariant && \; git checkout tags/v1.6.1. # Run Bazel build with additional flags to skip problematic configurations; RUN bazel build -c opt --noincremental --experimental_action_listener= //deepvariant:make_examples //deepvariant:call_variants //deepvariant:postprocess_variants || { \; echo ""Bazel build failed""; \; exit 1; }. # Final image; FROM base AS final. # Set environment variables; ENV VERSION=1.6.0; ENV PYTHON_VERSION=3.8; ENV PATH=""/opt/miniconda/bin:${PATH}"". # Install Python packages; RUN pip install --upgrade pip setuptools wheel --timeout=120 && \; pip install jaxlib jax --timeout=120 --extra-index-url https://storage.googleapis.com/jax-releases/jax_releases.html. # Copy DeepVariant binaries from the builder stage; COPY --from=builder /opt/deepvariant /opt/deepvariant; WORKDIR /opt/deepvariant. # Ensure executable scripts are correctly set up; RUN BASH_HEADER='#!/bin/bash' && \; for script in make_examples call_variants call_variants_slim postprocess_variants vcf_stats_report show_examples runtime_by_region_vis multisample_make_examples labeled_examples_to_vcf make_examples_somatic train run_deepvariant run_deepsomatic; do \; printf ""%s\n%s\n"" ""${BASH_HEADER}"" ""python3 /opt/deepvariant/bin/${script}.zip \""$@\"""" > /opt/deepvariant/bin/${script} && \; chmod +x /opt/deepvariant/bin/${script}; \; done. # Copy licenses and other necessary files; # Ensure these paths and URLs are correct and accessible; # Replace with valid URLs or remove if not needed; ADD https://storage.googleapis.com/deepvariant/models/DeepVariant/1.6.0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:2464,timeout,timeout,2464,,https://github.com/google/deepvariant/issues/871,2,['timeout'],['timeout']
Safety,"nt/deepvariant/postprocess_variants.py"", line 1249, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main; write_variants_to_vcf(; File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf; for variant in variant_iterable:; File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants; for variant in sorted_variants:; File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants; canonical_variant, predictions = merge_predictions(; File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check.; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? ; No, the quick start and also chr22 from the same sample ran through. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/517:5380,predict,predictions,5380,,https://github.com/google/deepvariant/issues/517,3,"['predict', 'sanity check']","['predictions', 'sanity check']"
Safety,"ocess_variants.py:1211] Using sample name from call_variants output. Sample name: sample1; multiprocessing.pool.RemoteTraceback:; """"""; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 125, in worker; result = (True, func(*args, **kwds)); File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 48, in mapstar; return list(map(*args)); File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1125, in _mappable_transform_call_variant_group_to_output_variant; return _transform_call_variant_group_to_output_variant(**kwargs); File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1036, in _transform_call_variant_group_to_output_variant; return add_call_to_variant(; File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 434, in add_call_to_variant; gq, variant.quality = compute_quals(predictions, index); File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 469, in compute_quals; genomics_math.ptrue_to_bounded_phred(predictions[prediction_index]); File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/third_party/nucleus/util/genomics_math.py"", line 143, in ptrue_to_bounded_phred; raise ValueError('ptrue must be between zero and one: {}'.format(ptrue)); ValueError: ptrue must be between zero and one: nan; """""". The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_i47tupw0/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849:3804,predict,predictions,3804,,https://github.com/google/deepvariant/issues/849,1,['predict'],['predictions']
Safety,"oftmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 640, in predict; preds_evaluated = mon_sess.run(predictions); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 754, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1259, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1360, in run; raise six.reraise(*original_exc_info); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/six_archive/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1345, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 14",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:13791,predict,prediction,13791,,https://github.com/google/deepvariant/issues/358,2,['predict'],"['prediction', 'predictions']"
Safety,"ons connected to node IteratorGetNext:; In[0] IteratorV2 (defined at /usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py:58). Operation defined at: (most recent call last); >>> File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; >>> tf.compat.v1.app.run(); >>> ; >>> File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 300, in run; >>> _run_main(main, args); >>> ; >>> File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 251, in _run_main; >>> sys.exit(main(argv)); >>> ; >>> File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; >>> call_variants(; >>> ; >>> File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants; >>> prediction = next(predictions); >>> ; >>> File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict; >>> features, input_hooks = self._get_features_from_input_fn(; >>> ; >>> File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn; >>> result, _, hooks = estimator_util.parse_input_fn_result(result); >>> ; >>> File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result; >>> result = iterator.get_next(); >>> . Original stack trace for 'IteratorGetNext':; File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/pbs.117",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:14134,predict,prediction,14134,,https://github.com/google/deepvariant/issues/564,2,['predict'],"['prediction', 'predictions']"
Safety,"or 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 622, in predict; features, None, ModeKeys.PREDICT, self.config); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1148, in _call_model_fn; model_fn_results = self._model_fn(features=features, **kwargs); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 914, in model_fn; is_training=mode == tf.estimator.ModeKeys.TRAIN); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 744, in create; return self._create(images, num_classes, is_training); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 1122, in _create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 587, in inception_v3; depth_multiplier=depth_multiplier); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 117, in inception_v3_b",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:17377,predict,predict,17377,,https://github.com/google/deepvariant/issues/358,1,['predict'],['predict']
Safety,"reV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:; File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>; tf.app.run(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants; prediction = next(predictions); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict; hooks=all_hooks) as mon_sess:; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session; return self._sess_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", li",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:8727,predict,predict,8727,,https://github.com/google/deepvariant/issues/166,1,['predict'],['predict']
Safety,"runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170 in main; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/absl_py/absl/app.py"", line 251 in _run_main; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180 in <module>; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.559681: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 283943 end: 284101; Fatal Python error: Aborted. Current thread 0x00007f84f61df740 (most recent call first):; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 67 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 675 in realign_reads; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581:18666,Abort,Aborted,18666,,https://github.com/google/deepvariant/issues/581,1,['Abort'],['Aborted']
Safety,"runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170 in main; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/absl_py/absl/app.py"", line 251 in _run_main; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180 in <module>; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.521271: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 276773 end: 276899; Fatal Python error: Aborted. Current thread 0x00007f1d0c68a740 (most recent call first):; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 67 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 675 in realign_reads; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581:23385,Abort,Aborted,23385,,https://github.com/google/deepvariant/issues/581,1,['Abort'],['Aborted']
Safety,"runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170 in main; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/absl_py/absl/app.py"", line 251 in _run_main; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180 in <module>; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.553988: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 277024 end: 277165; Fatal Python error: Aborted. Current thread 0x00007f1c6e524740 (most recent call first):; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 67 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 675 in realign_reads; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_brb4vywu/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581:21812,Abort,Aborted,21812,,https://github.com/google/deepvariant/issues/581,1,['Abort'],['Aborted']
Safety,"runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170 in main; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/absl_py/absl/app.py"", line 251 in _run_main; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/tmp/Bazel.runfiles_hpi9dr8x/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180 in <module>; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.554670: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 275948 end: 276106; Fatal Python error: Aborted. Current thread 0x00007f9594dae740 (most recent call first):; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 67 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 675 in realign_reads; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_cf7f414f/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581:20239,Abort,Aborted,20239,,https://github.com/google/deepvariant/issues/581,1,['Abort'],['Aborted']
Safety,"runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170 in main; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/absl_py/absl/app.py"", line 251 in _run_main; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180 in <module>; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.544568: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 271187 end: 271287; Fatal Python error: Aborted. Current thread 0x00007f4836ae3740 (most recent call first):; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 67 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 675 in realign_reads; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_amrrdv68/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581:17093,Abort,Aborted,17093,,https://github.com/google/deepvariant/issues/581,1,['Abort'],['Aborted']
Safety,"runfiles_o0nxhusg/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 474, in main; call_variants(; File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3153, in predict; rendezvous.raise_errors(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/error_handling.py"", line 150, in raise_errors; six.reraise(typ, value, traceback); File ""/tmp/Bazel.runfiles_o0nxhusg/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/tpu/tpu_estimator.py"", line 3142, in predict; for result in super(TPUEstimator, self).predict(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 623, in predict; with tf.compat.v1.train.MonitoredSession(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1035, in __init__; super(MonitoredSession, self).__init__(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.cr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:19342,predict,predict,19342,,https://github.com/google/deepvariant/issues/537,1,['predict'],['predict']
Safety,"runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170 in main; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/absl_py/absl/app.py"", line 251 in _run_main; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/tmp/Bazel.runfiles_r0rx38k1/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180 in <module>; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.668786: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 279152 end: 279350; Fatal Python error: Aborted. Current thread 0x00007f5d7f60d740 (most recent call first):; File ""/tmp/Bazel.runfiles_wddxsjgc/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 67 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_wddxsjgc/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_wddxsjgc/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 675 in realign_reads; File ""/tmp/Bazel.runfiles_wddxsjgc/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_wddxsjgc/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_wddxsjgc/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_wddxsjgc/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581:24958,Abort,Aborted,24958,,https://github.com/google/deepvariant/issues/581,1,['Abort'],['Aborted']
Safety,"runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 170 in main; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/absl_py/absl/app.py"", line 251 in _run_main; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/tmp/Bazel.runfiles_r72dsu_k/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180 in <module>; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2022-10-29 09:19:04.586897: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chr20"" start: 277206 end: 277403; Fatal Python error: Aborted. Current thread 0x00007f6797491740 (most recent call first):; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 67 in _candidates_from_reads; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 233 in select_windows; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 675 in realign_reads; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1244 in region_reads; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1123 in process; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1795 in make_examples_runner; File ""/tmp/Bazel.runfiles_kpkzlrts/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/581:15520,Abort,Aborted,15520,,https://github.com/google/deepvariant/issues/581,1,['Abort'],['Aborted']
Safety,"running call variants on ONT data using the command . `run_pepper_margin_deepvariant call_variant -b ./Sample/alignments/GRCh38/41195.minimap2.bam -f /data/Homo_sapiens_assembly38.fasta -o Sample -t 4 -s Sample --phased_output --ont_r9_guppy5_sup; `. no errors but it's stuck at Starting Candidate Finding and the CPU is at 0% for hours. Can I stop it and resume? why there is no error? will it ever finish?. ....; [12-24-2023 05:47:47] INFO: SUMMARY PROCESSED 7280/7280.; [12-24-2023 05:47:47] INFO: THREAD 0 FINISHED SUCCESSFULLY.; [12-24-2023 05:54:33] INFO: FINISHED PREDICTION; [12-24-2023 05:54:33] INFO: ELAPSED TIME: 867 Min 41 Sec; [12-24-2023 05:54:33] INFO: PREDICTION FINISHED SUCCESSFULLY.; [12-24-2023 05:54:33] INFO: TOTAL ELAPSED TIME FOR INFERENCE: 867 Min 44 Sec; [12-24-2023 05:54:33] INFO: STEP 3/3 FINDING CANDIDATES; [12-24-2023 05:54:33] INFO: OUTPUT: 41195/pepper/; [12-24-2023 05:55:25] INFO: STARTING CANDIDATE FINDING. - Operating system: Linux; - DeepVariant version: kishwars/pepper_deepvariant r0.8; - Installation method (Docker, built from source, etc.): Docker; ONT long reads",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/758:571,PREDICT,PREDICTION,571,,https://github.com/google/deepvariant/issues/758,2,['PREDICT'],['PREDICTION']
Safety,"se_input_fn_result; >>> result = iterator.get_next(); >>> . Original stack trace for 'IteratorGetNext':; File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict; features, input_hooks = self._get_features_from_input_fn(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn; result, _, hooks = estimator_util.parse_input_fn_result(result); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result; result = iterator.get_next(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 444, in get_next; flat_ret = gen_dataset_ops.iterator_get_next(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2865, in iterator_get_next; _, _, _op, _outputs = _op_def_library._apply_op_helper(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:15683,predict,prediction,15683,,https://github.com/google/deepvariant/issues/564,2,['predict'],"['prediction', 'predictions']"
Safety,"singularity run -B /slurm/home/yrd/sunlab/yangfeng/pub/WW/WGS/deepvirant/deepvariant_1.6.1.sif \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$reference \; --reads=$sample_dir2 \; --output_vcf=""${OUTPUT_DIR}""/${NameN}.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/${NameN}.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/${NameN}_intermediate_results_dir"" \; --num_shards=60. FATAL: Unable to handle docker://google/deepvariant:1.6.1 uri: failed to get checksum for docker://google/deepvariant:1.6.1: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp: lookup registry-1.docker.io on 114.114.114.114:53: read udp 172.16.10.108:58057->114.114.114.114:53: i/o timeout; FATAL: Unable to handle docker://google/deepvariant:1.6.1 uri: failed to get checksum for docker://google/deepvariant:1.6.1: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp: lookup registry-1.docker.io on 114.114.114.114:53: read udp 172.16.10.108:57892->114.114.114.114:53: i/o timeout; FATAL: Unable to handle docker://google/deepvariant:1.6.1 uri: failed to get checksum for docker://google/deepvariant:1.6.1: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp: lookup registry-1.docker.io on 114.114.114.114:53: read udp 172.16.10.108:49924->114.114.114.114:53: i/o timeout; FATAL: Unable to handle docker://google/deepvariant:1.6.1 uri: failed to get checksum for docker://google/deepvariant:1.6.1: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp: lookup registry-1.docker.io on 114.114.114.114:53: read udp 172.16.10.108:58178->114.114.114.114:53: i/o timeout",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/831:775,timeout,timeout,775,,https://github.com/google/deepvariant/issues/831,4,['timeout'],['timeout']
Safety,"sr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Ker. For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0619 14:57:56.059498 47403021002560 call_variants.py:563] Total 1 writing processes started.; I0619 14:57:56.063244 47403021002560 dv_utils.py:370] From /tmp/make_examples.tfrecord-00000-of-00010.gz.example_info; I0619 14:57:56.063441 47403021002560 call_variants.py:588] Shape of input examples: [100, 221, 7]; I0619 14:57:56.063909 47403021002560 call_variants.py:592] Use saved model: True; 2024-06-19 14:57:57.916727: F tensorflow/tsl/platform/env.cc:391] Check failed: -1 != path_length (-1 vs. -1); Fatal Python error: Aborted. Current thread 0x00002b1ce03a6740 (most recent call first):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 500 in _import_graph_de; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 414 in import_graph_def; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/function_def_to_graph.py"", line 87 in func; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/function_deserialization.py"", line 416 i; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 154 in __init__; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 958 in load_partial; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 828 in load; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 596 in call_; File ""/tmp/Baze",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:2891,Abort,Aborted,2891,,https://github.com/google/deepvariant/issues/833,1,['Abort'],['Aborted']
Safety,"standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?; [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]. Caused by op u'save_1/RestoreV2', defined at:; File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 411, in <module>; tf.app.run(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 401, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_9uQwMB/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 357, in call_variants; prediction = next(predictions); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/estimator/estimator.py"", line 567, in predict; hooks=all_hooks) as mon_sess:; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 921, in __init__; stop_grace_period_secs=stop_grace_period_secs); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 643, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session; return self._sess_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session; self.tf",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:8592,predict,prediction,8592,,https://github.com/google/deepvariant/issues/166,2,['predict'],"['prediction', 'predictions']"
Safety,"t/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 642, in predict; preds_evaluated = mon_sess.run(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 786, in run; return self._sess.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1315, in run; return self._sess.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1420, in run; raise six.reraise(*original_exc_info); File ""/tmp/pbs.1173981.omics/Bazel.runfiles_pfgek2w5/runfiles/six_archive/six.py"", line 703, in reraise; raise value; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1405, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1473, in run; outputs = _WrappedSession.run(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1236, in run; return self._sess.run(*args, **kwargs); File """,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:11137,predict,predictions,11137,,https://github.com/google/deepvariant/issues/564,1,['predict'],['predictions']
Safety,"tNext}}]]. Original stack trace for 'IteratorGetNext':; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict; features, input_hooks = self._get_features_from_input_fn(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn; result, _, hooks = estimator_util.parse_input_fn_result(result); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result; result = iterator.get_next(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 445, in get_next; flat_ret = gen_dataset_ops.iterator_get_next(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 3037, in iterator_get_next; _, _, _op, _outputs = _op_def_library._apply_op_helper(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 795, in _apply_op_helper; op = g._create_op_internal(op_type_name, inputs, dtypes=None,; File ""/usr/local/lib/python3.8/dist-packages/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:10521,predict,predict,10521,,https://github.com/google/deepvariant/issues/679,1,['predict'],['predict']
Safety,"tches, targets, options,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/client/session.py"", line 1397, in _do_call; raise type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter; tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict; features, input_hooks = self._get_features_from_input_fn(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn; result, _, hooks = estimator_util.parse_input_fn_result(result); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result; result = iterator.get_next(); Node: 'IteratorGetNext'; corrupted record at 484865306; [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_und",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:8881,predict,prediction,8881,,https://github.com/google/deepvariant/issues/679,2,['predict'],"['prediction', 'predictions']"
Safety,"tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/third_party/nucleus/io/tfrecord.py"", line 190, in write_tfrecords; for proto in protos:; File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 111, in _group_overlapping_variants; for variant in sorted_variants:; File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in _transform_call_variants_output_to_variants; yield _transform_call_variant_group_to_output_variant(**cvo_group_kwargs); File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1036, in _transform_call_variant_group_to_output_variant; return add_call_to_variant(; File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 434, in add_call_to_variant; gq, variant.quality = compute_quals(predictions, index); File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 469, in compute_quals; genomics_math.ptrue_to_bounded_phred(predictions[prediction_index]); File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/third_party/nucleus/util/genomics_math.py"", line 143, in ptrue_to_bounded_phred; raise ValueError('ptrue must be between zero and one: {}'.format(ptrue)); ValueError: ptrue must be between zero and one: nan; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. I'm running this code on an H100 GPU running nvidia driver - `535.183.06` and CUDA version is `12.2`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/901:5032,predict,predictions,5032,,https://github.com/google/deepvariant/issues/901,2,['predict'],['predictions']
Safety,"ts.py:1211] Using sample name from call_variants output. Sample name: default; I0105 16:01:06.676597 140416700553024 postprocess_variants.py:1313] CVO sorting took 0.006136405467987061 minutes; I0105 16:01:06.677379 140416700553024 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0105 16:01:06.677495 140416700553024 postprocess_variants.py:1318] Using 2 CPUs for parallelization of variant transformation.; I0105 16:01:06.808352 140416700553024 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: default; I0105 16:01:08.209710 140416700553024 postprocess_variants.py:1386] Processing variants (and writing to temporary file) took 0.01743464469909668 minutes; I0105 16:01:10.258949 140416700553024 postprocess_variants.py:1407] Finished writing VCF and gVCF in 0.03414338032404582 minutes. real 0m21.740s; user 0m13.473s; sys 0m2.305s. ***** Running the command:*****; time /opt/deepvariant/bin/vcf_stats_report --input_vcf ""./outputgpu/output.vcf.gz"" --outfile_base ""./outputgpu/output"". 2024-01-05 16:01:21.188421: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-01-05 16:01:21.188700: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; 2024-01-05 16:01:28.513759: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; I0105 16:01:28.547411 140591583876928 genomics_reader.py:222] Reading ./outputgpu/output.vcf.gz with NativeVcfReader. real 0m18.513s; user 0m11.281s; sys 0m1.577s. `",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:18214,detect,detected,18214,,https://github.com/google/deepvariant/issues/761,1,['detect'],['detected']
Safety,"type(e)(node_def, op, message) # pylint: disable=no-value-for-parameter; tensorflow.python.framework.errors_impl.DataLossError: Graph execution error:. Detected at node 'IteratorGetNext' defined at (most recent call last):; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 453, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict; features, input_hooks = self._get_features_from_input_fn(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn; result, _, hooks = estimator_util.parse_input_fn_result(result); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result; result = iterator.get_next(); Node: 'IteratorGetNext'; corrupted record at 484865306; [[{{node IteratorGetNext}}]]. Original stack trace for 'IteratorGetNext':; File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_y5t4ngn_/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/679:9024,predict,predict,9024,,https://github.com/google/deepvariant/issues/679,1,['predict'],['predict']
Safety,"uate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets; WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter; W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay; W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum; W1025 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:2593,Detect,Detecting,2593,,https://github.com/google/deepvariant/issues/722,1,['Detect'],['Detecting']
Safety,"ugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-07-03 17:21:58.247080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; Runs all 3 steps to go from input DNA reads to output VCF/gVCF files.; (... then -- the list of all options follows). If run on a proper BAM file with all options provided, all TF-TRT warning messages are periodically repeated as well as ; CUDA Version 11.3.1; 2024-07-02 22:47:07.493311: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-02 22:47:12.386498: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ...; and processing is performed on CPUs. . Also, these details are put in the log hudreds of times:; 2024-07-03 18:27:31.862526: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-03 18:27:31.862557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 8308a7bb3067; 2024-07-03 18:27:31.862563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 8308a7bb3067; 2024-07-03 18:27:31.862607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 555.42.6; 2024-07-03 18:27:31.862621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 555.42.6; 2024-07-03 18:27:31.862626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:3256,detect,detected,3256,,https://github.com/google/deepvariant/issues/844,1,['detect'],['detected']
Safety,"ultiprocessing/pool.py"", line 125, in worker; result = (True, func(*args, **kwds)); File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 48, in mapstar; return list(map(*args)); File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1125, in _mappable_transform_call_variant_group_to_output_variant; return _transform_call_variant_group_to_output_variant(**kwargs); File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1036, in _transform_call_variant_group_to_output_variant; return add_call_to_variant(; File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 434, in add_call_to_variant; gq, variant.quality = compute_quals(predictions, index); File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 469, in compute_quals; genomics_math.ptrue_to_bounded_phred(predictions[prediction_index]); File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/third_party/nucleus/util/genomics_math.py"", line 143, in ptrue_to_bounded_phred; raise ValueError('ptrue must be between zero and one: {}'.format(ptrue)); ValueError: ptrue must be between zero and one: nan; """""". The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1385, in main; tmp_variant_file = dump_variants_to_temp_file(variant_generator); File ""/tmp/Bazel.runfiles_i47tupw0/runfil",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849:3995,predict,predictions,3995,,https://github.com/google/deepvariant/issues/849,1,['predict'],['predictions']
Safety,"urred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 640, in predict; preds_evaluated = mon_sess.run(predictions); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 754, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1259, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1360, in run; raise six.reraise(*original_exc_info); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/six_archive/six.py"", line 686, in reraise; raise value; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1345, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1418, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py""",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:13934,predict,predict,13934,,https://github.com/google/deepvariant/issues/358,1,['predict'],['predict']
Safety,"ython/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 433, in call_variants; prediction = next(predictions); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 622, in predict; features, None, ModeKeys.PREDICT, self.config); File ""usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1148, in _call_model_fn; model_fn_results = self._model_fn(features=features, **kwargs); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 914, in model_fn; is_training=mode == tf.estimator.ModeKeys.TRAIN); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 744, in create; return self._create(images, num_classes, is_training); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/modeling.py"", line 1122, in _create; images, num_classes, create_aux_logits=False, is_training=is_training); File ""usr/local/lib/python3.6/dist-packages/tf_slim/nets/inception_v3.py"", line 587, in inception_v3; de",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:17235,predict,prediction,17235,,https://github.com/google/deepvariant/issues/358,2,['predict'],"['prediction', 'predictions']"
Safety,"{self._structured_signature_summary()} missing ""; TypeError: signature_wrapper(*, input_1) missing required arguments: input_1. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/local/scratch/haley.arnold/14698718/Bazel.runfiles_xx0yuppt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 570, in call_variants; predictions = model.signatures['serving_default'](batch[1]); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1474, in __call__; return self._call_impl(args, kwargs); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1487, in _call_impl; return self._call_with_flat_signature(args, kwargs,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1541, in _call_with_flat_signature; return self._call_flat(args, self.captured_inputs, cancellation_manager); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 138, in _call_flat; return super(_WrapperFunction, self)._call_flat(args, captured_inputs,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py"", line 1745, in _call_flat; retur",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:8103,predict,predictions,8103,,https://github.com/google/deepvariant/issues/797,1,['predict'],['predictions']
Security,"**Describe the issue:**; Hi, I am following the [deepvariant-quick-start](https://github.com/google/deepvariant/blob/r1.4/docs/deepvariant-quick-start.md) tutorial on singularity to try out DeepVariant on our study. When I do `singularity run` command, I get the error about `temple()` please see the error message below. I'm wondering if anyone can help with this. **Setup**; - Operating system: CentOS Linux 7 (Core); - Singularity version: 3.5-8.el7; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: WGS. **Steps to reproduce:**; - Command:; > singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; > docker://google/deepvariant:""1.4.0"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=${reference_genome} \; > --reads=${bam} \; > --regions=""chr1"" \; > --output_vcf=${vcf_dir}/${sample}.vcf.gz \; > --output_gvcf=${gvcf_dir}/${sample}.g.vcf.gz \; > --intermediate_results_dir ${tmp_dir} \; > --num_shards=${ncpu}. - Error trace: (if applicable); > Error in tempfile() using template /XXX/parXXXXX.par: Parent directory (/XXX/) does not exist at /usr/bin/parallel line 3889. **Additional comments:**; I also tried with `--no-home` flag which did not work at all. ; I don't have the root access since I am running this on a HPC Torque system managed by others.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/601:1283,access,access,1283,,https://github.com/google/deepvariant/issues/601,1,['access'],['access']
Security,"**Describe the issue:**; I Build the docker image; Inside Docker image: I am reading the checkpoint files to create a frozen graph; When doing ""import_meta_graph"" I get the error. Below is the stack trace; `tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed.`. **Setup**; - Operating system: Ubuntu 18.04 on Intel i7 CPU (no GPU or TPU); - DeepVariant version: r-0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - Error trace: ; `2020-08-26 18:04:05.695108: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; Traceback (most recent call last):; File ""tf2_mipso_convert.py"", line 35, in <module>; saver = tf.compat.v1.train.import_meta_graph(meta_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1453, in import_meta_graph; **kwargs)[0]; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/py",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/339:513,access,accessing,513,,https://github.com/google/deepvariant/issues/339,2,['access'],"['accessed', 'accessing']"
Security,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:; yes. **Describe the issue:**; Version 1.2 installed via docker on a linux server (over SSH login), running the quickstart test run:; - Expected behavior: when running without sudo, process uses current user's name privilege.; - What happened: file access denied if folder permission is 744. The run successfully returns if manually setting the relevant folders to permission 777, but output (vcf files and report) files were owned by nobody/nobody. . My understanding is that nobody is a special handle meant for OS housekeeping works. Is this an expected behavior? Is it docker?. **Setup**; - Operating system: CentOS 7 (`cat /etc/os-release`); - DeepVariant version: 1.2; - Installation method: docker; - Type of data: The test data and command described in [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). **Steps to reproduce:**; - Command: identical to those of [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). Environment variable setup lines were directly pasted into the shell, the 'run everything' command was pasted into a file `cmd.sh` which was then was ran with `. cmd.sh`. **Does the quick start test work on your system?**; Yes. Outputs are fine. **Any additional context:**; Except having to add `mkdir` and `chmod` lines to the script, I found the run successful. I can read/write to the files owned by nobody and the ownership will transfer automatically upon writing.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/478:340,access,access,340,,https://github.com/google/deepvariant/issues/478,1,['access'],['access']
Security,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**; I have processed around 30 samples albeit having some issues with GPU, possibly due to nvidia driver / cuda version. However, recently postprocess has started stalling with the same error. Any help troubleshooting this would be greatly appreciated!. **Setup**; - Operating system: ; NAME=Red Hat Enterprise Linux; VERSION=9.4 (Plow); - DeepVariant version: deepvariant:1.6.1-gpu; - Installation method (Docker, built from source, etc.): Docker (via podman); - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Illumina WGS, GCA_000001405.15_GRCh38_no_alt_analysis_set. **Steps to reproduce:**; - Command: ; `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""; `; - Error trace: (if applicable); ```; ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-10 12:07:21.275077: I tensorflow/core/platform/cpu_feature_guard.cc:193",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849:811,secur,security-opt,811,,https://github.com/google/deepvariant/issues/849,1,['secur'],['security-opt']
Security,"**ISSUE**; First of all, I found DeepVariant to be a very good and innovative tool. I'm considering including it in my exome analysis pipeline. I followed the tutorial (DeepVariant worked correctly with the Complete Genomics model), and I created my own model using Genome in a Bottle samples. To do this, I sequenced the same reference sample three times to use each BAM file for training, validation, and testing. I didn't encounter any errors during the model creation process, but when I tried to test it, the process got stuck at the call_variants step. **Setup**; - Operating system: Ubuntu 22.04.4 LTS; - DeepVariant version:1.6.1; - Installation method:docker; - Type of data: MGI DNBSEQ 400, exome sequencing. **Steps to reproduce:**; - Command:; _Create examples for trainning set_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" make_examples --mode training --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/26_r_groups.bam"" --examples ""/output/training_set.gz"" --truth_variants ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed""`; _Create examples for validation set_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" make_examples --mode training --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/27_r_groups.bam"" --examples ""/output/validation_set.gz"" --truth_variants ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed"" `; _Trainning Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/training_set.gz --output_pattern_prefix=""output/training_shuffled"" --output_dataset_name=""26"" --output_dataset_config_pbtxt=""output/training.pbtxt"" --job_name=shuffle-tfrecords`; _Validation Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:391,validat,validation,391,,https://github.com/google/deepvariant/issues/869,1,['validat'],['validation']
Security,-strings deepvariant/...; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018年 08月 24日 星期五 19:30:03 CST] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [2018年 08月 24日 星期五 19:30:03 CST] Stage 'Misc setup' starting; Hit:1 http://mirrors.aliyun.com/ubuntu xenial InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:3 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Ign:4 http://dl.google.com/linux/chrome/deb stable InRelease ; Hit:5 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Hit:6 http://dl.google.com/linux/chrome/deb stable Release ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Err:9 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists... Done; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; W: Target Packages (main/binary-amd64/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-i386/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Packages (main/binary-all/Packages) is configured multiple times in /etc/apt/sources.list.d/google-cloud-sdk.list:1 and /etc/apt/sources.list.d/google-cloud-sdk.list:2; W: Target Translations ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:4494,secur,security,4494,,https://github.com/google/deepvariant/issues/89,1,['secur'],['security']
Security,"./build-prereq.sh ; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Misc setup' starting; ========== [Tue Oct 29 17:26:45 IST 2019] Stage 'Update package list' starting; [sudo] password for bioinformatics: ; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:28:53 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:28:54 IST 2019] Stage 'Install python packaging infrastructure' starting; Python 2.7.16 :: Anaconda, Inc. pip 19.3.1 from /home/bioinformatics/.local/lib/python2.7/site-packages/pip (python 2.7); ========== [Tue Oct 29 17:28:57 IST 2019] Stage 'Install python packages' starting; ========== [Tue Oct 29 17:29:14 IST 2019] Stage 'Install TensorFlow pip package' starting; Installing Intel's CPU-only MKL TensorFlow wheel; ========== [Tue Oct 29 17:29:15 IST 2019] Stage 'Install other packages' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'run-prereq.sh complete' starting; ========== [Tue Oct 29 17:29:16 IST 2019] Stage 'Update package list' starting; W: GPG error: https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 51716619E084DAB9; W: The repository 'https://cloud.r-project.org/bin/linux/ubuntu xenial-cran35/ InRelease' is not signed.; ========== [Tue Oct 29 17:29:24 IST 2019] Stage 'Install development packages' starting; ========== [Tue Oct 29 17:29:25 IST 2019] Stage 'Install bazel' starting; [bazel INFO src/main/cpp/option_processor.cc:388] Looking for the following rc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/231:335,password,password,335,,https://github.com/google/deepvariant/issues/231,1,['password'],['password']
Security,".3 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.0.3); Requirement already satisfied, skipping upgrade: six<2dev,>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.11.0); Requirement already satisfied, skipping upgrade: google-auth>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (1.5.1); Requirement already satisfied, skipping upgrade: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (3.4.2); Requirement already satisfied, skipping upgrade: cachetools>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (2.1.0); Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python2.7/dist-packages (from google-auth>=1.4.1->google-api-python-client) (0.2.2); Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python2.7/dist-packages (from rsa>=3.1.4->google-auth>=1.4.1->google-api-python-client) (0.4.4); ========== [2018年 08月 24日 星期五 19:54:15 CST] Stage 'Install TensorFlow pip package' starting; Skipping tf-nightly as it is not installed.; Skipping tensorflow as it is not installed.; Skipping tf-nightly-gpu as it is not installed.; Skipping tensorflow-gpu as it is not installed.; Installing Google Cloud Platform optimized CPU-only TensorFlow wheel; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed; 0 0 0 0 0 0 0 0 --:--:-- 0:03:04 --:--:-- 0; curl: (56) GnuTLS recv error (-54): Error in the pull function.; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build_release_binaries.sh; [sudo] password for solokopi: ; build_release_binaries.sh: line 39: bazel: command not found; build_release_binaries.sh: line 43: bazel: command not found; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:19212,password,password,19212,,https://github.com/google/deepvariant/issues/89,1,['password'],['password']
Security,".py"", line 58, in <listcomp>; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 56, in _todict; return obj.to_dict(validate=validate, context=context); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 373, in to_dict; dct = super(TopLevelMixin, copy).to_dict(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 325, in to_dict; result = _todict(; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 60, in _todict; return {; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 61, in <dictcomp>; k: _todict(v, validate, context); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in _todict; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in <listcomp>; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 56, in _todict; return obj.to_dict(validate=validate, context=context); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 84, in _prepare_data; data = _pipe(data, data_transformers.get()); File ""/usr/local/lib/python3.8/dist-packages/toolz/functoolz.py"", line 628, in pipe; data = func(data); File ""/usr/local/lib/python3.8/dist-packages/toolz/functoolz.py"", line 304, in __call__; return self._partial(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/data.py"", line 19, in default_data_transformer; return curried.pipe(data, limit_rows(max_rows=max_rows), to_values); File ""/usr/local/lib/python3.8/dist-packages/toolz/functoolz.py"", line 628, in pipe; data = func(data); File ""/usr/local/lib/python3.8/dist-packages/toolz/functoolz.py"", line 304, in __call__; return self._partial(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-pa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/839:5134,validat,validate,5134,,https://github.com/google/deepvariant/issues/839,1,['validat'],['validate']
Security,"//apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. - 100%[====================================================================================================================================================================================================>] 3.07K --.-KB/s in 0s. 2021-10-11 18:34:23 (51.5 MB/s) - written to stdout [3145/3145]. OK; Get:2 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease [20.8 kB]; Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]; Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]; Get:6 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:3677,secur,security,3677,,https://github.com/google/deepvariant/issues/489,1,['secur'],['security']
Security,"/schemapi.py"", line 61, in <dictcomp>; k: _todict(v, validate, context); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in _todict; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in <listcomp>; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 56, in _todict; return obj.to_dict(validate=validate, context=context); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 373, in to_dict; dct = super(TopLevelMixin, copy).to_dict(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 325, in to_dict; result = _todict(; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 60, in _todict; return {; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 61, in <dictcomp>; k: _todict(v, validate, context); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in _todict; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in <listcomp>; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 56, in _todict; return obj.to_dict(validate=validate, context=context); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 84, in _prepare_data; data = _pipe(data, data_transformers.get()); File ""/usr/local/lib/python3.8/dist-packages/toolz/functoolz.py"", line 628, in pipe; data = func(data); File ""/usr/local/lib/python3.8/dist-packages/toolz/functoolz.py"", line 304, in __call__; return self._partial(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/data.py"", line 19, in default_data_transformer; return curried.pipe(data, limit_rows(max_rows=max_row",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/839:4853,validat,validate,4853,,https://github.com/google/deepvariant/issues/839,1,['validat'],['validate']
Security,"01 ========== Load config settings.; 0.103 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Install the runtime packages' starting; 0.104 ========== This script is only maintained for Ubuntu 22.04.; 0.104 ========== Load config settings.; 0.105 ========== [Thu Oct 31 21:28:00 UTC 2024] Stage 'Misc setup' starting; 1.955 W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/sbsa InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-updates InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-backports InRelease' is not signed.; 1.955 W: GPG error: http://ports.ubuntu.com/ubuntu-ports jammy-security InRelease: At least one invalid signature was encountered.; 1.955 E: The repository 'http://ports.ubuntu.com/ubuntu-ports jammy-security InRelease' is not signed.; ------; Dockerfile:50; --------------------; 49 |; 50 | >>> RUN ./build-prereq.sh \; 51 | >>> && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel; 52 |; --------------------; ERROR: failed to solve: process ""/bin/sh -c ./build-prereq.sh && PATH=\""${HOME}/bin:${PATH}\"" ./build_release_binaries.sh # PATH for bazel"" did not complete successfully: exit code: 100; ```. Looks like the repositories are either old or the sign has expired. How to fix this error?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/902:2680,secur,security,2680,,https://github.com/google/deepvariant/issues/902,2,['secur'],['security']
Security,"2_v4.2.1_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed""`; _Create examples for validation set_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" make_examples --mode training --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/27_r_groups.bam"" --examples ""/output/validation_set.gz"" --truth_variants ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed"" `; _Trainning Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/training_set.gz --output_pattern_prefix=""output/training_shuffled"" --output_dataset_name=""26"" --output_dataset_config_pbtxt=""output/training.pbtxt"" --job_name=shuffle-tfrecords`; _Validation Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/validation_set.gz --output_pattern_prefix=""output/validation_shuffled"" --output_dataset_name=""27"" --output_dataset_config_pbtxt=""output/validation.pbtxt"" --job_name=shuffle-tfrecords `; _Model trainning_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" train --config=/input/dv_config.py:base --config.train_dataset_pbtxt=""/output/training.pbtxt"" --config.tune_dataset_pbtxt=""/output/validation.pbtxt"" --config.num_epochs=10 --config.learning_rate=0.0001 --config.num_validation_examples=0 --strategy=mirrored --experiment_dir=""/output/"" --config.batch_size=512`; _Model test_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" /opt/deepvariant/bin/run_deepvariant --model_type WES --customized_model ""/output/checkpoints/ckpt-679"" --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/33_r_groups.bam"" --output_vcf ""/output/33.vcf.gz"" --output_gvcf ""/output/33.g.vcf.gz"" -intermediate_results_dir ""/output/intermediate_results_dir"" --nu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:2144,validat,validation,2144,,https://github.com/google/deepvariant/issues/869,1,['validat'],['validation']
Security,"80/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.1 Repository rule _tf_http_archive defined at:; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.2 (21:51:08) Loading: 0 packages loaded; #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.3 (21:51:09) INFO: Elapsed time: 12.546s; #16 1497.3 (21:51:09) INFO: 0 processes.; #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded); #16 1497.3 (21:51:09) FAILED: Build did NOT complete successfully (0 packages loaded); #16 ERROR: executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1; ------; > [builder 6/6] RUN ./build-prereq.sh && PATH=""${HOME}/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"" ./build_release_binaries.sh # PATH for bazel:; ------; executor failed running [/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel]: exit code: 1; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:8894,Checksum,Checksum,8894,,https://github.com/google/deepvariant/issues/608,1,['Checksum'],['Checksum']
Security,; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; build_and_test.sh: line 39: bazel: command not found; + PATH=/home/solokopi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018年 08月 24日 星期五 19:30:03 CST] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [2018年 08月 24日 星期五 19:30:03 CST] Stage 'Misc setup' starting; Hit:1 http://mirrors.aliyun.com/ubuntu xenial InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:3 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Ign:4 http://dl.google.com/linux/chrome/deb stable InRelease ; Hit:5 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Hit:6 http://dl.google.com/linux/chrome/deb stable Release ; Hit:7 http://mirrors.aliyun.com/ubuntu xenial-security InRelease ; Err:9 http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease ; Cannot initiate the connection to packages.cloud.google.com:80 (2404:6800:4008:802::200e). - connect (101: Network is unreachable) [IP: 2404:6800:4008:802::200e 80]; Fetched 107 kB in 12min 0s (148 B/s) ; Reading package lists...,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:3793,password,password,3793,,https://github.com/google/deepvariant/issues/89,1,['password'],['password']
Security,Access permission was nobody instead of user when `docker run` without sudo,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/478:0,Access,Access,0,,https://github.com/google/deepvariant/issues/478,1,['Access'],['Access']
Security,Accessing tensorflow model defintion,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/456:0,Access,Accessing,0,,https://github.com/google/deepvariant/issues/456,1,['Access'],['Accessing']
Security,Adds the ability to link to external posts. Add an `external_url` and `source` to the frontmatter of a post and it will use the external link and list the source like this:. ![image](https://user-images.githubusercontent.com/1536935/105360574-be57a800-5bc6-11eb-98f9-f3bccc3a1100.png). A blank page is created for the post but contains a redirect tag for users that access the post via RSS.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/407:366,access,access,366,,https://github.com/google/deepvariant/pull/407,1,['access'],['access']
Security,Can we get access to the graph def protobuf?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/84:11,access,access,11,,https://github.com/google/deepvariant/issues/84,1,['access'],['access']
Security,Consistent exclusion for contig validation,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/18:32,validat,validation,32,,https://github.com/google/deepvariant/pull/18,1,['validat'],['validation']
Security,"During make_examples, there is a validation step to be sure contigs; reasonably overlap. This excludes some contigs, like chrM and extra; chromosomes but does it inconsistently. The contigs get excluded from; the list to use but then not during validation. This leads to errors on; small test datasets (bcbio has a chr22/chrM dataset that exposes this,; chrM is removed and then only 50% of the bases overlap so it fails).",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/18:33,validat,validation,33,,https://github.com/google/deepvariant/pull/18,3,"['expose', 'validat']","['exposes', 'validation']"
Security,Error: validating pipeline: zones and regions cannot be specified together,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/96:7,validat,validating,7,,https://github.com/google/deepvariant/issues/96,1,['validat'],['validating']
Security,"File ""/tmp/Bazel.runfiles_xq721o6r/runfiles/com_google_deepvariant/deepvariant/vcf_stats_vis.py"", line 513, in _altair_chart_to_html; altair_chart.save(; File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 476, in save; result = save(**kwds); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/save.py"", line 79, in save; spec = chart.to_dict(); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 373, in to_dict; dct = super(TopLevelMixin, copy).to_dict(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 325, in to_dict; result = _todict(; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 60, in _todict; return {; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 61, in <dictcomp>; k: _todict(v, validate, context); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in _todict; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in <listcomp>; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 56, in _todict; return obj.to_dict(validate=validate, context=context); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 373, in to_dict; dct = super(TopLevelMixin, copy).to_dict(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 325, in to_dict; result = _todict(; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 60, in _todict; return {; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 61, in <dictcomp>; k: _todict(v, validate, context); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in _todict; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/839:4041,validat,validate,4041,,https://github.com/google/deepvariant/issues/839,1,['validat'],['validate']
Security,Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~),MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:5743,secur,security,5743,,https://github.com/google/deepvariant/issues/489,1,['secur'],['security']
Security,"HON_VERSION=3.8; ENV PATH=""/opt/miniconda/bin:${PATH}"". # Install Python packages; RUN pip install --upgrade pip setuptools wheel --timeout=120 && \; pip install jaxlib jax --timeout=120 --extra-index-url https://storage.googleapis.com/jax-releases/jax_releases.html. # Copy DeepVariant binaries from the builder stage; COPY --from=builder /opt/deepvariant /opt/deepvariant; WORKDIR /opt/deepvariant. # Ensure executable scripts are correctly set up; RUN BASH_HEADER='#!/bin/bash' && \; for script in make_examples call_variants call_variants_slim postprocess_variants vcf_stats_report show_examples runtime_by_region_vis multisample_make_examples labeled_examples_to_vcf make_examples_somatic train run_deepvariant run_deepsomatic; do \; printf ""%s\n%s\n"" ""${BASH_HEADER}"" ""python3 /opt/deepvariant/bin/${script}.zip \""$@\"""" > /opt/deepvariant/bin/${script} && \; chmod +x /opt/deepvariant/bin/${script}; \; done. # Copy licenses and other necessary files; # Ensure these paths and URLs are correct and accessible; # Replace with valid URLs or remove if not needed; ADD https://storage.googleapis.com/deepvariant/models/DeepVariant/1.6.0/savedmodels/deepvariant.hybrid.savedmodel/saved_model.pb /models/; WORKDIR /opt/deepvariant/bin/; COPY --from=builder /opt/conda /opt/conda; COPY --from=builder /opt/deepvariant/run-prereq.sh .; COPY --from=builder /opt/deepvariant/settings.sh .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/make_examples.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/call_variants_slim.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/postprocess_variants.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/vcf_stats_report.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/bin/deepvariant/show_examples.zip .; COPY --from=builder /opt/deepvariant/bazel-out/k8-opt/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/871:3336,access,accessible,3336,,https://github.com/google/deepvariant/issues/871,1,['access'],['accessible']
Security,"Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, ; Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt); ; **Code to train the model:** ; `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=gpu # standard node(s); #SBATCH --ntasks=48; #SBATCH --job-name=""deepvariant_training""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. L",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:248,validat,validation,248,,https://github.com/google/deepvariant/issues/797,1,['validat'],['validation']
Security,"Hello, . Previously I had an issue where the parameters I was using were not producing checkpoints in the model training step. I know that choosing parameters has a component of guesswork and iteration, and I was wondering if there are recommendations anywhere on how to choose a starting point for model training parameters, or if there are descriptions somewhere of what changing a particular parameter is likely to do. In the run described below, I am attempting to train the model on an individual using a second individual for the training data and a third individual for the validation data, but my goal is to use multiple individuals for both the training and validation sets, akin to the project described [here](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). . Secondly, I've been having an issue lately where I submit scripts to my computing cluster, and though they are granted resources and produce a log file, the log file is empty after several days of the code running indicating no progress has been made or that the program has even initialized. I am also asking my cluster resources about this, as I suspect it is more likely an issue with resource allocation, but I would also very much appreciate if someone could take a glance at the code I am submitting to make sure there are no obvious causes for this in the deepvariant commands that I'm just completely missing. . Thank you very much! . Best, . Haley. Here is the code: ; `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=5-48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=gpu-a100 # standard node(s); #SBATCH --ntasks=1; #SBATCH --job-name=""deepvariant_modeltraining""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/840:581,validat,validation,581,,https://github.com/google/deepvariant/issues/840,2,['validat'],['validation']
Security,"Hello, I'm currently attempting to train DV on a non-model organism. I believe I have the training examples being made properly. For ~170 samples, I take 30 out randomly to use as validation and shuffle both sets by splitting into 4096 files, then repeating a few times (shuffling the iterator as well). Training proceeds quickly on the Tesla GPU, but all stats end up being homozygous reference. I can't tell what is going wrong. Here's my make_examples; ```; /opt/deepvariant/bin/make_examples \; --mode=training \; --use_ref_for_cram=true \; --ref=${reference} \; --examples ${accession}.ds0.with_labels.examples \; --sample_name ${accession} \; --reads ${cram} \; --truth_variants=${accession}.vcf.gz \; --confident_regions=${accession}.bed \; --regions CM0XXXXXX.1 \; --write_run_info. ```. Shuffling code is similar to this, but repeated multiple time:; ```; raw_dataset = tf.data.TFRecordDataset(inputs); for raw_record in itershuffle(raw_dataset, 2000):; example = tf.train.Example(); example.ParseFromString(raw_record.numpy()); writer = random.choice(out_fhs); writer.write(example.SerializeToString()); ```. And training code is like this:; ```; /opt/deepvariant/bin/model_train \; --dataset_config_pbtxt=${config_path} \; --batch_size=256 \; --train_dir=${training_dir} \; --model_name=""inception_v3"" \; --learning_rate=0.008 \; --start_from_checkpoint=/opt/models/wgs/model.ckpt \; --number_of_steps=50000 \; --save_interval_secs 300; ```. Here is the run info for just one sample's examples set (only a single chromosome, for testing purposes, from the .run_info.pbtxt file):. ```; labeling_metrics {; n_truth_variant_sites: 3469; n_truth_variant_alleles: 3474; n_candidate_variant_sites: 9778; n_candidate_variant_alleles: 9943; n_non_confident_candidate_variant_sites: 2219; n_true_positive_sites: 3468; n_true_positive_alleles: 3845; n_false_negative_sites: 1; n_false_negative_alleles: 1; n_false_positive_sites: 6309; n_false_positive_alleles: 6469; n_inexact_position_matches: 1; n",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/251:180,validat,validation,180,,https://github.com/google/deepvariant/issues/251,5,"['access', 'validat']","['accession', 'validation']"
Security,"Hello, after some quite impressive results applying HiSeq-trained DeepVariant on MGISEQ-2000 data, I've been working on achieving even better performance by retraining DeepVariant specifically for the MGISEQ-2000. To do this I've been broadly following the sketch at https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md. I now have unshuffled tfrecords for six reference samples, and have some questions about next steps. Because it might be relevant to the questions, my data are structured as follows:; * 6 samples (4xHG001/NA12878, 2xHG005/NA24631), each with:; * 25 tfrecord shards (00000-00024) of chr1, for tuning (perhaps over-optimistically) ; * 247 tfrecord shards (00000-00246) of chr2-19, no downsampling, for training; * 247 tfrecord shards (00000-00246) of chr2-19, 50% downsampling, for training; * 17 tfrecord shards (00000-00016) of chr20-22, for validation. My questions are:; 1. Is it necessary to shuffle the training data? I ask as it's proving to be a bit laborious to set up, and so I'm hoping that I can get around it. Given I have so many shards, if I just shuffle the order of the chr2-19 shards when I supply them to the training loop, will this be almost as good as shuffling the whole dataset?; 2. Is it necessary to shuffle the validation data? The tutorial does this, but I'm not sure why.; 3. How can I supply multiple datasets to the training loop (here effectively 12 datasets: 6 samples x 2 downsampling settings)? In the tutorial, `model_train` is supplied a wildcard path of `validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz`, which seems like it would only work for a single sample, and I'm not sure how this will work with multiple samples.; 4. Have there been any changes to the code base to better support warmstarting, or is the advice at https://github.com/google/deepvariant/issues/185 still the best approach to fine-tuning the model?. DeepVariant is a fantastic tool and I'm very much looking forward to see",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312:899,validat,validation,899,,https://github.com/google/deepvariant/issues/312,1,['validat'],['validation']
Security,"Hello, when running Deepvariant with the following command: . ```bash; BIN_VERSION=""1.0.0""`; INPUT_DIR=""${PWD}/data""; OUTPUT_DIR=""${PWD}/output""; LOGDIR=""${PWD}/log""; N_SHARDS=$( /bin/ls output/ | wc -l ); ; sudo docker run --gpus 1 \; -v ${HOME}:${HOME} \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""gs://deepvariant/models/DeepVariant/1.0.0/DeepVariant-inception_v3-1.0.0+data-pacbio_standard/model.ckpt""; ```. the following error occurs:. ```; I1203 17:49:21.931325 140389904897792 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-12-03 17:49:32.284722: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/392:903,authenticat,authentication,903,,https://github.com/google/deepvariant/issues/392,1,['authenticat'],['authentication']
Security,"Hello,. I've been using DeepTrio for de novo variant analysis, and it's been performing excellently. However, I noticed from the logs that DeepTrio uses the CPU to prepare data, which is quite time-consuming. In my case, it took 12 hours for one trio analysis, with an additional 4 hours on the GPU. Given that renting GPU servers( usually with less CPU) is more expensive than CPU servers and access to privately owned GPU servers is limited, it seems inefficient to run lengthy CPU processes on a GPU server. It feels like a bit of a waste, and sometimes I half-jokingly feel there might be someone out there with murderous intent because of it!. Would it be possible in future updates to partition the DeepTrio analysis into separate steps? This way, CPU-intensive tasks could be completed on a CPU server, and then the job could be transferred to a GPU server for the remaining tasks. Alternatively, could the data preparation (CPU) and analysis (GPU) be run at the same time? This would help optimize resource usage and reduce costs. Thank you for considering these suggestions.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/873:394,access,access,394,,https://github.com/google/deepvariant/issues/873,1,['access'],['access']
Security,"Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5.; First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant.; I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp.; Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py.; The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below.; ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png); Can you give me a detailed explanation of this detail？ Thank you very much！ ; Finally, thank you very much for developing such a great tool！",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/616:145,access,access,145,,https://github.com/google/deepvariant/issues/616,1,['access'],['access']
Security,"Hi Developers,. I get the following error when I am trying to use the latest version (V0.7.0) of deepvariant for wes analysis. I encounter the same error even I used the example given [here](https://cloud.google.com/genomics/docs/tutorials/deepvariant). ; ./deepvariant_v0.7.0_UDN644883_wes_09202018.sh; ERROR: (gcloud.alpha.genomics.pipelines.run) INVALID_ARGUMENT: Error: validating pipeline: zones and regions cannot be specified together. I have attached my script for the reference. ; [deepvariant_v0.7.0_UDN644883_wes_09202018.sh.txt](https://github.com/google/deepvariant/files/2403088/deepvariant_v0.7.0_UDN644883_wes_09202018.sh.txt). I was able to run deepvariant for wes using V0.6.1 successfully however looking at the output vcf I 'think' the script is not restricting the variants to the bed file regions. ; [deepvariant_v0.6.1_UDN644883_wes_09132018.sh.txt](https://github.com/google/deepvariant/files/2403091/deepvariant_v0.6.1_UDN644883_wes_09132018.sh.txt); [deepvariant_v0.6.1_UDN644883_wes_09132018.yaml.txt](https://github.com/google/deepvariant/files/2403092/deepvariant_v0.6.1_UDN644883_wes_09132018.yaml.txt). Thanks,; Shruti. Shruti Marwaha, PhD.; Research Engineer,; Stanford Center for Undiagnosed Diseases; Stanford University",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/96:374,validat,validating,374,,https://github.com/google/deepvariant/issues/96,1,['validat'],['validating']
Security,"Hi, . This is more of a support question, but I wasn't sure where else to get help. I'm trying to build and test deepvariant inside of a docker image. I know that there is already an image published to google cloud, but for my purposes I prefer to build my own image. My docker file looks like this. ; ```; FROM ubuntu:16.04. RUN set -ex \; && buildDependencies=' \; ca-certificates \; curl \; wget \; git \; apt-transport-https \; xz-utils \; bzip2 \; make \; ' \; && apt-get update \; && apt-get install -y --no-install-recommends $buildDependencies \; # gsutil; && wget https://storage.googleapis.com/pub/gsutil.tar.gz \; && tar xfz gsutil.tar.gz -C $HOME && rm gsutil.tar.gz \; && export PATH=$PATH:$HOME/gsutil \; # deepvariant; && git clone https://github.com/google/deepvariant.git \; && cd deepvariant \; && git checkout v0.4.1 \; && ./build-prereq.sh \; && ./build_and_test.sh; ```; The `build_and_test.sh` script fails with these errors:; ```; + ./build_and_test.sh; + source settings.sh; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=0; ++ TF_ENABLE_XLA=0; ++ export TF_NEED_CUDA=0; ++ TF_NEED_CUDA=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=0; ++ TF_NEED_MKL=0; ++ export TF_NEED_MPI=0; ++ TF_NEED_MPI=0; ++ export TF_NEED_OPENCL=0; ++ TF_NEED_OPENCL=0; ++ export TF_NEED_OPENCL_SYCL=0; ++ TF_NEED_OPENCL_SYCL=0; ++ export TF_NEED_S3=0; ++ TF_NEED_S3=0; ++ export TF_NEED_VERBS=0; ++ TF_NEED_VERBS=0; ++ export TF_CUDA_VERSION=8.0; ++ TF_CUDA_VERSION=8.0; ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ export TF_CUDNN_VERSION=6; ++ TF_CUDNN_VERSION=6; ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ export DV_P",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:370,certificate,certificates,370,,https://github.com/google/deepvariant/issues/19,1,['certificate'],['certificates']
Security,"Hi, I had a quick question involving [training](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) DeepVariant from an existing one. On the training data [page](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-details-training-data.md), the footnotes say that in v1.1 you exclude HG003 and chr20-22 from the training data. . Is this true for all versions since then? Does that mean if I wanted to train a new model from an existing one, it's safe to train on chr1-19 and then validate on chr20-22 on NIST data without the model ever seeing the variants there? Is it safe to use HG003 everywhere even though they are a direct relative of HG002 (whose data is used extensively in the training)? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/788:530,validat,validate,530,,https://github.com/google/deepvariant/issues/788,1,['validat'],['validate']
Security,"Hi, I try to setup deepvariant on my ubuntu, following instructionsin section Download binaries, models, and test data, listed here.; https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.mddy. I get httplib.ResponseNotReady error.; ![screenshot from 2017-12-11 13-31-30](https://user-images.githubusercontent.com/16382685/33831300-c7d8b290-de77-11e7-957f-748aadf16347.png). I was able to run the repository docker, but I would like to use this variant caller on machine without internet access. Is this possible?. Kind regards,. --; Tomasz",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/11:511,access,access,511,,https://github.com/google/deepvariant/issues/11,1,['access'],['access']
Security,"Hi, I'm running DeepVariant with BQSR -adjusted bam files. I have sequencing data for hg002 and hg005 and I have to say the validation results are very impressive!. I wanted to test the option for using the original base quality scores with:. --parse_sam_aux_fields ; --use_original_quality_scores. but get the following error: . FATAL Flags parsing error: Unknown command line flag 'parse_sam_aux_fields'; Pass --helpshort or --helpfull to see help on flags. I was running DeepVariant with docker by following the whole genome sequencing case study -tutorial, but will next test the pipeline for multi-sample variant calling for my cohort of 50 samples. I'm wondering should I realign the reads or is it possible to use the original base quality scores from BQSR adjusted bam files? I was previously using the GATK4 pipeline, but the results are so much better with DeepVariant and as a bonus, it's a million times easier (and quicker). Thanks. Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/595:124,validat,validation,124,,https://github.com/google/deepvariant/issues/595,1,['validat'],['validation']
Security,"Hi, i am new to deepvariant and kind of learning some programming languages too as a beginner, i try to run these commands on my ubuntu 16.04 and had the following errors, both with Build_and_test.sh, Build-prereq.sh. please i need help on how to fix these errors and get deepvariant running. thank you. [sudo] password for solokopi: ; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=0; ++ TF_ENABLE_XLA=0; ++ export TF_NEED_CUDA=0; ++ TF_NEED_CUDA=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=0; ++ TF_NEED_MKL=0; ++ export TF_NEED_MPI=0; ++ TF_NEED_MPI=0; ++ export TF_NEED_OPENCL=0; ++ TF_NEED_OPENCL=0; ++ export TF_NEED_OPENCL_SYCL=0; ++ TF_NEED_OPENCL_SYCL=0; ++ export TF_NEED_S3=0; ++ TF_NEED_S3=0; ++ export TF_NEED_VERBS=0; ++ TF_NEED_VERBS=0; ++ export TF_CUDA_VERSION=8.0; ++ TF_CUDA_VERSION=8.0; ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ export TF_CUDNN_VERSION=6; ++ TF_CUDNN_VERSION=6; ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ DV_BAZEL_VERSION=0.15.0; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages; ++ DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages; ++ export DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages; ++ DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages; ++ export DV_TF_NIGHTLY_BUILD=0; ++ DV_TF_NIGHTLY_BUILD=0; ++ [[ 0 = \1 ]]; ++ export DV_CPP_TENSORFLOW_TAG=r1.9; ++ DV_CPP_TENSORFLOW_TAG=r1.9; ++ export DV_GCP_OPTIMIZED_TF_WHL_VERSION=1.9.0; ++ DV_GCP_OPTIMIZED_TF_WHL_VERSION=1.9.0; ++ export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=1.9",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:311,password,password,311,,https://github.com/google/deepvariant/issues/89,1,['password'],['password']
Security,"Hi, is it possible that we can get access to the `graph.pbtxt` file that was generated during your training? Or can we extract it somehow from the model.ckpt.meta file? I need this to load the model in https://github.com/tensorflow/lucid . I already tried adding `tf.train.write_graph(sess.graph_def, ""/tmp"", ""graph.pbtxt"", True)` during variant calling, and I get a valid graph def. But I suspect that the graph def that I get during variant calling is not the same as the one during training time, and I think I need the latter. Thank you very much.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/84:35,access,access,35,,https://github.com/google/deepvariant/issues/84,1,['access'],['access']
Security,"Hi,. From the RNA example doc, it states; > We will also restrict analysis to CDS regions on chromosome 20 to make this demonstration quicker. I'm not 100% on the biological nuances, but the total RNA samples I have access to should enable variant calling (nearly) genome-wide. Is that applicable with the RNA-seq model, or is that primarily trained on CDS/exome only?; ; Best,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/584:216,access,access,216,,https://github.com/google/deepvariant/issues/584,1,['access'],['access']
Security,"Hi,. I had a question regarding how train/validation split was determined for training the models. From the DeepVariant paper, I noticed that chromosome 1-19 was used for training. Within chr1-19, is there a reason to prefer one of the following training-validation splits:; 1. Designate chr-a, 1 <= a <= 19, as validation set, and remaining as training set.; 2. Use some random subset of all labeled images as a validation set, and the disjoint set as training set (the random split may be grouped by sites, or not). Specifically, I am interested in knowing whether there was a reason for preferring one split over the other, or whether it is an arbitrary choice, that is considered equivalent. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/224:42,validat,validation,42,,https://github.com/google/deepvariant/issues/224,4,['validat'],['validation']
Security,"Hi. when I try to pull the deepvariant docker image via singularity using the following command:; singularity pull docker://google/deepvariant:""1.3.0""; it return the following error:; WARNING: pull for Docker Hub is not guaranteed to produce the; WARNING: same image on repeated pull. Use Singularity Registry; WARNING: (shub://) to pull exactly equivalent images.; ERROR Authentication error, exiting.; Cleaning up...; ERROR: pulling container failed!. could you please help. I don't have the permission to install docker.; Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/513:372,Authenticat,Authentication,372,,https://github.com/google/deepvariant/issues/513,1,['Authenticat'],['Authentication']
Security,"I am running Docker image of your software. Pls help to solve the problem. Here are the codes for running it:. BIN_VERSION=""1.1.0""; export OUTPUT_DIR=/home/manager/deepvariant-run/input; sudo docker run -v ""${OUTPUT_DIR}"":/opt/deepvariant/output dajunluo/deepvariant python run_deepvariant.py \; --model_type=WGS \; --ref=Reference.fasta \; --reads=newtest.bam \; --output_vcf=../output/test_output.vcf.gz \; --output_gvcf=../test_output/output.g.vcf.gz; ls $OUTPUT_DIR. Result:. [sudo] password for manager: ; [E::hts_open_format] Failed to open file newtest.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open newtest.bam. real	0m6.581s; user	0m4.128s; sys	0m1.476s; Traceback (most recent call last):; File ""run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/420:487,password,password,487,,https://github.com/google/deepvariant/issues/420,1,['password'],['password']
Security,"I am trying to build DeepVariant from source, and **trying to use a custom python installation rather than the standard one.** However, ```bazel test ``` fails because it tries to use the standard library python. The requisite python is accessible as ""python"" because it is in the PATH variable, but bazel seems to ignore that and looks for python in the standard location. I am not an expert in bazel by any means, so any help in how to get around this issue is greatly appreciated. Here is the command used for build (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/loca",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/356:237,access,accessible,237,,https://github.com/google/deepvariant/issues/356,1,['access'],['accessible']
Security,"I am trying to install DeepVariant on an IBM Power 8 system within a docker container. The docker container has the following Bazel version installed: 0.15.0- (https://github.com/bazelbuild/bazel/releases/tag/0.15.0) . I installed tensorflow r1.11 from source inside the docker container for CPU-only execution. This same source-code is placed so that it is seen by the build-prereq.sh script. I set the `export DV_USE_PREINSTALLED_TF=1`. In settings.sh, I changed DV_BAZEL_VERSION to DV_BAZEL_VERSION=""0.15.0-"" (to match the bazel version above). I also removed the corei7 option in DV_COPT_FLAGS. . In build-prereq.sh, I hard-coded the following in: `DV_PLATFORM=""ubuntu-16""`, since `lsb_release` didn't match the case statement conditions there. The following is the result `lsb_release`.; root@1f07cee05809:~/deepvariant# lsb_release; LSB Version: core-9.20160110ubuntu0.2-noarch:core-9.20160110ubuntu0.2-ppc64el:security-9.20160110ubuntu0.2-noarch:security-9.20160110ubuntu0.2-ppc64el. After these changes, build-prereq.sh runs fine. However, build_and_test.sh fails with the following error:; (03:21:40) ERROR: /root/deepvariant/third_party/nucleus/protos/BUILD:424:1: ClifProtoLibraryGeneration third_party/nucleus/protos/reads_pyclif.h failed (Exit 2): proto failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; bazel-out/host/bin/external/clif/proto -c bazel-out/ppc-opt/genfiles/third_party/nucleus/protos/reads_pyclif.cc -h bazel-out/ppc-opt/genfiles/third_party/nucleus/protos/reads_pyclif.h '--strip_dir=bazel; -out/ppc-opt/genfiles' '--source_dir='\''.'\''' third_party/nucleus/protos/reads.proto); bazel-out/host/bin/external/clif/proto: 3: bazel-out/host/bin/external/clif/proto: __requires__: not found; bazel-out/host/bin/external/clif/proto: 4: bazel-out/host/bin/external/clif/proto: import: not found; bazel-out/host/bin/external/clif/proto: 5: bazel-out/host/bin/external/clif/p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/122:917,secur,security-,917,,https://github.com/google/deepvariant/issues/122,2,['secur'],['security-']
Security,"I have successfully run the training tutorial. However, I have a question about how to train a model if you have multiple samples. Do I have to create various examples for training and validation and then merge them? Thank you in advance for any clarification.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/805:185,validat,validation,185,,https://github.com/google/deepvariant/issues/805,1,['validat'],['validation']
Security,"I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**; - Centos 7; - deepvariant 1.3.0; - Singularity run pulling from here: docker://google/deepvariant:""1.3.0""; - quickstart example. **Steps to reproduce:**; ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service ; mkdir -p /mnt/share/jasontest; cd /mnt/share/jasontest; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; BIN_VERSION=""1.3.0""; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530:375,access,access,375,,https://github.com/google/deepvariant/issues/530,1,['access'],['access']
Security,"I tried to train the DeepVariant on CPU and it was very slow. I now have access to a google cloud instance with GPUs. I am new with google cloud and I am not sure how I can accelerate the training process with GPUs. According to the [Case Study](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md), I can use the use_tpu flag to accelerate the training process with TPUs, but I don't know if I can do the same thing with GPUs.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/162:73,access,access,73,,https://github.com/google/deepvariant/issues/162,1,['access'],['access']
Security,"I'm attempting to write a bam file of the realigned reads, as I'm seeing ADs in the vcf that do not line up with what is present in the input bam file. I'm mainly concerned with two specific locations in the genome. The full genome is approx 11 Gbp, the input bam file is about 190 GB, and the drive I'm attempting to output to has more than 4 TB available. When using `-emit_realigned_reads` and using `-realigner_diagnostics` to provide an output directory, the log file tells me that deepvariant attempts to write more than seven million bam files, making it impossible to access the directory before crashing due to running out of disk space. Is there some way of getting around this? I'm thinking either a more storage efficient way of getting the entire bam file, or a way of getting the bam file of the specific regions I'm interested in. I'm using deepvariant 1.0 from docker on Ubuntu 18.04. The data is short read Illumina data. . The command I'm using to run this:. ```; seq 0 $((60-1)) |\; parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples \; --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \; --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \; --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord@60.gz \; --sample_name sample --task {} 2> results/sample/deepvariant/tmp/make_examples.log ; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/370:576,access,access,576,,https://github.com/google/deepvariant/issues/370,1,['access'],['access']
Security,"I'm interested in retraining DeepVariant to work on our dataset with low allele fraction variants. In this [tutorial](https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/docs/deepvariant-training-case-study.md), the training and validation set only target one chromosome region, chromosome 1 and 21 respectively. I want to create training samples on all chromosome regions in .BAM files with parameters to select for low fraction variants (using --downsample_fraction and vsc_min_fraction_snps). Is it a good way to do or do you suggest I break the training examples into different chromosome regions? ; Also if I use one BAM file and create training set on all chr regions of that file, should I use another BAM file to create the validation set?. Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/698:261,validat,validation,261,,https://github.com/google/deepvariant/issues/698,2,['validat'],['validation']
Security,"I'm trying to train a deepvariant model with a very simple topology.; After a few thousands of training steps, the logged training loss starts to vibrate around a rather high value. However the performance of saved models still keeps improving on my validation data set.; Why does this happen?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/194:250,validat,validation,250,,https://github.com/google/deepvariant/issues/194,1,['validat'],['validation']
Security,Question about train/validation split,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/224:21,validat,validation,21,,https://github.com/google/deepvariant/issues/224,1,['validat'],['validation']
Security,Run OpenVINO processing in separate thread which let's to use common logging (https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 + https://github.com/google/deepvariant/pull/363/commits/9e69c4096fac8ddb788c3d29e4405fc50e85d1e3) . Please ignore test scripts from `.github/workflows` - they are not a part of patch but just used for validation.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/393:376,validat,validation,376,,https://github.com/google/deepvariant/pull/393,1,['validat'],['validation']
Security,"Running make_examples locally from the Docker container. It runs without errors on the NA12878_S1.chr20.10_10p1mb.bam dataset, but fails with my BAM file. See the stack trace below. My BAM file is too large to attach here. What do you recommend? Also what are the allowed values for --logging_level? Please advise. ```; # ./opt/deepvariant/bin/make_examples --logging_level DEBUG --mode calling --ref /dv2/reference/CFSAN000189.fasta --reads /dv2/samples/CFSAN000211/reads.sorted.bam --examples output.examples.tfrecord; WARNING: Logging before flag parsing goes to stderr.; I1228 21:10:23.407845 140668049200896 client.py:1004] Timeout attempting to reach GCE metadata service.; W1228 21:10:23.408325 140668049200896 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 969, in main; options = default_options(add_flags=True, flags=FLAGS); File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 207, in default_options; sample_name = extract_sample_name_from_reads(flags.reads); File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 406, in extract_sample_name_from_reads; raise ValueError('Expected a single sample, found {}'.format(samples)); ValueError: Expected a single sample, found set([]). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/28:816,access,accessible,816,,https://github.com/google/deepvariant/issues/28,1,['access'],['accessible']
Security,"Running several DeepVariant jobs (v0.9.0) on the same server, e.g., by splitting a dataset via the `--regions` option, results in corrupted files (DataLossError) because of non-unique file names for the temp files generated under `/tmp` on the server (confirmed, see below). > I just checked the code, and you're right that the temp file names will be the same:; > https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py#L264-L266; > For now, please pass in different `intermediate_results_dir` for each run.; > For example:; > `--intermediate_results_dir=""/tmp/deepvariant_tmp_output/chr1""` for chr1, and so on. > I'll think about how we want to improve this in the future. I can think of a few options for future improvements, such as :. > 1. Use a random name for the internal /tmp files. Given that these are not exposed to the users anyway.; > 2. Use a unique name derived from the output VCF file, instead of calling all temp files the same name. > For now, using the `--intermediate_results_dir` should hopefully resolve your issue. Let me know if it works. If you have a suggestion on what's the best future improvement for better user experience, please let me know. _Originally posted by @pichuan in https://github.com/google/deepvariant/issues/175#issuecomment-560625427_. @pichuan; I think as an immediate step, updating the docs and making this explicit (also the option of using the `intermediate_results_dir`) would be reasonable. Concerning a proper solution, creating a randomly named temp dir for all temp files of the job (which then could have non-random names) seems like a very straightforward way; ideally, this should also take care of the clean-up, e.g.,as it is provided by Python's `tempfile` module. I assume basic functionality like this exists in all relevant programming languages. +Peter",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/242:836,expose,exposed,836,,https://github.com/google/deepvariant/issues/242,1,['expose'],['exposed']
Security,"The principle of PL, and sometimes the genotype and mutation ratio may not match, and after one generation of validation, some genotype results are incorrect.How should I handle it?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/702:110,validat,validation,110,,https://github.com/google/deepvariant/issues/702,1,['validat'],['validation']
Security,Validating WGS outputs,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/153:0,Validat,Validating,0,,https://github.com/google/deepvariant/issues/153,1,['Validat'],['Validating']
Security,"When I tried to create a VM with 8 GPUs using this command line:; export IMAGE_FAMILY=""tf-latest-gpu""; export ZONE=""us-west1-a""; export INSTANCE_NAME=""deep""; export INSTANCE_TYPE=""n1-standard-8""; gcloud compute instances create $INSTANCE_NAME \; --zone=$ZONE \; --image-family=$IMAGE_FAMILY \; --image-project=deeplearning-platform-release \; --maintenance-policy=TERMINATE \; --accelerator=""type=nvidia-tesla-p100,count=8"" \; --machine-type=$INSTANCE_TYPE \; --boot-disk-size=200GB \; --metadata=""install-nvidia-driver=True"". I got this error:; ERROR: (gcloud.compute.instances.create) Could not fetch resource:; - Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '8'. Number of accelerator cards attached to an instance must be one of [1, 2, 4]. My Quota shows that I have access to 8 P100 GPUs:; ![screenshot from 2019-01-04 12-53-33](https://user-images.githubusercontent.com/19914123/50707923-cd8b4d80-101f-11e9-976d-b04df93f0f26.png)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/136:800,access,access,800,,https://github.com/google/deepvariant/issues/136,1,['access'],['access']
Security,"Your dockerhub repo is currently having issues when trying to pull it, regardless of how:. ```; $> docker pull google/deepvariant; Using default tag: latest; Error response from daemon: manifest for google/deepvariant:latest not found: manifest unknown: manifest unknown; ```. ```; $> singularity pull deepvariant.img docker://google/deepvariant:latest; FATAL: While making image from oci registry: failed to get checksum for docker://google/deepvariant:latest: Error reading manifest latest in docker.io/google/deepvariant: manifest unknown: manifest unknown; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/221:413,checksum,checksum,413,,https://github.com/google/deepvariant/issues/221,1,['checksum'],['checksum']
Security,"ages/tensorflow/python/training/monitored_session.py"", line 750, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1232, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 1237, in _create_session; return self._sess_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 903, in create_session; self.tf_sess = self._session_creator.create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session; self._scaffold.finalize(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize; self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default; saver = Saver(sharded=True, allow_empty=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__; self.build(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build; self._build(self._filename, build_save=True, build_restore=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build; self.saver_def = self._builder._build_internal( # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal; restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps; self._AddRestoreOps(; File ""usr/local/lib/pyt",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:15833,access,access,15833,,https://github.com/google/deepvariant/issues/537,2,['access'],['access']
Security,amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information... Done; Some packages could not be installed. This may mean that you have; requested an impossible situation or if you are using the unstable; distribution that some required packages have not yet been created; or been moved out of Incoming.; The following information may help to resolve the situation:. The following packages have unmet dependencies:; clang-11 : Depends: libclang-cpp11 (>= 1:11.1.0~++20211010011718+1fdec59bffc1) but it is not going to be installed; Depends: libgcc-s1 (>= 3.0) but it is not installable; Depends: libllvm11 (>= 1:9~svn298832-1~) but it is not going to be installed; Depends: libstdc++6 (>= 11),MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:5777,secur,security,5777,,https://github.com/google/deepvariant/issues/489,1,['secur'],['security']
Security,"amples ""/output/validation_set.gz"" --truth_variants ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed"" `; _Trainning Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/training_set.gz --output_pattern_prefix=""output/training_shuffled"" --output_dataset_name=""26"" --output_dataset_config_pbtxt=""output/training.pbtxt"" --job_name=shuffle-tfrecords`; _Validation Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/validation_set.gz --output_pattern_prefix=""output/validation_shuffled"" --output_dataset_name=""27"" --output_dataset_config_pbtxt=""output/validation.pbtxt"" --job_name=shuffle-tfrecords `; _Model trainning_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" train --config=/input/dv_config.py:base --config.train_dataset_pbtxt=""/output/training.pbtxt"" --config.tune_dataset_pbtxt=""/output/validation.pbtxt"" --config.num_epochs=10 --config.learning_rate=0.0001 --config.num_validation_examples=0 --strategy=mirrored --experiment_dir=""/output/"" --config.batch_size=512`; _Model test_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" /opt/deepvariant/bin/run_deepvariant --model_type WES --customized_model ""/output/checkpoints/ckpt-679"" --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/33_r_groups.bam"" --output_vcf ""/output/33.vcf.gz"" --output_gvcf ""/output/33.g.vcf.gz"" -intermediate_results_dir ""/output/intermediate_results_dir"" --num_shards 10`. - Error trace: ; ` - I0822 07:51:54.272576 127450974123840 make_examples_core.py:301] Task 17/20: Writing example info to /output/intermediate_results_dir/make_examples.tfrecord-00017-of-00020.gz.example_info.json; I0822 07:51:54.272647 127450974123840 make_examples_core.py:2958] example_shape = [100, 221, 7]; I0822 07:51:54.272740 127450974123840 make_e",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:2467,validat,validation,2467,,https://github.com/google/deepvariant/issues/869,1,['validat'],['validation']
Security,"at.v1.train.import_meta_graph(meta_path); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1453, in import_meta_graph; **kwargs)[0]; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def; producer_op_list=producer_op_list); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 501, in _import_graph_def_internal; graph._c_graph, serialized, options) # pylint: disable=protected-access; tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed`. **Does the quick start test work on your system?** ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Quick start works on my system -- I can perform make_examples, call_variants and post_processing. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`). Code snippet :; `import tensorflow as tf. meta_path = '/opt/models/w",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/339:2335,access,access,2335,,https://github.com/google/deepvariant/issues/339,1,['access'],['access']
Security,"bam \; --output_vcf ""deepvariant/output.vcf.gz"" \; --num_shards 24 -v 2; ```; - Error trace: (if applicable); ```bash; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 2. real 0m35.091s; user 0m1.452s; sys 0m1.237s; I0205 10:26:31.374659 47922265431040 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 252.; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; - I have limited access to docker in general, and no access to docker on this machine. I've been running v1.0.0 on singularity on different input types (human WGS) regularly without errors. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/419:2531,access,access,2531,,https://github.com/google/deepvariant/issues/419,2,['access'],['access']
Security,c/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:5048,secur,security,5048,,https://github.com/google/deepvariant/issues/489,2,['secur'],['security']
Security,chive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 pac,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:4954,secur,security,4954,,https://github.com/google/deepvariant/issues/489,1,['secur'],['security']
Security,ckages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgr,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:5014,secur,security,5014,,https://github.com/google/deepvariant/issues/489,1,['secur'],['security']
Security,"create_session(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 661, in create_session; self._scaffold.finalize(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/monitored_session.py"", line 236, in finalize; self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 607, in _get_saver_or_default; saver = Saver(sharded=True, allow_empty=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 836, in __init__; self.build(); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 848, in build; self._build(self._filename, build_save=True, build_restore=True); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 876, in _build; self.saver_def = self._builder._build_internal( # pylint: disable=protected-access; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 509, in _build_internal; restore_op = self._AddShardedRestoreOps(filename_tensor, per_device,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 383, in _AddShardedRestoreOps; self._AddRestoreOps(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 335, in _AddRestoreOps; all_tensors = self.bulk_restore(filename_tensor, saveables, preferred_shard,; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py"", line 583, in bulk_restore; return io_ops.restore_v2(filename_tensor, names, slices, dtypes); File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_io_ops.py"", line 1490, in restore_v2; _, _, _op, _outputs = _op_def_library._apply_op_helper(; File ""usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 748, in _apply_op_helper; op = g._create",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/537:16472,access,access,16472,,https://github.com/google/deepvariant/issues/537,2,['access'],['access']
Security,"d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':; #16 1497.0 Traceback (most recent call last):; #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl; #16 1497.0 ctx.download_and_extract(; #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.1 (21:51:08) INFO: Repository llvm-raw instantiated at:; #16 1497.1 /opt/deepvariant/WORKSPACE:102:14: in <toplevel>; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/tensorflow/workspace3.bzl:42:9: in workspace; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/llvm/workspace.bzl:10:20: in repo; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.1 Repository rule _tf_http_archive defined at:; #16 1497.1 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.2 (21:51:08) Loading: 0 packages loaded; #16 1497.3 (21:51:08) ERROR: no such package '@tf_runtime//': java.io.IOException: Error downloading [http://m",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:7450,Checksum,Checksum,7450,,https://github.com/google/deepvariant/issues/608,1,['Checksum'],['Checksum']
Security,"e term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant?. **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.nature.com/articles/s41598-018-36177-7/tables/1. Do you have other metrics with comparisons on completely independent samples?. Thank you very much for your help!. Sincerely,; Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165:1435,access,accessible,1435,,https://github.com/google/deepvariant/issues/165,1,['access'],['accessible']
Security,"e tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal/ llvm-toolchain-focal-11 main'; Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease; Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease; Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease; Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 not available; deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic main; deb-src http://apt.llvm.org/bionic/ llvm-toolchain-bion",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:8363,secur,security,8363,,https://github.com/google/deepvariant/issues/489,1,['secur'],['security']
Security,"ed DeepVariant on MGISEQ-2000 data, I've been working on achieving even better performance by retraining DeepVariant specifically for the MGISEQ-2000. To do this I've been broadly following the sketch at https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md. I now have unshuffled tfrecords for six reference samples, and have some questions about next steps. Because it might be relevant to the questions, my data are structured as follows:; * 6 samples (4xHG001/NA12878, 2xHG005/NA24631), each with:; * 25 tfrecord shards (00000-00024) of chr1, for tuning (perhaps over-optimistically) ; * 247 tfrecord shards (00000-00246) of chr2-19, no downsampling, for training; * 247 tfrecord shards (00000-00246) of chr2-19, 50% downsampling, for training; * 17 tfrecord shards (00000-00016) of chr20-22, for validation. My questions are:; 1. Is it necessary to shuffle the training data? I ask as it's proving to be a bit laborious to set up, and so I'm hoping that I can get around it. Given I have so many shards, if I just shuffle the order of the chr2-19 shards when I supply them to the training loop, will this be almost as good as shuffling the whole dataset?; 2. Is it necessary to shuffle the validation data? The tutorial does this, but I'm not sure why.; 3. How can I supply multiple datasets to the training loop (here effectively 12 datasets: 6 samples x 2 downsampling settings)? In the tutorial, `model_train` is supplied a wildcard path of `validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz`, which seems like it would only work for a single sample, and I'm not sure how this will work with multiple samples.; 4. Have there been any changes to the code base to better support warmstarting, or is the advice at https://github.com/google/deepvariant/issues/185 still the best approach to fine-tuning the model?. DeepVariant is a fantastic tool and I'm very much looking forward to seeing what it can do with these data, so many thanks in advance.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/312:1293,validat,validation,1293,,https://github.com/google/deepvariant/issues/312,1,['validat'],['validation']
Security,"es_xq721o6r/runfiles/com_google_deepvariant/deepvariant/vcf_stats_vis.py"", line 532, in _save_html; html_string = _altair_chart_to_html(; File ""/tmp/Bazel.runfiles_xq721o6r/runfiles/com_google_deepvariant/deepvariant/vcf_stats_vis.py"", line 513, in _altair_chart_to_html; altair_chart.save(; File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 476, in save; result = save(**kwds); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/save.py"", line 79, in save; spec = chart.to_dict(); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 373, in to_dict; dct = super(TopLevelMixin, copy).to_dict(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 325, in to_dict; result = _todict(; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 60, in _todict; return {; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 61, in <dictcomp>; k: _todict(v, validate, context); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in _todict; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in <listcomp>; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 56, in _todict; return obj.to_dict(validate=validate, context=context); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 373, in to_dict; dct = super(TopLevelMixin, copy).to_dict(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 325, in to_dict; result = _todict(; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 60, in _todict; return {; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 61, in <dictcomp>; k: _todict(v, validate, context); File ""/usr/local/lib/python3.8/dist-pac",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/839:3909,validat,validate,3909,,https://github.com/google/deepvariant/issues/839,1,['validat'],['validate']
Security,"hecked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: CentOS Linux release 7.9.2009; - DeepVariant version: deepvariant:0.9.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); - Illumina, HG38, standard capture panel. **Steps to reproduce:**; - Command: Snakemake command:; - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12; - actual command (XXXXX = removed for security purposes) ; - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/550:1056,secur,security,1056,,https://github.com/google/deepvariant/issues/550,1,['secur'],['security']
Security,"i.py"", line 58, in _todict; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in <listcomp>; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 56, in _todict; return obj.to_dict(validate=validate, context=context); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 373, in to_dict; dct = super(TopLevelMixin, copy).to_dict(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 325, in to_dict; result = _todict(; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 60, in _todict; return {; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 61, in <dictcomp>; k: _todict(v, validate, context); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in _todict; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in <listcomp>; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 56, in _todict; return obj.to_dict(validate=validate, context=context); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 84, in _prepare_data; data = _pipe(data, data_transformers.get()); File ""/usr/local/lib/python3.8/dist-packages/toolz/functoolz.py"", line 628, in pipe; data = func(data); File ""/usr/local/lib/python3.8/dist-packages/toolz/functoolz.py"", line 304, in __call__; return self._partial(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/data.py"", line 19, in default_data_transformer; return curried.pipe(data, limit_rows(max_rows=max_rows), to_values); File ""/usr/local/lib/python3.8/dist-packages/toolz/functoolz.py"", line 628, in pipe; data = func(data); File ""/usr/local/l",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/839:4985,validat,validate,4985,,https://github.com/google/deepvariant/issues/839,1,['validat'],['validate']
Security,"k: . **Setup**; - Operating system: Linux HPC; - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WES. **Steps to reproduce:**; ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p c_compute_wgp; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=02:00:00; #SBATCH --time=072:00:00; #SBATCH --mem-per-cpu=32GB. module purge; module load singularity; module load parallel. set -eu. cd /scratch/c.c21087028/; BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \; --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/deepvariant:1.3.0: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp 52.0.218.102:443: connect: network is unreachable``. This may be an error on my behalf, but I have tried all other options and asked lots of different people. Thanks,; Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/522:1971,checksum,checksum,1971,,https://github.com/google/deepvariant/issues/522,1,['checksum'],['checksum']
Security,"ld_restore=True); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1333, in _build; build_save=build_save, build_restore=build_restore); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 775, in _build_internal; restore_sequentially, reshape); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 453, in _AddShardedRestoreOps; name=""restore_shard"")); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 422, in _AddRestoreOps; assign_ops.append(saveable.restore(saveable_tensors, shapes)); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 113, in restore; self.op.get_shape().is_fully_defined()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/state_ops.py"", line 219, in assign; validate_shape=validate_shape); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_state_ops.py"", line 60, in assign; use_locking=use_locking, name=name); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py"", line 787, in _apply_op_helper; op_def=op_def); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 3414, in create_op; op_def=op_def); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py"", line 1740, in __init__; self._traceback = self._graph._extract_stack() # pylint: disable=protected-access. InvalidArgumentError (see above for traceback): Assign requires shapes of both tensors to match. lhs shape= [3,3,6,32] rhs shape= [3,3,7,32]; 	 [[Node: save_1/Assign_3 = Assign[T=DT_FLOAT, _class=[""loc:@InceptionV3/Conv2d_1a_3x3/weights""], use_locking=true, validate_shape=true, _device=""/job:localhost/replica:0/task:0/device:CPU:0""](InceptionV3/Conv2d_1a_3x3/weights, save_1/RestoreV2:3)]]. Could you please check and let us know about possible reason of this error ?. Thanks; Najeeb",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/117:9563,access,access,9563,,https://github.com/google/deepvariant/issues/117,1,['access'],['access']
Security,"ltair/utils/save.py"", line 79, in save; spec = chart.to_dict(); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 373, in to_dict; dct = super(TopLevelMixin, copy).to_dict(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 325, in to_dict; result = _todict(; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 60, in _todict; return {; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 61, in <dictcomp>; k: _todict(v, validate, context); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in _todict; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in <listcomp>; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 56, in _todict; return obj.to_dict(validate=validate, context=context); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 373, in to_dict; dct = super(TopLevelMixin, copy).to_dict(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 325, in to_dict; result = _todict(; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 60, in _todict; return {; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 61, in <dictcomp>; k: _todict(v, validate, context); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in _todict; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in <listcomp>; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 56, in _todict; return obj.to_dict(validate=validate, context=context); File ""/usr/local/lib/python3.8/dist-packages/alt",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/839:4336,validat,validate,4336,,https://github.com/google/deepvariant/issues/839,2,['validat'],['validate']
Security,"me in a Bottle samples. To do this, I sequenced the same reference sample three times to use each BAM file for training, validation, and testing. I didn't encounter any errors during the model creation process, but when I tried to test it, the process got stuck at the call_variants step. **Setup**; - Operating system: Ubuntu 22.04.4 LTS; - DeepVariant version:1.6.1; - Installation method:docker; - Type of data: MGI DNBSEQ 400, exome sequencing. **Steps to reproduce:**; - Command:; _Create examples for trainning set_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" make_examples --mode training --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/26_r_groups.bam"" --examples ""/output/training_set.gz"" --truth_variants ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed""`; _Create examples for validation set_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" make_examples --mode training --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/27_r_groups.bam"" --examples ""/output/validation_set.gz"" --truth_variants ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed"" `; _Trainning Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/training_set.gz --output_pattern_prefix=""output/training_shuffled"" --output_dataset_name=""26"" --output_dataset_config_pbtxt=""output/training.pbtxt"" --job_name=shuffle-tfrecords`; _Validation Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/validation_set.gz --output_pattern_prefix=""output/validation_shuffled"" --output_dataset_name=""27"" --output_dataset_config_pbtxt=""output/validation.pbtxt"" --job_name=shuffle-tfrecords `; _Model trainning_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:1217,validat,validation,1217,,https://github.com/google/deepvariant/issues/869,1,['validat'],['validation']
Security,"nd #416, maybe I should just move on :wink:). The error during call_variants is below (and similar to #432); ```; File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 416, in call_variants; ie_estimator = OpenVINOEstimator(; File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 90, in __init__; freeze_graph(model, checkpoint_path, tensor_shape, openvino_model_pb); File ""/tmp/Bazel.runfiles_ya87e4x1/runfiles/com_google_deepvariant/deepvariant/openvino_estimator.py"", line 77, in freeze_graph; graph_def = optimize_for_inference_lib.optimize_for_inference(; NameError: name 'optimize_for_inference_lib' is not defined; ```. Which comes from [here](https://github.com/google/deepvariant/blob/d2a3aca8691318221e794594ea08e7c88e21359b/deepvariant/openvino_estimator.py#L42). However, after playing around inside the image, the line `from tensorflow.python.tools import optimize_for_inference_lib` works fine as I can successfully run; ```; python -c 'from tensorflow.python.tools import optimize_for_inference_lib'; ```. The real issue is openvino is not installed ; ```; python -c 'from openvino.runtime import Core, AsyncInferQueue, Type'; Traceback (most recent call last):; File ""<string>"", line 1, in <module>; ModuleNotFoundError: No module named 'openvino'; ```. which triggers the ImportError and pass statement skipping the import of optimize_for_inference_lib. Not to expose my limited understanding of dockerfiles, but [here](https://hub.docker.com/layers/deepvariant/google/deepvariant/latest/images/sha256-83ce0d6bbe3695bcbaa348b73c48737bdbfaeaea2272b0105dd4bdfa7a804f18?context=explore) it appears that the current latest build has ` ENV DV_OPENVINO_BUILD=0`. I've seen a lot of back and forth with openvino no longer being as helpful, but then there has been some recent updates, so not sure if it is still recommended or deprecated as it has disappeared from some docs. Best,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/541:1605,expose,expose,1605,,https://github.com/google/deepvariant/issues/541,1,['expose'],['expose']
Security,"nsorflow/python/training/monitored_session.py"", line 643, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1107, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 1112, in _create_session; return self._sess_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 800, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 557, in create_session; self._scaffold.finalize(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py"", line 213, in finalize; self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access; File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 878, in _get_saver_or_default; saver = Saver(sharded=True, allow_empty=True); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1094, in __init__; self.build(); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1106, in build; self._build(self._filename, build_save=True, build_restore=True); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1143, in _build; build_save=build_save, build_restore=build_restore); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 781, in _build_internal; restore_sequentially, reshape); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 459, in _AddShardedRestoreOps; name=""restore_shard"")); File ""/root/.local/lib/python2.7/site-packages/tensorflow/pyth",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:10000,access,access,10000,,https://github.com/google/deepvariant/issues/166,1,['access'],['access']
Security,"or/packaging/version.py"", line 193, in Version; _regex = re.compile(; File ""/usr/lib/python3.8/re.py"", line 252, in compile; return _compile(pattern, flags); File ""/usr/lib/python3.8/re.py"", line 304, in _compile; p = sre_compile.compile(pattern, flags); File ""/usr/lib/python3.8/sre_compile.py"", line 768, in compile; code = _code(p, flags); File ""/usr/lib/python3.8/sre_compile.py"", line 607, in _code; _compile(code, p.data, flags); File ""/usr/lib/python3.8/sre_compile.py"", line 156, in _compile; _compile(code, av[2], flags); File ""/usr/lib/python3.8/sre_compile.py"", line 168, in _compile; _compile(code, p, _combine_flags(flags, add_flags, del_flags)); File ""/usr/lib/python3.8/sre_compile.py"", line 148, in _compile; _compile(code, av[2], flags); File ""/usr/lib/python3.8/sre_compile.py"", line 120, in _compile; charset, hascased = _optimize_charset(av, iscased, tolower, fixes); File ""/usr/lib/python3.8/sre_compile.py"", line 389, in _optimize_charset; charmap = bytes(charmap) # should be hashable; MemoryError; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_3qjgcvn2/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_3qjgcvn2/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 43, in <module>; import numpy as np; File ""/usr/local/lib/python3.8/dist-packages/numpy/__init__.py"", line 151, in <module>; from . import polynomial; File ""/usr/local/lib/python3.8/dist-packages/numpy/polynomial/__init__.py"", line 120, in <module>; from .hermite_e import HermiteE; File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load; File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked; File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked; File ""<frozen importlib._bootstrap_external>"", line 844, in exec_module; File ""<frozen importlib._bootstrap_external>"", line 939, in get_code; File ""<frozen importlib._bootstrap",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/854:7047,hash,hashable,7047,,https://github.com/google/deepvariant/issues/854,1,['hash'],['hashable']
Security,"or/packaging/version.py"", line 193, in Version; _regex = re.compile(; File ""/usr/lib/python3.8/re.py"", line 252, in compile; return _compile(pattern, flags); File ""/usr/lib/python3.8/re.py"", line 304, in _compile; p = sre_compile.compile(pattern, flags); File ""/usr/lib/python3.8/sre_compile.py"", line 768, in compile; code = _code(p, flags); File ""/usr/lib/python3.8/sre_compile.py"", line 607, in _code; _compile(code, p.data, flags); File ""/usr/lib/python3.8/sre_compile.py"", line 156, in _compile; _compile(code, av[2], flags); File ""/usr/lib/python3.8/sre_compile.py"", line 168, in _compile; _compile(code, p, _combine_flags(flags, add_flags, del_flags)); File ""/usr/lib/python3.8/sre_compile.py"", line 148, in _compile; _compile(code, av[2], flags); File ""/usr/lib/python3.8/sre_compile.py"", line 120, in _compile; charset, hascased = _optimize_charset(av, iscased, tolower, fixes); File ""/usr/lib/python3.8/sre_compile.py"", line 389, in _optimize_charset; charmap = bytes(charmap) # should be hashable; MemoryError; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_wycra2sl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_wycra2sl/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 43, in <module>; import numpy as np; File ""/usr/local/lib/python3.8/dist-packages/numpy/__init__.py"", line 141, in <module>; File ""/usr/local/lib/python3.8/dist-packages/numpy/core/__init__.py"", line 57, in <module>; File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load; File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked; File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked; File ""<frozen importlib._bootstrap_external>"", line 844, in exec_module; File ""<frozen importlib._bootstrap_external>"", line 939, in get_code; File ""<frozen importlib._bootstrap_external>"", line 1037, in get_data; MemoryError; parallel: This j",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/854:10418,hash,hashable,10418,,https://github.com/google/deepvariant/issues/854,1,['hash'],['hashable']
Security,"orflow/third_party/tf_runtime/workspace.bzl:12:20: in repo; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:113:21: in tf_http_archive; #16 1497.0 Repository rule _tf_http_archive defined at:; #16 1497.0 /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl:66:35: in <toplevel>; #16 1497.0 (21:51:08) WARNING: Download from http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException GET returned 404 Not Found; #16 1497.0 (21:51:08) WARNING: Download from https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz failed: class com.google.devtools.build.lib.bazel.repository.downloader.UnrecoverableHttpException Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be9389f50652b4b4a14c5a8a96e7e15ad73e8f234d8d09ebe; #16 1497.0 (21:51:08) ERROR: An error occurred during the fetch of repository 'tf_runtime':; #16 1497.0 Traceback (most recent call last):; #16 1497.0 File ""/root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/org_tensorflow/third_party/repo.bzl"", line 53, column 33, in _tf_http_archive_impl; #16 1497.0 ctx.download_and_extract(; #16 1497.0 Error in download_and_extract: java.io.IOException: Error downloading [http://mirror.tensorflow.org/github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz, https://github.com/tensorflow/runtime/archive/64c92c8013b557087351c91b5423b6046d10f206.tar.gz] to /root/.cache/bazel/_bazel_root/617054f44dc1f1e9b3fe3174f8eb2580/external/tf_runtime/temp12516918929418979294/64c92c8013b557087351c91b5423b6046d10f206.tar.gz: Checksum was 8383b3247286016e450b0b20e805d26b88ab4638b4e4e3cc4a6923debaf7ad1e but wanted f16fcf09b34e0c7be93",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:6489,Checksum,Checksum,6489,,https://github.com/google/deepvariant/issues/608,1,['Checksum'],['Checksum']
Security,r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.o,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:4827,secur,security,4827,,https://github.com/google/deepvariant/issues/489,1,['secur'],['security']
Security,rchive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lis,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:4861,secur,security,4861,,https://github.com/google/deepvariant/issues/489,2,['secur'],['security']
Security,"run_setup; exec(compile(code, __file__, 'exec'), locals()); File ""setup.py"", line 499, in <module>; setup_package(); File ""setup.py"", line 479, in setup_package; generate_cython(); File ""setup.py"", line 274, in generate_cython; raise RuntimeError(""Running cythonize failed!""); RuntimeError: Running cythonize failed!; [end of output]; ; note: This error originates from a subprocess, and is likely not a problem with pip.; error: metadata-generation-failed. × Encountered error while generating package metadata.; ╰─> See above for output. note: This is an issue with the package mentioned above, not pip.; hint: See above for details.; /*********************************************/; My conda environment contains the following libraries:; conda list; # packages in environment at /opt/miniconda3/envs/deepvariant:; #; # Name Version Build Channel; _libgcc_mutex 0.1 main ; _openmp_mutex 5.1 1_gnu ; absl-py 2.1.0 pypi_0 pypi; argparse 1.4.0 pypi_0 pypi; blas 1.0 mkl ; bzip2 1.0.8 h5eee18b_6 ; ca-certificates 2024.7.2 h06a4308_0 ; chex 0.1.86 pypi_0 pypi; clu 0.0.9 pypi_0 pypi; contextlib2 21.6.0 pypi_0 pypi; cython 3.0.10 pypi_0 pypi; enum34 1.1.8 pypi_0 pypi; etils 1.7.0 pypi_0 pypi; flax 0.8.5 pypi_0 pypi; fsspec 2024.6.1 pypi_0 pypi; importlib-resources 6.4.0 pypi_0 pypi; intel-openmp 2023.1.0 hdb19cb5_46306 ; intervaltree 3.0.2 pypi_0 pypi; jax 0.4.31 pypi_0 pypi; jaxlib 0.4.31 pypi_0 pypi; ld_impl_linux-64 2.38 h1181459_1 ; libffi 3.4.4 h6a678d5_1 ; libgcc-ng 11.2.0 h1234567_1 ; libgomp 11.2.0 h1234567_1 ; libstdcxx-ng 11.2.0 h1234567_1 ; libuuid 1.41.5 h5eee18b_0 ; markdown-it-py 3.0.0 pypi_0 pypi; mdurl 0.1.2 pypi_0 pypi; mkl 2023.1.0 h213fc3f_46344 ; mkl-service 2.4.0 py310h5eee18b_1 ; mkl_fft 1.3.8 py310h5eee18b_0 ; mkl_random 1.2.4 py310hdb19cb5_0 ; ml-collections 0.1.1 pypi_0 pypi; ml-dtypes 0.4.0 pypi_0 pypi; mock 5.1.0 pypi_0 pypi; msgpack 1.0.8 pypi_0 pypi; ncurses 6.4 h6a678d5_0 ; nest-asyncio 1.6.0 pypi_0 pypi; numpy 1.24.3 py310h5f9d8c6_1 ; numpy-base 1.24.3 p",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:11884,certificate,certificates,11884,,https://github.com/google/deepvariant/issues/859,1,['certificate'],['certificates']
Security,"s latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. ; First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, ; Haley . Here is the error traceback: ; `Traceback (most recent call last):; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>; from . import multiarray; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>; from . import overrides; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>; from numpy.core._multiarray_umath import (; ModuleN",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793:1559,validat,validation,1559,,https://github.com/google/deepvariant/issues/793,2,['validat'],['validation']
Security,"save(; File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 476, in save; result = save(**kwds); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/save.py"", line 79, in save; spec = chart.to_dict(); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 373, in to_dict; dct = super(TopLevelMixin, copy).to_dict(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 325, in to_dict; result = _todict(; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 60, in _todict; return {; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 61, in <dictcomp>; k: _todict(v, validate, context); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in _todict; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in <listcomp>; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 56, in _todict; return obj.to_dict(validate=validate, context=context); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 373, in to_dict; dct = super(TopLevelMixin, copy).to_dict(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 325, in to_dict; result = _todict(; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 60, in _todict; return {; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 61, in <dictcomp>; k: _todict(v, validate, context); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in _todict; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in <listcomp>; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/839:4190,validat,validate,4190,,https://github.com/google/deepvariant/issues/839,1,['validat'],['validate']
Security,"singularity run -B /slurm/home/yrd/sunlab/yangfeng/pub/WW/WGS/deepvirant/deepvariant_1.6.1.sif \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$reference \; --reads=$sample_dir2 \; --output_vcf=""${OUTPUT_DIR}""/${NameN}.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/${NameN}.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/${NameN}_intermediate_results_dir"" \; --num_shards=60. FATAL: Unable to handle docker://google/deepvariant:1.6.1 uri: failed to get checksum for docker://google/deepvariant:1.6.1: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp: lookup registry-1.docker.io on 114.114.114.114:53: read udp 172.16.10.108:58057->114.114.114.114:53: i/o timeout; FATAL: Unable to handle docker://google/deepvariant:1.6.1 uri: failed to get checksum for docker://google/deepvariant:1.6.1: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp: lookup registry-1.docker.io on 114.114.114.114:53: read udp 172.16.10.108:57892->114.114.114.114:53: i/o timeout; FATAL: Unable to handle docker://google/deepvariant:1.6.1 uri: failed to get checksum for docker://google/deepvariant:1.6.1: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp: lookup registry-1.docker.io on 114.114.114.114:53: read udp 172.16.10.108:49924->114.114.114.114:53: i/o timeout; FATAL: Unable to handle docker://google/deepvariant:1.6.1 uri: failed to get checksum for docker://google/deepvariant:1.6.1: pinging container registry registry-1.docker.io: Get ""https://registry-1.docker.io/v2/"": dial tcp: lookup registry-1.docker.io on 114.114.114.114:53: read udp 172.16.10.108:58178->114.114.114.114:53: i/o timeout",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/831:523,checksum,checksum,523,,https://github.com/google/deepvariant/issues/831,4,['checksum'],['checksum']
Security,"t install clang-11. root@4f3323c7fe90:/# wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | apt-key add - && \; > add-apt-repository ""deb http://apt.llvm.org/$(lsb_release -sc)/ llvm-toolchain-$(lsb_release -sc)-11 main""; --2021-10-11 18:34:18-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)...; 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. - 100%[====================================================================================================================================================================================================>] 3.07K --.-KB/s in 0s. 2021-10-11 18:34:23 (51.5 MB/s) - written to stdout [3145/3145]. OK; Get:2 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease [20.8 kB]; Get:3 http://archive.ubuntu.com/ubuntu bionic InRelease [242 kB]; Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]; Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]; Get:6 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic/main amd64 Packages [19.3 kB]; Get:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]; Get:8 http://archive.ubuntu.com/ubuntu bionic/main amd64 Packages [1344 kB]; Get:9 http://archive.ubuntu.com/ubuntu bionic/multiverse amd64 Packages [186 kB]; Get:10 http://archive.ubuntu.com/ubuntu bionic/universe amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:3643,secur,security,3643,,https://github.com/google/deepvariant/issues/489,1,['secur'],['security']
Security,"t problems, you have held broken packages.; ```. After that, I've tried to build your docker image - same error:; ```bash; + wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key; + apt-key add -; --2021-10-11 15:29:09-- https://apt.llvm.org/llvm-snapshot.gpg.key; Resolving apt.llvm.org (apt.llvm.org)... Warning: apt-key output should not be parsed (stdout is not a terminal); 151.101.114.49, 2a04:4e42:1b::561; Connecting to apt.llvm.org (apt.llvm.org)|151.101.114.49|:443... connected.; HTTP request sent, awaiting response... 200 OK; Length: 3145 (3.1K) [application/octet-stream]; Saving to: 'STDOUT'. 0K ... 100% 37.6M=0s. 2021-10-11 15:29:12 (37.6 MB/s) - written to stdout [3145/3145]. OK; ++ lsb_release -sc; ++ lsb_release -sc; + add-apt-repository 'deb http://apt.llvm.org/focal/ llvm-toolchain-focal-11 main'; Hit:2 http://archive.ubuntu.com/ubuntu focal InRelease; Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease; Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease; Hit:5 http://security.ubuntu.com/ubuntu focal-security InRelease; Get:1 https://apt.llvm.org/focal llvm-toolchain-focal-11 InRelease [5526 B]; Get:6 https://apt.llvm.org/focal llvm-toolchain-focal-11/main amd64 Packages [9008 B]; Fetched 14.5 kB in 13s (1133 B/s); Reading package lists...; + apt-get update -qq -y; + apt-get install -qq -y clang-11 libclang-11-dev libgoogle-glog-dev libgtest-dev libllvm11 llvm-11-dev python3-dev python3-pyparsing zlib1g-dev; E: Unable to correct problems, you have held broken packages. real 0m54.858s; user 0m12.058s; sys 0m4.272s; The command '/bin/sh -c ./build-prereq.sh && PATH=""${HOME}/bin:${PATH}"" ./build_release_binaries.sh # PATH for bazel' returned a non-zero code: 100. ```. According to this link: https://apt.llvm.org/ only 12 and 13 version are mensioned.; ```; Bionic LTS (18.04) - Last update : Mon, 11 Oct 2021 13:24:17 UTC / Revision: 20211011091508+7ae8f392a161; # i386 not available; deb http://apt.llvm.org/bionic/ llvm-toolchain-bio",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:8330,secur,security,8330,,https://github.com/google/deepvariant/issues/489,1,['secur'],['security']
Security,"tart to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant.; I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right?. EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/716:1707,access,access,1707,,https://github.com/google/deepvariant/issues/716,1,['access'],['access']
Security,"tensorflow/python/training/monitored_session.py"", line 549, in __init__; self._sess = _RecoverableSession(self._coordinated_creator); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1012, in __init__; _WrappedSession.__init__(self, self._create_session()); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 1017, in _create_session; return self._sess_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 706, in create_session; self.tf_sess = self._session_creator.create_session(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 468, in create_session; self._scaffold.finalize(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/monitored_session.py"", line 212, in finalize; self._saver = training_saver._get_saver_or_default() # pylint: disable=protected-access; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 856, in _get_saver_or_default; saver = Saver(sharded=True, allow_empty=True); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1284, in __init__; self.build(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1296, in build; self._build(self._filename, build_save=True, build_restore=True); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 1333, in _build; build_save=build_save, build_restore=build_restore); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 775, in _build_internal; restore_sequentially, reshape); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"", line 453, in _AddShardedRestoreOps; name=""restore_shard"")); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/117:7609,access,access,7609,,https://github.com/google/deepvariant/issues/117,1,['access'],['access']
Security,"todict; return obj.to_dict(validate=validate, context=context); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 373, in to_dict; dct = super(TopLevelMixin, copy).to_dict(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 325, in to_dict; result = _todict(; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 60, in _todict; return {; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 61, in <dictcomp>; k: _todict(v, validate, context); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in _todict; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 58, in <listcomp>; return [_todict(v, validate, context) for v in obj]; File ""/usr/local/lib/python3.8/dist-packages/altair/utils/schemapi.py"", line 56, in _todict; return obj.to_dict(validate=validate, context=context); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/v4/api.py"", line 84, in _prepare_data; data = _pipe(data, data_transformers.get()); File ""/usr/local/lib/python3.8/dist-packages/toolz/functoolz.py"", line 628, in pipe; data = func(data); File ""/usr/local/lib/python3.8/dist-packages/toolz/functoolz.py"", line 304, in __call__; return self._partial(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/altair/vegalite/data.py"", line 19, in default_data_transformer; return curried.pipe(data, limit_rows(max_rows=max_rows), to_values); File ""/usr/local/lib/python3.8/dist-packages/toolz/functoolz.py"", line 628, in pipe; data = func(data); File ""/usr/local/lib/python3.8/dist-packages/toolz/functoolz.py"", line 304, in __call__; return self._partial(*args, **kwargs); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/data.py"", line 149, in to_values; data = sanitize_dataframe(data); File ""/usr/local/lib/python3.8/dist-packages/altair/utils/core.py"", line 283, in",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/839:5280,validat,validate,5280,,https://github.com/google/deepvariant/issues/839,2,['validat'],['validate']
Security,"udy.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. ; First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, ; Haley . Here is the error traceback: ; `Traceback (most recent call last):; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>; from . import multiarray; File ""/project/pbarc/haley.ar",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793:1301,validat,validation,1301,,https://github.com/google/deepvariant/issues/793,1,['validat'],['validation']
Security,"ver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def; producer_op_list=producer_op_list); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 501, in _import_graph_def_internal; graph._c_graph, serialized, options) # pylint: disable=protected-access; tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed`. **Does the quick start test work on your system?** ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Quick start works on my system -- I can perform make_examples, call_variants and post_processing. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`). Code snippet :; `import tensorflow as tf. meta_path = '/opt/models/wgs/model.ckpt.meta'. ckpt_folder = '/opt/models/wgs'. with tf.compat.v1.Session() as sess:. saver = tf.compat.v1.train.import_meta_graph(meta_path). print(""\n**Import Sucessful\n**""). saver.restore(sess,tf.compat.v1.train.latest_checkpoint(ckpt_folder)); `",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/339:2649,access,accessing,2649,,https://github.com/google/deepvariant/issues/339,2,['access'],"['accessed', 'accessing']"
Security,verse amd64 Packages [11.3 MB]; Get:11 http://archive.ubuntu.com/ubuntu bionic/restricted amd64 Packages [13.5 kB]; Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [34.4 kB]; Get:13 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [638 kB]; Get:14 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2210 kB]; Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2801 kB]; Get:16 http://archive.ubuntu.com/ubuntu bionic-backports/main amd64 Packages [11.3 kB]; Get:17 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [11.4 kB]; Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [606 kB]; Get:19 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [26.7 kB]; Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2365 kB]; Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1431 kB]; Get:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease [5527 B]; Get:22 https://apt.llvm.org/bionic llvm-toolchain-bionic-11/main amd64 Packages [8985 B]; Fetched 23.6 MB in 10s (2248 kB/s); Reading package lists... Done; root@4f3323c7fe90:/#; root@4f3323c7fe90:/# apt update; Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease; Hit:3 http://ppa.launchpad.net/openjdk-r/ppa/ubuntu bionic InRelease; Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease; Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease; Hit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease; Hit:1 https://apt.llvm.org/bionic llvm-toolchain-bionic-11 InRelease; Reading package lists... Done; Building dependency tree; Reading state information... Done; 53 packages can be upgraded. Run 'apt list --upgradable' to see them.; root@4f3323c7fe90:/# apt install clang-11; Reading package lists... Done; Building dependency tree; Reading state information...,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/489:5136,secur,security,5136,,https://github.com/google/deepvariant/issues/489,1,['secur'],['security']
Testability," ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error?. The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python; def test_make_labeler_ref(self, candidates, truths, expected_start,; expected_end, bufsize):; expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader; labeler = _make_labeler(); labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(; ranges.make_range('20', expected_start, expected_end)); self.assertEqual(labeler_ref.start, expected_start); self.assertEqual(labeler_ref.end, expected_end); self.assertEqual(; labeler_ref.bases(expected_start, expected_end), expected_bases); ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python; def make_labeler_ref(self, candidates, true_variants, bufsize=20):; all_variants = candidates + true_variants; contig = all_variants[0].reference_name; start = min(x.start for x in all_variants); end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object; ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]; ## change the above type to int becomes ""1"", then the region.end will be 1 to cause test fail; contig_nbp = self._ref_reader.contig(contig).n_bases ; region = ranges.make_range(contig, max(start - 1, 0),; min(end + bufsize, contig_nbp)); ref_bases = self._ref_reader.query(region); return ReferenceRegion(ref_bases, start=region.start); ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154:1219,assert,assertEqual,1219,,https://github.com/google/deepvariant/issues/154,5,"['Mock', 'assert', 'mock', 'test']","['Mock', 'assertEqual', 'mock', 'test']"
Testability," --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/c_elegans.PRJEB28388.WS274.genomic.fa \; --reads=""${INPUT_DIR}""/maddog_bam_trim_bwaMEM_sort_dedupped.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. The error looks like:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:1622,log,logs,1622,,https://github.com/google/deepvariant/issues/292,1,['log'],['logs']
Testability," --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_child.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_child.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent1.tfrecord.gz"" --outfile ""/out_dir/199710.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent1.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent1.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent2.tfrecord.gz"" --outfile ""/out_dir/199718.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent2.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent2.log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.705964 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; real	0m3.173s; user	0m3.003s; sys	0m3.160s; real	0m3.194s; user	0m3.299s; sys	0m4.216s; real	0m3.254s; user	0m3.024s; sys	0m2.808s; post_process returns: [0, 0, 0]; real	2008m37.771s; user	78330m54.158s; sys	730m9.042s; ```. **Does the quick start test work on your system?** Yes.; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes, see below:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:dee",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/429:3248,log,log,3248,,https://github.com/google/deepvariant/issues/429,1,['log'],['log']
Testability," -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as in",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:2527,test,test,2527,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability," -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:2203,test,test,2203,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability," -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring ou",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:2446,test,test,2446,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability," -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:2365,test,test,2365,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability," -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:2284,test,test,2284,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability," -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```; ## Run `call_variants`; echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log.""; ( time sudo docker run \; -v ""${BASE}"":""${BAS",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:2752,log,log,2752,,https://github.com/google/deepvariant/issues/151,1,['log'],['log']
Testability," /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7; ```. I checked the lra bam with samtools view and the base quality scores are there.; I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**; - Operating system: Ubuntu 20.04.4; - DeepVariant version: pepper_deepvariant:r0.8-gpu; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**; - Command: ; ```; 	docker run --ipc=host \; 	--gpus all \; 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; 	-v ""${BASE}"":""${BASE}"" \; 	-v ""${REF}"":""${REF}"" \; 	-v ""${BAMPATH}"":""${BAMPATH}"" \; 	kishwars/pepper_deepvariant:r0.8-gpu \; 	run_pepper_margin_deepvariant call_variant \; 	-o ""${OUTPUT_DIR}"" \; 	-b ""${BAM}"" \; 	-f ""${REF}"" \; 	-p ""${OUTPUT_PREFIX}"" \; 	-t ${THREADS} \; 	-g \; 	--ont_r9_guppy5_sup; ```. - Error trace: (if applicable); ; [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? no. **Any additional context:** Ultra-long reads",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/539:2716,log,log,2716,,https://github.com/google/deepvariant/issues/539,4,"['log', 'test']","['log', 'test']"
Testability," /opt/deepvariant/bin/make_examples --mode calling --ref /media/euphrasie/DATA/reference_genome/hg38/hg38_GenDev.fa --reads /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PHASED.PEPPER_MARGIN.haplotagged.bam --examples /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/dv_intermediate_outputs/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup none --min_base_quality 1 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 10000 --proposed_variants /media/euphrasie/Alienware_May202/HG002_PAG07506/pmdv/HG002_PAG07506_38_lra/output/intermediate_files/PEPPER_VARIANT_OUTPUT_VARIANT_CALLING_SNPs.vcf.gz --norealign_reads --sample_name Sample --sort_by_haplotypes --variant_caller vcf_candidate_importer --task 7; ```. I checked the lra bam with samtools view and the base quality scores are there.; I wonder what is wrong with my lra aligned reads. The full `5.1_DeepVariant_SNP.log` is attached. **Setup**; - Operating system: Ubuntu 20.04.4; - DeepVariant version: pepper_deepvariant:r0.8-gpu; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**; - Command: ; ```; 	docker run --ipc=host \; 	--gpus all \; 	-v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; 	-v ""${BASE}"":""${BASE}"" \; 	-v ""${REF}"":""${REF}"" \; 	-v ""${BAMPATH}"":""${BAMPATH}"" \; 	kishwars/pepper_deepvariant:r0.8-gpu \; 	run_pepper_margin_deepvariant call_variant \; 	-o ""${OUTPUT_DIR}"" \; 	-b ""${BAM}"" \; 	-f ""${REF}"" \; 	-p ""${OUTPUT_PREFIX}"" \; 	-t ${THREADS} \; 	-g \; 	--ont_r9_guppy5_sup; ```. - Error trace: (if applicable); ; [5.1_DeepVariant_SNP.log](https://github.com/google/deepvariant/files/8785347/5.1_DeepVariant_SNP.log). **Does the quick start test work on your system?** yes ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-sta",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/539:1939,log,log,1939,,https://github.com/google/deepvariant/issues/539,1,['log'],['log']
Testability," /opt/deepvariant/bin/run_deepvariant --model_type=WGS --customized_model=/opt/dv_models/ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log; -------; STARTING DEEPVARIANT; I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/; I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****; ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_outpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:22589,log,logs,22589,,https://github.com/google/deepvariant/issues/491,1,['log'],['logs']
Testability," 2020-09-24 03:47:45.654628: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call; return fn(*args); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn; target_list, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun; run_metadata); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Ba",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:12707,log,log,12707,,https://github.com/google/deepvariant/issues/358,1,['log'],['log']
Testability," 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |; | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**; - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0; - DeepVariant version:deeptrio-1.4.0; - Installation method (Docker, built from source, etc.):Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); HiFi data,those data download links follows:; * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/; * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb; * HG004:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/Google_15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG004/PacBio_HiFi/HudsonAlpha_15kb/PBmixSequel733_2_B01_PBSU_30hours_15kbV2PD_70pM_HumanHG004_CCS/; **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689:7698,test,test,7698,,https://github.com/google/deepvariant/issues/689,2,['test'],['test']
Testability," CALLING MODULE SELECTED; [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND; -------; mkdir -p /cromwell_root/pepper_output; ; mkdir -p /cromwell_root/pepper_output/logs; ; mkdir -p /cromwell_root/pepper_output/intermediate_files;; -------; [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND; -------; time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log; -------; [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED.; [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING.; [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041; [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/; [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES; [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']; [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895; [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS; [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]; ...; [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]; [11-03-2021 13:42:49] INFO: 480/483 COMPLETE (99%) [ELAPSED TIME: 2 Min 8 Sec]; [11-03-2021 13:42:49] THREAD 0 FINISHED SUCCESSFUL",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:2530,log,log,2530,,https://github.com/google/deepvariant/issues/491,1,['log'],['log']
Testability," Docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) quick start data . **Steps to reproduce:**; - Command: sudo docker run --platform linux/amd64 google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/quickstart-output/output.vcf.gz --output_gvcf=/quickstart-output/output.g.vcf.gz --intermediate_results_dir /quickstart-output/intermediate_results_dir --num_shards=1; - Error trace: (if applicable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/471:1298,test,testdata,1298,,https://github.com/google/deepvariant/issues/471,1,['test'],['testdata']
Testability," File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next; not_done = self._cc_iterable.PythonNext(record); RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0; ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```; singularity exec DeepVariant_1.6.1.sif bash; pip install --user google-nucleus; run_deepvariant --model_type=WGS \; 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; 	--regions ""chr20:10,000,000-10,010,000"" \; 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; 	--num_shards=12; ```. Error:. ```; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 49, in <module>; import tensorflow as tf; File ""/usr/local/li",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812:4229,test,test,4229,,https://github.com/google/deepvariant/issues/812,1,['test'],['test']
Testability," File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 141, in _raw_next; not_done = self._cc_iterable.PythonNext(record); ValueError: Data loss: Failed to parse SAM record. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2246, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2236, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2106, in make_examples_runner; candidates, examples, gvcfs, runtimes = region_processor.process(region); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1540, in process; reads = self.region_reads(region); File ""/tmp/Bazel.runfiles_y3fqbfu4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1616, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5.; If you cannot find out the reason why this error is occurring, please report to https://github.com/google/deepvariant/issues; root@a8d04f73bc21:/opt/deepvariant/bin# exit. i also test version 1.0.0, it also return same exception",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/455:9583,test,test,9583,,https://github.com/google/deepvariant/issues/455,1,['test'],['test']
Testability," OUTPUT_DIR=./output/$sID. mkdir -p $OUTPUT_DIR; mkdir -p ./tmp; export TMPDIR=`realpath ./tmp`. if [ ! -f $sBAM.bai ]; then; echo producing bai index for $sBAM; samtools index $sBAM; fi. if [ ! -f ""${OUTPUT_DIR}/dv.log"" ];then; bedtools coverage -g genome.file -sorted -d -a genome.bed -b ""$sBAM"" | awk '{if ($5>=3) print $1""\t""($4-1)""\t""$4""\t""$5}' | bedtools merge -d 1 -c 4 -o mean -i - > ${OUTPUT_DIR}/cov3x.bed; fi. apptainer run -B /public:/public,/public3:/public3,/public2:/public2,/fast3:/fast3,/public4:/public4 \; /public4/software/deepvariant/1.6.1/cpuver/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --make_examples_extra_args=""normalize_reads=true"" \; --model_type=WES \; --ref=$REF \; --reads=""$sBAM"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --regions=""${OUTPUT_DIR}/cov3x.bed"" \; --num_shards=$CPU > ${OUTPUT_DIR}/dv.log 2>&1. `. Inspecting the tail of the log, it appears that the program gets stuck at the make_examples step, with many threads reporting finding 0 examples:; 'I0812 17:25:00.705988 139682501986112 make_examples_core.py:301] Task 14/32: Overhead for preparing inputs: 270 seconds; I0812 17:25:00.763086 139682501986112 make_examples_core.py:301] Task 14/32: 0 candidates (0 examples) [0.06s elapsed]; I0812 17:25:01.273164 139627217889088 genomics_reader.py:222] Reading ../mapped/SRR18493715.RNA-Seq.Camellia_sp._multipetala.leaf/Aligned.sortedByCoord.out.bam with NativeSamReader; I0812 17:25:01.308415 139627217889088 make_examples_core.py:301] Task 18/32: Writing gvcf records to /public4/courses/ec3121/shareddata/Camellia_Sect_Chrysantha/star_hapbetter/deepvariant/tmp/tmp0n4wz07d/gvcf.tfrecord-00018-of-00032.gz; I0812 17:25:01.325705 139627217889088 make_examples_core.py:301] Task 18/32: Writing examples to /public4/courses/ec3121/shareddata/Camellia_Sect_Chrysantha/star_hapbetter/deepvariant/tmp/tmp0n4wz07d/make_examples.tfrecord-00018-of-00032.gz; I0812 17:25:01.326699 139627217",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/867:1681,log,log,1681,,https://github.com/google/deepvariant/issues/867,1,['log'],['log']
Testability," _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 186, in main; make_examples_core.make_examples_runner(options); File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2183, in make_examples_runner; runtimes) = region_processor.process(region); File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1285, in process; sample_reads = self.region_reads_norealign(; File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1376, in region_reads_norealign; reads = itertools.chain(reads, sam_reader.query(region)); File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 247, in query; return self._reader.query(region); File ""/tmp/Bazel.runfiles_ztv_d7ra/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 250, in query; return self._reader.query(region); ValueError: FAILED_PRECONDITION: Cannot query without an index; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /media/nils/nils_ssd_01/Genomics_prac_guide/reference/GRCh37/hs37d5.fa --reads /media/nils/nils_ssd_01/Calling/HiFI_sequencing/data/bam/GFX.bam --examples /tmp/tmpwjk24y8t/make_examples.tfrecord@22.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --noparse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --nosort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 1; ; ```; ; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Quickstart works. This issue also happens when I try to run the pipeline with docker-only.; ; **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/666:6760,test,test,6760,,https://github.com/google/deepvariant/issues/666,2,['test'],['test']
Testability," am trying to run DeepVariant but ...; here is my command; `time seq 0 $((N_SHARDS-1)) |parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python bin/make_examples.zip --mode calling --ref ""${REF}"" --reads ""${BAM}"" --sample_name FalconSet --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" --task {}`. And here is the output. ```; When using programs that use GNU Parallel to process data for publication please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; and it won't cost you a cent.; Or you can get GNU Parallel without this requirement by paying 10000 EUR. To silence this citation notice run 'parallel --bibtex' once or use '--no-notice'. Computers / CPU cores / Max jobs to run; 1:local / 48 / 40. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 40 AVG: 0.00s local:40/0/100%/0.0s WARNING: Logging before flag parsing goes to stderr.; I0601 15:22:01.182291 140355759671040 make_examples.py:1024] Preparing inputs; 2018-06-01 15:22:01.188982: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I0601 15:22:01.189755 140355759671040 genomics_reader.py:174] Reading ../Falcon_Unzip/out.bam with NativeSamReader; I0601 15:22:01.543628 140355759671040 make_examples.py:946] Common contigs are [u'000000F', u'000001F', u'000002F', u'000003F', u'000004F', u'000005F', u'000006F', u'000007F', u'000009F', u'000010F', u'000011F', u'000012F', u'000013F', u'000014F', u'000015F', u'000016F', u'000017F', u'000018F', u'000019F', u'000020F', u'000021F', u'000022F', u'000023F', u'000024F', u'000025F', u'000026F', u'000027F', u'000028F', u'000029F', u'000030F', u'000031F', u'000032F', u'000033F', u'000034F', u'000035F', u'000036F', u'000037F', u'000038F', u'000039F', u'000040F', u'000041F', u'000042F', u'000043F', u'000045F', u'000046F', u'000047F', u'000048F', u'000049F', u'0000",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/77:993,Log,Logging,993,,https://github.com/google/deepvariant/issues/77,1,['Log'],['Logging']
Testability," encountered this error previously and I cannot figure out what is causing the issue. Looks like something to do with the reference file?. user@node1784:~/MyData$ /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=./hg19.fa --reads=./NA12878_S1.bam --output_vcf=./NA12878_DeepVariant_output.vcf.gz --num_shards=1 ; I1027 14:35:49.384760 139774463268608 run_deepvariant.py:273] Re-using the directory for intermediate results in /tmp/tmps6oyff7s. ***** Intermediate results will be written to /tmp/tmps6oyff7s in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""./hg19.fa"" --reads ""./NA12878_S1.bam"" --examples ""/tmp/tmps6oyff7s/make_examples.tfrecord@1.gz"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. I1027 14:35:51.541710 140702132172544 genomics_reader.py:223] Reading ./NA12878_S1.bam with NativeSamReader; I1027 14:35:51.552782 140702132172544 make_examples.py:587] Preparing inputs; I1027 14:35:51.576705 140702132172544 genomics_reader.py:223] Reading ./NA12878_S1.bam with NativeSamReader; I1027 14:35:51.590540 140702132172544 make_examples.py:587] Common contigs are ['chrM', 'chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX', 'chrY']; I1027 14:35:56.576697 140702132172544 make_examples.py:587] Writing examples to /tmp/tmps6oyff7s/make_examples.tfrecord-00000-of-00001.gz; I1027 1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/372:1017,log,login,1017,,https://github.com/google/deepvariant/issues/372,1,['log'],['login']
Testability," extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 24日 星期五 19:54:09 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: numpy==1.14 in /usr/local/lib/python2.7/dist-packages (1.14.0); Requirement already satisfied: requests>=2.18 in /usr/local/lib/python2.7/dist-packages (2.19.1); Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2018.8.13); Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (3.0.4); Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (1.23); Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2.7); Requirement already satisfied: scipy==1.0 in /usr/local/lib/python2.7/dist-packages (1.0.0); Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python2.7/dist-pa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:14147,mock,mock,14147,,https://github.com/google/deepvariant/issues/89,1,['mock'],['mock']
Testability, gvcf files like so:; ```; -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz; ...; -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:1,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1141,test,test,1141,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability," in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:; ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```; rule deepvariant:; input:; bam=rules.apply_bqsr.output.bam,; ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'; output:; vcf=""results/deepvariant/{sample}.vcf.gz""; params:; model=""WES""; threads: ; 64; resources:; mem_mb=163840; log:; ""logs/deepvariant/{sample}/stdout.log""; singularity:; ""singularity/deepvariant_1.4.0.sif""; # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU; shell:; """"""; /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'; """"""; ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? ; I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:1515,log,log,1515,,https://github.com/google/deepvariant/issues/602,2,['log'],"['log', 'logs']"
Testability," most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error?. The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python; def test_make_labeler_ref(self, candidates, truths, expected_start,; expected_end, bufsize):; expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader; labeler = _make_labeler(); labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(; ranges.make_range('20', expected_start, expected_end)); self.assertEqual(labeler_ref.start, expected_start); self.assertEqual(labeler_ref.end, expected_end); self.assertEqual(; labeler_ref.bases(expected_start, expected_end), expected_bases); ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python; def make_labeler_ref(self, candidates, true_variants, bufsize=20):; all_variants = candidates + true_variants; contig = all_variants[0].reference_name; start = min(x.start for x in all_variants); end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object; ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]; ## change the above type to int becomes ""1"", then the region.end will be 1 to cause test fail; contig_nbp = self._ref_reader.contig(contig).n_bases ; region = ranges.make_range(contig, max(start - 1, 0),; min(end + bufsize, contig_nbp)); ref_bases = self._",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154:1117,assert,assertEqual,1117,,https://github.com/google/deepvariant/issues/154,1,['assert'],['assertEqual']
Testability," operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o; neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs; I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']; I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs; I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']; I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Readi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:9242,test,testdata,9242,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability," sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/_0_Диссертация/Exp/seq1/seq1.fa"" --reads ""/home/platon/_0_Диссертация/Exp/seq1/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Second attempt. This time with paths consisting only of latin characters.; `sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/platon/test/seq1.fa --reads=/home/platon/test/seq1.bam --output_vcf=/home/platon/test/seq1.vcf`. ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}. [E::hts_open_format] Failed to open file /home/platon/test/seq1.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/219:3333,test,test,3333,,https://github.com/google/deepvariant/issues/219,1,['test'],['test']
Testability," the Exome dataset (for the provided alignment from Genos). I’ve re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets was also about a day. However, I also noticed that the upload from cloud servers to the precisionFDA was much quicker (so, maybe part of this is my apartment internet connection). To be fair, with the $300 credit, DeepVariant is essentially free to use on Google Cloud. However, if I was evaluating Cloud options in the long term (prior to conducting my own test), I think the examples of $0.20 per Exome (my own was <$1.00, but I think it was more than $0.20) and $2-3 per WGS (compared to $10+) may not be representative for somebody who is thinking about using the Cloud (versus local server + storage) for processing 100s or 1000s of samples (or even more, although I assume that would probably be for more than most individual labs or citizen scientists). I think 24 hour run-time was similar to running GATK on my local computer (with 8 GB of RAM and 4 cores), so I’m not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didn’t match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and I’m not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:2358,test,test,2358,,https://github.com/google/deepvariant/issues/171,1,['test'],['test']
Testability," the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:; ```; #!/bin/bash. #SBATCH --job-name=Deepvariant_debug; #SBATCH --cpus-per-task=2 # change this according to your needs; #SBATCH --mem=8G # change this according to your needs; #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes; #SBATCH --output=myrun.o%j; #SBATCH --error=myrun.e%j. mkdir -p output; mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000; BIN_VERSION=""1.2.0""; # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder; export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output ; export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata ; # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add; # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container; # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif); singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \; /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \; --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \; --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed; --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/515:1024,test,testdata,1024,,https://github.com/google/deepvariant/issues/515,1,['test'],['testdata']
Testability," to reproduce:**; - Command:; ```; singularity run \; -B /usr/lib/locale/:/usr/lib/locale/ \; -B /paedyl01/disk1/louisshe/ref/GIAB/HG005/hs37d5/novoalign_bam/:/input_reads \; -B /paedyl01/disk1/louisshe/out/GIAB/HG005/heterozygous_deletions/heterozygous_sites/:/output \; -B /tmp:/tmp \; -B /paedyl01/disk1/louisshe/ref/hs37d5:/ref/hs37d5 \; -B /paedyl01/disk1/louisshe/ref/hg19:/ref/hg19 \; --home /paedyl01/disk1/louisshe/ref/GIAB/HG005/hs37d5/ \; --contain \; /paedyl01/disk1/louisshe/tools/DeepVariant/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/ref/hs37d5/hs37d5.fa \; --reads=/input_reads/HG005.hs37d5.30x.bam \; --output_vcf=/output/HG005.dv.vcf.gz \; --output_gvcf=/output/HG005.dv.g.vcf.gz \; --num_shards=10 \; --intermediate_results_dir=/tmp \; --logging_dir=/output/log \; --dry_run=false \; --par_regions_bed=/ref/hg19/ucsc.hg19.par.bed \; --haploid_contigs=""chrX,chrY""; ```; - Error trace:; Error trace below is from `HG005_deppvariant.log`. No error prompts prior to this step.; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examp. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Ker. For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0619 14:57:56.059498 47403021002560 call_variants.py:563] Total 1 writing processes started.; I0619 14:57:56.063244 47403021002560 dv_utils.py:370] From /tmp/make_examples.tfrecord-00000-of-00010.gz.example_info; I0619 14:57:56.063441 47403021002560 call_variants.py:588] Shape of input examples: [100, 221, 7]; I061",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:1679,log,log,1679,,https://github.com/google/deepvariant/issues/833,1,['log'],['log']
Testability," up to use the group permission. I do not think this is the issue. . Any suggestions would be greatly appreciated. Andy. ```; (base) -bash-4.2$ groups; giuser kimlab docker; (base) -bash-4.2$ ; ```. ```; docker run -v /public/home/dkim142/quickstart-testdata:/input \; -v /public/home/dkim142/quickstart-output:/output google/deepvariant:0.9.0 \; /opt/deepvariant/bin/run_deepvariant --model_type=WGS \; --ref=/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads=/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions chr20:10,000,000-10,010,000 \; --output_vcf=/public/home/dkim142/quickstart-output/output.vcf.gz \; --output_gvcf=/public/home/dkim142/quickstart-output/output.g.vcf.gz \; --num_shards=1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling \; --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2019-12-08 02:35:44.105906: F tensorflow/core/platform/cpu_feature_guard.cc:37] ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m1.146s; user	0m1.709s; sys	0m4.191s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise Call",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/248:1371,test,testdata,1371,,https://github.com/google/deepvariant/issues/248,1,['test'],['testdata']
Testability," using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2021-06-11 15:22:06.016750: I third_party/nucleus/io/sam_reader.cc:662] Setting HTS_OPT_BLOCK_SIZE to 134217728; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /reference/GRCh38_no_alt_analysis_set.fasta --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions /input/idt_capture_novogene.grch38.bed --task 0. real	1m16.629s; user	1m9.338s; sys	0m1.008s; I0611 15:22:08.176606 140688071014144 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""/input/HG003.novaseq.wes_idt.100x.dedup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""/input/idt_capture_novogene.grch38.bed"" --task {} )' returned non-zero exit status 247.; ```. **Does the quick start test work on your system?** No; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/462:6188,test,test,6188,,https://github.com/google/deepvariant/issues/462,2,['test'],['test']
Testability,"""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>; import numpy as np; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>; from . import core; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the numpy C-extensions failed. This error can happen for; many reasons, often due to issues with your setup or how NumPy was; installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3""; * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect.; Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**; As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610:2739,test,test,2739,,https://github.com/google/deepvariant/issues/610,2,['test'],['test']
Testability,"""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules; module load singularity; module load cuda-dcgm/2.2.9.1; module load cuda11.4/toolkit; module load cuda11.4/blas; module load cuda11.4/nsight; module load cuda11.4/profiler; module load cuda11.4/fft; source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh; conda activate TensorFlow_GPU. # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; --nv \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir""; ```. And here's my error:; ```; 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>; _ll.load_library(_main_dir); File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library; py_tf.TF_LoadLibrary(lib); tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZNK10tensorflow8OpKernel11TraceStringERKNS_15OpKernelContextEb; ```. I'm wondering if this error can help highlight the error I'm experiencing? . Is there something I can run with CUDA to test that implementation on our new GPU server?. Thanks!; Phil",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/514:2562,test,test,2562,,https://github.com/google/deepvariant/issues/514,1,['test'],['test']
Testability,"# Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.8.0/DeepVariant-inception_v3-0.8.0+data-wgs_standard; IMAGE_VERSION=0.8.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_gVCF_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://files_jays/bam/bqsr.realign.markdup.sorted.merged.bam \; --bai gs://files_jays/bam/bqsr.realign.markdup.sorted.merged.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --preemptible \; --max_preemptible_tries 5 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". # logs on one of the VMs; /bin/bash: gcsfuse: command not found; parallel: This job failed:; mkdir -p ./input-gcsfused-0 && gcsfuse --implicit-dirs files_jays /input-gcsfused-0; /bin/bash: gcsfuse: command not found; parallel: This job failed:; mkdir -p ./input-gcsfused-1 && gcsfuse --implicit-dirs files_jays /input-gcsfused-1; /bin/bash: gcsfuse: command not found",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/210:1336,log,logging,1336,,https://github.com/google/deepvariant/issues/210,3,['log'],"['log', 'logging', 'logs']"
Testability,"######################################; **Any additional context:**; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; [E::hts_open_format] Failed to open file ""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options; samples_in_order, sample_role_to_train = one_sample_from_flags(; File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 88, in one_sample_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678:3625,test,testdata,3625,,https://github.com/google/deepvariant/issues/678,1,['test'],['testdata']
Testability,"${MODEL}"", \; DOCKER_IMAGE=""${DOCKER_IMAGE}"", \; DOCKER_IMAGE_GPU=""${DOCKER_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.0`; 3. `[u'Error in job call-varia--root--180321-233157-28 - code 9: Quota CPUS exceeded in region us-central1']`. The `...-stderr.log` file written to `staging-folder` also begins with the errors; ```; /tmp/ggp-896952821: line 16: type: gsutil: not found; debconf: delaying package configuration, since apt-utils is not installed; debconf: delaying package configuration, since apt-utils is not installed; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3746C208A7317B0F; W: The repository 'http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease' is not signed.; debconf: delaying package configuration, since apt-utils is not installed; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 663 100 663 0 0 5012 0 --:--:-- --:--:-- --:--:-- 5022; debconf: delaying package configuration, since apt-utils is not installed; WARNING: Logging before flag parsing goes to stderr.; ```. But I then see many messages about candidate variants it's found. . The directory `staging-folder/examples/0/` also includes 8 `.gz` files like `examples_output.tfrec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:2903,log,log,2903,,https://github.com/google/deepvariant/issues/60,1,['log'],['log']
Testability,"${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```; ## Run `call_variants`; echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log.""; ( time sudo docker run \; -v ""${BASE}"":""${BASE}"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) 2>&1 | tee ""${LOG_DIR}/call_variants.log""; echo ""Done.""; echo; ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the vcfs after or something. And where in this shading would post processing of variants fit in to generate the VCF -- can that be part of a reduce step pulling all sharded call_variants outputs together one one machine? Any recommendations @pichuan @akolesnikov ?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:3988,log,log,3988,,https://github.com/google/deepvariant/issues/151,2,"['log', 'test']","['log', 'test']"
Testability,"'.format(ptrue)); ValueError: ptrue must be between zero and one: nan; """""". The above exception was the direct cause of the following exception:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1385, in main; tmp_variant_file = dump_variants_to_temp_file(variant_generator); File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1067, in dump_variants_to_temp_file; tfrecord.write_tfrecords(variant_protos, temp.name); File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/third_party/nucleus/io/tfrecord.py"", line 190, in write_tfrecords; for proto in protos:; File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/tmp/Bazel.runfiles_i47tupw0/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 111, in _group_overlapping_variants; for variant in sorted_variants:; File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 420, in <genexpr>; return (item for chunk in result for item in chunk); File ""/usr/lib/python3.8/multiprocessing/pool.py"", line 868, in next; raise value; ValueError: ptrue must be between zero and one: nan; ```. **Does the quick start test work on your system?** Yes; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849:6005,test,test,6005,,https://github.com/google/deepvariant/issues/849,2,['test'],['test']
Testability,") and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 558, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py"", line 1047, in assert_consumed; raise AssertionError(; AssertionError: Some objects had attributes which were not restored: ; <tf.Variable 'conv2d/kernel:0' shape=(3, 3, 7, 32) dtype=float32, numpy=; ; My knowledge in deep learning models is not the best, so if you could please tell me how to overcome this error, as the RNA model seems to have very promising results for RNA variant calling and i want to use it. **Setup**; - Operating system: Ubuntu 20.0; - DeepVariant version: Latest version 1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: GIAB benchmark data used in the deepvariant-rnaseq-case-study.md but not restricted to chr20. **Steps to reproduce:**; - Command: ; docker run -v ""$(pwd):$(pwd)"" -w $(pwd) google/deepvariant:latest run_deepvariant --model_type=WES --customized_model=model/model.ckpt --ref=GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta --reads=STAR/Mapping/marked_split.bam --output_vcf=STAR/Mapping/deepvariant.rna.vcf --num_shards=$(nproc)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845:2670,Assert,AssertionError,2670,,https://github.com/google/deepvariant/issues/845,3,"['Assert', 'benchmark']","['AssertionError', 'benchmark']"
Testability,"* Build Docker image with OpenVINO support; ```; docker build -t deepvariant . --build-arg DV_OPENVINO_BUILD=1; ```. * Run; ```bash; export INPUT_DIR=""${PWD}/quickstart-testdata""; export OUTPUT_DIR=""${PWD}/quickstart-output"". docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; deepvariant \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --call_variants_extra_args=""use_openvino=True"" \; --num_shards=1; ```. (added extra flag `--call_variants_extra_args=""use_openvino=True""` comparing to original Getting Started)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/363:169,test,testdata,169,,https://github.com/google/deepvariant/pull/363,1,['test'],['testdata']
Testability,* DeepVariant Logo.; * DeepVariant RNA-seq case study.; * DeepVariant RNA-seq release.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/576:14,Log,Logo,14,,https://github.com/google/deepvariant/pull/576,1,['Log'],['Logo']
Testability,"* Install OpenVINO by pip; * Update OpenVINO to latest 2021.3 version; * Use `enum34==1.1.8` to fix ""AttributeError: module 'enum' has no attribute 'IntFlag'"" (https://github.com/python-poetry/poetry/issues/1122#issuecomment-628037127). test run: https://github.com/dkurt/deepvariant/actions/runs/755874669",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/442:237,test,test,237,,https://github.com/google/deepvariant/pull/442,1,['test'],['test']
Testability,"**Describe the issue:**. DV calls two adjacent SNPs rather than one larger variant - eventho these variants are on the same reads. The DV call looks as follows:. `chr17 63951760 . G T 53 PASS . GT:GQ:DP:AD:VAF:PL 0/1:53:139:64,75:0.539568:53,0,62. chr17 63951761 . A T 45.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:45:139:62,75:0.539568:45,0,55; `; Expected for this locus (same BAM file, with Freebayes):. `chr17 63951760 . GA TT 1766.67 . AB=0.515152;ABP=3.27351;AC=1;AF=0.5;AN=2;AO=68;CIGAR=2X;DP=132;DPB=132;DPRA=0;EPP=3.0103;EPPR=3.15039;GTI=0;LEN=2;MEANALT=3;MQM=60;MQMR=60;NS=1;NUMALT=1;ODDS=361.082;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2481;QR=2251;RO=62;RPL=25;RPP=13.3567;RPPR=3.57068;RPR=43;RUN=1;SAF=29;SAP=6.20364;SAR=39;SRF=25;SRP=8.05372;SRR=37;TYPE=mnp;technology.ILLUMINA=1 GT:DP:AD:RO:QR:AO:QA:GL 0/1:132:62,68:62:2251:68:2481:-184.277,0,-163.588; `. BAM file (+/-150 bases): https://www.dropbox.com/s/hcxmotqgxzhtm9k/test.bam?dl=0; BAI file: https://www.dropbox.com/s/fnkzzi8mh1qhwsl/test.bam.bai?dl=0. Reference genome: hg38 (no ALT). **Setup**; - Operating system:; - DeepVariant version: 1.3.0, latest ; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) 2*150bp Illumina, NovaSeq600, Exome. . **Steps to reproduce:**; - Command: Call variants with run_deepvariant wrapper script. ; - Error trace: (if applicable). ![igv_snapshot](https://user-images.githubusercontent.com/22975/154966285-a761d2b4-4eba-46e2-a1f4-4f3af93ddbc8.png)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/520:934,test,test,934,,https://github.com/google/deepvariant/issues/520,2,['test'],['test']
Testability,"**Describe the issue:**. Hi, I am wondering if there's been a study on the cost-benefit of running DV in the GPU mode.; Back in the days of PEPPER-DeepVariant-Margin, I remember trying to profile (not as a rigorous study) what benefits there'd be if we were to run the pipeline in the GPU mode.; The conclusion back then from my anecdotal runs is that it's not worth it (we can get the CPU version to <$100/sample with little to minimum effort on optimizing cloud resource allocations, but the GPU version is ~$200/sample with P100). Now given that DV has undergone quite a lot of changes since then, I wonder if the conclusion is changed.; So I did a test run on a PacBio Hifi 30X bam with DV 1.5.0, and collected the GPU resource log (using `gpustat -a -i 1 `, log attached below).; Looking at the log file, it doesn't look like GPU is used much still. So I wonder if you have done any study on this subject and if so, can share some insights. Thank you!. Steve. [gpu.usages.log.zip](https://github.com/google/deepvariant/files/11473421/gpu.usages.log.zip)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/650:652,test,test,652,,https://github.com/google/deepvariant/issues/650,6,"['log', 'test']","['log', 'test']"
Testability,"**Describe the issue:**; After running, no VCF is found, the logs however are available. **Setup**; - Operating system: ubuntu 22.04 (WSL2); - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): docker; - Type of data: (I find variant only in chr17 for easier reading and faster speed); - input read: aligned HG004 to CHM13 (https://downloads.pacbcloud.com/public/revio/2022Q4/HG004-rep1/); - reference genome: CHM13 (https://github.com/marbl/CHM13). **Steps to reproduce:**; - Command:; `docker run --volume ""/root/deepvariant/input"":""/input"" --volume ""/root/deepvariant/output"":""/output"" google/deepvariant:""1.6.1"" /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=/input/chm13v2.0.fa --reads=/input/aligntest.bam --regions ""chr17"" --output_vcf=/output.vcf.gz --output_gvcf=/output.g.vcf.gz --intermediate_results_dir /output/intermediate_results --logging_dir=/output`; ; **Any additional context:**; [make_examples.log](https://github.com/user-attachments/files/16189177/make_examples.log); [call_variants.log](https://github.com/user-attachments/files/16189180/call_variants.log); [postprocess_variants.log](https://github.com/user-attachments/files/16189182/postprocess_variants.log); [vcf_stats_report.log](https://github.com/user-attachments/files/16189186/vcf_stats_report.log)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/850:61,log,logs,61,,https://github.com/google/deepvariant/issues/850,9,['log'],"['log', 'logs']"
Testability,"**Describe the issue:**; After upgrading to v1.6, we noticed this strange behavior, where the program hangs on a sharded BAM that holds only alt-contig mapping reads. **Setup**; - Operating system: on GCE via Google Life Sciences API (through Cromwell); - DeepVariant version: v1.6; - Installation method (Docker, built from source, etc.): official v1.6 docker; - Type of data: Both PacBio HiFi and ONT (10.4), on GRCh38. . **Steps to reproduce:**. - Command. ```; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=GCA_000001405.15_GRCh38_no_alt_analysis_set.fa \; --haploid_contigs chrX,chrY \; --par_regions_bed GRCh38.PAR.bed \; --reads=/cromwell_root/<sample_id>.alts.bam \; --output_vcf=/cromwell_root/dv_output/<sample_id>.alts.deepvariant.vcf.gz \; --output_gvcf=/cromwell_root/dv_output/<sample_id>.alts.deepvariant.g.vcf.gz \; --num_shards=16; ```. - Relevant log ; (note it says ""0 examples"", so I suspect it is when no examples are available, not just when there's only alt-mapping reads, e.g. if one simulates reads error-free from the reference itself, it probably will have the same issue). ```; /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord-00011-of-00016.gz.example_info.json; I0203 17:23:03.253894 135328978921280 make_examples_core.py:2958] example_shape = None; I0203 17:23:03.254237 135328978921280 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 7, 9, 10]; I0203 17:23:03.255900 135328978921280 make_examples_core.py:301] Task 11/16: Found 0 candidate variants; I0203 17:23:03.256017 135328978921280 make_examples_core.py:301] Task 11/16: Created 0 examples; I0203 17:23:04.930985 137565708298048 make_examples_core.py:301] Task 7/16: Writing example info to /cromwell_root/tmp.cd83af44/tmpuzrx3yrs/make_examples.tfrecord-00007-of-00016.gz.example_info.json; I0203 17:23:04.931358 137565708298048 make_examples_core.py:2958] example_shape = None; I0203 17:23:04.931699 137565708298048 make_examples_core.py:2959] example_chann",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/769:888,log,log,888,,https://github.com/google/deepvariant/issues/769,1,['log'],['log']
Testability,"**Describe the issue:**; Attempting to install deepvariant using conda and python 3 fails due to missing `tensorflow` and `tensorflow-estimator` dependencies. **Setup**; - Operating system: Amazon Linux 2023; - DeepVariant version: N/A, but we can narrow the focus down to 1.5, which is the latest available on conda; - Installation method (Docker, built from source, etc.): Conda (mamba). **Steps to reproduce:**; - Command: `mamba install deepvariant -c bioconda`; - Error trace: ; ```; Pinned packages:; - python 3.10.*. Could not solve for environment specs; The following packages are incompatible; └─ deepvariant is installable with the potential options; ├─ deepvariant [0.10.0|0.7.2|0.8.0|0.9.0] would require; │ └─ tensorflow 1.12.* , which does not exist (perhaps a missing channel);; ├─ deepvariant [0.10.0|1.0.0] would require; │ └─ tensorflow 2.0.* , which does not exist (perhaps a missing channel);; ├─ deepvariant [0.4.1|0.6.0|0.6.1|0.7.0] would require; │ └─ python [2.7* |>=2.7,<2.8.0a0 ], which can be installed;; ├─ deepvariant [0.7.1|0.7.2] would require; │ └─ tensorflow 1.11.* , which does not exist (perhaps a missing channel);; └─ deepvariant [1.0.0|1.1.0|...|1.5.0] would require; └─ tensorflow-estimator 2.0.* , which does not exist (perhaps a missing channel).; ```. **Does the quick start test work on your system?**; N/A. **Any additional context:**; My goal was to install the latest version available (1.5.0). Looking at the `tensorflow-estimator` releases on conda-forge, version 2.0 is skipped entirely, which explains the error. https://anaconda.org/conda-forge/tensorflow-estimator/files?page=8",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/736:1318,test,test,1318,,https://github.com/google/deepvariant/issues/736,1,['test'],['test']
Testability,"**Describe the issue:**; Hello everyone, i am trying to run a Pacbio Workflow with deepvariant in it but i get an error in the make example step ( Rule and log below) i allready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: 1.5.0; - Tensorflow 2.11.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: PacBio HIFI reads. **Steps to reproduce:**; ```; rule deepvariant_make_examples:; input:; bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",; bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",; reference=config[""ref""][""fasta""],; output:; tfrecord=temp(; f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz""; ),; nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>; log:; f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",; benchmark:; f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv""; container:; f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}""; params:; vsc_min_fraction_indels=""0.12"",; pileup_image_width=199,; shard='{shard}',; examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=25",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:156,log,log,156,,https://github.com/google/deepvariant/issues/677,1,['log'],['log']
Testability,"**Describe the issue:**; Hello, I want to know what is an efficient way to build and run locally. My intent: make a change in call_variant.py and observe the effect. ; Do I have to always build the docker? ; OR which shell scripts can I use to achieve my purpose?. **Setup**; - Operating system: Ubuntu 18.04 LTS; - DeepVariant version: 0.8.0; - Installation method: build from source; - Type of data: NA. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/340:498,test,test,498,,https://github.com/google/deepvariant/issues/340,2,['test'],['test']
Testability,"**Describe the issue:**; I am not obtaining any output files even though there are no major issues in the log file, (see attached); I ran it with the same data first, only for the chr20, and everything went fine. For all the genome now, I don't have the vcfs.; [deepvariant_run.log](https://github.com/user-attachments/files/16596005/deepvariant_run.log). **Setup**; - Operating system: Windows, WSL2 (5.15.146.1-microsoft-standard-WSL2); - DeepVariant version: 1.4.0; - Installation method: Docker; - Type of data: NA12878, bam file. **Steps to reproduce:**; sudo docker run \; -v ""/mnt/c/Users/pinto/OneDrive - Universidade de Lisboa/Revisao bibliografica/Scoping Review/alg_testing:/input"" \; -v ""/mnt/c/Users/pinto/OneDrive - Universidade de Lisboa/Revisao bibliografica/Scoping Review/alg_testing/output:/output"" \; google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/genome.fa \; --reads=/input/sorted.bam \; --output_vcf=/output/outputdeepvar.vcf \; --output_gvcf=/output/outputdeepvar.g.vcf \; --num_shards=4 \; > ""/mnt/c/Users/pinto/OneDrive - Universidade de Lisboa/Revisao bibliografica/Scoping Review/alg_testing/output/deepvariant_run.log"" 2>&1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/868:106,log,log,106,,https://github.com/google/deepvariant/issues/868,4,['log'],['log']
Testability,"**Describe the issue:**; I follow the quick start guidelines, and meet this error. **Setup**; - Operating system: MacBook Air (M1, 2020); - DeepVariant version: 19.03.14; - Installation method (Docker, built from source, etc.): Docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) quick start data . **Steps to reproduce:**; - Command: sudo docker run --platform linux/amd64 google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/quickstart-output/output.vcf.gz --output_gvcf=/quickstart-output/output.g.vcf.gz --intermediate_results_dir /quickstart-output/intermediate_results_dir --num_shards=1; - Error trace: (if applicable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickst",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/471:531,test,testdata,531,,https://github.com/google/deepvariant/issues/471,2,['test'],['testdata']
Testability,"**Describe the issue:**; I have been using DV via the official Docker container; and I have not found a way yet to make DV tell me its version. The Docker container is versioned, obviously, but the included tools have no ""--version"" flag - which makes it a bit tricky to dump out version information at run time (for logging purposes, for example). . **Setup**; Any system would have this issue, I think. **Steps to reproduce:**; N/A",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/332:317,log,logging,317,,https://github.com/google/deepvariant/issues/332,1,['log'],['logging']
Testability,"**Describe the issue:**; I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : ; 1. non-parallel + bam ✅; 2. non-parallel + cram ✅ ; 3. parallel + bam ✅ ; 4. non-parallel + cram 🔴 . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**; - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64; - DeepVariant version: v1.6.0; - Installation method (Docker, built from source, etc.): HPC, sorry I don't know; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); Not special, I used common toy data. **Steps to reproduce:**; - Command: ; ```; seq 0 $((N_SHARDS-1)) \; | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \; --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \; make_examples --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples output/examples.tfrecord@${N_SHARDS}.gz\; --channels insert_size \; --task {} \; || exit 1; ```; - Error trace: (if applicable); ```; META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/786:1091,log,logs-parallel,1091,,https://github.com/google/deepvariant/issues/786,3,['log'],"['log', 'logs-parallel']"
Testability,"**Describe the issue:**; I'm doing a series of test of how can I run DV faster with my resources. I'm trying splitting the bam file - run make examples on separate - run Call Variants of both at the same time and call variants. Everything goes fine until the postprocess The 2 different gvcf are name as followed:. SPLIT2.gvcf.tfrecord-00000-of-00030.gz; SPLIT.gvcf.tfrecord-00000-of-00030.gz. Both in the same directory. I know that ideally would run on separate all the way then merge the two gvcf, but I'm,m asking if there is any tweak I can do to overcome this problem... I tought on rename the files from 0:59-of-00060.gz but can someone also tell me the implications of that move? . **Setup**; - Linux; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WGS from shallow resequencing data. **Steps to reproduce:**; - Command: "" /opt/deepvariant/bin/postprocess_variants --ref ""/ref/100kbPrad_v1_scaffolds.fasta"" --infile ""/input/call_variants_output.tfrecord.gz"" --outfile ""/output/MergedSplited.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" --gvcf_outfile ""/output/MergedSplited.output.g.vcf.gz"" --vcf_stats_report=False"" ; ; - Error trace: ; Singularity> time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/100kbPrad_v1_scaffolds.fasta"" --infile ""/input/call_variants_output.tfrecord.gz"" --outfile ""/output/MergedSplited.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/input/gvcf.tfrecord@30.gz"" --gvcf_outfile ""/output/MergedSplited.output.g.vcf.gz"" --vcf_stats_report=False; 2021-01-26 15:54:17.883294: I deepvariant/postprocess_variants.cc:88] Read from: /input/call_variants_output.tfrecord.gz; 2021-01-26 16:15:59.645306: I deepvariant/postprocess_variants.cc:103] Total #entries in single_site_calls = 144590159; I0126 17:29:21.938455 140157300115200 postprocess_variants.py:1079] CVO sorting took 95.07083837985992 minutes; I0126 17:29:21.940265 140157300115200 postprocess_variants",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/413:47,test,test,47,,https://github.com/google/deepvariant/issues/413,1,['test'],['test']
Testability,"**Describe the issue:**; In ```make_examples```: The middle base of reference sequence in the window doesn't match first character of variant.reference_bases. **Setup**; - Operating system: CentOS Linux v7; - DeepVariant version: 1.1.0; - Installation method: Docker; - Type of data: WGS (Illumina 150nt pairs from GIAB HG002). **Steps to reproduce:**; - Command: ; - Error trace: (if applicable). **Does the quick start test work on your system?** Yes, it does.; Is there any way to reproduce the issue by using the quick start? No. **Any additional context:**; The goal is to call SNPs and indels in GIAB HG002 WGS data, and to compare the results with a truthset. High-confidence intervals and the truthset are at https://github.com/genome-in-a-bottle/giab_latest_release. Please see the attached bash script (command line) and output files. Two questions:; - Is ```make_examples``` parameterized correctly (see attached script and output files)?; - Can someone please explain what this error message means and suggest an appropriate approach to troubleshooting and fixing it?. [vcall.log](https://github.com/google/deepvariant/files/5858295/vcall.log); [vcall.sh.txt](https://github.com/google/deepvariant/files/5858303/vcall.sh.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/411:421,test,test,421,,https://github.com/google/deepvariant/issues/411,3,"['log', 'test']","['log', 'test']"
Testability,"**Describe the issue:**; The prints that read base quality scores cannot be read, as result, no variants are reported. However, I can visualize these values in the reads in IGV. How is that these values cannot be read? This is the line with the error, which repeats one after. 2021-03-26 19:12:43.550815: W third_party/nucleus/io/sam_reader.cc:534] Could not read base quality scores m64036_210113_122249/147655225/ccs: Not found: Could not read base quality scores. **Setup**; - Operative system: Ubuntu 20.04; - DeepVariant version: 1.1.0 (latest); - Installation method: docker; - Type of data: PacBio HiFi. BAM files aligned to the reference with `minimap2 -ax map-pb`. **Steps to reproduce:**; - Command:; ```; docker run \; -v /home/user/working_directory:/input \; -v /home/user/working_directory:/output \; google/deepvariant:1.1.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/reference.fa \; --reads=/input/file.bam \; --output_vcf=/output/file.vcf \; --call_variants_extra_args=""use_openvino=true"" \; --num_shards=4 \; --logging_dir=/output/logs; ```. **Does the quick start test work on your system?**; Yes. The test works without problem.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/434:1085,log,logs,1085,,https://github.com/google/deepvariant/issues/434,3,"['log', 'test']","['logs', 'test']"
Testability,"**Describe the issue:**; When I try to run DeepVariant using the examples in the quickstart document I receive the following output:. ```; INFO: Using cached SIF image; --ref is required.; Pass --helpshort or --helpfull to see help on flags.run_deepvariant.sh: line 13: --ref=/home/sk2847/scratch60/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory; ```. I am able to open the FASTA file at that path, so I know that it exists. The full script I am using is:. ```; #!/bin/sh. BIN_VERSION=""1.0.0""; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity run --cleanenv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1; ```. **Setup**; - Operating system: Linux, cluster; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker, through Singularity; - Type of data: The data from the quickstart . **Steps to reproduce:**; - Command: See above; - Error trace: See above. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. My issue is with the quickstart. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/402:310,test,testdata,310,,https://github.com/google/deepvariant/issues/402,4,['test'],"['test', 'testdata']"
Testability,"**Describe the issue:**; `/opt/deepvariant/bin/run_deepvariant` crashes when there are no variants, because a VCF file gets written with 'default' as the sample name'. This happens because I use targetted sequencing, and when the capture fails, I get hardly any reads. This can be simulated by downsampling the quickstart data to 0.1%, see below. **Setup**; - HPC; - google/deepvariant:0.10.0; - Docker; - Targetted PacBio sequencing, aligned against HG38. **Does the quick start test work on your system?**; Yes. **Workaround, for version 1.0.0 only**; This error does not occur when using version 1.0.0, and explicitly passing the `--sample_name` flag to `run_deepvariant`. . **Steps to reproduce, using the quickstart data:**; ```bash; MODEL_TYPE=PACBIO; NUM_SHARDS=4; READS=NA12878_0.1_percent.bam. # Downsample the reads to 0.1%; samtools view -s 0.001 -b NA12878_S1.chr20.10_10p1mb.bam -o ${READS} --write-index. docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${MODEL_TYPE} \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/${READS} \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=${NUM_SHARDS}; ```. **Error trace**; ```bash; $ docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=${MODEL_TYPE} --ref=/input/ucsc.hg19.chr20.unittest.fasta --reads=/input/${READS} --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --intermediate_results_dir /output/intermediate_results_dir --num_shards=${NUM_SHARDS}; I0921 06:50:39.795207 140086398105344 run_deepvariant.py:241] Re-using the directory for intermediate results in /output/intermediate_results",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/354:480,test,test,480,,https://github.com/google/deepvariant/issues/354,1,['test'],['test']
Testability,"**Describe the issue:**; while trying to install deepvariant with conda its is running for dour days, still nothing is getting installed. **Setup**; - OS: CentOS Linux release 7.4.1708 (Core); - DeepVariant version:conda install bioconda/label/cf201901::deepvariant; - Installation method (Docker, built from source, etc.): Conda; - Type of data: NA. **Steps to reproduce:**; - Command: conda install bioconda/label/cf201901::deepvariant; - Error trace: ; '''conda install bioconda/label/cf201901::deepvariant -y; Collecting package metadata: done; Solving environment: '''. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? NA. **Any additional context:**; NA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/806:598,test,test,598,,https://github.com/google/deepvariant/issues/806,2,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/459:490,test,test,490,,https://github.com/google/deepvariant/issues/459,2,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; I am debugging a set of false negative calls in a benchmarking set (NA12878, Agilent exome provided by a collaborator). . In the process, I came across a call that makes no sense to me and was wondering what a plausible explanation might be:. Final VCF:; `chr1 109161996 rs678238 A G 39 . AC=1;AF=0.5;AN=2;AQ=39;DP=218 GT:AD:DP:GQ:PL:RNC 0/1:0,218:218:15:39,0,14:.; `. And the gVCF:; `chr1 109161996 . A G,<*> 39.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:15:218:0,218,0:1,0:39,0,14,990,990,990; `. The true gtenotype at this position should be G|G. . However, note that the genotype is shown as 0|1 - even tho the ref allele as a depth of 0. This is supported by a manual inspection of the alignment. There really isn't an A there and it does not seem to be a ""problematic"" locus with long runs of A or G. The reads align perfectly without any gaps. . Screenshot: https://www.dropbox.com/s/sp2n2gfy3li2rjl/dv_locus_error.JPG?dl=0 , Allele frequency as per alignment: G: 100%. . So how come Deepvariant calls it like that? It really makes no sense to me :(. **Setup**; - Operating system: Centos 7, Docker container; - DeepVariant version: 1.1.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Exome (Agilent V7, genome-in-a-bott reference). **Steps to reproduce:**; - Command: Not possible without the raw data...available upong request. ; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? . No. . **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/470:167,benchmark,benchmarking,167,,https://github.com/google/deepvariant/issues/470,3,"['benchmark', 'test']","['benchmarking', 'test']"
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: **YES**. **Describe the issue:**. Manually selected regions (a single region is formed by a locus extending 500 bp to both sides) were used in my project to make examples, and it was also succeed in calling variants. However, when I running postprocess_variants, something went wrong. I check the log, and I guess it was related to the wrong ""call_variant_outputs"". So I printed one ""call_variant_outputs"" out of the whole tfrecord, and found out there are several repeated variant in one call. Where did I go wrong?. **The log file is attached.**; [postprocess_variants.log](https://github.com/google/deepvariant/files/7149887/postprocess_variants.log). **Setup**; - Operating system: ubuntu **16**; - DeepVariant version: **0.7.0**; - Installation method (Docker, built from source, etc.): **built from source**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **NO**. **Steps to reproduce:**; - Command:; - Error trace: (if applicable). > W0912 23:51:01.891268 140429229119232 postprocess_variants.py:331] Alt allele indices found from call_variants_outputs for variant reference_bases: ""C""; alternate_bases: ""A""; calls {; info {; key: ""AD""; value {; values {; int_value: 17; }; values {; int_value: 4; }; }; }; info {; key: ""DP""; value {; values {; int_value: 21; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 0.190476190476; }; }; }; genotype: -1; genotype: -1; call_set_name: ""XY406-1""; }; end: 10147; reference_name: ""1""; start: 10146; is [[0], [0], [0]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 874, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_goo",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/485:388,log,log,388,,https://github.com/google/deepvariant/issues/485,4,['log'],['log']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**: yes. **Describe the issue:**; I am building the deep variant Dockerfile (v1.2) off the Databricks Runtime base image (Ubuntu 18.04).; Run into issues at Stage 'Install CLIF binary', I get the error,. `ModuleNotFoundError: No module named 'apt_pkg'`. I see in the build-prereq.sh script this comment,. `Build clif binary from scratch. Might not be ideal because it installs a; bunch of dependencies, but this works fine when we used this in a Dockerfile; because we don't do build-prereq.sh in the final image.`. Please advise how to get around this when building your own Docker Image. Cheers,. William. **Setup**; - Operating system: Ununtu 18.04; - DeepVariant version: 1.2; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/476:1035,test,test,1035,,https://github.com/google/deepvariant/issues/476,2,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; can deepvariant detect multiallelic positions, for example, Ref is A, and Alt is C, G. And the GT is denoted as 1/2",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/480:490,test,test,490,,https://github.com/google/deepvariant/issues/480,2,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:; Yes. **Describe the issue:**. When running WDL workflows backed with PAPI, I get PAPI error 10, which indicates the disk is full. **Setup**; - Operating system: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`; - DeepVariant version: Docker image coming with DV-Margin-Pepper: `kishwars/pepper_deepvariant:r0.4.1`; - Installation method (Docker, built from source, etc.): Docker; - Type of data: ONT, GRCh38, process by chromosome. **Steps to reproduce:**. ```; # This is the command from Pepper, but judged from the log, the command failed during the DV stage.; run_pepper_margin_deepvariant \; call_variant \; -b ~{bam} \; -f ~{ref_fasta} \; -t ""${num_core}"" \; -s ""${SM}"" \; -o ""~{output_root}"" \; -p ""~{prefix}"" \; --gvcf \; --phased_output \; --ont; ```; Relevant part of the log file (which is over 200MB):. ```; run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont; [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED; [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND; -------; mkdir -p /cromwell_root/pepper_output; ; mkdir -p /cromwell_root/pepper_output/logs; ; mkdir -p /cromwell_root/pepper_output/intermediate_files;; -------; [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND; -------; time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-8",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:640,log,log,640,,https://github.com/google/deepvariant/issues/491,2,['log'],['log']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:; yes. **Describe the issue:**; Version 1.2 installed via docker on a linux server (over SSH login), running the quickstart test run:; - Expected behavior: when running without sudo, process uses current user's name privilege.; - What happened: file access denied if folder permission is 744. The run successfully returns if manually setting the relevant folders to permission 777, but output (vcf files and report) files were owned by nobody/nobody. . My understanding is that nobody is a special handle meant for OS housekeeping works. Is this an expected behavior? Is it docker?. **Setup**; - Operating system: CentOS 7 (`cat /etc/os-release`); - DeepVariant version: 1.2; - Installation method: docker; - Type of data: The test data and command described in [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). **Steps to reproduce:**; - Command: identical to those of [quick-start](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md). Environment variable setup lines were directly pasted into the shell, the 'run everything' command was pasted into a file `cmd.sh` which was then was ran with `. cmd.sh`. **Does the quick start test work on your system?**; Yes. Outputs are fine. **Any additional context:**; Except having to add `mkdir` and `chmod` lines to the script, I found the run successful. I can read/write to the files owned by nobody and the ownership will transfer automatically upon writing.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/478:183,log,login,183,,https://github.com/google/deepvariant/issues/478,4,"['log', 'test']","['login', 'test']"
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**; (A clear and concise description of what the issue is.); Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**; - Operating system: Linux HPC; - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WES. **Steps to reproduce:**; ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p c_compute_wgp; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=02:00:00; #SBATCH --time=072:00:00; #SBATCH --mem-per-cpu=32GB. module purge; module load singularity; module load parallel. set -eu. cd /scratch/c.c21087028/; BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \; --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/d",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/522:500,log,login,500,,https://github.com/google/deepvariant/issues/522,1,['log'],['login']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**; The error arises during the ""postprocess_variants"" step. The quick-test and a run on chr22 from the same sample ran through without any issue. I tried to use `group_variants=false` as suggested [here](https://github.com/google/deepvariant/issues/341#issuecomment-686657676). But a similar error/crash occurs at a different variant/location. A similar problem was reported [here](https://github.com/google/deepvariant/issues/485), but the final fix is not provided. **Setup**; - Operating system: CentOS 7; - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity image built from docker image; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) WGS, Illumina x10. **Steps to reproduce:**; - Command: ; ```; # Modified script; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; -B ${INPUT_PATH}:/input \; compute_envs/deepvariant_latest.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=hs37d5_PhiX.fa \; --reads=/input/${pid}/alignment/${prefix}_${pid}_merged.mdup.bam \; --intermediate_results_dir=/input/${pid}/deepvariant_calling/tmp/${prefix}/ \; --output_vcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz \; --output_gvcf=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.g.vcf.gz \; --num_shards=15; ```; I have also tried postprocessing with `group_variants`, which also produces a similar error.; ```; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; -B ${INPUT_PATH}:/input \; compute_envs/deepvariant_latest.sif \; /opt/deepvariant/bin/postprocess_variants \; --group_variants=false \; --ref=hs37d5_PhiX.fa \; --infile=/input/${pid}/deepvariant_calling/tmp/${prefix}/call_variants_output.tfrecord.gz \; --outfile=/input/${pid}/deepvariant_calling/${prefix}_${pid}_deepvariant.vcf.gz; ```; - Error t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/517:188,test,test,188,,https://github.com/google/deepvariant/issues/517,1,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/525:490,test,test,490,,https://github.com/google/deepvariant/issues/525,2,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:; YES. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: ubuntu **16.04**; - DeepVariant version: **1.1.0**; - Installation method (Docker, built from source, etc.): **built from source**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than “**AD**“ did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than “**AD**“ did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/529:1068,log,log,1068,,https://github.com/google/deepvariant/issues/529,1,['log'],['log']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**. When using the singularity-gpu version, the make_examples step will only run sequentially (i.e., one shard processed at a time using only a single CPU) no matter what value I supply to ```--num_shards```. **Setup**; - Operating system: CentOS 7; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); WES data from the tutorial. **Steps to reproduce:**; - Command:; ; ```; #!/usr/bin/env bash. INPUT_DIR=""input""; OUTPUT_DIR=""output"". BIN_VERSION=1.4.0; export TMPDIR=""$PWD/tmp_dir"". singularity run \; --nv -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}""-gpu \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=""${INPUT_DIR}""/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions=""${INPUT_DIR}""/idt_capture_novogene.grch38.bed \; --output_vcf=""${OUTPUT_DIR}""/HG003.output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/HG003.output.g.vcf.gz \; --intermediate_results_dir=""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=28; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Yes, though only sequentially. **Any additional context:**. Based on the the documentation and by looking at the code, I _assume_ that the value for ```--num_shards``` is supposed to indicate how many chunks of sequence should be processed in parallel by the ```make_examples``` command, but this does not seem to be working for me. Any suggestions or ideas?. Thanks!; Dave",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/546:1333,test,test,1333,,https://github.com/google/deepvariant/issues/546,2,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: yes. **Describe the issue:**; I am running DeepVariant on a custom genome assembly using a hybrid of pacbio hifi and illumina short reads and it's been running for 17days. I wonder if something is wrong and is there a way to speed thing up? I am already using 30 shards. **Setup**; - Operating system: centOS 7; - DeepVariant version: 1.4.0 and 1.1.0 (tried version 1.1.0 been running for 17 days then I'm trying the 1.4.0 and it's been running for 3 days now); - Installation method (Docker, built from source, etc.): converted docker image to singularity image; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) I am using custom genome. **Steps to reproduce:**; - Command: ; ```; singularity exec ~/virtual_server/deepvariant.sif \; bash -c ""; /opt/deepvariant/bin/run_deepvariant \; --model_type=""HYBRID_PACBIO_ILLUMINA"" \; --ref=""${REF_DIR}""/scaffolds_FINAL.fasta \; --reads=""${INPUT_DIR}""/hybrid_hifi_Kapa_combined.bam \; --output_vcf=""${OUTPUT_DIR}""/A673.HiFi.Kapa.scaffolds_FINAL_hap1.deepvar.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/A673.HiFi.Kapa.scaffolds_FINAL_hap1.deepvar.g.vcf.gz \; --num_shards=$SLURM_CPUS_PER_TASK \; --logging_dir=""${OUTPUT_DIR}""/logs \; --intermediate_results_dir=""${OUTPUT_DIR}""/tmp""; ```; - Error trace: (if applicable) attached r_deepvariant_hybrid_2.txt. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? I was able to run quick start. I was also able to run DeepVariant on the same singularity system with pacbio HiFi reads only, using human reference genome hg19. **Any additional context:**; [r_deepvariant_hybrid_2_662510.txt](https://github.com/google/deepvariant/files/9853335/r_deepvariant_hybrid_2_662510.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/578:1314,log,logs,1314,,https://github.com/google/deepvariant/issues/578,3,"['log', 'test']","['logs', 'test']"
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated.; (A clear and concise description of what the issue is.). **Setup**; - Operating system:Mac OS ; - DeepVariant version: Latest; - Installation method (Docker, built from source, etc.): Docker; - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true; - Error trace: No error.(if applicable)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/561:445,Test,Test,445,,https://github.com/google/deepvariant/issues/561,1,['Test'],['Test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.); Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**; - Operating system: MacOs (Mac mini/ m1 chip); - DeepVariant version:1.4.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); The test data from GitHub; **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/545:729,test,test,729,,https://github.com/google/deepvariant/issues/545,3,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; According to the running log: the setlocale failed when trying to change LC_ALL to 'en_US.UTF-8'. **Setup**; - Operating system: CentOS7; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): docker pull ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) DNA seq. **Steps to reproduce:**; - Command: ; - singularity run \; -B ""/paedyl01/disk1/yangyxt,/usr/lib/locale"" \; --env LANG=""en_US.UTF-8"" \; --env LC_ALL=""C"" \; --env LANGUAGE=""en_US.UTF-8"" \; --env LC_CTYPE=""UTF-8"" \; ...... - Error trace: (if applicable); ![image](https://user-images.githubusercontent.com/40780228/190950415-84faaa5d-7371-42a7-9e13-f6caf53a3dea.png). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/566:142,log,log,142,,https://github.com/google/deepvariant/issues/566,3,"['log', 'test']","['log', 'test']"
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; Running singularity on the test data I get the following:; ```. OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata"". singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,HYBRID_PACBIO_ILLUMINA]**; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional.; --num_shards=20 \ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.**. My error: . ...; ...; Try --helpfull to get a list of all flags.; deepvariant.sing.sh: line 13: --ref=/mnt/scratch/username/software/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta: No such file or directory; deepvariant.sing.sh: line 18: make_examples: command not found; deepvariant.sing.sh: line 18: --num_shards=20: command not found. ```; I have checked and these paths and files exist and can be opened used the above links. . **Setup**; - Operating system: linux; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) test data tutorial. **Steps to reproduce:**; - Command: ; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/558:144,test,test,144,,https://github.com/google/deepvariant/issues/558,6,['test'],"['test', 'testdata']"
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:; Yes. **Describe the issue:**; I was following the quick start guide for running singularity on a gpu node. Initially, I encounter the dynamic cast failed error similar to #559 . After installing the google-nucleus package, I encountered this new error about protobuf package. I tried protobuf version 3.20.3 and 4.21.9, but the error message is the same. In order to run DeepVariant successfully, what additional packages (version) should I install besides cloning the singularity image?. **Setup**; - Operating system: ; - DeepVariant version: 1.4.0; - Installation method: singularity; - Type of data: quick start test; ; **Steps to reproduce:**; ; ```; SINGULARITY_TMPDIR=/scratch/midway3/weilu1/tmp SINGULARITY_CACHEDIR=/scratch/midway3/weilu1/cache singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_1.4.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1. INFO: Converting SIF file to temporary sandbox...; WARNING: underlay of /usr/bin/nvidia-smi required more than 50 (469) bind mounts; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 33, in <module>;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/580:708,test,test,708,,https://github.com/google/deepvariant/issues/580,1,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:; Yes; **Describe the issue:**; A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants.; (A clear and concise description of what the issue is.). **Setup**; - Operating system: CentOS7; - DeepVariant version: 1.4; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); - 150bp paired-end Illumina data. **Steps to reproduce:**; - Command: ; `/opt/deepvariant/bin/run_deepvariant \; --model_type=${model_type} \; --ref=""${ref_genome}"" \; --reads=""${bam_file}"" \; --make_examples_extra_args=""normalize_reads=true"" \; ${region_arg} \; --output_vcf=""${output_vcf}"" \; --output_gvcf=""${output_gvcf}"" \; --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \; --num_shards=${threads}`; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):; ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/612:1203,test,test,1203,,https://github.com/google/deepvariant/issues/612,2,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**; 'CUDA_ERROR_UNKNOWN' using DeepVariant GPU version. **Setup**; - Operating system: CentOS Linux release 7.4.1708 (Core), Linux 5.10.150-1.el7.x86_64; - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): singularity image build from dockerhub; - Type of data: nothing special that is unlike the case studies. **Steps to reproduce:**; - Command: /opt/deepvariant/bin/run_deepvariant --version; - Error trace: (if applicable); ```; tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error; tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: ; tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: ; tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: INVALID_ARGUMENT: expected %d.%d, %d.%d.%d, or %d.%d.%d.%d form for driver version; got ""1""; tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5; DeepVariant version 1.4.0; ```; The `hostname` is privacy. **Does the quick start test work on your system?:** No. **Any additional context:** ; The GPU is NVIDIA GeForce 3090; The GPU Driver Version: 520.61.05; The CUDA version in the host is V11.8.89 as followings:; ![image](https://user-images.githubusercontent.com/43125963/225341539-aa2ee3c6-c376-4758-a582-c8fd871b0508.png); It seems that the Deepvariant v1.4.0 in the singularity image has already installed CUDA v11.3. ; ![image](https://user-images.githubusercontent.com/43125963/225343337-d0924a9b-4b9d-4b03-848f-e8e9753eb377.png). I don't know whether it causes the program crash.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/619:1227,test,test,1227,,https://github.com/google/deepvariant/issues/619,1,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**; Having problem running deeptrio examples [https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md](https://github.com/google/deepvariant/blob/r1.5/docs/deeptrio-wgs-case-study.md) . **Setup**; - Operating system: Ubuntu 22.04, Docker 23+; - DeepVariant version: deeptrio-1.5.0-gpu; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). It always give ```Error: The directory ""/reference/GRCh38_no_alt_analysis_set.sdf"" already exists. Please remove it first or choose a different directory.``` even after I ensure that there are no GRCh38_no_alt_analysis_set.sdf exist in said directory; ```; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta""; ```. And that being said, this command also raises another error showing ```Error: An IO problem occurred: ""Not in GZIP format""```; ```; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/output"":""/output"" \; realtimegenomics/rtg-tools mendelian \; -i ""/output/HG002_trio_merged.vcf.gz"" \; -o ""/output/HG002_trio_annotated.output.vcf.gz"" \; --pedigree=/reference/trio.ped \; -t /reference/GRCh38_no_alt_analysis_set.sdf \; | tee output/deepvariant.input_rtg_output.txt; ``` . **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? **Quick start on single variant analysis is optimal**. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/632:1690,test,test,1690,,https://github.com/google/deepvariant/issues/632,2,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: Yes. **Describe the issue:**; Hello,. Using WES model, deepvariant calls the following variant in the vcf file:; ```; NC_000001.11	84574341	.	CAGCAGCGCT	C,T	.	.	.	GT:GQ:DP:AD:VAF:PL	1/0:3:97:25,45,26:0.463918,0.268041:36,0,47,0,16,44; ```. For this variant, the genotype is 1/0, meaning that one allele is REF, and the other allele is C. ; What is confusing is that deepvariant also calls a T however this is not referenced anywhere in the GT field. What is the point of calling T if it does not occur in the sample?. Here is the screenshot of the original alignment:. ![dv1](https://user-images.githubusercontent.com/22151692/223809460-b6cdeed1-e332-4014-879d-8ee44123f793.png). And here is the screenshot for the realigned reads for this position:. ![dv2](https://user-images.githubusercontent.com/22151692/223808774-d63a161c-e9f9-4e54-9546-2be3b9d5f492.png). **Setup**; - Operating system: Ubuntu; - DeepVariant version: 1.2.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/618:1289,test,test,1289,,https://github.com/google/deepvariant/issues/618,2,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/651:490,test,test,490,,https://github.com/google/deepvariant/issues/651,2,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Slurm based ; - DeepVariant version: deepvariant_1.5.0.sif; - Installation method : singularity image ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**; - Command:. projDir=/home1/***/***/deepvaraint/; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:720,test,test,720,,https://github.com/google/deepvariant/issues/717,3,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**; - Operating system: linux（Centos）; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.):singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**; - Error trace: (if applicable); <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/668:947,test,test,947,,https://github.com/google/deepvariant/issues/668,2,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; When calling a bam file aligned to hs37d5_decoy.fasta so the hs37d5 version with decoys. The produced vcfs contains no decoys despite there are such reads in the file but instead those calls were aligned to chrY. This is problematic in several ways. Thereby one can't distinguish between chromosome Y and decoy and it is impossible to determining the sex of the sample using just the vcf. . **Setup**; - Operating system: Linux 5.15.0-78-generic #85-Ubuntu SMP; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; 1. Align any WGS sequencing reads to hs37d5 with decoys. Maybe this can be repeated with any GRCh38 reference fasta containing decoys as well but I did not test that. ; 2. Run deepvariant with default parameters to call the file.; 3. Open the vcf and see if the file contains decoy regions or not and observe if such calls were catched by chromosome Y; 4. ; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/695:965,test,test,965,,https://github.com/google/deepvariant/issues/695,3,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:; Yes. Same error msgs were observed. But I was lunching deepvariant with singularity; **Describe the issue:**; (A clear and concise description of what the issue is.); The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity.; **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable); module load singularity; BIN_VERSION=""1.5.0""; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools""; INPUT_DIR=""${LABASE}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; OUTPUT_DIR=""${LABASE}/quickstart-output""; mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ls -1 ${INPUT_DIR}; mkdir -p ${OUTPUT_DIR}; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678:366,test,testing,366,,https://github.com/google/deepvariant/issues/678,2,['test'],"['testdata', 'testing']"
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). Hello All,. I have been testing ONT datasets on the HPC cluster to benchmark and optimize them. While using the mapped ONT BAM files from the HG002 and HG003 datasets from the UCSC studies, I observed that DeepVariant gets stuck at the make_examples stage. Even after 24 hours, it remains in the same stage which is unsual. I would appreciate your input on this issue. **Setup**; - Operating system: Linux, HPC cluster; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) ; -ONT : https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/HG002_GRCh38_ONT-UL_UCSC_20200508.phased.bam; reference -hg38 . **Steps to reproduce:**; - Command: . apptainer exec ; --bind Deepvariant/HG002_HG003_1.5.0 deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant ; --model_type ONT_R104 ; --ref Homo_sapiens_assembly38.fasta ; --reads HG002_GRCh38_ONT-UL_UCSC_20200508.phased.bam ; --output_vcf HG002_chr1.output.vcf.gz ; --output_gvcf HG002_chr1.output.g.vcf.gz ; --regions chr1 --num_shards 56 --logging_dir chr1 ; --intermediate_results_dir chr1/intermediate_results . - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, it did work. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/856:200,test,testing,200,,https://github.com/google/deepvariant/issues/856,4,"['benchmark', 'test']","['benchmark', 'test', 'testing']"
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; Am getting the error as ""Fatal Python error: Segmentation fault"". **Setup**; - Operating system: Ubuntu 22.04.2 LTS ; - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): Docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Its a Pabcio CLR data. Read Input is provided in Fastq format and reference in FASTA format. . **Steps to reproduce:**; - Command: sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/RILWLs1.fasta \; --reads=/input/Out.fastq \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=15. - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Yes. Test data works fine. ; ![Screenshot from 2024-04-17 12-24-22](https://github.com/google/deepvariant/assets/68117296/41ac66ff-ff52-493f-b18f-f017921caa86). Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/807:1028,test,test,1028,,https://github.com/google/deepvariant/issues/807,3,"['Test', 'test']","['Test', 'test']"
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; I am having some errors while fetching variants from chrX, Could you please have a look I added a error.txt file for reference. ; Thank you. > INVALID_ARGUMENT: Couldn't fetch bases for reference_name: ""chrX"" start: 14000 end: 15000. **Setup**; - Operating system:linux; - DeepVariant version:latest; - Installation method (Docker, built from source, etc.):udocker; - Type of data: test file for checking Calling variants in non-autosomal contigs [this](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-xy-calling-case-study.md). **Steps to reproduce:**; - Command:; BIN_VERSION=""1.6.1"". ```; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002.pfda_challenge.grch38.chrXY.bam""; THREADS=$(nproc); REGION=""chrX chrY""; HAPLOID_CONTIGS=""chrX,chrY""; PAR_BED=""GRCh38_PAR.bed"". udocker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLOID_CONTIGS}"" \; --par_regions_bed ""${INPUT_DIR}/${PAR_BED}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}"" ; ```. - Error trace: ; ; [error.txt](https://github.com/user-attachments/files/16281125/error.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/853:501,test,test,501,,https://github.com/google/deepvariant/issues/853,1,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; I am running deepvariant 1.6.1 through singularity (apptainer) on both WGS and RNAseq bams. The WGS bam was much larger in file size, but was processed much more quickly than the RNAseq bams produced by STAR. ; Because deepvariant 1.6.1 does not support an rnaseq model, so I just ran the WES model on it, providing a BED file containing all regions with at least 3X read depth. Here is the script I used:. `sID=$1 #sample ID; sBAM=$2 #full path to BAM; REF=$3 #full path to fasta ref; CPU=$4 #number of CPUs to use. module load apptainer/1.2.5; module load clusterbasics; module load samtools; module load bedtools. OUTPUT_DIR=./output/$sID. mkdir -p $OUTPUT_DIR; mkdir -p ./tmp; export TMPDIR=`realpath ./tmp`. if [ ! -f $sBAM.bai ]; then; echo producing bai index for $sBAM; samtools index $sBAM; fi. if [ ! -f ""${OUTPUT_DIR}/dv.log"" ];then; bedtools coverage -g genome.file -sorted -d -a genome.bed -b ""$sBAM"" | awk '{if ($5>=3) print $1""\t""($4-1)""\t""$4""\t""$5}' | bedtools merge -d 1 -c 4 -o mean -i - > ${OUTPUT_DIR}/cov3x.bed; fi. apptainer run -B /public:/public,/public3:/public3,/public2:/public2,/fast3:/fast3,/public4:/public4 \; /public4/software/deepvariant/1.6.1/cpuver/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --make_examples_extra_args=""normalize_reads=true"" \; --model_type=WES \; --ref=$REF \; --reads=""$sBAM"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --regions=""${OUTPUT_DIR}/cov3x.bed"" \; --num_shards=$CPU > ${OUTPUT_DIR}/dv.log 2>&1. `. Inspecting the tail of the log, it appears that the program gets stuck at the make_examples step, with many threads reporting finding 0 examples:; 'I0812 17:25:00.705988 139682501986112 make_examples_core.py:301] Task 14/32: Overhead for preparing inputs: 270 seconds; I0812 17:25:00.763086 139682501986112 make_examples_core.py:301] Task 14/32: 0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/867:951,log,log,951,,https://github.com/google/deepvariant/issues/867,1,['log'],['log']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; I tried to test run deepvariant following the quick-start guide at https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md and I got `Fatal Python error: Segmentation fault`. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: r1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: exact same data in the quick start guide. **Steps to reproduce:**; - Command:; ``` ; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; deepvbuild:latest \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```; - Error trace:; ```; I0906 02:45:46.585311 275767425675280 run_deepvariant.py:519] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0906 02:45:51.909050 257960059396112 genomics_reader.py:222] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0906 02:45:51.913105 257960059396112 make_examples_core.py:301] Preparing inputs; I0906 02:45:51.913431 257960059396112 genomics_reader.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879:130,test,test,130,,https://github.com/google/deepvariant/issues/879,1,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: 1.6; - Installation method (Docker, built from source, etc.): singularity image built form Docker Hub; - Type of data: bacteria whole genome. **Steps to reproduce:**; - Command:; smakemake pipeline; rule run_deepvariant:; output:; vcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.vcf.gz"",; gvcf = ""../results/deepVariant/{dataset}/{sample}/vcf/{sample}.deepVariant.g.vcf.gz""; input:; reference_fasta = ""/project/databases/bacteroides_genome/reference_genomic.fna"",; reads = rules.sam2bam.output.sorted_bam; params:; inter_dir = ""../../results/deepVariant/{dataset}/{sample}/intermediate"",; log_dir = ""../../results/deepVariant/{dataset}/{sample}/log"",; work_dir = ""/project/"",; deepvariant = ""/project/software/deepVariant.sif""; shell:; """"""; module load singularity/3.7.0; singularity exec -B {params.work_dir} {params.deepvariant} /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref={input.reference_fasta} \; --reads={input.reads} \; --output_vcf={output.vcf} \; --output_gvcf={output.vcf} \; --make_examples_extra_args --channels=insert_size \; --intermediate_results_dir {params.inter_dir} \; --num_shards=6 \; --logging_dir={params.log_dir}; """"""; - Error trace: ; ***** Running the command:*****; time /opt/deepvariant/bin/vcf_stats_report --input_vcf ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz"" --outfile_base ""../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant"". I0626 19:01:30.369722 139699125458752 genomics_reader.py:222] Reading ../results/deepVariant/KO_PV/<sample_name>/vcf/<sample_name>.deepVariant.vcf.gz with NativeVcfReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_xq721o6r/runfiles/com_google_deepvariant/deepvariant/vcf_stats_report.py"", line 103, in <module>; tf",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/839:869,log,log,869,,https://github.com/google/deepvariant/issues/839,1,['log'],['log']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**; (A clear and concise description of what the issue is.); `run_deepvariant` is erroring out in the `postprocess_variants` step. **Setup**; - Operating system: Running inside docker image - `google/deepvariant:1.6.0-gpu`; - DeepVariant version: `1.6.0`; - Installation method (Docker, built from source, etc.): Docker image - `google/deepvariant:1.6.0-gpu`; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: Running the quickstart cmd --; ```; /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/opt/deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/opt/deepvariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/opt/deepvariant/quickstart-output/output.vcf.gz --output_gvcf=/opt/deepvariant/quickstart-output/output.g.vcf.gz --intermediate_results_dir /opt/deepvariant/quickstart-output/intermediate_results_dir --num_shards=1 --verbosity=2; ```. - Error trace: (if applicable) In the `postprocess_variants` step; ```; ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/opt/deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/opt/deepvariant/quickstart-output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/opt/deepvariant/quickstart-output/output.vcf.gz"" --cpus ""1"" --gvcf_outfile ""/opt/deepvariant/quickstart-output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/opt/deepvariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"". 2024-10-31 20:36:34.101345: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/901:750,test,testdata,750,,https://github.com/google/deepvariant/issues/901,2,['test'],['testdata']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes; **Describe the issue:**; This isn't a code problem, but rather a documentation issue. I've run DeepVariant via your docker with success. To integrate it with our project I would like to install it via conda. I was able to do that but it isn't clear how to run deep variant. Do you have documentation/examples of what commands to send? . When using docker, we invoke the google/deepvariant:1.6.1 image and send it the command ""/opt/deepvariant/bin/run_deepvariant"" with appropriate arguments. What do we run when using conda? . Note the docs/deepvariant-quick-start.md has examples for docker (very useful and they work with our data) but nothing for conda. **Setup**; - Operating system: linux; - DeepVariant version: 1.5.0 (latest from conda); - Installation method (Docker, built from source, etc.): conda; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. Do you have plans to update conda with the latest deepvariant version? It is still at 1.5.0. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/865:1110,test,test,1110,,https://github.com/google/deepvariant/issues/865,2,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/743:490,test,test,490,,https://github.com/google/deepvariant/issues/743,2,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**; I am struggling to get DeepTrio to run to completion on a small dataset. It completes at the end of call_variants.py but my system just collapses when at postprocess_variants.; Through using --dry_run=true, I'm able to keep going only after being sufficiently confident the last step has completed without error.; So in short, is it possible to re-run the wrapper command and have the analysis pipeline pick up where it left off? . **Setup**; - Operating system: Rocky Linux 8; - DeepVariant version: 1.6; - Installation method (Docker, built from source, etc.): through Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) illumina, 151bp, same reference as case studies; - RAM 64 GB; - CPUs 32 (c6i.8xlarge). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?** Yes they do. they complete because they are small. ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; Unfortunately, i cant run it on g4dn.8xlarge available to me since that EC2 running Amazon Linux 2, and GPU DeepVariant seems to need Ubuntu.; In short, a ""step_x_completed"" sentinel file at end of each step would be great IMO. . Thanks,; -Daniel",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/749:984,test,test,984,,https://github.com/google/deepvariant/issues/749,2,['test'],['test']
Testability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**; When variant is not detected, the program will freeze in the last step；. **Setup**; - Operating system:Centos7.6; - DeepVariant version: 1.6 ; - Installation method (Docker, built from source, etc.): singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) PACBIO-SMART；A reference sequence for a normal person；. **Steps to reproduce:**; - Command: /bin/singularity run -B /work/:/work/ /work/deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --ref=human_geneA_reference.fa --reads=reference.align.bam --output_vcf=out.vcf --output_gvcf=out.gvcf --num_shards=32; - Error trace: Last line： I0119 11:43:53.450599 47012502976320 call_variants.py:623] Complete: call_variants（Stuck at this step）. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. [deepvariant_1.6.pdf](https://github.com/google/deepvariant/files/13986125/deepvariant_1.6.pdf)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/764:921,test,test,921,,https://github.com/google/deepvariant/issues/764,2,['test'],['test']
Testability,"**ISSUE**; First of all, I found DeepVariant to be a very good and innovative tool. I'm considering including it in my exome analysis pipeline. I followed the tutorial (DeepVariant worked correctly with the Complete Genomics model), and I created my own model using Genome in a Bottle samples. To do this, I sequenced the same reference sample three times to use each BAM file for training, validation, and testing. I didn't encounter any errors during the model creation process, but when I tried to test it, the process got stuck at the call_variants step. **Setup**; - Operating system: Ubuntu 22.04.4 LTS; - DeepVariant version:1.6.1; - Installation method:docker; - Type of data: MGI DNBSEQ 400, exome sequencing. **Steps to reproduce:**; - Command:; _Create examples for trainning set_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" make_examples --mode training --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/26_r_groups.bam"" --examples ""/output/training_set.gz"" --truth_variants ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed""`; _Create examples for validation set_; `sudo docker run -v ""${PWD}/input"":""/input"" -v ""${PWD}/REF"":""/ref"" -v ""${PWD}""/output:""/output"" google/deepvariant:""1.6.1"" make_examples --mode training --ref ""/ref/GRCh38.p14.genome.fa"" --reads ""/input/27_r_groups.bam"" --examples ""/output/validation_set.gz"" --truth_variants ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.vcf.gz"" --confident_regions ""/ref/HG001_GRCh38_1_22_v4.2.1_benchmark_ROCHE.bed"" `; _Trainning Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=output/training_set.gz --output_pattern_prefix=""output/training_shuffled"" --output_dataset_name=""26"" --output_dataset_config_pbtxt=""output/training.pbtxt"" --job_name=shuffle-tfrecords`; _Validation Shuffling_; `python3 scripts/shuffle_tfrecords_beam.py --input_pattern_list=",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:407,test,testing,407,,https://github.com/google/deepvariant/issues/869,2,['test'],"['test', 'testing']"
Testability,"**Issue**; I am using the docker you provided, while working on a remote machine.; Using Pycharm Professional's Services tab, I configured my interpreter to run the code I have on my local clone of the entire git. ; I am trying to run the ""make_examples.py"" file line-by-line, to understand it better. The entire clone is on my remote machine, and it runs with the docker container's interpreter. When I debug the code, there are many unresolved references. ; Some examples are:; `from third_party.nucleus.protos import reads_pb2`; `from deepvariant.protos import deepvariant_pb2`; `from deepvariant.python import pileup_image_native`; `from deepvariant.protos import deepvariant_pb2`; `from deepvariant.python import allelecounter`; `from third_party.nucleus.io.python import hts_verbose`; ...; I looked for these files, and they aren't there.; I understand there is something very basic that I misunderstand, so thanks in advance for your patience!. **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 1.0.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: irrelevant. **Does the quick start test work on your system?**; I have succeeded in running the quick start example on the remote machine, through the terminal.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/359:1143,test,test,1143,,https://github.com/google/deepvariant/issues/359,1,['test'],['test']
Testability,"**hello,; I tested DeepVariant 1.5.0 on pacbio public data.; The data link is:; https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG003_NA24149_father/PacBio_MtSinai_NIST/PacBio_minimap2_bam/HG003_PacBio_GRCh37.bam; But it failed.; The log :** ; 2023-04-13 03:58:10.677743: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0413 03:58:12.505982 140621665130304 run_deepvariant.py:364] Re-using the directory for intermediate results in intermediate_results_dir; ***** Intermediate results will be written to intermediate_results_dir in docker. ****; ***** Running the command:*****; time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/sfs-GCS/ann-BIstorage/DB/data/sentieon/hs37d5/hs37d5.fasta"" --reads ""HG003_PacBio_GRCh37.bam"" --examples ""intermediate_results_dir/make_examples.tfrecord@32.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --gvcf ""intermediate_results_dir/gvcf.tfrecord@32.gz"" --max_reads_per_partition ""600"" --min_mapping_quality ""1"" --parse_sam_aux_fields --partition_size ""25000"" --phase_reads --pileup_image_width ""199"" --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels ""0.12"" --task {}; ; 2023-04-13 03:58:35.887616: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-04-13 03:58:36.520424: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimiz",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:12,test,tested,12,,https://github.com/google/deepvariant/issues/631,2,"['log', 'test']","['log', 'tested']"
Testability,", line 224, in main; make_examples_core.make_examples_runner(options); File ""/flashscratch/kimkw/tmp/Bazel.runfiles_hubgarxp/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847, in make_examples_runner; region_example_shape = region_processor.writes_examples_in_region(; File ""/flashscratch/kimkw/tmp/Bazel.runfiles_hubgarxp/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566, in writes_examples_in_region; for example in self.create_pileup_examples(; File ""/flashscratch/kimkw/tmp/Bazel.runfiles_hubgarxp/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2333, in create_pileup_examples; pileup_images = self.pic.create_pileup_images(; File ""/flashscratch/kimkw/tmp/Bazel.runfiles_hubgarxp/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 602, in create_pileup_images; pileup = _pileup_for_pair_of_alts(alts); File ""/flashscratch/kimkw/tmp/Bazel.runfiles_hubgarxp/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 544, in _pileup_for_pair_of_alts; ref_image = self.build_pileup(; File ""/flashscratch/kimkw/tmp/Bazel.runfiles_hubgarxp/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 490, in build_pileup; build_pileup_for_one_sample(reads_for_samples[i], sample); File ""/flashscratch/kimkw/tmp/Bazel.runfiles_hubgarxp/runfiles/com_google_deepvariant/deepvariant/pileup_image.py"", line 400, in build_pileup_for_one_sample; self._encoder.encode_reference(refbases); ImportError: numpy.core.multiarray failed to import; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref ./quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /flashscratch/kimkw/tmp/tmppin2lwy5/make_examples.tfrecord@1.gz --channels insert_size --gvcf /flashscratch/kimkw/tmp/tmppin2lwy5/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m6.183s; user 0m3.271s; sys 0m1.140s. ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/746:5329,test,testdata,5329,,https://github.com/google/deepvariant/issues/746,2,['test'],['testdata']
Testability,"- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@$",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:2041,test,test,2041,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```; ## Run `call_variants`; echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log.""; ( time sudo docker run \; -v ""${BASE}"":""${BASE}"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) 2>&1 | tee ""${LOG_DIR}/call_variants.log""; echo ""Done.""; echo; ```. Is there some magic pattern recognition that knows to look for files o",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:3080,log,log,3080,,https://github.com/google/deepvariant/issues/151,1,['log'],['log']
Testability,"--------|------|---------|------|--------|------|------|----------|----------|----------|----------|-------------|-------------|-------------|-------------|; | HG002 | SNP | PASS | 3365127 | 3361925 | 3202 | 4312006 | 7891 | 942045 | 1197 | 719 | 0.999048 | 0.997658 | 0.21847 | 0.998353 | 2.100128487 | 1.823839956 | 1.581195853 | 1.50423718 |; | HG003 | INDEL | PASS | 504501 | 501414 | 3087 | 1009147 | 3989 | 471526 | 1814 | 1831 | 0.993881 | 0.99258 | 0.467252 | 0.99323 | | | 1.489759281 | 2.02565724 |; | HG003 | SNP | PASS | 3327495 | 3323623 | 3872 | 4265460 | 4910 | 936912 | 1118 | 617 | 0.998836 | 0.998525 | 0.219651 | 0.998681 | 2.102574954 | 1.831128594 | 1.535137772 | 1.484295493 |; | HG004 | INDEL | PASS | 510519 | 507376 | 3143 | 1013737 | 4102 | 469356 | 1887 | 1729 | 0.993844 | 0.992465 | 0.462996 | 0.993154 | | | 1.516130736 | 2.075927402 |. analysising result：Using the same test data as the scattered samples, it can be found that the variation detection results of the HG002/3/4 family sample are relatively poor when tested using the GIAB standard set，but I don't understand the reason for this difference. **Setup**; - Operating system: image of singularity, transforming from docker image of deeptrio-1.4.0; - DeepVariant version:deeptrio-1.4.0; - Installation method (Docker, built from source, etc.):Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); HiFi data,those data download links follows:; * HG002:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/15kb/;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG002/hpp_HG002_NA24385_son_v1/PacBio_HiFi/20kb/; * HG003:https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/Google_15kb;https://s3-us-west-2.amazonaws.com/human-pangenomics/NHGRI_UCSC_panel/HG003/PacBio_HiFi/HudsonAlpha_15kb; * HG004:https://s3-us-west-2.amazonaws.com/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689:6270,test,test,6270,,https://github.com/google/deepvariant/issues/689,2,['test'],"['test', 'tested']"
Testability,--track_ref_reads --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.08 --task 39; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /work/09505/s223885/data/common/human/hg38bundle/Homo_sapiens_assembly38.fasta --reads /scratch/09505/s223885/ONT_WGS/HH/FL9-1/FL9-1.chr10.bam --examples /scratch/09505/s223885/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/intermediate_results/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /scratch/09505/s223885/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/intermediate_results/gvcf.tfrecord@64.gz --max_reads_per_partition 600 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.08 --task 52; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /work/09505/s223885/data/common/human/hg38bundle/Homo_sapiens_assembly38.fasta --reads /scratch/09505/s223885/ONT_WGS/HH/FL9-1/FL9-1.chr10.bam --examples /scratch/09505/s223885/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/intermediate_results/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /scratch/09505/s223885/ONT_WGS/HH/FL9-1_deepvaraint/FL9-1/chr10/intermediate_results/gvcf.tfrecord@64.gz --max_reads_per_partition 600 --min_mapping_quality 5 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.08 --task 57. real	0m4.925s; user	0m4.781s; sys	0m19.092s. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; Did not face this error in DeepVariant version: 1.5.0,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/854:15386,test,test,15386,,https://github.com/google/deepvariant/issues/854,2,['test'],['test']
Testability,-bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds.; Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:1507,test,test,1507,,https://github.com/google/deepvariant/issues/717,1,['test'],['test']
Testability,"-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam file? The .fai file was created from the .fa file using samtools index command. . ValueError: Reference contigs span 3270284521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""CM000663.2"" is 248956422 bp and IS MISSING, ""KI270706.1"" is 175055 bp and IS MISSING, ""KI270707.1"" is 32032 bp and IS MISSING, ""KI270708.1"" is 127682 bp and IS MISSING, ""KI270709.1"" is 66860 bp and IS MISSING, ""KI270710.1"" is 40176 bp and IS MISSING... Any feedback would be appreciated. Thanks, -Matt. [staging_folder1_logs_make_exa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/225:1605,log,logging,1605,,https://github.com/google/deepvariant/issues/225,1,['log'],['logging']
Testability,"-intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds.; Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Launcher: Job 6 completed in 0 seconds.; Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:2568,test,test,2568,,https://github.com/google/deepvariant/issues/717,2,['test'],['test']
Testability,"-output_vcf_parent1=""/out_dir/199710.output.vcf.gz"" \; --output_vcf_parent2=""/out_dir/199718.output.vcf.gz"" \; --logging_dir=""/out_dir/${trioName}/"" \; --num_shards=$(nproc) \; --vcf_stats_report=true \; ```. - Error trace: (if applicable); ```; I0307 04:23:48.405982 46912496319168 call_variants.py:458] Processed 9497443 examples in 18550 batches [0.321 sec per 100]; I0307 04:23:48.406211 46912496319168 call_variants.py:461] Done calling variants from a total of 9497443 examples.; real	507m54.839s; user	17892m22.565s; sys	172m54.026s; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_child.tfrecord.gz"" --outfile ""/out_dir/199713.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_child.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_child.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent1.tfrecord.gz"" --outfile ""/out_dir/199710.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent1.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent1.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent2.tfrecord.gz"" --outfile ""/out_dir/199718.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent2.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent2.log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires bot",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/429:2418,log,log,2418,,https://github.com/google/deepvariant/issues/429,1,['log'],['log']
Testability,"-v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. Then I modified the shell script to run my sample ; > I'm using the custom ref - included fa, fai, .gz. gzi files in the input dir. >RHA; CTGGG ..... > I aligned my reads to the ref and extracted only mapped paired-end reads. @HD VN:1.6 SO:coordinate; @SQ SN:RHA LN:911; @PG ID:bwa PN:bwa VN:0.7.17-r1194-dirty CL:bwa mem -M -t 10 /..... > now when I run the docker tool, I get the following error message. 2019-09-24 15:23:14.405094: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring:; I0924 15:23:14.405213 139913087186688 genomics_reader.py:218] Reading /input/test.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 316, in default_options; sample_name = extract_sample_name_from_sam_reader(sam_reader); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 458, in extract_sample_name_from_sam_reader; 'No non-empty sample name found in the input reads. Please provide the '; ValueError: No non-empty sample name found in the input reads. Please provide the n",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/222:1249,test,test,1249,,https://github.com/google/deepvariant/issues/222,1,['test'],['test']
Testability,". Estimated time remaining: 30s; > Polishing 92% complete (1235/1342). Estimated time remaining: 14s; > Starting merge; > Merging results from 1342 chunks.; > Merging took 7s; > Merge cleanup took 0s; Separated reads with divisions: H1 475116, H2 453908, and H0 159194; > Wrote haplotyped bams in 1m 43s; > Finished phasing in 18m 46s. real	18m47.373s; user	245m51.236s; sys	2m8.903s; mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file; [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND; -------; time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log; -------; [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED; [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET.; [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305; [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/; [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:; [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']; [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376; [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS; [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]; [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 M",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:9747,log,logs,9747,,https://github.com/google/deepvariant/issues/491,1,['log'],['logs']
Testability,". Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error?. The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python; def test_make_labeler_ref(self, candidates, truths, expected_start,; expected_end, bufsize):; expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader; labeler = _make_labeler(); labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(; ranges.make_range('20', expected_start, expected_end)); self.assertEqual(labeler_ref.start, expected_start); self.assertEqual(labeler_ref.end, expected_end); self.assertEqual(; labeler_ref.bases(expected_start, expected_end), expected_bases); ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python; def make_labeler_ref(self, candidates, true_variants, bufsize=20):; all_variants = candidates + true_variants; contig = all_variants[0].reference_name; start = min(x.start for x in all_variants); end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object; ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]; ## change the above type to int becomes ""1"", then the region.end will be 1 to cause test fail; contig_nbp = self._ref_reader.contig(contig).n_bases ; region = ranges.make_range(contig, max(start - 1, 0),; min(end + bufsize, contig_nbp)); ref_bases = self._ref_reader.query(region); return ReferenceRegion(ref_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154:1170,assert,assertEqual,1170,,https://github.com/google/deepvariant/issues/154,1,['assert'],['assertEqual']
Testability,".04 (bionic); - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): SINGULARITY sif made as follows:; BIN_VERSION=""1.5.0""; singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}""; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); EXAMPLE DATA PROVIDED. **Steps to reproduce:**; - Command:. INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ \; /fh/fast/furlan_s/grp/sifs/deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz. - Error trace: (if applicable) SEE BELOW. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. YES THIS IS WITH THE QUICK START EXAMPLE. **Any additional context:**. Message:. 2023-05-02 14:40:43.757041: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0502 14:40:56.961649 140501830911808 run_deepvariant.py:364] Re-using the directory for intermediate results in /fh/scratch/delete90/furlan_s/targ_reseq/230117_Sami/AML_1101_merge/tmp_dir/tmpzz53zv8p. ***** Intermediate results will be written to /fh/scratch/delete90/furlan_s/targ_reseq/230117_Sami/AML_1101_merge/tmp_dir/tmpzz53zv8p in docker. ****. ***** Running the command:*****; time ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640:1582,test,test,1582,,https://github.com/google/deepvariant/issues/640,2,['test'],['test']
Testability,".pb; > -rw-r----- 1 haley.arnold proj-pbarc 80 Aug 6 22:51 example_info.json. Here is the error log file: . > 2024-08-09 20:05:25.101938: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; > To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; > I0809 20:05:40.093672 139993880950592 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmp4wzl_5p3; > Traceback (most recent call last):; > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>; app.run(main); > File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); > File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 693, in main; commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir); > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 572, in create_all_commands_and_logfiles; check_flags(); > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 544, in check_flags; raise RuntimeError(; > RuntimeError: The model files /90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/modeltrainout/2fullindividualmodel/checkpoints/ckpt-14902* do not exist. Potentially relevant issue: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#why-cant-it-find-one-of-the-input-files-eg-could-not-open. Can someone please help me figure out what's going on? The link provided showed a different set of files than the ones I have. Am I missing files? Is something upstream not functioning properly? I have trained models before, with the same kinds out output, and have been able to test them before. What am I missing? . Thank you for your help!. Best,; Haley Arnold",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/866:3862,test,test,3862,,https://github.com/google/deepvariant/issues/866,1,['test'],['test']
Testability,".reads) as sam_reader:; File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_a5JjgU/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open newtest.bam. real	0m6.581s; user	0m4.128s; sys	0m1.476s; Traceback (most recent call last):; File ""run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Reference.fasta"" --reads ""newtest.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}' returned non-zero exit status 1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Reference.fasta"" --reads ""newtest.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --task {}. data newtest.bam Reference Reference.fasta Test Test.bam. ------------------; (program exited with code: 0); Press return to continue. Thx for helping me",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/420:3077,Test,Test,3077,,https://github.com/google/deepvariant/issues/420,2,['Test'],['Test']
Testability,/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds.; Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/*,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:1593,test,test,1593,,https://github.com/google/deepvariant/issues/717,2,['test'],['test']
Testability,"/input/wes2_38_3col.sorted.bed --task 2. I have ran the following command with a successful docker installation:; 	BIN_VERSION=""1.2.0"". 	sudo docker run \; 	-v ""${PWD}/input"":""/input"" \; 	-v ""${PWD}/output"":""/output"" \; 	-v ""${PWD}/reference"":""/reference"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type WES \; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta \; 	--reads /input/wes_deepvarfast_38.sorted.bam \; 	--regions /input/wes2_38_3col.sorted.bed \; 	--output_vcf /output/output_38.vcf.gz \; 	--output_gvcf /output/output_38.g.vcf.gz \; 	--num_shards=8 \; 	--intermediate_results_dir /output/intermediate_results_dir; with bam and bed files I've created of my own sample (paired end sequencing result of a human genome). The alignment of the bam file was successful (used bwa and samtools) and created the bed file out of the bam file by bedtools. . I've further checked FAQ and tried to run the following command, to better understand what is the error or where it fails:; 	BIN_VERSION=""1.2.0"". 	sudo docker run; 	-v ""${PWD}/input"":""/input""; 	-v ""${PWD}/output"":""/output""; 	-v ""${PWD}/reference"":""/reference""; 	google/deepvariant:""${BIN_VERSION}""; 	/opt/deepvariant/bin/make_examples; 	--mode calling; 	--ref /reference/GRCh38_no_alt_analysis_set.fasta; 	--reads /input/wes_deepvarfast_38.sorted.bam; 	--examples ""/output/make_examples.tfrecord@1.gz""; 	--gvcf ""/output/gvcf.tfrecord@1.gz""; 	--regions ""/input/wes2_38_3col.sorted.bed"" \. However I get no error message, some lines of this kind are printed: ""Adding interval chr1:1523790-1523940 to intervaltree"" and than it finishes without creating any files. Any Idea of what happens and how can I make deepvariant work on my sample and create a vcf file?. (**Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes it works)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/483:2305,test,test,2305,,https://github.com/google/deepvariant/issues/483,2,['test'],['test']
Testability,"/make_examples_core.py"", line 2019, in processing_regions_from_options; ref_contigs = fasta.IndexedFastaReader(; File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__; self._reader = reference.IndexedFastaReader.from_file(; ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Yes, the quick test run as normal.; ```. 3. reference index does; ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. *************",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:7244,test,test,7244,,https://github.com/google/deepvariant/issues/653,1,['test'],['test']
Testability,"/r0.9/docs/deepvariant-quick-start.md . They show using sudo to run the docker. I do not have sudo permission on this machine. The machine is set up to use the group permission. I do not think this is the issue. . Any suggestions would be greatly appreciated. Andy. ```; (base) -bash-4.2$ groups; giuser kimlab docker; (base) -bash-4.2$ ; ```. ```; docker run -v /public/home/dkim142/quickstart-testdata:/input \; -v /public/home/dkim142/quickstart-output:/output google/deepvariant:0.9.0 \; /opt/deepvariant/bin/run_deepvariant --model_type=WGS \; --ref=/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads=/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions chr20:10,000,000-10,010,000 \; --output_vcf=/public/home/dkim142/quickstart-output/output.vcf.gz \; --output_gvcf=/public/home/dkim142/quickstart-output/output.g.vcf.gz \; --num_shards=1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling \; --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2019-12-08 02:35:44.105906: F tensorflow/core/platform/cpu_feature_guard.cc:37] ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m1.146s; user	0m1.709s; sys	0m4.191s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/248:1289,test,testdata,1289,,https://github.com/google/deepvariant/issues/248,1,['test'],['testdata']
Testability,"0-36-generic linux-image-4.13.0-36-generic linux-image-extra-4.13.0-36-generic; Use 'sudo apt autoremove' to remove them.; 0 upgraded, 0 newly installed, 0 to remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 24日 星期五 19:54:09 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: numpy==1.14 in /usr/local/lib/python2.7/dist-packages (1.14.0); Requirement already satisfied: requests>=2.18 in /usr/local/lib/python2.7/dist-packages (2.19.1); Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2018.8.13); Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (3.0.4); Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:13858,mock,mock,13858,,https://github.com/google/deepvariant/issues/89,1,['mock'],['mock']
Testability,"0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/c_elegans.PRJEB28388.WS274.genomic.fa \; --reads=""${INPUT_DIR}""/maddog_bam_trim_bwaMEM_sort_dedupped.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. The error looks like:. ```; [31mFATAL: [0m Image file already exists: ""deepvariant_0.10.0.sif"" - will not overwrite; time=""2020-03-31T17:40:13-07:00"" level=warning msg=""\""/run/user/3019658\"" dire",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:1701,log,logs,1701,,https://github.com/google/deepvariant/issues/292,1,['log'],['logs']
Testability,"0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?; [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]; ```. This is the script that I am running DeepVariant:. ```; OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant; REF=/mnt/efs-genome/Ref/hg19.gatk.fasta; BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam; MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs; N_SHARDS=4. #mkdir -p ""${LOGDIR}""; #time seq 0 $((N_SHARDS-1)) | \; # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \; # sudo docker run \; # -v /mnt/efs-genome:/mnt/efs-genome \; # gcr.io/deepvariant-docker/deepvariant \; # /opt/deepvariant/bin/make_examples \; # --mode calling \; # --ref ""${REF}"" \; # --reads ""${BAM}"" \; # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \; -v /mnt/efs-genome:/mnt/efs-genome \; gcr.io/deepvariant-docker/deepvariant \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""${MODEL}""; ```. Can you please help me troubleshoot?. I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,; Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:13077,LOG,LOGDIR,13077,,https://github.com/google/deepvariant/issues/166,4,"['LOG', 'log']","['LOGDIR', 'log']"
Testability,"07:17:31.684022 140029649073920 make_examples.py:1086] Writing examples to /home/chungtsai_su/quickstart-output/examples.tfrecord.gz; 2018-12-20 07:17:31.684869: I third_party/nucleus/io/sam_reader.cc:561] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2018-12-20 07:17:31.688126: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.688252 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.884236 140029649073920 make_examples.py:1119] Task 0: 6 candidates (6 examples) [0.20s elapsed]; I1220 07:17:33.209975 140029649073920 make_examples.py:1134] Writing MakeExamplesRunInfo to /home/chungtsai_su/quickstart-output/examples.tfrecord.gz.run_info.pbtxt; I1220 07:17:33.241107 140029649073920 make_examples.py:1137] Found 76 candidate variants; I1220 07:17:33.241497 140029649073920 make_examples.py:1138] Created 82 examples; ```. Also, the problem can be detected in test case. ```; //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s; //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s; //deepvariant/vendor:timer_test PASSED in 0.8s; //deepvariant:make_examples_test FAILED in 2 out of 2 in 1.7s; Stats over 2 runs: max = 1.7s, min = 1.6s, avg = 1.7s, dev = 0.1s; /home/chungtsai_su/.cache/bazel/_bazel_chungtsai_su/959496e1d4e585c03b8886e389170de9/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /home/chungtsai_su/.cache/bazel/_bazel_chungtsai_su/959496e1d4e585c03b8886e389170de9/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test PASSED in 49.7s; Stats over 10 runs: max = 49.7s, min = 2.6s, avg = 9.1s, dev = 13.6s; //deepvariant:model_train_test PASSED in 127.0s; Stats over 10 runs: max = 127.0s, min = 2.7s, avg = 42.5s, dev = 47.3s. Executed 38 out of 38 tests: 37 tests ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131:5376,test,test,5376,,https://github.com/google/deepvariant/issues/131,1,['test'],['test']
Testability,"08322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont; [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED; [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND; -------; mkdir -p /cromwell_root/pepper_output; ; mkdir -p /cromwell_root/pepper_output/logs; ; mkdir -p /cromwell_root/pepper_output/intermediate_files;; -------; [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND; -------; time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log; -------; [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED.; [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING.; [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041; [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/; [11-03-2021 13:40:41] STEP 1: GENERATING IMAGES; [11-03-2021 13:40:41] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']; [11-03-2021 13:40:41] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895; [11-03-2021 13:40:41] STARTING THREAD: 0 FOR 483 INTERVALS; [11-03-2021 13:40:41] INFO: 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]; ...; [11-03-2021 13:42:49] INFO: 470/483 COMPLETE (97%) [ELAPSED TIME: 2 Min 8 Sec]; [11-03-2021 13:42:49] IN",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:2512,log,logs,2512,,https://github.com/google/deepvariant/issues/491,1,['log'],['logs']
Testability,"0p1mb.bam \; --regions chr20:10,000,000-10,010,000 \; --output_vcf=/public/home/dkim142/quickstart-output/output.vcf.gz \; --output_gvcf=/public/home/dkim142/quickstart-output/output.g.vcf.gz \; --num_shards=1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling \; --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2019-12-08 02:35:44.105906: F tensorflow/core/platform/cpu_feature_guard.cc:37] ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m1.146s; user	0m1.709s; sys	0m4.191s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 1; (base) -bash-4.2$ ; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/248:2557,test,testdata,2557,,https://github.com/google/deepvariant/issues/248,2,['test'],['testdata']
Testability,"1. How can i use deepvariant if I have built it from source?(By bazel-bin/deepvariant/*runfiles?); 2. If i want to change the source of deepvariant/pileup_image_native.cc, how can I test it after changing the source.(It means how can i rerun deepvariant if i change the source code, do i need to recompile and how?); Thank you for your help and patience!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/747:182,test,test,182,,https://github.com/google/deepvariant/issues/747,1,['test'],['test']
Testability,"1.2.0""; # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder; export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output ; export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata ; # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add; # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container; # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif); singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \; /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \; --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \; --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed; --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \; --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \; --call_variants_extra_args=""use_openvino=true"" \; --num_shards=$(nproc) \; --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \; --dry_run=true; ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/515:1830,test,testdata,1830,,https://github.com/google/deepvariant/issues/515,1,['test'],['testdata']
Testability,"16 run_deepvariant.py:519] Re-using the directory for intermediate results in /flashscratch/kimkw/tmp/tmppin2lwy5. ***** Intermediate results will be written to /flashscratch/kimkw/tmp/tmppin2lwy5 in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""./quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/flashscratch/kimkw/tmp/tmppin2lwy5/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/flashscratch/kimkw/tmp/tmppin2lwy5/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I1202 23:23:46.123890 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.133658 46912500266816 make_examples_core.py:301] Preparing inputs; I1202 23:23:46.139615 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.140348 46912500266816 make_examples_core.py:301] Common contigs are ['chr20']; I1202 23:23:46.141555 46912500266816 make_examples_core.py:301] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; I1202 23:23:46.150200 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.240882 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.241135 46912500266816 make_examples_core.py:301] Writing gvcf records to /flashscratch/kimkw/tmp/tmppin2lwy5/gvcf.tfrecord-00000-of-00001.gz; I1202 23:23:46.248160 46912500266816 make_examples_core.py:301] Writing examples to /flashscratch/kimkw/tmp/tmppin2lwy5/make_examples.tfrecord-00000-of-00001.gz; I1202 23:23",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/746:1713,test,testdata,1713,,https://github.com/google/deepvariant/issues/746,1,['test'],['testdata']
Testability,"199 --norealign_reads --regions chr17 --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 2. real 0m4.188s; user 0m3.295s; sys 0m0.885s`. **this is the form of bam files I have used:**; `molecule/22487242 0 chr1 11823 22 1S22=165N39=1X71=1X27=1X9=1X9=1X16=1D3=1X10=1X12=1X13=385N109=761N9=1I16=1X104=1X70=1X91=1I37=1X24=1X54=1D18=1X67=1X1=1X132=1X132=1X49=1X55=1X58= * 0 2590 GTAAACGAGATTGCCAGCACCGGGTGTCTGACTTCCAGCAACTGCTGGCCTGTGCCAGGGTGGAAGCTGAGCACTGGAGTGGAGTTTTCCTGTGGAGAGGAGCCATGCCTAGAGTGGGATGGGCCATTGTTCATATTCTGGCCCCTGTTGTCTGCATGTAACCTAATACCACGACCAGGCATGGGGGAAAGATTGGAGGAAGTTGAGTGAGAGGATCAACTTCTCTGACAACCTAGGCCAGTGTGTGGTGATGCCAGGCATGCCCTTCCCCAGCATCAGGTCTCCAGAGCTGCAGAAGACGACGGCCGACTTGGATCACACTCTTGTGAGTGTCCCCAGTGTTGCAGAGGCAGCTGCACCCACTGCCTGGCGCTGCGCCCTTCCTTTGCTCTGCCCGCTGGAGACGGTGTTTGTCATGGGCCTGGTCTGCAGGGATCCTGCTACAAAGGTGAAACCCAGGAGAGTGTGGAGTCCAGAGTGATGCCAGGACCCAGGCACAGGCATTAGTGCCCGTTGGAGAAAACAGGGGAATCCCGAAGAAATGGTGGGTCTTGGCCATCCGTGAGATCTTCCCAGGGCAGCTCCCCTCTGTGGAATCCAATCTGTCTTCCATCCTGCGTGGCCGAGGGCCAGGCTTCTCACTGGGGCCTCTGCAGGAGGCTGCCATTTGTCCTGCCCACCGTCTTAGAAGCGAGACGGAGCAGACTCATCTGCTACTGCCCTTTCTATAATAACTAAAGTTAGCTGCCCTGGACTATTCACCCCTAGTCTCAATTTAAAAAGATCCCCATGGCCACAGGGCCCCTGCCTGGGGGCTTGTCACCTCCCCCACCTTCTTCCTGAGTCACTTCTGCAGCCTTGCTCCCTAACCTGCCCCACAGCCTTGCCTGGATTTCTATCTCCCTGGCTTGGTGCCAGTTCCTCCAAGTCGATGGCACCTCCCTCCCTCTCAACCACTTGAGCAAACTCCAAGACATCTTCTTCCCCAACACCAGCAATTGTGCCAAGGGCCATTAGGCTCTCAGCATGACTATTTTTAGAGACCCCGTGTCTGTCACTGAAACCTTTTTTGTGGGAGACTATTCCTCCCATCTGCAACAGCTGCCCCTGCTGACGGCCCTTCTCTCCTCCCTCTCATCCCAGAGAAACAGGTCAGCTGGGAGCTCCTGCCCCCACTGCCTAGGGACCAACAGGGGCAGGAGGCAGTCACTGACCCCGAGAAGTTTGCATCCTGCACAGCTAGAGATCCTTTATTAAAAGCACACTGTTGGTTTCTGCTC * CB:Z:CCAACTCACATTGAAG XA:Z:XM-CB XM:Z:AGACAATCCGTA ic:i:1 im:Z:m84210_240422_080753_s1/229444864/ccs/5985_7265 is:i:1 it:Z:AGACAATCCGTACCAACTCACATTGAAG rc:i:1 RG:Z:e4927d21 zm:i:22487242 mg:f:98.1265 NM:i:24`. **Also, when I used test files, I got the final results without any errors**. All advice much appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/877:8356,test,test,8356,,https://github.com/google/deepvariant/issues/877,1,['test'],['test']
Testability,"2018年 08月 24日 星期五 19:54:09 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: numpy==1.14 in /usr/local/lib/python2.7/dist-packages (1.14.0); Requirement already satisfied: requests>=2.18 in /usr/local/lib/python2.7/dist-packages (2.19.1); Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2018.8.13); Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (3.0.4); Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (1.23); Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2.7); Requirement already satisfied: scipy==1.0 in /usr/local/lib/python2.7/dist-packages (1.0.0); Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python2.7/dist-packages (from scipy==1.0) (1.14.0); Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:14257,mock,mock,14257,,https://github.com/google/deepvariant/issues/89,1,['mock'],['mock']
Testability,"270744v1 168472 13 0; chrUn_KI270745v1 41891 0 0; chrUn_KI270746v1 66486 2 0; chrUn_KI270747v1 198735 2 0; chrUn_KI270748v1 93321 2 0; chrUn_KI270749v1 158759 0 0; chrUn_KI270750v1 148850 0 0; chrUn_KI270751v1 150742 4 0; chrUn_KI270752v1 27745 0 0; chrUn_KI270753v1 62944 1 0; chrUn_KI270754v1 40191 0 0; chrUn_KI270755v1 36723 0 0; chrUn_KI270756v1 79590 3 0; chrUn_KI270757v1 71251 2 0; chrUn_GL000214v1 137718 9 0; chrUn_KI270742v1 186739 9 0; chrUn_GL000216v2 176608 17 0; chrUn_GL000218v1 161147 3 0; chrEBV 171823 10 0; * 0 0 0; ```; - Error trace: (if applicable); ```; Fatal Python error: Segmentation fault. Current thread 0x00002b91584d7740 (most recent call first):; File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/very_sensitive_caller.py"", line 67 in get_candidate_positions; File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2047 in candidates_in_region; File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1734 in process; File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner; File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 424 in main; File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_n9w2sjmt/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 434 in <module>; ```; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Unfortunately I cannot run Docker on my environment. **Any additional context:**; The issue cannot be reproduced with `WES` model and Illumina WES data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/724:8400,test,test,8400,,https://github.com/google/deepvariant/issues/724,2,['test'],['test']
Testability,"29297437696 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0624 02:14:01.826225 47429297437696 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0624 02:14:01.954994 47429297437696 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --infile ""/output/intermediate_resu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465:1216,log,log,1216,,https://github.com/google/deepvariant/issues/465,1,['log'],['log']
Testability,"2: 89987; # class0: 33161; # class1: 24300. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 147448; ```. The training command looks like this:. ```; LR=0.001; BS=1024. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such training run:. ```; I1031 10:55:27.365902 140558597089024 logging_writer.py:48] [0] epoch=0, train/categorical_accuracy=0.91796875, train/categorical_crossentropy=0.6384725570678711, train/f1_het=0.7428571581840515, train/f1_homalt=0.964401364326477, train/f1_homref=0.902255654335022, train/f1_macro=0.; 8698380589485168, train/f1_micro=0.91796875, train/f1_weighted=0.9241795539855957, train/false_negatives=34.0, train/false_positives=14.0, train/learning_rate=9.999999747378752e-06, train/loss=0.6384731531143188, train/precision=0.9406779408454895, train/precision_het=0.702702701091; 7664, train/precision_homalt=0.978723406791687, train/precision_homref=1.0, train/recall=0.8671875, train/recall_het=1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:1612,test,tests,1612,,https://github.com/google/deepvariant/issues/904,1,['test'],['tests']
Testability,"3 14:53:07.275555 139714691082048 make_examples_core.py:163] Task 53/64: 2400 candidates (2566 examples) [15.76s elapsed]; I1103 14:53:07.719906 140657934407488 make_examples_core.py:163] Task 27/64: 2739 candidates (3035 examples) [5.45s elapsed]; I1103 14:53:07.775277 140126785840960 make_examples_core.py:163] Task 16/64: 2308 candidates (2374 examples) [2.44s elapsed]; I1103 14:53:08.681667 139823122659136 make_examples_core.py:163] Task 45/64: 2652 candidates (2750 examples) [5.88s elapsed]; I1103 14:53:08.499621 140345388750656 make_examples_core.py:163] Task 50/64: 2517 candidates (2651 examples) [4.04s elapsed]; I1103 14:53:08.077846 139826026686272 make_examples_core.py:163] Task 55/64: 2412 candidates (2556 examples) [8.96s elapsed]; I1103 14:53:08.165700 140447748351808 make_examples_core.py:163] Task 29/64: 2805 candidates (2883 examples) [2.81s elapsed]; I1103 14:53:08.086294 140152994068288 make_examples_core.py:163] Task 4/64: 2265 candidates (2381 examples) [3.39s elapsed]; I1103 14:53:08.115124 140349764978496 make_examples_core.py:163] Task 58/64: 2401 candidates (2511 examples) [13.20s elapsed]; I1103 14:53:07.834557 140529397729088 make_examples_core.py:163] Task 44/64: 2614 candidates (2702 examples) [1.68s elapsed]; I1103 14:53:08.208366 140388734826304 make_examples_core.py:163] Task 13/64: 2206 candidates (2302 examples) [8.06s elapsed]; # the program died here; ```. For one failed task, the input BAM size is 19GB, and allocated disk size is 300GB. **Does the quick start test work on your system?**. Some inputs finish, while others fail using the exact same workflow (PAPI error 10), so it's unlikely to be a coding issue. **Any additional context:**. We have successful runs with inputs of similar sizes that failed with PAPI 10. So I'm wondering if there's an empirical formula for predicting disk space usage. Additionally, is there a way to make DV less verbose? The log file goes to hundreds of MB, which makes debugging less easy. Thanks!; Steve",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:35914,test,test,35914,,https://github.com/google/deepvariant/issues/491,2,"['log', 'test']","['log', 'test']"
Testability,"3 haley.arnold proj-pbarc 4.0K Jul 21 23:11 ckpt-14902; > -rw-r----- 1 haley.arnold proj-pbarc 54K Aug 6 22:51 ckpt-7451.index; > -rw-r----- 1 haley.arnold proj-pbarc 250M Aug 6 22:51 ckpt-7451.data-00000-of-00001; > -rw-r----- 1 haley.arnold proj-pbarc 54K Aug 6 22:51 ckpt-14902.index; > -rw-r----- 1 haley.arnold proj-pbarc 250M Aug 6 22:51 ckpt-14902.data-00000-of-00001; > -rw-r----- 1 haley.arnold proj-pbarc 266 Aug 6 22:51 checkpoint. and finally, here are the contents of ckpt-14902: . > total 7.6M; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 1 22:49 ..; > drwxr-s--- 2 haley.arnold proj-pbarc 4.0K Jul 1 22:49 variables; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 21 23:11 .; > -rw-r----- 1 haley.arnold proj-pbarc 6.9M Aug 6 22:51 saved_model.pb; > -rw-r----- 1 haley.arnold proj-pbarc 677K Aug 6 22:51 keras_metadata.pb; > -rw-r----- 1 haley.arnold proj-pbarc 55 Aug 6 22:51 fingerprint.pb; > -rw-r----- 1 haley.arnold proj-pbarc 80 Aug 6 22:51 example_info.json. Here is the error log file: . > 2024-08-09 20:05:25.101938: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; > To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; > I0809 20:05:40.093672 139993880950592 run_deepvariant.py:519] Re-using the directory for intermediate results in /tmp/tmp4wzl_5p3; > Traceback (most recent call last):; > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 722, in <module>; app.run(main); > File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); > File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); > File ""/opt/deepvariant/bin/run_deepvariant.py"", line 693, in main; commands_logfiles = create_all_commands_and_logfiles(intermediate_results_dir); ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/866:2042,log,log,2042,,https://github.com/google/deepvariant/issues/866,1,['log'],['log']
Testability,"4. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such training run:. ```; I1031 10:55:27.365902 140558597089024 logging_writer.py:48] [0] epoch=0, train/categorical_accuracy=0.91796875, train/categorical_crossentropy=0.6384725570678711, train/f1_het=0.7428571581840515, train/f1_homalt=0.964401364326477, train/f1_homref=0.902255654335022, train/f1_macro=0.; 8698380589485168, train/f1_micro=0.91796875, train/f1_weighted=0.9241795539855957, train/false_negatives=34.0, train/false_positives=14.0, train/learning_rate=9.999999747378752e-06, train/loss=0.6384731531143188, train/precision=0.9406779408454895, train/precision_het=0.702702701091; 7664, train/precision_homalt=0.978723406791687, train/precision_homref=1.0, train/recall=0.8671875, train/recall_het=1.0, train/recall_homalt=0.8789808750152588, train/recall_homref=0.7945205569267273, train/true_negatives=498.0, train/true_positives=222.0; I1031 11:18:53.873582 140558597089024 logging_writer.py:48] [100] epoch=0, train/categorical_accuracy=0.9428125023841858, train/cate",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:1970,log,log,1970,,https://github.com/google/deepvariant/issues/904,1,['log'],['log']
Testability,"4.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta; output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta; output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta; (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics; output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics; output/models/current.metrics output/models/model.ckpt-31078.metrics; ```. But `model_eval` just sits there like this (until a new checkpoint appears):; ```; I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models; ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time?. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/611:3839,test,test,3839,,https://github.com/google/deepvariant/issues/611,2,['test'],['test']
Testability,"43 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1631,test,test,1631,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"4571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_exam",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1712,test,test,1712,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${n",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1798,test,test,1798,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"7:49:02.877284 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.117142 139897470359360 make_examples.py:1110] Preparing inputs; 2019-07-16 17:49:03.117644: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.117749 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.118745 139897470359360 make_examples.py:1034] Common contigs are [u'chr20']; I0716 17:49:03.120177 139897470359360 make_examples.py:1116] Writing examples to examples.tfrecord-00000-of-00001.gz; 2019-07-16 17:49:03.121118: I third_party/nucleus/io/sam_reader.cc:600] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-07-16 17:49:03.124279: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.124422 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/home/nyakovenko/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1225, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1127, in make_examples_runner; candidates, examples, gvcfs = region_processor.process(region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 849, in process; self.in_memory_sam_reader.replace_reads(self.region_reads(region)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 889, in",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199:1803,test,testdata,1803,,https://github.com/google/deepvariant/issues/199,1,['test'],['testdata']
Testability,"9-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --alt_aligned_pileup diff_channels --reads /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/HG002.merged.bam --ref Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --norealign_reads --regions 20 --sample_name HG002 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --use_openvino --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./HG002.vcf ) > ./postprocess_variants.log 2>&1`. When I try and run with HYBRID model, everything go",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/458:1928,LOG,LOGDIR,1928,,https://github.com/google/deepvariant/issues/458,6,"['LOG', 'log']","['LOGDIR', 'log', 'logs']"
Testability,"9.7, build 2d0083d; Bowtie 2; Samtools 1.9; DeepVariant 0.9.0. Original source files.; - _SRR062634.filt.fastq_ from ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/HG00096/sequence_read/; - _Homo_sapiens.GRCh38.dna.primary_assembly.fa_ from ftp://ftp.ensembl.org/pub/release-98/fasta/homo_sapiens/dna/. Actions.; 1. Bowtie 2: indexing _Homo_sapiens.GRCh38.dna.primary_assembly.fa_; 2. Bowtie 2: aligning _SRR062634.filt.fastq_ on _Homo_sapiens.GRCh38.dna.primary_assembly.fa_; 3. Samtools: converting _SRR062634.sam_ to _SRR062634.bam_; 4. Samtools: indexing _SRR062634.filt.fastq_; 5. DeepVariant: trying to call SNPs. DeepVariant command syntax.; `sudo docker run -v ""/home/platon/_0_Диссертация/Exp/seq1/bowtie2/"":""/input"" -v ""/home/platon/_0_Диссертация/Exp/seq1/bowtie2/"":""/output"" google/deepvariant:""0.9.0"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/SRR062634.filt.fastq --reads=/input/SRR062634.bam --output_vcf=/output/SRR062634.vcf.gz --num_shards=4`. Part of error log.; ```; ***** Running the command:*****; time seq 0 3 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/SRR062634.filt.fastq"" --reads ""/input/SRR062634.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@4.gz"" --task {}. I1208 19:49:03.680470 140573386819328 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1208 19:49:03.681448 140573386819328 genomics_reader.py:223] Reading /input/SRR062634.bam with NativeSamReader; W1208 19:49:03.681570 140573386819328 make_examples.py:558] No non-empty sample name found in the input reads. DeepVariant will use default as the sample name. You can also provide a sample name with the --sample_name argument.; I1208 19:49:03.742767 140573386819328 make_examples.py:1324] Preparing inputs; I1208 19:49:05.745795 140573386819328 genomics_reader.py:223] Reading /input/SRR062634.bam with NativeSamReader; Traceba",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/250:1044,log,log,1044,,https://github.com/google/deepvariant/issues/250,1,['log'],['log']
Testability,"9713-199710-199718/call_variants_output_parent2.tfrecord.gz"" --outfile ""/out_dir/199718.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent2.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent2.log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.705964 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; real	0m3.173s; user	0m3.003s; sys	0m3.160s; real	0m3.194s; user	0m3.299s; sys	0m4.216s; real	0m3.254s; user	0m3.024s; sys	0m2.808s; post_process returns: [0, 0, 0]; real	2008m37.771s; user	78330m54.158s; sys	730m9.042s; ```. **Does the quick start test work on your system?** Yes.; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes, see below:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=/input/GRCh38_no_alt_analysis_set.fasta \; --reads_child=/input/HG002.chr20.10_10p1mb.bam \; --reads_parent1=/input/HG003.chr20.10_10p1mb.bam \; --reads_parent2=/input/HG004.chr20.10_10p1mb.bam \; --output_vcf_child /output/HG002.output.vcf.gz \; --output_vcf_parent1 /output/HG003.output.vcf.gz \; --output_vcf_parent2 /output/HG004.output.vcf.gz \; --sample_name_child 'HG002' \; --sample_name_parent1 'HG003' \; --sample_name_parent2 'HG004' \; --num_shards $(nproc) \; --regions ""chr20:10,000,000-10,010,000"" \; --intermediate_results_dir /output/intermediate_results_dir \; ```. **Any additional context:**; DeepVariant's",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/429:3974,test,test,3974,,https://github.com/google/deepvariant/issues/429,1,['test'],['test']
Testability,9:34:11.184109 47167827691328 make_examples_core.py:257] 7332195 candidates (8325720 examples) [13.01s elapsed]; I0720 09:34:37.427601 47167827691328 make_examples_core.py:257] 7334598 candidates (8328193 examples) [26.24s elapsed]; I0720 09:34:53.436255 47167827691328 make_examples_core.py:257] 7336041 candidates (8329660 examples) [16.01s elapsed]; I0720 09:35:18.419953 47167827691328 make_examples_core.py:257] 7338676 candidates (8332321 examples) [24.98s elapsed]; I0720 09:35:32.331245 47167827691328 make_examples_core.py:257] 7340143 candidates (8333833 examples) [13.91s elapsed]; I0720 09:35:48.913797 47167827691328 make_examples_core.py:257] 7342460 candidates (8336192 examples) [16.58s elapsed]; I0720 09:36:05.716144 47167827691328 make_examples_core.py:257] 7344261 candidates (8338017 examples) [16.80s elapsed]; I0720 09:36:24.168858 47167827691328 make_examples_core.py:257] 7347004 candidates (8340828 examples) [18.45s elapsed]; I0720 09:36:42.921647 47167827691328 make_examples_core.py:257] 7349633 candidates (8343603 examples) [18.75s elapsed]; I0720 09:36:47.287455 47167827691328 make_examples_core.py:257] 7350149 candidates (8344123 examples) [4.37s elapsed]; I0720 09:37:06.694804 47167827691328 make_examples_core.py:257] 7352217 candidates (8346237 examples) [19.41s elapsed]; I0720 09:37:27.305462 47167827691328 make_examples_core.py:257] 7354095 candidates (8348169 examples) [20.61s elapsed]; I0720 09:37:48.644136 47167827691328 make_examples_core.py:257] 7356001 candidates (8350103 examples) [21.34s elapsed]. My question is: ; Is the long running time due to the complexity of my samples? Do I need to allocate more threads to speed up this process? As I recall make_examples is a single-thread program. . **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Yes; **Any additional context:**,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/683:5251,test,test,5251,,https://github.com/google/deepvariant/issues/683,2,['test'],['test']
Testability,":40:12.133007 139624775427840 run_deepvariant.py:321] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 332, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 319, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 31 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmpsowmvllp/make_examples.tfrecord@32.gz"" --gvcf ""/tmp/tmpsowmvllp/gvcf.tfrecord@32.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}' returned non-zero exit status 252.; ```; The command I run is the following:. ```; :~# cat command.sh; docker run -v ""/root/quickstart-testdata"":""/input"" -v ""/root/quickstart-output"":""/output"" google/deepvariant:latest /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/ucsc.hg19.chr20.unittest.fasta --reads=/input/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/output/output.vcf.gz --output_gvcf=/output/output.g.vcf.gz --num_shards=32; ```; And the content of ```testdata``` dir is:. ```; :~# ls quickstart-testdata/; NA12878_S1.chr20.10_10p1mb.bam ucsc.hg19.chr20.unittest.fasta; NA12878_S1.chr20.10_10p1mb.bam.bai ucsc.hg19.chr20.unittest.fasta.fai; test_nist.b37_chr20_100kbp_at_10mb.bed ucsc.hg19.chr20.unittest.fasta.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz ucsc.hg19.chr20.unittest.fasta.gz.fai; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. Thanks a lot for any help!; -A",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/325:3119,test,testdata,3119,,https://github.com/google/deepvariant/issues/325,3,['test'],['testdata']
Testability,"=0; ++ DV_GPU_BUILD=0; ++ export DV_USE_GCP_OPTIMIZED_TF_WHL=1; ++ DV_USE_GCP_OPTIMIZED_TF_WHL=1; ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ export DV_TF_NIGHTLY_BUILD=0; ++ DV_TF_NIGHTLY_BUILD=0; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/home/huangl/publib/bin/python; ++ PYTHON_BIN_PATH=/home/huangl/publib/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ DV_COPT_FLAGS='--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ export DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; ++ DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; (08:09:38) INFO: Current date is 2017-12-08; (08:09:38) WARNING: /home/huangl/.cache/bazel/_bazel_huangl/008c6ca154d923f28d39cff9fad40a7f/external/org_tensorflow/tensorflow/core/BUILD:1806:1: in includes attribute of cc_library rule @org_tensorflow//tensorflow/core:framework_headers_lib: '../../../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'external/org_tensorflow/tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/huangl/.cache/bazel/_bazel_huangl/008c6ca154d923f28d39cff9fad40a7f/external/org_tensorflow/tensorflow/tensorflow.bzl:1100:30; (08:09:38) INFO: Analysed 241 targets (0 packages loaded).; (08:09:38) INFO: Found 18",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6:2289,test,test,2289,,https://github.com/google/deepvariant/issues/6,1,['test'],['test']
Testability,"Any idea why I can not run the docker?. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. In the quick start guide https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md . They show using sudo to run the docker. I do not have sudo permission on this machine. The machine is set up to use the group permission. I do not think this is the issue. . Any suggestions would be greatly appreciated. Andy. ```; (base) -bash-4.2$ groups; giuser kimlab docker; (base) -bash-4.2$ ; ```. ```; docker run -v /public/home/dkim142/quickstart-testdata:/input \; -v /public/home/dkim142/quickstart-output:/output google/deepvariant:0.9.0 \; /opt/deepvariant/bin/run_deepvariant --model_type=WGS \; --ref=/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads=/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions chr20:10,000,000-10,010,000 \; --output_vcf=/public/home/dkim142/quickstart-output/output.vcf.gz \; --output_gvcf=/public/home/dkim142/quickstart-output/output.g.vcf.gz \; --num_shards=1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling \; --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2019-12-08 02:35:44.105906: F tensorflow/core/platform/cpu_feature_guard.cc:37] ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m1.146s; user	0m1.709s; sys	0m4.191s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/248:607,test,testdata,607,,https://github.com/google/deepvariant/issues/248,3,['test'],['testdata']
Testability,AssertionError: Some objects had attributes which were not restored:,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845:0,Assert,AssertionError,0,,https://github.com/google/deepvariant/issues/845,2,['Assert'],['AssertionError']
Testability,Benchmark with low precision and recall on indels with HG002 PacBio Revio data,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/641:0,Benchmark,Benchmark,0,,https://github.com/google/deepvariant/issues/641,1,['Benchmark'],['Benchmark']
Testability,Benchmarking of NA24385 sample with v1.5.0,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/626:0,Benchmark,Benchmarking,0,,https://github.com/google/deepvariant/issues/626,1,['Benchmark'],['Benchmarking']
Testability,Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights/RMSProp_1|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/weights|InceptionV3/Logits/Conv2d_1c_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/weights/RMSProp|InceptionV3/Conv2d_2a_3x3/weights/RMSProp_1|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/weights|InceptionV3/Mixed_7c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/weights|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights/RMSProp|InceptionV3/Mixed_5d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/weights|InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_variance/Expo,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:36693,Log,Logits,36693,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,"Build and test works, binaries do not",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47:10,test,test,10,,https://github.com/google/deepvariant/issues/47,1,['test'],['test']
Testability,Building from source failed tests that import `random`,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/32:28,test,tests,28,,https://github.com/google/deepvariant/issues/32,1,['test'],['tests']
Testability,Can't run the test data and get `subprocess.CalledProcessError` with status 252,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345:14,test,test,14,,https://github.com/google/deepvariant/issues/345,1,['test'],['test']
Testability,"Checkpoint ""Model files do not exist"" when testing custom model",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/866:43,test,testing,43,,https://github.com/google/deepvariant/issues/866,1,['test'],['testing']
Testability,"Compiled from source, tests pass but ""make examples"" does not run with test data.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199:22,test,tests,22,,https://github.com/google/deepvariant/issues/199,2,['test'],"['test', 'tests']"
Testability,"DER_NAME=wes_staging; OUTPUT_FILE_NAME=wes_output.vcf; # Model for calling exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; #; # Changing the number of chards changes the output for some reason; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://deepvariant/exome-case-study-testdata/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam \; --bai gs://deepvariant/exome-case-study-testdata/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai \; --ref gs://deepvariant/exome-case-study-testdata/hs37d5.fa.gz \; --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \; --shards 64 \; --make_examples_workers 8 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 100 \; --call_variants_workers 1 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --max_preemptible_tries 5 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west1 \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. Changing `--shards` to `128` changes the output in the VCF file. Running a diff between the two outputs shows that they are not the same. Why would that happen?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/112:1311,test,testdata,1311,,https://github.com/google/deepvariant/issues/112,3,"['log', 'test']","['log', 'logging', 'testdata']"
Testability,"Dear All,. I am trying to run gcloud alpha genomics but have recurrently encountered the same issues about authentification and docker run. . The bash file for Deep Variant and error logs are below:; BASH file [https://storage.googleapis.com/wgs-test-shan/test_samples/deepVariant.sh](url); YAML file [https://storage.googleapis.com/wgs-test-shan/test_samples/deepvariant_wes_pipeline.yaml](url); LOG file [https://storage.googleapis.com/wgs-test-shan/test_samples/runner_logs/ENjW7s2JLBjf3aql19nvyv8BIKeM6-b_FyoPcHJvZHVjdGlvblF1ZXVl-stderr.log](url). I have contacted Cloud support center and obtained suggestions as below. However this did not mend the problem. What is your suggestion? ; [https://enterprise.google.com/supportcenter/managecases#Case/001f200001TaEgT/U-14552728; ](url); Thank you.; I will appreciate your help.; Best,; Shan",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/27:183,log,logs,183,,https://github.com/google/deepvariant/issues/27,6,"['LOG', 'log', 'test']","['LOG', 'log', 'logs', 'test-shan']"
Testability,"Dear Authors,; I just install deepvariant using conda and prepare to test the software with HiFi data. I noticed in the recently published DipAsm [package](https://github.com/shilpagarg/DipAsm) the parameters 'model_type' was set as 'PACBIO' in 'dv.sh' file, but there are only two options supplied in conda-deepvariant(wgs & wes). Since the alleged HiFidata base accuracy is very close to Illumina data, I wonder can I use wgs model for now? . Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/395:69,test,test,69,,https://github.com/google/deepvariant/issues/395,1,['test'],['test']
Testability,"Dear Deepvariant team,. I was attempting to run Deepvariant on GCP by following the sample scripts from the tutorials, but it failed. I have checked the configuration regarding the Compute Engine quota and it should meet the requirements (i.e. CPU, Persistent Disk and In-use IP addresses). The error message from the log is like:; ""RuntimeError: Job failed with error ""run"": operation ""projects/deepvariant-phh/operations/7761698599878123803"" failed: executing pipeline: Execution failed: action 2: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION)"". I have read some of the related discussed issues but still can't solve my problem. The log files and my script file are attached. Your help is appreciated. . [staging_temp%2Frunner_logs_20181118_014355.log](https://github.com/google/deepvariant/files/2592663/staging_temp.2Frunner_logs_20181118_014355.log); [log.txt](https://github.com/google/deepvariant/files/2592666/log.txt). [script.txt](https://github.com/google/deepvariant/files/2592665/script.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/120:318,log,log,318,,https://github.com/google/deepvariant/issues/120,6,['log'],['log']
Testability,"Dear Devs, . I am currently training a model (starting from wgs.1.6.1) for use in a fish species. The programs are running well, I have confident regions and truth variants defined, and am currently tuning hyperparameters to optimise the training. . However . . . . I notice when tracking the model eval stats (specifically f1, precision, recall), that the hom_ref classifications are much less reliable than hom_alt and het classes. My question is whether this is to be expected, or whether there might be something wrong with my training setup, or perhaps the examples. . The test example set I am using to tune the hyperparams looks like this:. ```; # Generated by shuffle_tfrecords_beam.py; # class2: 89987; # class0: 33161; # class1: 24300. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 147448; ```. The training command looks like this:. ```; LR=0.001; BS=1024. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such traini",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:578,test,test,578,,https://github.com/google/deepvariant/issues/904,1,['test'],['test']
Testability,"Dear all. I am new to deepvariant. We are trying to use deepvariant on a HPC cluster with singularity.; We installed nvidia and cuda drivers through conda, and tested it with other python programs that used gpu with success.; I also managed to run the CPU version with deepvariant with singularity with success. ; However when running deepvariant on a gpu node with the following command, deepvariant complained that certain libraries are not found which prevented it from using the GPU:. `apptainer run --nv -B /public:/public,/public3:/public3,/public2:/public2,/fast3:/fast3 \; /public/software/deepvariants/1.6.0/gpuver/deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$REF \; --reads=""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam"" \; --regions ""NC_037590.1:200,000-950,000"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --num_shards=2`. Error messages:; `==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available.; Use the NVIDIA Container Toolkit to start this container with GPU support; see; https://docs.nvidia.com/datacenter/cloud-native/ . 2024-01-05 15:52:56.748367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enabl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:160,test,tested,160,,https://github.com/google/deepvariant/issues/761,1,['test'],['tested']
Testability,"Dear deepvariant developers,. I'm using deepvariant (0.10.0) to call variants in HG002 human genome using Pacbio hifi sequencing data. However, I am getting a seg-fault error in the process. . ```; REF=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; BAM=output.primary.bam; MODEL=""/opt/models/pacbio/model.ckpt""; N_SHARDS=24; CALL_VARIANTS_OUTPUT=""output/call_variants_output.tfrecord.gz"". mkdir input output logs. /usr/bin/time seq 0 $((N_SHARDS-1)) \; | parallel -P ${SLURM_CPUS_PER_TASK} --eta --halt 2 \; --joblog ""logs/log"" --res ""logs"" \; make_examples --mode calling \; --ref ""${REF}"" \; --reads ""${wd}/${BAM}"" \; --examples output/examples.tfrecord@${N_SHARDS}.gz\; --task {} \; || exit 1; ```. Here is the [output log file](https://github.com/google/deepvariant/files/4714025/slurm-58983130.log). Please let me know if I can share more details.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/313:410,log,logs,410,,https://github.com/google/deepvariant/issues/313,6,['log'],"['log', 'logs']"
Testability,"Dears, . I just followed ""DeepVariant quick start"" guide, which was successful. ; Then, I've replaced the reference and reads with my own files and ran following commands but failed to get vcf files. . > OUTPUT_DIR=""${PWD}/quickstart-output""; > INPUT_DIR=""${PWD}/quickstart-testdata""; > mkdir -p ""${OUTPUT_DIR}""; > BIN_VERSION=""0.9.0"". > sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}:/output"" \; > gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WES \; > --ref=/input/genome.fa \; > --reads=/input/HC3-BC_RG_bwa.bam \; > --regions ""20:10,000,000-10,100,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=12. Here is a log: ; ```; ***** Running the command:*****; time seq 0 11 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/genome.fa"" --reads ""/input/HC3-BC_RG_bwa.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@12.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@12.gz"" --regions ""20:10,000,000-10,100,000"" --task {}. I1208 07:03:00.340749 140610455504640 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1208 07:03:00.598805 140610455504640 genomics_reader.py:223] Reading /input/HC3-BC_RG_bwa.bam with NativeSamReader; I1208 07:03:00.619033 140610455504640 make_examples.py:1324] Preparing inputs; I1208 07:03:00.814711 140610455504640 genomics_reader.py:223] Reading /input/HC3-BC_RG_bwa.bam with NativeSamReader; I1208 07:03:00.841161 140610455504640 make_examples.py:1248] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; I1208 07:03:00.851603 140610455504640 make_examples.py:1330] Writing examples to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00000-of",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/249:274,test,testdata,274,,https://github.com/google/deepvariant/issues/249,2,"['log', 'test']","['log', 'testdata']"
Testability,"Dears,. I get an error trying to reproduce the test example on my Bio-Linux Ubuntu 14.04.6 LTS where I run a 1.6.2 docker. The docker installation was successful:. ```; REPOSITORY TAG IMAGE ID CREATED VIRTUAL SIZE; google/deepvariant 0.10.0 4745891a5ab0 3 months ago 3.866 GB; google/deepvariant latest 4745891a5ab0 3 months ago 3.866 GB; ```. But I get this error:. ```; I0715 10:39:51.140211 139624775427840 run_deepvariant.py:241] Re-using the directory for intermediate results in /tmp/tmpsowmvllp. ***** Intermediate results will be written to /tmp/tmpsowmvllp in docker. ****. ***** Running the command:*****; time seq 0 31 | parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/tmpsowmvllp/make_examples.tfrecord@32.gz"" --gvcf ""/tmp/tmpsowmvllp/gvcf.tfrecord@32.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 2; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 0; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /tmp/tmpsowmvllp/make_examples.tfrecord@32.gz --gvcf /tmp/tmpsowmvllp/gvcf.tfrecord@32.gz --regions chr20:10,000,000-10,010,000 --task 3. real 0m20.988s; user 0m7.822s; sys 3m7.414s; I0715 10:40:12.133007 139624775427840 run",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/325:47,test,test,47,,https://github.com/google/deepvariant/issues/325,1,['test'],['test']
Testability,"Dears,; I'm working on Ubuntu 16.04.5 LTS, and Docker API version 1.39,; I have downloaded the data according to the script:. INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see h",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/223:155,test,testdata,155,,https://github.com/google/deepvariant/issues/223,2,['test'],['testdata']
Testability,DeepTrio Quickstart Test Command error,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/632:20,Test,Test,20,,https://github.com/google/deepvariant/issues/632,1,['Test'],['Test']
Testability,"DeepVariant fails to run with test data, giving error:; ""RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed"" . **Setup**; running from HPC; OS info:; `cat /etc/os-release`; output:. ```; NAME=""AlmaLinux""; VERSION=""9.3 (Shamrock Pampas Cat)""; ID=""almalinux""; ID_LIKE=""rhel centos fedora""; VERSION_ID=""9.3""; PLATFORM_ID=""platform:el9""; PRETTY_NAME=""AlmaLinux 9.3 (Shamrock Pampas Cat)""; ANSI_COLOR=""0;34""; LOGO=""fedora-logo-icon""; CPE_NAME=""cpe:/o:almalinux:almalinux:9::baseos""; HOME_URL=""https://almalinux.org/""; DOCUMENTATION_URL=""https://wiki.almalinux.org/""; BUG_REPORT_URL=""https://bugs.almalinux.org/"". ALMALINUX_MANTISBT_PROJECT=""AlmaLinux-9""; ALMALINUX_MANTISBT_PROJECT_VERSION=""9.3""; REDHAT_SUPPORT_PRODUCT=""AlmaLinux""; REDHAT_SUPPORT_PRODUCT_VERSION=""9.3""; ```. - DeepVariant version: **1.6.1**; - Installation method (Docker, built from source, etc.): **Docker**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **Test data provided in documentation.** . **Steps to reproduce:**; - Command: ; ``` ; run_deepvariant --model_type=WGS \; 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; 	--regions ""chr20:10,000,000-10,010,000"" \; 	--output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; 	--intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; 	--num_shards=12; ```. - Error trace: (if applicable). ```; I0423 14:28:39.396079 139638090712896 make_examples_core.py:301] Task 0/12: Overhead for preparing inputs: 0 seconds; 2024-04-23 14:28:39.402994: W ./third_party/nucleus/util/proto_clif_converter.h:75] Failed to cast type N6google8protobuf14DynamicMessageE; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/absl_py/ab",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812:30,test,test,30,,https://github.com/google/deepvariant/issues/812,4,"['LOG', 'Test', 'log', 'test']","['LOGO', 'Test', 'logo-icon', 'test']"
Testability,"Deepvariant 1.0 is still not considering polyploid polyclonal tumor data analysis in consideration. Anything in the pipeline or any plug in?. **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/352:458,test,test,458,,https://github.com/google/deepvariant/issues/352,2,['test'],['test']
Testability,"Deepvariant fails without clear reason. . **Setup**; JHU Rockfish HPC; Singularity 3.8.7; singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:; ```; #!/bin/bash; #SBATCH --job-name=deep64_13448198; #SBATCH --time=24:00:00; #SBATCH --nodes=2; #SBATCH --ntasks-per-node=1; #SBATCH --cpus-per-task=32; #SBATCH --mem=0. ml anaconda; conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref c_elegans.PRJNA13758.WS245.genomic.fa \; --reads aln13448198.pbmm2.bam \; --output_vcf aln13448198.pbmm2.dv.vcf.gz \; --num_shards 64; ```. Here's a long snippet of slurm output:; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****; time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]; # this part is likely unimportant. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/614:1039,sandbox,sandbox,1039,,https://github.com/google/deepvariant/issues/614,1,['sandbox'],['sandbox']
Testability,"Describe the issue:; I can't get vcf output after bind mount a root directory. Setup; - Operating system: Windows 11, but mount an Ubuntu VM through multipass; - Type of data: fasta, bam and vcf file. Steps to reproduce:; - Command:; #Configure the DeepVariant environment variables (missing input directory,...); BIN_VERSION=""1.5.0"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}"". # Pull the image; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; PWD=/mountpoint/fastQ; INPUT_DIR=""${PWD}/testdata_input""; mkdir -p ${INPUT_DIR}; OUTPUT_DIR=""${PWD}/04.deepvariant_out""; mkdir -p ""${OUTPUT_DIR}"". sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/input/Homo_sapiens_assembly38.fasta \; --reads=/input/$FQ.align.sort.marked.bam \; --output_vcf=/output/$FQ.vcf.gz \; --output_gvcf=/output/$FQ.g.vcf.gz \; --num_shards=2 ; - Error trace: ; ; It displays: Task reading input the .bam file but it ends up with 0 candidates.; I suppose it can read the input files. Does the quick start test work on your system?; This the tutorial I've used: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-quick-start.md",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/675:1160,test,test,1160,,https://github.com/google/deepvariant/issues/675,1,['test'],['test']
Testability,Different results on test data,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/239:21,test,test,21,,https://github.com/google/deepvariant/issues/239,1,['test'],['test']
Testability,"Does anyone know what caused the below error?; I use deepvariant image on singularity and running it on a cluster but this error happens on many machines.; I don't know what causes this error. I0624 02:14:00.095050 47429297437696 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0624 02:14:01.826225 47429297437696 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0624 02:14:01.954994 47429297437696 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. r",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465:423,log,logs,423,,https://github.com/google/deepvariant/issues/465,3,['log'],['logs']
Testability,"During make_examples, there is a validation step to be sure contigs; reasonably overlap. This excludes some contigs, like chrM and extra; chromosomes but does it inconsistently. The contigs get excluded from; the list to use but then not during validation. This leads to errors on; small test datasets (bcbio has a chr22/chrM dataset that exposes this,; chrM is removed and then only 50% of the bases overlap so it fails).",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/18:288,test,test,288,,https://github.com/google/deepvariant/pull/18,1,['test'],['test']
Testability,"ER_NAME=stage; OUTPUT_FILE_NAME=deeptest_FB4_chr20.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/; IMAGE_VERSION=0.7.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://public_bed/CHR20.bed \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --ref_fai gs://ms_bam/Homo_sapiens_assembly38.fasta.fai \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones europe-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}"". 1. I have quoted #set -euo pipefail out as it returns an error.; 2. The bed file is located in a public bucket #119 ; 3. I have tried with docker image 0.7.1 which returns following error:. [12/12/2018 14:14:08 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 14:37:23 ERROR gcp_deepvariant_runner.py] Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/5187520767668161022"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attem",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:1342,log,logging,1342,,https://github.com/google/deepvariant/issues/129,1,['log'],['logging']
Testability,Error on testing deepvariant for WES,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/743:9,test,testing,9,,https://github.com/google/deepvariant/issues/743,1,['test'],['testing']
Testability,Error running quickstart test command,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/262:25,test,test,25,,https://github.com/google/deepvariant/issues/262,1,['test'],['test']
Testability,Error while running tests on Calling variants in non-autosomal contigs,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/853:20,test,tests,20,,https://github.com/google/deepvariant/issues/853,1,['test'],['tests']
Testability,Errors on testing DeepTrio on PacBio samples,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/742:10,test,testing,10,,https://github.com/google/deepvariant/issues/742,1,['test'],['testing']
Testability,"F_WHL_VERSION=1.9.0; ++ export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=1.9.0; ++ DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=1.9.0; ++ export DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=1.9.0; ++ DV_TENSORFLOW_STANDARD_CPU_WHL_VERSION=1.9.0; ++ export DV_GPU_BUILD=0; ++ DV_GPU_BUILD=0; ++ export DV_USE_GCP_OPTIMIZED_TF_WHL=1; ++ DV_USE_GCP_OPTIMIZED_TF_WHL=1; ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-1.9.0.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-1.9.0.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/home/viniws/anaconda3/bin/python; ++ PYTHON_BIN_PATH=/home/viniws/anaconda3/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; ./build_and_test.sh: line 39: bazel: command not found; + PATH=/home/viniws/bin:/home/viniws/anaconda3/bin:/home/viniws/anaconda3/bin:/home/viniws/anaconda3/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; ./build_and_test.sh: line 54: bazel: command not found; ```. I don't know where to start troubleshooting this. In the first error, apparently the script couldn't retrieve my Ubuntu version, but I don't know where to edit it to place it there manually.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98:4093,test,test,4093,,https://github.com/google/deepvariant/issues/98,1,['test'],['test']
Testability,"GATK on my local computer (with 8 GB of RAM and 4 cores), so I’m not really complaining about the Cloud run-time that I encountered (I am just saying that the estimates provided on the README didn’t match my own experience, even with an almost identical command on Google Cloud). **1b)** I realize that it would take some time (and I’m not sure what would be the benefits versus other projects). However, have you considered allowing users to upload their run-time information (and estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)?. Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that?. **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:3789,test,test,3789,,https://github.com/google/deepvariant/issues/171,1,['test'],['test']
Testability,"Goal: Installing deepvariant using Docker Desktop on a Mac (apple silicon - M1/M2). I have been troubleshooting for days and to build from source, but failed to do so. ; Now I ended up installing Ubuntu 20.04 using mac's UTM, but still facing a lot of problems. . Is there a detailed step-by-step instruction on how to install on a mac (apple silicon)?. You mentioned: ""It can likely be built and run on other unix-based systems with some minimal modifications to these scripts.""; from https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-build-test.md; What is the ""minimal modifications"" in here? Changing everything about the build-prereq.sh, setting.sh, tools/build_clif.sh, and other .sh, proves to be a hard task. Otherwise, I can try to explain the problem of Ubuntu 20.04 using mac's UTM. Thank you for your help!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/657:557,test,test,557,,https://github.com/google/deepvariant/issues/657,1,['test'],['test']
Testability,"Hello DeepVariant team, thanks for great tool. After we tried to upgrade our deepvariant installation to the latest release we encountered a problem with `numpy` installation, which I described [here](https://github.com/pypa/wheel/issues/389). The problem is caused by `wheel` - not by `numpy` itself, but this error raised several questions for me:. - For now looks like installing deepvariant with https://github.com/google/deepvariant/blob/r1.1/build-prereq.sh will fail on non `Ubuntu 16.04` due to `numpy` problem. This problem won't be fixed in older versions of `numpy` I think - only `wheel` can fix it now. Or you can switch `numpy 1.18.5` for `1.19.3+`?. - Part of this problem origins from using Python 3.6 if I understand correctly [this](https://github.com/pypa/wheel/issues/331#issuecomment-579285573), maybe you should update version of Python installing by script?. - I've just tested your installation script with one change: I added `Ubuntu 18.04` to this [check](https://github.com/google/deepvariant/blob/r1.1/run-prereq.sh#L120) - looks like all good. If a problem was in `Ubuntu 14.04` - maybe wide this check a bit? Even `Ubuntu 20.04` is released half a year ago.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/394:894,test,tested,894,,https://github.com/google/deepvariant/issues/394,1,['test'],['tested']
Testability,"Hello DeepVariant team. We trying to implement your tool, it works, but we have different results on test data:. <details>; <summary>Our test VCF (DeepVariant v0.8.0)</summary>. ```; ##fileformat=VCFv4.2; ##FILTER=<ID=PASS,Description=""All filters passed"">; ##FILTER=<ID=RefCall,Description=""Genotyping model thinks this site is reference."">; ##FILTER=<ID=LowQual,Description=""Confidence in this variant being real is below calling threshold."">; ##INFO=<ID=END,Number=1,Type=Integer,Description=""End position (for use with symbolic alleles)"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Conditional genotype quality"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Read depth"">; ##FORMAT=<ID=MIN_DP,Number=1,Type=Integer,Description=""Minimum DP observed within the GVCF block."">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Read depth for each allele"">; ##FORMAT=<ID=VAF,Number=A,Type=Float,Description=""Variant allele fractions."">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype likelihoods rounded to the closest integer"">; ##contig=<ID=chr20,length=63025520>; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	NA12878; chr20	9999996	.	A	ACT	44.4	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:41:44:0,44:1:44,44,0; chr20	10000117	.	C	T	36.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:34:55:25,30:0.545455:36,0,37; chr20	10000211	.	C	T	34.1	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:34:59:30,29:0.491525:34,0,51; chr20	10000439	.	T	G	40	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:39:72:0,72:1:40,45,0; chr20	10000598	.	T	A	62.2	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:54:46:0,46:1:62,55,0; chr20	10000694	.	G	A	35.1	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:35:48:26,22:0.458333:35,0,47; chr20	10000758	.	T	A	69.1	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:56:56:0,56:1:69,55,0; chr20	10001019	.	T	G	2.1	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:4:44:31,13:0.295455:0,2,34; chr20	10001298	.	T	A	54.7	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:49:43:0,43:1:54,50,0; chr20	10001436	.	A	AAGGCT	38.7	PASS	.	GT:G",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/239:101,test,test,101,,https://github.com/google/deepvariant/issues/239,2,['test'],['test']
Testability,Hello DeepVariant team. We're trying to use your great tool and I'm creating our own Docker for this.; Yesterday I finally fixed all issues with v0.8.0 version - all tests from build_and_test.sh passed and saw v0.9.0 release happened several days before - I tried to switch to in and get error at build_and_test.sh stage:; ```; deepvariant/variant_calling.cc:36:20: fatal error: optional: No such file or directory; #include <optional>; ^; compilation terminated.; ```; Looks like you added new include lines in `deepvariant/variant_calling.cc` between releases - [this](https://github.com/google/deepvariant/blob/r0.9/deepvariant/variant_calling.cc#L36) line causing my error.; Can you help me please? Do I need to install some external dependencies or what?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/236:166,test,tests,166,,https://github.com/google/deepvariant/issues/236,1,['test'],['tests']
Testability,"Hello, . I am very sorry because this seems like a kind of s*** I could figure out but I have spent the afternoon to no avail. So, the following command WORKS in dry run mode, no error, but fails when in real operative mode. . ```; #!/usr/bin/zsh. OUTPUT_DIR=""${PWD}""; INPUT_DIR=""${PWD}"". BIN_VERSION=""1.5.0"". OUTPUT_VCF=vaga_lab_hifi_standing_variation.vcf.gz; OUTPUT_GVCF=vaga_lab_hifi_standing_variation.vcf.gz; BAM=HiFi_vaga.sorted.bam. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref=/input/Adineta_vaga.fsa --make_examples_extra_args=vsc_min_count_snps=2,vsc_min_fraction_snps=0.12 --reads=/input/BAM --output_vcf=/output/OUTPUT_VCF --output_gvcf=/output/OUTPUT_GVCF --num_shards=$(nproc) --logging_dir=/output/logs) 2>&1 | tee -a generallog.log; ```; and the output tells me . ```; ValueError: NOT_FOUND: Could not open /input/BAM; [E::hts_open_format] Failed to open file ""/input/BAM"" : No such file or directory; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/Adineta_vaga.fsa --reads /input/BAM --examples /tmp/tmpbkwxdhbf/make_examples.tfrecord@48.gz --add_hp_channel --alt_aligned_pileup diff_channels --gvcf /tmp/tmpbkwxdhbf/gvcf.tfrecord@48.gz --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_count_snps 2 --vsc_min_fraction_indels 0.12 --vsc_min_fraction_snps 0.12 --task 15. ```; the bam file is there, 100% sure, at least to my eyes. But it seems docker fails to see it. Any idea? I have the full log available if needed. Again, I am sorry because it looks like some easy stuff, but my colleague and I can't find it. . Thanks a lot",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/685:839,log,logs,839,,https://github.com/google/deepvariant/issues/685,3,['log'],"['log', 'logs']"
Testability,"Hello, . I have followed along with the advanced training case study, and I believe I was successful in training a model (at least, there were no errors thrown in that step that I could see). I am using one chromosome for the training set, one for validation, and one for testing the model. I am running this remotely on a cluster using apptainer and was able to specify a gpu node for the training step. . When I went to test the model, my script at first appears to run fine, but it seems when it hits the call_variants step, it throws a warning, after which it does not fail but also does not progress--just stays stagnant. The main issue seems to be that my ""input shape and model shape do not match,"" but I'm not sure functionally what that means I need to fix or where I went wrong. Any suggestions on how to resolve this would be very much appreciated! Below is the code I used to train the model, and then to test the model, as well as the error code thrown when testing the mode. I will also attach the output file as a whole so you can see exactly where it stops. Thank you so much for any insight! . Best, ; Haley . [deepvariant_modeltest-14698718-Atlas-0021.out.txt](https://github.com/google/deepvariant/files/14795403/deepvariant_modeltest-14698718-Atlas-0021.out.txt); ; **Code to train the model:** ; `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=gpu # standard node(s); #SBATCH --ntasks=48; #SBATCH --job-name=""deepvariant_training""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltrain-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltrain-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. L",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:272,test,testing,272,,https://github.com/google/deepvariant/issues/797,4,['test'],"['test', 'testing']"
Testability,"Hello, . Sorry for writing again about some questions in the analysis, I don't have any expertise on DeepVariant/variant benchmarking around me :( . I wanted to benchmark identified variants using hap.py as you suggest in the tutorial, and the output surprised me a bit. For the variants I called the recall/precision were from 0 to 0.4, which is very low... I did not change default pacbio parameters when running DeepVariant, except for asking to keep supplementary alignments. Maybe it is partially because of the complexity of HLA region, but I am not sure what is the reason. What do you think, is there something obviously wrong?. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref $REFERENCE \; --reads $BAM_FILE \; --make_examples_extra_args=keep_supplementary_alignments=true \; --output_vcf $VCF_FILE \; --num_shards 12 \; --regions chr6:32541543-32701886. I am using version 1.6.1. . Kind regards,; Alisa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/896:121,benchmark,benchmarking,121,,https://github.com/google/deepvariant/issues/896,2,['benchmark'],"['benchmark', 'benchmarking']"
Testability,"Hello, . When running DeepVariant I have a persistent error that the .fa and .fai reference genome files don't exist. I have checked that the given path is correct by displaying the files via copying the path given in the error sheet - the paths are correct and I don't have this problem with the input bam files, . I'm running the program via a script on a Linux Ubuntu server. I'm using singularity v3.5.3, which is pre-installed and loaded as a module. The data is Illumina short read which has been mapped with BWA-Kit. The following is the script I'm using is: . # Load modules needed; . /etc/profile.d/modules.sh; module load xxxxx/singularity/3.5.3. # inputs; reference=$2; bam=$1.final.bam; sampleid=$1; outdir=deepvar. # Create output directories; if [ ! -e deepvar ]; then mkdir deepvar; fi; if [ ! -e deepvar/$sampleid ]; then mkdir deepvar/$sampleid; fi. # Set singularity caches; if [ ! -e ${PWD}/.singularity ]; then mkdir ${PWD}/.singularity; fi; export SINGULARITY_TMPDIR=$PWD/.singularity; export SINGULARITY_CACHEDIR=$PWD/.singularity. # Download the image; if [ ! -e deepvariant.sif ]; then singularity build deepvariant.sif docker://google/deepvariant:latest; fi. # Run Deepvariant; singularity exec -p -B ${TMPDIR} -B ${PWD} deepvariant.sif /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${reference} \; --reads=${bam} \; --output_vcf=deepvar/${sampleid}/${sampleid}.vcf.gz \; --output_gvcf=deepvar/${sampleid}/${sampleid}.g.vcf.gz \; --num_shards=${NSLOTS}. I can run the test data on the command line but have the same problem when I use the above script to run it. I've not been able to find a fix, and have tried fixes suggested for similar issues on this site. . Very appreciative of any suggestion for a solve. . [runDV.sh.o21362497.txt](https://github.com/google/deepvariant/files/8985669/runDV.sh.o21362497.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/543:1513,test,test,1513,,https://github.com/google/deepvariant/issues/543,1,['test'],['test']
Testability,"Hello, ; I have tested PacBio data on version 1.5 of the deepTrio image. I have performed pbmm2 and whatshap haplotag on the BAM file. However, I encountered an error message stating that the parameter ""use_hp_information"" is missing. What could be the reason for this?. `docker pull google/deepvariant:deeptrio-1.5.0`; ![1697527223540](https://github.com/google/deepvariant/assets/70870741/bd8eca89-b44b-464d-acbd-5889f9e408a8). Looking forward to your reply.; Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/718:16,test,tested,16,,https://github.com/google/deepvariant/issues/718,1,['test'],['tested']
Testability,"Hello, ; Operatin system: Linux HPC ; Version: 1.3.0 ; Installation: Singularity ; Data: WES - with Agilent SureSelect DNA Human All ExonV5_hg38 bed file. **Steps to reproduce:**; **Command**; ```; `#!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p compute; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-23; #SBATCH --time=12:00:00; #SBATCH --mem-per-cpu=128GB. module purge; module load parallel; module load singularity; EXOME_IDs_FILE=Polyposis_Exome_Analysis_JOB27/fastp/All_fastp_input/IDswithoutR1R2_JOB27; HG38_REFERENCE=Polyposis_Exome_Analysis_JOB27/bwa/index/indexhumanrefseq_output/samtools_faidx/GRCh38_latest_genomic.fna; PICARDMARKDUPLICATES_SORTEDBAM=Polyposis_Exome_Analysis_JOB27/picard/markduplicate/markedduplicates/{}PE_markedduplicates.bam; BED_REGIONS=Polyposis_Exome_Analysis_JOB27/deepvariant/bed/AgilentSureSelectDNASureSelectXTHumanAllExonV5_hg38_recoded_nocol4.bed; OUTPUT_VCF=Polyposis_Exome_Analysis_JOB27/deepvariant/vcf/{}PE_output.vcf.gz; OUTPUT_GVCF=Polyposis_Exome_Analysis_JOB27/deepvariant/gvcf/{}PE_output.vcf.gz; INTERMEDIATE_RESULTS=Polyposis_Exome_Analysis_JOB27/deepvariant/intermediateresults/{}PE_output_intermediate. # Set bash error trapping to exit on first error.; set -eu. cd /scratch/c.c21087028/. sed -n ""${SLURM_ARRAY_TASK_ID}p"" $EXOME_IDs_FILE | parallel -j 1 ""singularity run -B /usr/lib/locale/:/usr/lib/locale/ containers/deepvariant_1.3.0.sif /opt/deepvariant/bin/run_deepvariant --model_type=WES \; --ref=$HG38_REFERENCE \; --reads=$PICARDMARKDUPLICATES_SORTEDBAM \; --regions=$BED_REGIONS \; --output_vcf=$OUTPUT_VCF \; --output_gvcf=$OUTPUT_GVCF \; --intermediate_results_dir=$INTERMEDIATE_RESULTS""; ```. **Error trace:**. ***** Intermediate results will be written to Polyposis_Exome_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/542:213,log,login,213,,https://github.com/google/deepvariant/issues/542,1,['log'],['login']
Testability,"Hello, ; a rather simple issue, I am following this tutorial ; https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-docker.md. and at this step ; `gsutil -m cp gs://deepvariant/quickstart-testdata/* input/; `; I get; `zsh: no matches found: gs://deepvariant/quickstart-testdata/*`. Thanks a lot",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/61:200,test,testdata,200,,https://github.com/google/deepvariant/issues/61,2,['test'],['testdata']
Testability,"Hello, I am trying to run DeepVariant but ...; here is my command; `time seq 0 $((N_SHARDS-1)) |parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python bin/make_examples.zip --mode calling --ref ""${REF}"" --reads ""${BAM}"" --sample_name FalconSet --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" --task {}`. And here is the output. ```; When using programs that use GNU Parallel to process data for publication please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; and it won't cost you a cent.; Or you can get GNU Parallel without this requirement by paying 10000 EUR. To silence this citation notice run 'parallel --bibtex' once or use '--no-notice'. Computers / CPU cores / Max jobs to run; 1:local / 48 / 40. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 40 AVG: 0.00s local:40/0/100%/0.0s WARNING: Logging before flag parsing goes to stderr.; I0601 15:22:01.182291 140355759671040 make_examples.py:1024] Preparing inputs; 2018-06-01 15:22:01.188982: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I0601 15:22:01.189755 140355759671040 genomics_reader.py:174] Reading ../Falcon_Unzip/out.bam with NativeSamReader; I0601 15:22:01.543628 140355759671040 make_examples.py:946] Common contigs are [u'000000F', u'000001F', u'000002F', u'000003F', u'000004F', u'000005F', u'000006F', u'000007F', u'000009F', u'000010F', u'000011F', u'000012F', u'000013F', u'000014F', u'000015F', u'000016F', u'000017F', u'000018F', u'000019F', u'000020F', u'000021F', u'000022F', u'000023F', u'000024F', u'000025F', u'000026F', u'000027F', u'000028F', u'000029F', u'000030F', u'000031F', u'000032F', u'000033F', u'000034F', u'000035F', u'000036F', u'000037F', u'000038F', u'000039F', u'000040F', u'000041F', u'000042F', u'000043F', u'000045F', u'000046F', u'000047F', u'000048F', u'000049F'",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/77:132,LOG,LOGDIR,132,,https://github.com/google/deepvariant/issues/77,4,"['LOG', 'log']","['LOGDIR', 'log', 'login']"
Testability,"Hello, I am trying to train my pacbio model using ”DeepVariant-inception_v3-1.4.0+data-pacbio_standard“ on ”HG003.pacbio-hifi.21x.haplotag.grch38.bam“. I noticed that the pre-trained model accepted the following channels: ""channels"": [1, 2, 3, 4, 5, 6, 7, 9, 10]}. I tried the following command:. ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${BASE}:${BASE} \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_003}"" \; --examples ""${MEDATA_DIR}/validation_set.hg003.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF_003}"" \; --confident_regions ""${TRUTH_BED_003}"" \; --task {} \; --regions ' ""chr20"" ' \; 	 --sort_by_haplotypes \; 	 --parse_sam_aux_fields \; 	 --add_hp_channel \; 	 --channels ' ""read_mapping_percent,avg_base_quality,identity,gap_compressed_identity,gc_content,is_homopolymer,homopolymer_weighted,blank,insert_size"" ' \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log"". get ”example_channels = [1, 2, 3, 4, 5, 6, 7, 11, 12, 13, 14, 15, 16, 17, 18, 19]“. I'm curious how do I get channels 9 and 10？Thanks！",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/589:1038,log,log,1038,,https://github.com/google/deepvariant/issues/589,1,['log'],['log']
Testability,"Hello, I trained a customized model, and am now trying to test it. However, when I try to run it, it says that the model files in the checkpoint do not exist. . Here is the command I tried to run: . > module load apptainer; > ; > apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/run_deepvariant \; > --model_type WGS \; > --customized_model ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/modeltrainout/2fullindividualmodel/checkpoints/ckpt-14902"" \; > --ref ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/Bactrocera_dorsalis_rearing_male_mt_chr_unpl.fasta"" \; > --reads ""${filesdir}_mapped/${sample}.bam"" \; > --output_vcf ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_fulltest/output/modeltrainout/modeltestout/2fullindividualmodeltest/${sample}.vcf.gz"". Here are the contents of the checkpoints folder for this training: . > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jun 29 01:06 ..; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 1 22:49 .; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 21 23:11 ckpt-14902; > -rw-r----- 1 haley.arnold proj-pbarc 54K Aug 6 22:51 ckpt-7451.index; > -rw-r----- 1 haley.arnold proj-pbarc 250M Aug 6 22:51 ckpt-7451.data-00000-of-00001; > -rw-r----- 1 haley.arnold proj-pbarc 54K Aug 6 22:51 ckpt-14902.index; > -rw-r----- 1 haley.arnold proj-pbarc 250M Aug 6 22:51 ckpt-14902.data-00000-of-00001; > -rw-r----- 1 haley.arnold proj-pbarc 266 Aug 6 22:51 checkpoint. and finally, here are the contents of ckpt-14902: . > total 7.6M; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 1 22:49 ..; > drwxr-s--- 2 haley.arnold proj-pbarc 4.0K Jul 1 22:49 variables; > drwxr-s--- 3 haley.arnold proj-pbarc 4.0K Jul 21 23:11 .; > -rw-r----- 1 haley.arnold proj-pbarc 6.9M Aug 6 22:51 saved_model.pb; > -rw-r----- 1 haley.arnold proj-pbarc 677K Aug 6 22:51 keras_metadata.pb; > -rw-r----- 1 haley.arnold proj-pbarc 55 Aug 6 22:51 fingerprint.pb; > -rw-r----- 1 haley.arnold proj-pbarc 80 Aug 6 22",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/866:58,test,test,58,,https://github.com/google/deepvariant/issues/866,1,['test'],['test']
Testability,"Hello, I'm getting error below - please let me know if I can fix it in anyway.. . [root@localhost processed]# docker run -v /home/imusayev/Documents/RNAseq:/input -v /home/imusayev/Documents/RNAseq:/output google/deepvariant:1.4.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --regions=chr16:56191390-56357444 --ref=/input/hg38/hg38.fa --reads=/input/60d/processed/work/89/e7822adaaaf824f73f3d6acd1334bb/MUT60d.sorted.bam --output_vcf=/output/MUT60d.sorted.bam.vcf --output_gvcf=/output/MUT60d.sorted.bam.gvcf --call_variants_extra_args=use_openvino=true --num_shards=2 --logging_dir=/output/logs; Emulate Docker CLI using podman. Create /etc/containers/nodocker to quiet msg. I1214 05:45:20.217978 140442762327872 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmpiy9bfzyx. ***** Intermediate results will be written to /tmp/tmpiy9bfzyx in docker. ****. ***** Running the command:*****; time seq 0 1 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/hg38/hg38.fa"" --reads ""/input/60d/processed/work/89/e7822adaaaf824f73f3d6acd1334bb/MUT60d.sorted.bam"" --examples ""/tmp/tmpiy9bfzyx/make_examples.tfrecord@2.gz"" --channels ""insert_size"" --gvcf ""/tmp/tmpiy9bfzyx/gvcf.tfrecord@2.gz"" --regions ""chr16:56191390-56357444"" --task {}. I1214 05:45:33.914664 140555214505792 genomics_reader.py:222] Reading /input/60d/processed/work/89/e7822adaaaf824f73f3d6acd1334bb/MUT60d.sorted.bam with NativeSamReader; I1214 05:45:33.947415 140555214505792 make_examples_core.py:243] Task 1/2: Preparing inputs; I1214 05:45:34.038768 140555214505792 genomics_reader.py:222] Reading /input/60d/processed/work/89/e7822adaaaf824f73f3d6acd1334bb/MUT60d.sorted.bam with NativeSamReader; I1214 05:45:34.078675 140555214505792 make_examples_core.py:243] Task 1/2: Common contigs are ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18',",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/597:601,log,logs,601,,https://github.com/google/deepvariant/issues/597,1,['log'],['logs']
Testability,"Hello, latest bazel build (5.0.0) dropped support of `--incompatible_prohibit_aapt1` flag ass you can see in patch notes https://blog.bazel.build/2022/01/19/bazel-5.0.html#android and here is my error:; ```; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings --experimental_build_setting_api deepvariant/...; [0m[91mINFO: Reading rc options for 'test' from /soft/tensorflow/.bazelrc:; Inherited 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --noincompatible_prohibit_aapt1 --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2; [0m[91mERROR: --noincompatible_prohibit_aapt1 :: Unrecognized option: --noincompatible_prohibit_aapt1; ```. Tensorflow removed this flag from their `.bazelrc` in June 2021 https://github.com/tensorflow/tensorflow/pull/50310 . Now deepvariant image cannot be build with latest `bazel` due to this - I ask you to update tensorflow version where this is fixed.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/511:216,test,test,216,,https://github.com/google/deepvariant/issues/511,2,['test'],['test']
Testability,"Hello, when running Deepvariant with the following command: . ```bash; BIN_VERSION=""1.0.0""`; INPUT_DIR=""${PWD}/data""; OUTPUT_DIR=""${PWD}/output""; LOGDIR=""${PWD}/log""; N_SHARDS=$( /bin/ls output/ | wc -l ); ; sudo docker run --gpus 1 \; -v ${HOME}:${HOME} \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""gs://deepvariant/models/DeepVariant/1.0.0/DeepVariant-inception_v3-1.0.0+data-pacbio_standard/model.ckpt""; ```. the following error occurs:. ```; I1203 17:49:21.931325 140389904897792 call_variants.py:335] Shape of input examples: [100, 221, 6]; 2020-12-03 17:49:32.284722: W tensorflow/core/platform/cloud/google_auth_provider.cc:178] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with ""Not found: Could not locate the credentials file."". Retrieving token from GCE failed with ""Aborted: All 10 retry attempts failed. The last failure: Unavailable: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Couldn't resolve host 'metadata'"".; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_a7fwubx_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 481, in main; use_tpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/392:146,LOG,LOGDIR,146,,https://github.com/google/deepvariant/issues/392,2,"['LOG', 'log']","['LOGDIR', 'log']"
Testability,"Hello,. I am trying to build deepavariant on a HPC node on which all the required dependency is met except pyclif. I do not have root privileges to install it under /usr/local/clif. Hence I downloaded pyclif source code and ran the INSTALL.sh to get it successfully built and installed under $HOME and activated the pyclif virtualenv. . (clif) [test-node]$ which pyclif; ~/opt/clif/bin/pyclif. However build_and_test.sh fails with the below error. Any changes required to deepvariant build setup to pick up the pyclif installation in my home directory?; (18:02:14) ERROR: missing input file '@clif//:clif/bin/pyclif'",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/93:345,test,test-node,345,,https://github.com/google/deepvariant/issues/93,1,['test'],['test-node']
Testability,"Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```; sudo sh run_deepvariant.sh; I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0; 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA; 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters; W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ; I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:38,test,testing,38,,https://github.com/google/deepvariant/issues/166,1,['test'],['testing']
Testability,"Hello,. I'm trying to debug my installation of the singularity GPU version for a new C4140 GPU node with Tesla V100s. I've run the CPU version successfully in production and am very happy with it, but the shift to GPU is giving me trouble, likely running into an issue with CUDA or TensorFlow. . I have several CUDA modules loaded, but perhaps I'm missing one of the key libraries? ; I have TensorFlow in a conda environment (although that's probably satisfied inside the singularity image)?. Here's the code I'm running from the Quickstart:; ```; OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""1.3.0"". # Load modules; module load singularity; module load cuda-dcgm/2.2.9.1; module load cuda11.4/toolkit; module load cuda11.4/blas; module load cuda11.4/nsight; module load cuda11.4/profiler; module load cuda11.4/fft; source /mnt/common/Precision/Miniconda3/opt/miniconda3/etc/profile.d/conda.sh; conda activate TensorFlow_GPU. # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}-gpu"". # Run; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; --nv \; docker://google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir""; ```. And here's my error:; ```; 2022-02-07 11:50:52.952780: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/home/BCRICWH.LAN/prichmond/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/514:616,test,testdata,616,,https://github.com/google/deepvariant/issues/514,1,['test'],['testdata']
Testability,"Hello,. I'm trying to run Deepvariant using singularity. I just followed the ""Notes on Singularity"" section in quick start test (https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md), and I got an error regarding numpy as below. Could you help me resolve this issue? I used deepvariant_1.6.0 image. ```; 2023-12-02 23:23:35.126320: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I1202 23:23:41.449015 46912500266816 run_deepvariant.py:519] Re-using the directory for intermediate results in /flashscratch/kimkw/tmp/tmppin2lwy5. ***** Intermediate results will be written to /flashscratch/kimkw/tmp/tmppin2lwy5 in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""./quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/flashscratch/kimkw/tmp/tmppin2lwy5/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/flashscratch/kimkw/tmp/tmppin2lwy5/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I1202 23:23:46.123890 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.133658 46912500266816 make_examples_core.py:301] Preparing inputs; I1202 23:23:46.139615 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.140348 46912500266816 make_examples_core.py:301] Common contigs are ['chr20']; I1202 23:23:46.141555 46912500266816 make_examples_core.py:301] Starting from v0.9.0, --use_ref_for_cram is default to true. If you ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/746:123,test,test,123,,https://github.com/google/deepvariant/issues/746,1,['test'],['test']
Testability,"Hello,. I've been using DeepTrio for de novo variant analysis, and it's been performing excellently. However, I noticed from the logs that DeepTrio uses the CPU to prepare data, which is quite time-consuming. In my case, it took 12 hours for one trio analysis, with an additional 4 hours on the GPU. Given that renting GPU servers( usually with less CPU) is more expensive than CPU servers and access to privately owned GPU servers is limited, it seems inefficient to run lengthy CPU processes on a GPU server. It feels like a bit of a waste, and sometimes I half-jokingly feel there might be someone out there with murderous intent because of it!. Would it be possible in future updates to partition the DeepTrio analysis into separate steps? This way, CPU-intensive tasks could be completed on a CPU server, and then the job could be transferred to a GPU server for the remaining tasks. Alternatively, could the data preparation (CPU) and analysis (GPU) be run at the same time? This would help optimize resource usage and reduce costs. Thank you for considering these suggestions.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/873:129,log,logs,129,,https://github.com/google/deepvariant/issues/873,1,['log'],['logs']
Testability,"Hello,. Thank you very much for providing the software. However, when I run deepvariant v1.1.0, I encounter the following file error. I did not encounter this error before. I hope you can help me solve this problem. Thank you very much for your assistance. Below is my commond:. BIN_VERSION=""1.1.0""; INPUT_DIR=""/data/lilab/hli1/Workspace/DATA/run_out_data/sorted_BAM/vs_F/6_2_vsF""; #DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; OUTPUT_DIR=""${PWD}/run_DeepVariant_6-20231219""; mkdir -p ""${OUTPUT_DIR}"". docker run \; 	-v ""${INPUT_DIR}"":""/input"" \; 	-v ""${OUTPUT_DIR}"":""/output"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type=PACBIO \; 	--ref=/input/F_unphased.Chr.v2.fa \; 	--reads=/input/6ccs_2_vsF.sorted.bam \; 	--output_vcf=/output/output.vcf.gz \; 	--output_gvcf=/output/output.g.vcf.gz \; 	--intermediate_results_dir /output/intermediate_results_dir \; 	--num_shards=30; [DeepVariant-v1.1.0.docx](https://github.com/google/deepvariant/files/13722568/DeepVariant-v1.1.0.docx)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/757:452,test,testdata,452,,https://github.com/google/deepvariant/issues/757,1,['test'],['testdata']
Testability,"Hello,. We noticed that adjacent variants of the same haplotype (i.e. MNVs) are being called as separate variants in the DeepVariant and DeepTrio outputs with VCF and gVCF files. During downstream processing these MNVs are then treated as two individual SNVs at two different loci, leading to faulty assessments. . For example two variants for a site of interest (reference TCG -> Serine) were separated between two lines in the DeepVariant/DeepTrio output VCF and then categorised as containing a nonsynonymous (T**G**G -> Tryptophan) and synonymous mutation (TC**A** -> Serine). Whereas the correct and desired way to handle this, at least for us but I imagine others too, would seem to be to recognise both mutations on a single line in the VCF as a combined substitution, which could then be identified as resulting in a stopgain (T**GA** -> Nonsense mutation). Are there plans to support these MNV calls in the DeepVariant/DeepTrio outputs? Or alternatively are there any current post-processing approaches that you may be using and can recommend to handle these cases? Can understand these may be challenging to manage in some aspects but could be important to flag given some recent literature around this topic. For reference this was using hg38 with WGS. We initially identified this using the original DeepTrio release (docker image deeptrio:1.0.1rc), but then also using the most recent DeepVariant release (via docker, v1.2). We also tested this to see whether the change to the unfiltered GLnexus config could be contributing to this for processing of DeepTrio gVCFs due to the joint genotyping parameter, but reverting back to the WGS config did not result in a merged MNV in this instance. . Many thanks,; Macabe.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/486:1446,test,tested,1446,,https://github.com/google/deepvariant/issues/486,1,['test'],['tested']
Testability,"Hello,. When I trained DeepVariant, I set pileup_image_height=75. My question is, when I run DeepVariant to evaluate with a trained model, do I have to add this parameter --make_examples_extra_args=""pileup_image_height=75"" . I tested running with and without parameter and I noticed I have better precision and accuracy not using this parameter. However, I have 'call_variants.py:623] Input shape [100, 221, 7] and model shape [75, 221, 7] does not match.' in my log file when I do not add this parameter --make_examples_extra_args=""pileup_image_height=75"". . Which way is the correct way to do?. Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/893:227,test,tested,227,,https://github.com/google/deepvariant/issues/893,2,"['log', 'test']","['log', 'tested']"
Testability,"Hello,; I tested the T7 model on WGS data using DV1.6, but I keep getting the following error message. I generated the test data using the T7 platform for sequencing. Could you please tell me what went wrong?; My cmd:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref ${fasta} \; --reads ${Input.bam} \; --output_vcf output/output.vcf.gz \; --output_gvcf output/output.g.vcf.gz \; --num_shards 32 \; --intermediate_results_dir output/intermediate_results_dir \; --regions chr20 \; --customized_model model/weights-51-0.995354.ckpt; ```. Error message:; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""output/intermediate_results_dir/make_examples.tfreco; rd@42.gz"" --checkpoint ""model/weights-51-0.995354.ckpt"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I1102 03:54:58.936793 139651363960640 call_variants.py:471] Total 1 writing processes started.; I1102 03:55:00.378331 139651363960640 dv_utils.py:365] From output/intermediate_results_dir/make_examples.tfrecord-00000-of-00042.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I1102 03:55:00.378495 139651363960640 call_variants.py:506] Shape of input examples: [100, 221, 7]; I1102 03:55:00.381343 139651363960640 call_variants.py:510] Use saved model: False; /usr/local/lib/python3.8/dist-packages/keras/applications/inception_v3.py:138: UserWarning: This model usually",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/725:10,test,tested,10,,https://github.com/google/deepvariant/issues/725,2,['test'],"['test', 'tested']"
Testability,"Hello,; When I tested Deepvariant, I found one site reported as Refcall by deepvariant.; `chr1	155205518	.	C	G	0.3	RefCall	.	GT:GQ:DP:AD:VAF:PL	./.:12:391:275,116:0.296675:0,11,29`; But the result of GATK is: `0/1:581,245:826:99:4620,0,14057`. How should I understand RefCall? The VAF of these sites is less than 0.3 due to misalignment caused by high homology in this region. Is there a way for deepvariant to recognize them as PASS?. Looking forward to your reply.; Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/750:15,test,tested,15,,https://github.com/google/deepvariant/issues/750,1,['test'],['tested']
Testability,"Hello. OS: Scicore Cluster, Linux; Deep Variant version:1.2.0; Installation: Singularity; Instrument: Ilumina; Data type: Whole exome sequencing analysis. I used the script given from this site:https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-exome-case-study.md. I edited the script to run in the cluster here:; ```; #!/bin/bash. #SBATCH --job-name=Deepvariant_debug; #SBATCH --cpus-per-task=2 # change this according to your needs; #SBATCH --mem=8G # change this according to your needs; #SBATCH --qos=30min # this was just for testing, but the example runs in less than 30 minutes; #SBATCH --output=myrun.o%j; #SBATCH --error=myrun.e%j. mkdir -p output; mkdir -p /scicore/home/cichon/GROUP/Ilumina/output/intermediate_results_dir. ulimit -u 10000; BIN_VERSION=""1.2.0""; # OUTPUT_DIR and INPUT_DIR should reside and exist inside your $HOME folder; export OUTPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output ; export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata ; # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add; # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container; # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif); singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \; /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \; --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \; --regions=/scicore/home/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/515:546,test,testing,546,,https://github.com/google/deepvariant/issues/515,1,['test'],['testing']
Testability,"Hello！ I run the rawdata of NA12878 download from [NCBI SRA](https://trace.ncbi.nlm.nih.gov/Traces/?view=run_browser&acc=ERR1905890&display=data-access) []() and I got it's capture kit is Agilent_V5.; First, I run the **oqfe protocol** to align, and the output CRAM as the input of Deepvariant.; I run Deepvariant in WES model **3 times**, the first one didn't have --region parameter, the second one use a adding **50** bp buffer on each side of the custom target regions in BED format, the last one is adding **100** bp.; Next, I got the **truth** Benchmarking variant calls form GIAB and it's confident call regions to run hap.py.; The final outcome is very good, but I find a detail didn't make sense: as the bed lengthened，the SNP performed better and better, but INDEL on the contrary that it's getting worse since the number is decreasing, but I think it is making sense that the number becomes more as the bed gets longer, just like SNP. As shown in the figure below.; ![image](https://user-images.githubusercontent.com/63234787/220512170-4506359f-8c72-44ff-8585-e4357f24c20b.png); Can you give me a detailed explanation of this detail？ Thank you very much！ ; Finally, thank you very much for developing such a great tool！",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/616:550,Benchmark,Benchmarking,550,,https://github.com/google/deepvariant/issues/616,1,['Benchmark'],['Benchmarking']
Testability,"Hello，; Thanks for this fast and useful germline calling tool. When I used DeepVariant 1.6.0 for single sample WES germline calling, I found that some real germline mutations with VAF (variant allele frequency) values less than 0.3 to 0.4 could not be called. IGV view figures of these variants are below. May I ask if DeepVariant considers VAF parameters during runtime or sets threshold filtering for VAF parameters? We look forward to your reply. ; Our Codes(All variables have been defined):; `singularity run \; -B ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"" \; deepvariant_1.6.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/input/testinput/human_g1k_v37_modified.fasta \; --reads=/input/${i}.sorted.markdup.BQSR.bam \; --regions /input/testinput/use_agilent_region_padding_100.bed \; --output_vcf=/output/${i}.vcf.gz \; --output_gvcf=/output/${i}.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir/${i} \; --num_shards=10; `; IGV figures:; ![EP90t_R](https://github.com/google/deepvariant/assets/174405155/6e9263f9-ff9e-4495-b51d-54127dbfc837); ![EP40b](https://github.com/google/deepvariant/assets/174405155/0b8f1fec-b5d9-4bb5-be97-6431c38135b0)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/843:671,test,testinput,671,,https://github.com/google/deepvariant/issues/843,2,['test'],['testinput']
Testability,"Helo everyone,. I tested DeepVariant 1.5.0 on quickstart-testdata, using WGS model and got the following error in call_variants.py script:. I0405 09:07:48.961947 140199643854656 call_variants.py:317] From ./examples.tfrecord-00000-of-00032.gz.example_info.json: Shape of input examples: [100, 221, 6], Channels of input examples: [1, 2, 3, 4, 5, 6].; Traceback (most recent call last):; File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 363, in call_variants; raise ValueError('The number of channels in examples and checkpoint '; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 7 channels while the examples have 6. My command line looks like this:. export HOME=/root && N_SHARDS=32 && GVCF_TFRECORDS=""./gvcf.tfrecord@${N_SHARDS}.gz"" &&",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/625:18,test,tested,18,,https://github.com/google/deepvariant/issues/625,2,['test'],"['testdata', 'tested']"
Testability,"Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads bamlink \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; ```; And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:; 1) Why doesn't this work?; 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks!. Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options); But I can't find the equivalent just for the make_examples section",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/149:281,Log,Log,281,,https://github.com/google/deepvariant/issues/149,3,"['Log', 'log']","['Log', 'log']"
Testability,"Hi , when i run call_variant , it arises this warn which means can't use the gpu,but i can make sure that the tensorflow can use the gpu.There are the screen shots of the warn and the existence of the gpu. - the gpu existence; ![image](https://github.com/google/deepvariant/assets/71956115/367b1a98-123c-48fb-b170-3f8e4aae7d30). ```python; tensorflow.test.is_gpu_available(); WARNING:tensorflow:From <stdin>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.; Instructions for updating:; Use `tf.config.list_physical_devices('GPU')` instead.; 2024-05-12 21:36:00.744470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /device:GPU:0 with 15089 MB memory: -> device: 0, name: Vega 20, pci bus id: 0000:26:00.0; True; ```. - the warn ; ![image](https://github.com/google/deepvariant/assets/71956115/246d5cfd-a9b3-4ac5-aea7-bf4c89401c76). ```shell; warnings.warn(; 2024-05-12 21:43:29.067332: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ```. - Operating system: Linux ; - DeepVariant version: 1.6.1-gpu; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/820:351,test,test,351,,https://github.com/google/deepvariant/issues/820,1,['test'],['test']
Testability,"Hi . I am a Newbie/Student with some ""best practices"" questions. I am using the docker version 0.9.0 and following the script in the quick start example. I have two data sets of Illumina reads one was from a control group, the other from a treatment group. I think the number of reads is comparable. One batch has been generating examples for over 3.5 days. The other batch is running on 32 vCPU machine. It is still generating images after 1.5 days. I checked the OS level stats, the machines are not swapping. all cpu's are at 100%. Still alot of unused memory and disk. 1) Any idea how I might estimate the expected run time? . 2) Any idea of how I do a better job sizing compute resources?. 3) How do I know if docker is making progress or not?; I did a run a couple of weeks ago. I think it took a total of 66 CPU hrs. It seemed like after all the examples where constructed the docker did not produce log message for a long time. Eventually, the docker completed. The results were really good!. 4) I run on ubuntu and use nohub. I want to run time, my script, report 'data up' on completions. I always put nohub job in the background. It seems like after examples have been constructed I will not see my nohub in the jobs list, however, if I use top I see all my cpu's are running python and are at 100%. Is there a better way to run my script. thanks. Andy. p.s. I am running in AWS . not sure if that makes a difference or not. p.p.s. Is there a better place to ask questions like this?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/260:907,log,log,907,,https://github.com/google/deepvariant/issues/260,1,['log'],['log']
Testability,"Hi . I am using the quick start docker image. I think all the examples have been created at this point. When it first started creating examples top showed that all 64 of my cpu's where at 100% utilization, and there was still lots of available memory. I have not seen any new log files in over a day. I have check top several times over the last 2 days. It only shows 2 python processes and each of them is at 800% utilization. In my experience training models takes a long time, however making predictions is quick. Should I kill my job and try and start over again? I ran into a problem like this before on a much smaller machine. After 11 days I killed the jobs. I do not know much about docker. I looked in /var/lib/docker/containers. I did not find anything that looked a like a log file. any debugging tips would be appreciated. Andy. config ; ```; google/deepvariant:0.9.0; --model_type=WES; --regions=/input/agilent_sureselect_human_all_exon_v5_b37_targets.bed; --num_shards=64; ```; Looks like make_example completed; ```; I0208 03:49:00.939260 140440947410688 make_examples.py:1330] Writing examples to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00063-of-00064.gz; I0208 03:49:00.940793 140440947410688 make_examples.py:1334] Writing gvcf records to /tmp/deepvariant_tmp_output/gvcf.tfrecord-00063-of-00064.gz; I0208 03:49:01.427521 140440947410688 make_examples.py:905] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; 2020-02-08 03:49:01.428281: I third_party/nucleus/io/sam_reader.cc:660] Setting HTS_OPT_BLOCK_SIZE to 134217728; I0208 03:49:01.743115 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam with NativeSamReader; I0208 03:49:01.755232 140440947410688 genomics_reader.py:223] Reading /kimLab/kras.ipsc/bulk.data/day.5/ctrl.1/star.out/pass.2/Aligned.out.q11.sorted.bam wit",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/269:276,log,log,276,,https://github.com/google/deepvariant/issues/269,2,['log'],['log']
Testability,"Hi @pichuan,; Thank you for posting these Singularity images. I tested them on a computing cluster on the test data, following the instructions above and ran into errors similar to those posted in another issue, below. I'm wondering if you have any suggestions for workarounds? Thank you in advance for your help! (I realized posting in a new issue made more sense than adding to a closed issue). ; Best,; ```. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/labs/jandr/walter/tb/test/deepV/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. ImportError: No module named _multiarray_umath; ImportError: No module named _multiarray_umath; ImportError: numpy.core._multiarray_umath failed to import; ImportError: numpy.core.umath failed to import; 2020-01-28 19:06:29.164168: F tensorflow/python/lib/core/bfloat16.cc:675] Check failed: PyBfloat16_Type.tp_base != nullptr . real	0m4.153s; user	0m0.699s; sys	0m1.614s. ```. _Originally posted by @ksw9 in https://github.com/google/deepvariant/issues/243#issuecomment-579406829_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/265:64,test,tested,64,,https://github.com/google/deepvariant/issues/265,7,"['log', 'test']","['login', 'test', 'testdata', 'tested']"
Testability,"Hi DeepVariant team,. I have been using DeepVariant （v1.1.0）to call small variants for a human genome with approximately 50X coverage of HiFi reads. During my analysis, I noticed that DeepVariant consistently reports some false positives in some regions. For example, at position chr10:89013075-89013077, there is a SNV (see the IGV snapshot of both Illumina and HiFi reads), but DeepVariant identifies three variants at that location. The vcf records by deepvariant:; ```; chr10 89013075 . T TC 31.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:31:63:35,28:0.444444:31,0,43; chr10 89013076 . CA C 28.2 PASS . GT:GQ:DP:AD:VAF:PL 0/1:27:63:35,28:0.444444:28,0,34; chr10 89013077 . A C 30.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:34:0,34:1:30,0,17; ```; The IGV snapshots:. ![image](https://github.com/google/deepvariant/assets/44404441/9618a4b9-7594-42e0-bb65-173761f803a8). I haven't identified any patterns in these regions. Have you encountered similar situations before? Could you please explain the possible reasons behind these false positive calls? I have provided three examples, including small BAM files, resulting VCF files, and my code, for you to replicate and investigate. . [deepvariant_fp.zip](https://github.com/google/deepvariant/files/11735375/deepvariant_fp.zip). Other information of my test: . - Operating system: Red Hat 4.8.5-36; - DeepVariant version: 1.1.0; - Installation method (Docker, built from source, etc.): Docker; - reference : GRCh38_full_analysis_set_plus_decoy_hla.fa; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); ~50x Human HiFi seuquencing data. Thank you very much for your attention and support. I look forward to your response. Best regards,; Peng Jia",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/660:1286,test,test,1286,,https://github.com/google/deepvariant/issues/660,1,['test'],['test']
Testability,"Hi DeepVariant team,. I'm running DeepVariant via Singularity via Snakemake on a HPC cluster and overall, if I create an interactive session, my entire WES pipeline runs fine until an odd DeepVariant job fails (by that I mean that let's say 30 deepvariant jobs finish ok and the 31st fails). I can then restart the Snakemake run and the same job will run fine, followed by more jobs that will also run fine until another odd jobs fails. This is also particularly true when I use the --cluster command in Snakemake (i.e., send it to a SLURM job scheduler from the master node) - in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:; ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```; rule deepvariant:; input:; bam=rules.apply_bqsr.output.bam,; ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'; output:; vcf=""results/deepvariant/{sample}.vcf.gz""; params:; model=""WES""; threads: ; 64; resources:; mem_mb=163840; log:; ""logs/deepvariant/{sample}/stdout.log""; singularity:; ""singularity/deepvariant_1.4.0.sif""; # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU; shell:; """"""; /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'; """"""; ```. Below is the begening and end of",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:704,log,log,704,,https://github.com/google/deepvariant/issues/602,1,['log'],['log']
Testability,"Hi Deepvariant developer:. I paste the error report when I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \; --mode calling \; --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: ; I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs; 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: ; I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 998, in processing_regions_from_options; options.exclude_calling_regions",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/155:83,test,testing,83,,https://github.com/google/deepvariant/issues/155,4,['test'],"['testdata', 'testing']"
Testability,"Hi Mark (@depristo),. Sorry, I meant to put this together a while ago - regarding https://github.com/google/deepvariant/issues/27#issuecomment-364683474 - but got a bit swamped with a research deadline. In any case, this is purely for intellectual curiosity and discussion. Regarding the first point, where differences in allocated CPUs might be the cause for the timing, that could be remedied by specifying a minimal CPU requirement, as noted here:. https://cloud.google.com/compute/docs/instances/specify-min-cpu-platform. So to control for the variability in the test, the two options are either: a) to set the `--min-cpu-platform` setting to the maximum available (`""Intel Sandy Bridge""`), or b) to keep requesting and canceling instances until the desired one is allocated on which all tests should be performed, thus satisfying consistency. Just as a quick inspection, by looking at the CPU cycles utilization, I just ran a performance analysis of 0.4 and 0.5.1 on `make_examples` - since it displayed the initial discrepancy - and there seem to be some slight increases in `0.5.1`, which might cumulatively affect things. In any case, below is the top of the call-graph of percent utilization by method (per version):. #### DV 0.4. ```; # Samples: 186K of event 'cpu-clock'; # Event count (approx.): 46604750000; #; # Children, Self,Command ,Shared Object ,Symbol ; 50.33% , 8.80% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--42.49%--PyEval_EvalFrameEx; | | ; | |--30.79%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.34%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--27.87%--ssw_align; | | | | ; | | | |--14.65%--sw_sse2_word; | | | | ; | | | |--8.32%--sw_sse2_byte; | | | | ; | | | |--2.91%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.36%--ssw_init; | | | ; | | --0.89%--qP_byte; | | ; | |--3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.04%--lea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:567,test,test,567,,https://github.com/google/deepvariant/issues/50,2,['test'],"['test', 'tests']"
Testability,"Hi There,. I compiled the whole deepvariant on ppc64le with IBM Advance Toolchain 11.0 with all its dependencies on RHEL 7.5. And most test cases passed, but only 2 test cases failed. Found one root cause today of ""//deepvariant/labeler:haplotype_labeler_test"" as following. While suppose this is not related to platform/environment issue? Would you please kindly help to comment how to fix this error?. The detailed root cause please refer to the comments inline in the code, thanks in advance :). In the test file of ""deepvariant/labeler/haplotype_labeler_test.py"", the function of ""test_make_labeler_ref"". ```python; def test_make_labeler_ref(self, candidates, truths, expected_start,; expected_end, bufsize):; expected_bases = 'A' * (expected_end - expected_start). ## generate a Mock object instead of real object of InMemoryFastaReader; labeler = _make_labeler(); labeler._ref_reader.query.return_value = expected_bases. labeler_ref = labeler.make_labeler_ref(candidates, truths, bufsize=bufsize). labeler._ref_reader.query.assert_called_once_with(; ranges.make_range('20', expected_start, expected_end)); self.assertEqual(labeler_ref.start, expected_start); self.assertEqual(labeler_ref.end, expected_end); self.assertEqual(; labeler_ref.bases(expected_start, expected_end), expected_bases); ```. So when in the file of ""deepvariant/labeler/haplotype_labeler.py"", the function of ""make_labeler_ref"" will generate an incorrect output as ""self._ref_reader"" is mock. ```python; def make_labeler_ref(self, candidates, true_variants, bufsize=20):; all_variants = candidates + true_variants; contig = all_variants[0].reference_name; start = min(x.start for x in all_variants); end = max(x.end for x in all_variants). ## always output contig_nbp = 1, as self._ref_reader is Mock object; ## in fact contig_nbp=[<MagicMock name='mock.contig().n_bases' id='70366068929488'>]; ## change the above type to int becomes ""1"", then the region.end will be 1 to cause test fail; contig_nbp = self._ref_reader.con",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154:135,test,test,135,,https://github.com/google/deepvariant/issues/154,4,"['Mock', 'test']","['Mock', 'test']"
Testability,"Hi all,. I am currently running some tests using deepvariant and getting error codes that I do not understand.; I hope you can help me finding the error. (Sorry for the long post, but the devil is probably in the details!!). Script:; #!/bin/bash; #set -euo pipefail; # Set common settings.; PROJECT_ID=ms-deepvariant; OUTPUT_BUCKET=gs://ms_bam/deep_output; STAGING_FOLDER_NAME=stage; OUTPUT_FILE_NAME=deeptest_FB4_chr20.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/; IMAGE_VERSION=0.7.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://public_bed/CHR20.bed \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --ref_fai gs://ms_bam/Homo_sapiens_assembly38.fasta.fai \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones europe-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}"". 1. I have quoted #set -euo pipefail out as it returns an error.; 2. The bed file is located in a public bucket #119 ; 3. I have tried with docker image 0.7.1 which returns following error:. [12/12/2018 14:14:08 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:37,test,tests,37,,https://github.com/google/deepvariant/issues/129,1,['test'],['tests']
Testability,"Hi all,. I have encountered the following error when running deepvaraint on a wgs file. ‘[E::vcf_parse_format] Incorrect number of FORMAT fields at 1:19355\n.’ I have googled it but I am not sure what is causing this issue and how to resolve it. I will appreciate your help. ; I have attached the full error log, bash and yaml files. . error:; code: 10; message: |-; 11: Docker run failed: ASS""\ncalls {\n info {\n key: ""AD""\n value {\n values {\n int_value: 8\n }\n values {\n int_value: 7\n }\n }\n }\n info {\n key: ""DP""\n value {\n values {\n int_value: 15\n }\n }\n }\n info {\n key: ""GQ""\n value {\n values {\n int_value: 12\n }\n }\n }\n…….; ……….; ………..Reading /mnt/data/output/gs/gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/UDN631726_deepVariant_v0.6.1.g.vcf with NativeVcfReader\nI0806 01:37:07.984452 140434439055104 postprocess_variants.py:593] Writing output to VCF file: /mnt/data/output/gs/gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/UDN631726_deepVariant_v0.6.1.g.vcf\nI0806 01:37:08.052251 140434439055104 genomics_writer.py:118] Writing /mnt/data/output/gs/gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/UDN631726_deepVariant_v0.6.1.g.vcf with NativeVcfWriter\n**[E::vcf_parse_format] Incorrect number of FORMAT fields at 1:19355\n.** See logs at gs://gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/deepvariant_staging_folder/logs/']]. Operation ID: ENqznd7QLBi1jMjtufCo3ckBIK2c48-5AyoPcHJvZHVjdGlvblF1ZXVl. Best,; Shruti. Shruti Marwaha, PhD.; Research Engineer,; Stanford Center for Undiagnosed Diseases; Stanford University. [UDN631726_gvcf_error_08052018.log](https://github.com/google/deepvariant/files/2263671/UDN631726_gvcf_error_08052018.log); Since GIT does not allow me to attach .sh or .yaml files, I am saving them as text files and attaching.; [deepvariant_v0.6.1_UDN631726.yaml.txt](https://github.com/google/deepvariant/files/2263680/deepvariant_v0.6.1_UDN631726.yaml.txt); [deepvariant_v0.6.1_UDN631726.sh.txt](https://github.com/google/d",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/86:308,log,log,308,,https://github.com/google/deepvariant/issues/86,1,['log'],['log']
Testability,"Hi everyone,. I tested DeepVariant 1.1.0 on PACBIO data (HG002, chr20), using PACBIO model and got following error in call_variants.py script:; `I0519 14:15:32.843033 139847781275392 call_variants.py:338] Shape of input examples: [100, 221, 8]; Traceback (most recent call last):; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/458:16,test,tested,16,,https://github.com/google/deepvariant/issues/458,1,['test'],['tested']
Testability,"Hi guys, . I have been doing some test on DeepVariant genotyping call. I run the suit and got very good results, but as part of experiments I need to generate a cohort VCF (multi-sample). As suggested here on github, I generated GVCF for all my samples, but when I tried to use GATK's CombineGVCFs or GenotypeGVCFs neither worked because they don't recognize the alternative allele `<*>`, instead GATK moved to `<NON_REF>` on the more recent versions.; After identified the problem, I run a simple substitution using `sed` to replace all occurrences of `<*>` for `<NON_REF>` and the commands ran fine. . `zcat SAMPLE.deepvar.g.vcf.gz | sed 's/<*>/<NON_REF>/g' | bgzip -c > SAMPLE.gvcf.gz`. May I suggest this update for your software to keep the compatibility with GATK?. Best,; André Santos",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/83:34,test,test,34,,https://github.com/google/deepvariant/issues/83,1,['test'],['test']
Testability,"Hi guys, . I'm running DeepVariant—pretty much exactly as outlined at https://cloud.google.com/genomics/deepvariant —but I can't seem to get it to work. . My `deepvariant_configuration.yaml` (unmodified from guide) is ; ```; name: deepvariant_pipeline; inputParameters:; - name: PROJECT_ID; - name: OUTPUT_BUCKET; - name: MODEL; - name: DOCKER_IMAGE; - name: DOCKER_IMAGE_GPU; - name: STAGING_FOLDER_NAME; - name: OUTPUT_FILE_NAME; docker:; imageName: gcr.io/deepvariant-docker/deepvariant_runner; cmd: |; ./opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ""${PROJECT_ID}"" \; --zones 'us-*' \; --docker_image ""${DOCKER_IMAGE}"" \; --outfile ""${OUTPUT_BUCKET}""/""${OUTPUT_FILE_NAME}"" \; --staging ""${OUTPUT_BUCKET}""/""${STAGING_FOLDER_NAME}"" \; --model ""${MODEL}"" \; --bam gs://deepvariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --ref gs://deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta.gz \; --regions ""chr20:10,000,000-10,010,000""; ```. My `runner.sh` (changed `PROJECT_ID`,`OUTPUT_BUCKET`, and `STAGING_FOLDER_NAME`) is; ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=udndv-197518 #changed; OUTPUT_BUCKET=gs://udnXXXXXX #changed; STAGING_FOLDER_NAME=staging-folder #changed; OUTPUT_FILE_NAME=output.vcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-182548131.data-wgs_standard; # Model for calling exome sequencing data.; # MODEL=gs://deepvariant/models/DeepVariant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-181413382.data-wes_standard; IMAGE_VERSION=0.5.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; DOCKER_IMAGE_GPU=gcr.io/deepvariant-docker/deepvariant_gpu:""${IMAGE_VERSION}"". # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --pipeline-file deepvariant_pipeline.yaml \; --logging ""${OUTPUT_BUCKET}""/runner_logs \; --zones us-west1-b \; --inputs `echo \; PROJECT_ID=""${PROJEC",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:811,test,testdata,811,,https://github.com/google/deepvariant/issues/60,2,['test'],['testdata']
Testability,"Hi there!. I tried to combine the deepvariant's gvcf files using GATK and it returned a error because of the ""NON_REF"".; Do you have any other recommendation? Or did you tested your gvcf in GATK?. This is the error that I got using GATK:; A USER ERROR has occurred: The list of input alleles must contain <NON_REF> as an allele but that is not the case at position 10325413; please use the Haplotype Caller with gVCF output to generate appropriate records. The DeepVariant ""<NON_REF>"" allele is ""<*>"". _Originally posted by @jaqueytw in https://github.com/google/deepvariant/issues/45#issuecomment-481414559_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/170:170,test,tested,170,,https://github.com/google/deepvariant/issues/170,1,['test'],['tested']
Testability,"Hi there,. I have no problem running the CPU version but the GPU version was not function normally either through slurm request or logging in to the GPU node. I found no GPU process through $nvidia-smi with my codes attached as follow:. ```; singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; --bind /cluster/home/cx/variant_calling/slurm/DV/in:/in \; --bind /cluster/home/cx/variant_calling/slurm/DV/out:/out \; docker://google/deepvariant:1.5.0-gpu \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/in/GRCh37_primary/GRCh37.primary_assembly.genome.fa \; --reads=/in/SRR2962694.sorted.bam \; --regions=/in/all_19.bed \; --output_vcf=/out/SRR2962694.vcf.gz; ```. Here is the GPU status after running the code above, no process on GPU is running.; ![微信图片_20230728102005](https://github.com/google/deepvariant/assets/80762999/ae17ac46-1c73-449e-9943-c627a9bc13a7)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/688:131,log,logging,131,,https://github.com/google/deepvariant/issues/688,1,['log'],['logging']
Testability,"Hi there,. We ran deepvariant on test data, and we had the following error:. I0416 16:33:15.202579 46954465520640 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 3 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""reference/GRCh38_no_alt_analysis_set.fasta"" --reads ""input/HG003.GRCh38.chr20.pFDA_truthv2.bam"" --examples ""/tmp/tmpmxakjtgh/make_examples.tfrecord@4.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --norealign_reads --regions ""chr20"" --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 2. We converted docker image to singularity sandbox. And our command is like this:; singularity exec -B /data -B /home -B /localhd/ \; ../deepvariant-cpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant1/output.vcf.gz \; --num_shards ${nproc} \; --regions chr20. I am guessing we may have misconfigured some module. Any idea how to fix it?. Thanks. George",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/444:33,test,test,33,,https://github.com/google/deepvariant/issues/444,2,"['sandbox', 'test']","['sandbox', 'test']"
Testability,"Hi! I compiled from source on Ubuntu 14.4. . Tests pass, including `./bazel-bin/deepvariant/make_examples_test`. However, running as simple `make_examples` with downloaded example data fails with confusing (for me) error in realign script. Same script worked fine when I ran pre-compiled binary from the Docker container 🤷‍♂️. ```; input=""./quickstart-testdata"". ./bazel-bin/deepvariant/make_examples \; --ref=$input/ucsc.hg19.chr20.unittest.fasta \; --reads=$input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --examples examples.tfrecord@1.gz \; --mode calling \; --logging_every_n_candidates 10 \; --realign_reads; ```. ```; ./make_examples_demo.sh ; 2019-07-16 17:49:02.877175: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:02.877284 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.117142 139897470359360 make_examples.py:1110] Preparing inputs; 2019-07-16 17:49:03.117644: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.117749 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.118745 139897470359360 make_examples.py:1034] Common contigs are [u'chr20']; I0716 17:49:03.120177 139897470359360 make_examples.py:1116] Writing examples to examples.tfrecord-00000-of-00001.gz; 2019-07-16 17:49:03.121118: I third_party/nucleus/io/sam_reader.cc:600] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-07-16 17:49:03.124279: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.124422 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199:45,Test,Tests,45,,https://github.com/google/deepvariant/issues/199,3,"['Test', 'test']","['Tests', 'testdata']"
Testability,"Hi, . I had trouble running Deepvariant using conda. I ran the following command.; ```; dv_make_examples.py --sample {MY_SAMPLE} --ref {MY_FASTA}.fasta --reads {MY_BAM}.bam --logdir ./log/ --examples examples/; ```. and I got an error like this:; ```; ETA: 0s Left: 1 AVG: 0.00s local:1/0/100%/0.0s sh: 1: unzip: not found; Traceback (most recent call last):; File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 193, in _run_module_as_main; ""__main__"", mod_spec); File ""/opt/conda/envs/deepvariant/lib/python3.6/runpy.py"", line 85, in _run_code; exec(code, run_globals); File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 252, in <module>; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 187, in Main; File ""/opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip/__main__.py"", line 138, in GetRepositoriesImports; FileNotFoundError: [Errno 2] No such file or directory: '/tmp/Bazel.runfiles_qwsw52c7/runfiles'; parallel: This job failed:; /opt/conda/envs/deepvariant/bin/python /opt/conda/envs/deepvariant/share/deepvariant-0.10.0-3/binaries/DeepVariant/0.10.0/DeepVariant-0.10.0/make_examples.zip --mode calling --ref E.coli_K12_MG1655.fa --reads SRR1770413.bam --examples examples//SRR1770413.tfrecord@1.gz --task 0; ```. When I installed unzip by `conda install`, the command worked fine. When using conda to install deepvariant, should it not have to be installed together?. Thanks,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/314:175,log,logdir,175,,https://github.com/google/deepvariant/issues/314,2,['log'],"['log', 'logdir']"
Testability,"Hi, . This is more of a support question, but I wasn't sure where else to get help. I'm trying to build and test deepvariant inside of a docker image. I know that there is already an image published to google cloud, but for my purposes I prefer to build my own image. My docker file looks like this. ; ```; FROM ubuntu:16.04. RUN set -ex \; && buildDependencies=' \; ca-certificates \; curl \; wget \; git \; apt-transport-https \; xz-utils \; bzip2 \; make \; ' \; && apt-get update \; && apt-get install -y --no-install-recommends $buildDependencies \; # gsutil; && wget https://storage.googleapis.com/pub/gsutil.tar.gz \; && tar xfz gsutil.tar.gz -C $HOME && rm gsutil.tar.gz \; && export PATH=$PATH:$HOME/gsutil \; # deepvariant; && git clone https://github.com/google/deepvariant.git \; && cd deepvariant \; && git checkout v0.4.1 \; && ./build-prereq.sh \; && ./build_and_test.sh; ```; The `build_and_test.sh` script fails with these errors:; ```; + ./build_and_test.sh; + source settings.sh; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=0; ++ TF_ENABLE_XLA=0; ++ export TF_NEED_CUDA=0; ++ TF_NEED_CUDA=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=0; ++ TF_NEED_MKL=0; ++ export TF_NEED_MPI=0; ++ TF_NEED_MPI=0; ++ export TF_NEED_OPENCL=0; ++ TF_NEED_OPENCL=0; ++ export TF_NEED_OPENCL_SYCL=0; ++ TF_NEED_OPENCL_SYCL=0; ++ export TF_NEED_S3=0; ++ TF_NEED_S3=0; ++ export TF_NEED_VERBS=0; ++ TF_NEED_VERBS=0; ++ export TF_CUDA_VERSION=8.0; ++ TF_CUDA_VERSION=8.0; ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ export TF_CUDNN_VERSION=6; ++ TF_CUDNN_VERSION=6; ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ export DV_P",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:108,test,test,108,,https://github.com/google/deepvariant/issues/19,1,['test'],['test']
Testability,"Hi, ; I was testing the variants calling by deepvariant (DV) using PacBio CCS long reads https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/PacBio_SequelII_CCS_11kb/HG002.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.10x.trio.bam.; But I found some issues in the region 22:16,977,867-16,978,040 (hs37d5) show in the following IGV screenshot.; ![fig1](https://user-images.githubusercontent.com/27715065/68264666-81b49500-0084-11ea-8887-2283259b3df3.png). In the region, DV can call SNPs at position 16977891(A->G) and 17977975(A->G). A strange question, all the variants come out simultaneously in some reads at pos 16977870(A->T), 16977879(7bp insertion), 16977911(G->A), 16977924(T->C), 16977984(A->G) and 16978027(2bp deletion), the read counts support reference and variant allele is 15:12. What's more, I also test GATK4, both DV and GATK4 don't give these variants.; So I doubt that why DV doesn't give these positions as variants, if so, are there some tips for filtering these positions? Looking forward to your answer. Best!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/233:12,test,testing,12,,https://github.com/google/deepvariant/issues/233,2,['test'],"['test', 'testing']"
Testability,"Hi, ; I'm intersested in de novo variant identification from human Illumina WGS data. As DeepTrio seems very powerful for this task since the 1.6.0 verison of DeepVaraint, I have performed some tests and it indeed seems that it achieves great results. . In my workflow, I apply an series of empirical filters with bcftools view to the glnexus merged trio vcf generated by DeepTrio. Filters include the following metrcs : GT, DP, VAF extracted using AD, and others. While results are OK, there are still false positive (particularly for indels) and a few false negatives too. . In the blogpost you state that you have yield impressive resluts: ""for chr20 at 30x HG002-HG003-HG004, false negatives reduced from 8 to 0 with DeepTrio v1.4, false positives reduced from 5 to 0"". ; I imagine you must have applied a dedicated method to isolate de novo variants from trio data? If so, would you be willing to share this method? . Thanks for your great software. ; François",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/779:194,test,tests,194,,https://github.com/google/deepvariant/issues/779,1,['test'],['tests']
Testability,"Hi, @chapmanb; Sorry for asking so many questions. I looked for the `run_deepvariant` command in the conda package, but the I couldn't find it.; So to generate the vcf file, I created the following code. ```; dv_make_examples.py \; --cores {threads} \; --ref {ref} \; --reads {bam} \; --sample {samplename} \; --examples {output_dir}/{samplename} \; --logdir {log_dir} . dv_call_variants.py \; --cores {threads} \; --outfile {output_dir}/{samplename}.tmp \; --sample {samplename} \; --examples {output_dir}/{samplename} \; --model {model} . dv_postprocess_variants.py \; --ref {ref} \; --infile {output_dir}/{samplename}.tmp \; --outfile {vcf}; ```. Or is there a way to run the `run_deepvariant` command that I am just not aware of?; I'd like to generate a gvcf file if possible.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/316:352,log,logdir,352,,https://github.com/google/deepvariant/issues/316,1,['log'],['logdir']
Testability,"Hi, I get a fail info when i try run the build_and_test.sh. So the fail info will affect my normal using?; ![image](https://user-images.githubusercontent.com/15261087/33794972-3a17c480-dd11-11e7-9276-68e701695cd0.png). the log info is ; ![image](https://user-images.githubusercontent.com/15261087/33794983-50dbcc52-dd11-11e7-8f0d-bc1a80445beb.png)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/7:223,log,log,223,,https://github.com/google/deepvariant/issues/7,1,['log'],['log']
Testability,"Hi, I got an error when running ; `( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21 chr22'"" \; ) >""${LOG_DIR}/validation_set.with_label.make_examples.log"" 2>&1`. the log message in validation_set.with_label.make_examples.log is as below:. `[E::hts_open_format] Failed to open file /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/184:108,log,log,108,,https://github.com/google/deepvariant/issues/184,4,['log'],['log']
Testability,"Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose?; 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features.; 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed?. Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/790:423,test,test,423,,https://github.com/google/deepvariant/issues/790,1,['test'],['test']
Testability,"Hi, I try to setup deepvariant on my ubuntu, following instructionsin section Download binaries, models, and test data, listed here.; https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.mddy. I get httplib.ResponseNotReady error.; ![screenshot from 2017-12-11 13-31-30](https://user-images.githubusercontent.com/16382685/33831300-c7d8b290-de77-11e7-957f-748aadf16347.png). I was able to run the repository docker, but I would like to use this variant caller on machine without internet access. Is this possible?. Kind regards,. --; Tomasz",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/11:109,test,test,109,,https://github.com/google/deepvariant/issues/11,1,['test'],['test']
Testability,"Hi, I wanted to try adding some custom channels to experiment with like in the tutorial [here](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). I was wondering if there were any developer's notes on setting up an environment for good syntax highlighting, autocomplete, etc. for C++ with this project so I can better understand how to use the Nucleus library. The [build guide](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md) didn't seem to have anything to this effect. I tried using VSCode and CLion Nova, but can't seem to get the autocomplete to work on either. I'm guessing this is because the project is built using the Dockerfile and incorporates a few different languages, so it's not a ""pure"" C++ project. . Could any developers share some tips on their setup, or point me to a developer's guide? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/792:479,test,test,479,,https://github.com/google/deepvariant/issues/792,1,['test'],['test']
Testability,"Hi, I'm running DeepVariant with BQSR -adjusted bam files. I have sequencing data for hg002 and hg005 and I have to say the validation results are very impressive!. I wanted to test the option for using the original base quality scores with:. --parse_sam_aux_fields ; --use_original_quality_scores. but get the following error: . FATAL Flags parsing error: Unknown command line flag 'parse_sam_aux_fields'; Pass --helpshort or --helpfull to see help on flags. I was running DeepVariant with docker by following the whole genome sequencing case study -tutorial, but will next test the pipeline for multi-sample variant calling for my cohort of 50 samples. I'm wondering should I realign the reads or is it possible to use the original base quality scores from BQSR adjusted bam files? I was previously using the GATK4 pipeline, but the results are so much better with DeepVariant and as a bonus, it's a million times easier (and quicker). Thanks. Karoliina",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/595:177,test,test,177,,https://github.com/google/deepvariant/issues/595,2,['test'],['test']
Testability,"Hi, I've gotten a test run successfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:; ```; -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz; ...; -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:18,test,test,18,,https://github.com/google/deepvariant/issues/151,11,['test'],['test']
Testability,"Hi, is it possible to get some information regarding training of default PacBio models in v1.0.0? Specifically,. 1. What genomes were used in training?; 2. What was the source of BAM or FASTQ files of genomes used for training, and are they publicly available?; 3. What ground truth variant call set was used for training? If GIAB benchmark variants were used, then which version was used?. I see this information is available [here ](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md)for Illumina models but not for PacBio. Also, can you please tell us which is the latest PacBio model trained on v3.3.2 of GIAB benchmark variant calls?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/381:331,benchmark,benchmark,331,,https://github.com/google/deepvariant/issues/381,2,['benchmark'],['benchmark']
Testability,"Hi, so sorry for asking something again. But I really want to use mostly DeepVariant for variant calling. . **Setup**; - Operating system: Red Hat Enterprise Linux 9; - DeepVariant version: 1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: WES mapped to hg19. **My code:**; - Commands: ; ```; #!/bin/bash; #$ -l m_mem_free=200G; #$ -l os=rhel9; #$ -m bea; #$ -cwd; #$ -pe smp 2; #$ -o deepvariant_output.log; #$ -e deepvariant_error.log. cd path/to/deepvariant. BAM_DIR=.; VCF_DIR=deepvariant_output/; REFERENCE=Reference_HLA/human_g1k_v37_decoy.fasta. export SINGULARITY_CACHEDIR=""path/to/deepvariant/.singularity-$(whoami)""; export SINGULARITY_TMPDIR=""path/to/deepvariant/.singularity-$(whoami)"". BIN_VERSION=""1.6.1"". for BAM_FILE in ""${BAM_DIR}""/*.bam; do; # Extract the base name of the BAM file (without the directory and extension); BASE_NAME=$(basename ""${BAM_FILE}"" .bam). # Define the output VCF file name; VCF_FILE=""${VCF_DIR}/${BASE_NAME}.vcf.gz""; echo $BAM_FILE; echo $VCF_FILE; singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref $REFERENCE \; --reads $BAM_FILE \; --regions 6:32509320-32669663 \; --output_vcf $VCF_FILE \; --num_shards 12; done; ``` . - Error trace: ; ```; ***** Running the command:*****; time seq 0 11 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""Reference_HLA/chr6_hg19.fa"" --reads ""./MDC05_1463_3.final.bam"" --examples ""/tmp/7361351.1.gpu.q/tmpzsp9g_vq/make_examples.tfrecord@12.gz"" --channels ""insert_size"" --regions ""chr6:32509320-32669663"" --task {}. [libprotobuf ERROR external/com_google_protobuf/src/google/protobuf/wire_format_lite.cc:584] String field 'nucleus.genomics.v1.Program.command_line' contains invalid UTF-8 data when serializing a protocol buffer. Use the 'bytes' type if you intend to send raw bytes.; [libprotobuf ERROR external/com_google_protobuf/src/g",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/870:442,log,log,442,,https://github.com/google/deepvariant/issues/870,2,['log'],['log']
Testability,"Hi, with WGS (mm10, but the documentation says WGS human model should work for this), I got make_examples running fine with 4 cores and shards, but as soon as I go above 4 I get a strange lag. I'm running this recommended command:. ```; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads bamlink \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo. ```; With 4, I get a nice stdout update per shard, first a description of the contigs:. `I0201 17:01:29.167016 140183113676544 make_examples.py:946] Common contigs are [u'chr1', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'chr17', u'chr18', u'chr19', u'chr1_GL456210_random', u'chr1_GL456211_random', u'chr1_GL456212_random', u'chr1_GL456213_random', u'chr1_GL456221_random', u'chr2', u'chr3', u'chr4', u'chr4_GL456216_random', u'chr4_JH584292_random', u'chr4_GL456350_random', u'chr4_JH584293_random', u'chr4_JH584294_random', u'chr4_JH584295_random', u'chr5', u'chr5_JH584296_random', u'chr5_JH584297_random', u'chr5_JH584298_random', u'chr5_GL456354_random', u'chr5_JH584299_random', u'chr6', u'chr7', u'chr7_GL456219_random', u'chr8', u'chr9', u'chrX', u'chrX_GL456233_random', u'chrY', u'chrY_JH584300_random', u'chrY_JH584301_random', u'chrY_JH584302_random', u'chrY_JH584303_random', u'chrUn_GL456239', u'chrUn_GL456367', u'chrUn_GL456378', u'chrUn_GL456381', u'chrUn_GL456382', u'chrUn_GL456383', u'chrUn_GL456385', u'chrUn_GL456390', u'chrUn_GL456392', u'chrUn_GL456393', u'chrUn_GL456394', u'chrUn_GL456359', u'chrUn_GL456360', u'chrUn_GL456396', u'chrUn_GL456372', u'chrUn_GL456387', u'chrUn_GL456389', u'chrUn_GL456370', u'chrUn_GL456379', u'chrUn_GL456366', u'chrUn_GL456368', u'chr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/150:273,Log,Log,273,,https://github.com/google/deepvariant/issues/150,3,"['Log', 'log']","['Log', 'log']"
Testability,"Hi,. As a follow-up to my [previous question](https://github.com/google/deepvariant/issues/166), I am trying to run the **postprocess_variants** command for DeepVariant from a Docker container on an AWS instance. I am getting the following error message:. ```; terminate called after throwing an instance of 'std::bad_alloc'; what(): std::bad_alloc; ```. I thought this might have been an issue with memory allocation, but I have been testing the same command with increasing computational resources (I am currently using a m5.4xlarge instance). **1)** Am I totally wrong about the underlying cause? If so, is there anything you can suggest to troubleshoot this issue?. **2)** Based upon my Google searches, it seemed like this might have something to do with using TensorFlow. Is it easy to tell if that is correct? If so, does that mean you are still doing variant calling/prediction from the **postprocess_variants** command?. This is the command that I am running:. ```. OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant; REF=/mnt/efs-genome/Ref/hg19.gatk.fasta. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". FINAL_OUTPUT_VCF=""${OUTPUT_DIR}/output.vcf.gz"". sudo docker run \; -v /mnt/efs-genome:/mnt/efs-genome \; gcr.io/deepvariant-docker/deepvariant \; /opt/deepvariant/bin/postprocess_variants \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${FINAL_OUTPUT_VCF}""; ```. Thank you very much for your assistance!. Sincerely,; Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/167:435,test,testing,435,,https://github.com/google/deepvariant/issues/167,1,['test'],['testing']
Testability,"Hi,. I am working on a [deepvariant pipeline](https://github.com/nf-core/deepvariant/tree/dev) written in nextflow which installs deepvariant via conda. The command for `make_examples` will evaluate to something like this:; ```bash; time seq 0 !{numberShardsMinusOne} | \; parallel --eta --halt 2 \; python /opt/conda/pkgs/deepvariant-0.7.0-py27h5d9141f_0/share/deepvariant-0.7.0-0/binaries/DeepVariant/0.7.0/DeepVariant-0.7.0+cl-208818123/make_examples.zip \; --mode calling \; --ref chr20.fa.gz \; --reads test.bam \; --examples shardedExamples/examples.tfrecord@2.gz; --task {}; ```. I noticed that there is `dv_make_examples.py` on the PATH. Does this executable carry out the same/similar function as `python make_examples.zip`. If so how could I modify the code above to make it work?. Many thanks in advance",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/101:508,test,test,508,,https://github.com/google/deepvariant/issues/101,1,['test'],['test']
Testability,"Hi,. I encountered a problem running DeepVariant v1.5.0. I ran the following code:. ```; singularity run -B /scratch \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --customized_model=${MODEL_DIR}/model.ckpt \; --ref=""${FASTA_DIR}""/genome.fa \; --reads=""${READ_DIR}""/SRR6006655Aligned.sortedByCoord.out.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --num_shards=${NCPU} #\ **How many cores the `make_examples` step uses. Change it to the number of CPU cores you have.** ; ```. The error log is attached. ; [deepvariant_test.txt](https://github.com/google/deepvariant/files/14489514/deepvariant_test.txt). The error is about numpy compatibility. `RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem`. I also got the same error with DeepVariant 1.6.0. Please advise. Thanks and good day.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/782:743,log,log,743,,https://github.com/google/deepvariant/issues/782,1,['log'],['log']
Testability,"Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:; ```; time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=s3-mount/deepvariant_training/script/dv_config.py:base \; --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \; --config.num_epochs=0 \; --config.learning_rate=0.02 \; --config.num_validation_examples=0 \; --experiment_dir=""model_train"" \; --strategy=mirrored \; --config.batch_size=512 \; --debug 'true'; ```. I received an error regarding about the checkpoint: ```No checkpoint found.```; I also attached my log for training step here: ; [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802:983,log,log,983,,https://github.com/google/deepvariant/issues/802,3,['log'],['log']
Testability,"Hi,. I followed the instructions on deepvariant quick start (https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md) to create deepvariant_1.6.0.sif and deepvariant_1.6.0-gpu.sif successfully using apptainer. . Then, I followed the complete genomics T7 case study (https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-complete-t7-case-study.md) to have some test runs. . 1. CPU version; I run the following command:; ```apptainer run \; -B input:/input \; -B output_apptainer_cpu:/output \; deepvariant_1.6.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf=output_apptainer_cpu/HG001.apptainer.cpu.output.vcf.gz \; --output_gvcf=output_apptainer_cpu/HG001.apptainer.cpu.output.g.vcf.gz \; --num_shards=$(nproc) \; --customized_model=input/weights-51-0.995354.ckpt; ```; It was successful. Both vcf and gvcf were generated. 2. GPU version; I run the following command:; ```apptainer run --nv \; -B input:/input \; -B output_apptainer_gpu:/output \; deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf=output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz \; --output_gvcf=output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz \; --num_shards=$(nproc) \; --customized_model=input/weights-51-0.995354.ckpt; ```. It seems there are some errors and GPU was not used. These are the output (part of the output were removed due to the limit of the characters of this post):. ```; ➜ t7 apptainer run --nv \; -B input:/input \; -B output_apptainer_gpu:/output \; deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=input/HG001.complete_t7.E100030471QC",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:395,test,test,395,,https://github.com/google/deepvariant/issues/774,1,['test'],['test']
Testability,"Hi,. I have a big load of data to genotype and I did some tests using subsets of my data. My question is if will work use DV default parameter to a Plant genome. I have no gold set for training model. There is any suggestions?. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/357:58,test,tests,58,,https://github.com/google/deepvariant/issues/357,1,['test'],['tests']
Testability,"Hi,. I have encountered a somewhat unexpected behavior related to the sample name written to the output VCF (possibly caused by the presence of chrEBV?!). **Describe the issue:**; Sample name ""default"" is used in the output VCF for ""chrEBV"", all other chromosome VCFs contain correct sample name (chr1-chr22,chrX,chrY,chrM). **Setup**; - Operating system: CentOS; - DeepVariant version: v1.2; - Installation method (Docker, built from source, etc.): Singularity v3.5.2; - Type of data: PacBio HiFi, Sequel-II. **Steps to reproduce:**; - Command:; ```; /opt/deepvariant/bin/run_deepvariant \; --model_type=""PACBIO"" \; --regions ""$CHROM""; [ ... otherwise default options ...]; ```; - note: the input BAM alignment contains the sample name in the header (i.e. `SM:HG002`); - Error trace: (n/a - run finishes). **Does the quick start test work on your system?**; can't be used to reproduce the problem. **Any additional context:**; In the log for each chromosome run, I see that all chromosomes except chrEBV are listed in lines like this; ```; I0112 10:31:04.917984 47443049531200 \; make_examples_core.py:236] \; Task 6/12: Common contigs are [ HERE: list of all chromosomes except for chrEBV]; ```. Best,; Peter",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/603:830,test,test,830,,https://github.com/google/deepvariant/issues/603,2,"['log', 'test']","['log', 'test']"
Testability,"Hi,. I observed that CPU memory usage in training DeepVariant is very high and increased by proceeding training. I used DeepVariant v0.7.1 via docker.; The tested server (standalone, in house server) has 512GB memory with 56CPU, 8GB swap region without GPU/TPU.; I followed your tutorials and made tfrecords including 241,896 pileup images from our data. I tested the following 3 patterns of training and observed increasing memory usage in all cases:. - When batch size=32,; 　- Training was successfully finished (total 50,000 steps); 　- Memory usage at the end of this training was 268GB; ![batch_size32 mem](https://user-images.githubusercontent.com/16730135/52196627-ddf14b00-289f-11e9-90dc-05eff144191d.jpg); - When batch size=64 (source code default),; 　- At ~16,000 steps, all memory (512 GB) was completely used; 　- At ~19,000 steps, all swap region was used (and killed); ![batch_size64 mem](https://user-images.githubusercontent.com/16730135/52196624-ddf14b00-289f-11e9-983a-5dcc3de48227.jpg); - When batch size=512 (tutorial default),; 　- At ~3,900 steps, all memory (512 GB) was completely used; 　- At ~4,200 steps, all swap region was used (and killed); ![batch_size512 mem](https://user-images.githubusercontent.com/16730135/52196626-ddf14b00-289f-11e9-8910-67a7c64b324d.jpg). Are these swelling memory usage due to tensorflow itself or some memory leak?; I want to know how to prevent these. Best,. Masaru",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/147:156,test,tested,156,,https://github.com/google/deepvariant/issues/147,2,['test'],['tested']
Testability,"Hi,. I tested DeepVariant 1.5.0 on PACBIO data (HG002, chr20), using PACBIO model and got following error in call_variants.py script:. ```; 2023-04-07 15:47:02.393512: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0407 15:47:04.372007 140275094697792 call_variants.py:317] From ./examples.tfrecord-00000-of-00032.gz.example_info.json: Shape of input examples: [100, 221, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; I0407 15:47:04.374644 140275094697792 call_variants.py:317] From /opt/models/pacbio/model.ckpt.example_info.json: Shape of input examples: [100, 199, 9], Channels of input examples: [1, 2, 3, 4, 5, 6, 7, 9, 10].; Traceback (most recent call last):; File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 513, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 36, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in m",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/628:7,test,tested,7,,https://github.com/google/deepvariant/issues/628,1,['test'],['tested']
Testability,"Hi,. I want to create training examples for multiple samples (specifically HG001 to HG007). Each sample has different coverages. For this command below, do I have to repeat this command for each of the .BAM file I have, or can I have multiple `--reads` for .BAM files of the same sample with different coverages. What would be a good approach here?; `( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo nvidia-docker run \; -v ${HOME}:${HOME} \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; ) >""${LOG_DIR}/training_set.with_label.make_examples.log"" 2>&1; `. Also in the shuffling step, what is a correct way to shuffle training examples: shuffling all examples of each sample file (e.g HG001) or shuffling all examples of all sample files once?. Thank",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/706:424,log,log,424,,https://github.com/google/deepvariant/issues/706,2,['log'],['log']
Testability,"Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/696:9,test,testing,9,,https://github.com/google/deepvariant/issues/696,4,['test'],"['test', 'tested', 'testing']"
Testability,"Hi,. I'm tring to test Deepvariant in my own panel data and set --regins to the interest regin, but something happend in my output. I provided reference gene hg19 and index it with samtools index, and provided .bam file with index named .bam.bai, when i run following commend, it's can work and create files :. BIN_VERSION=""1.5.0""; ; INPUT_DIR=""${PWD}/test_input""; OUTPUT_DIR=""${PWD}/test_output""; mkdir -p ""${OUTPUT_DIR}""; ; docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.fasta \; --reads=/input/my_test.bam \; --regions chx:xxxxx-xxxxxx \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1 \. The following figure is my question: when i run the test files provided by Deepvariant, it can work rightly, but if i run it on my own dataset, it create this .html file:; ![image](https://github.com/google/deepvariant/assets/139957165/9fd3f9d2-9a33-4aa1-a7b8-d54d75af7163); I checked the Depth item in the website, it's 1091, and I use command ""samtools view xxxx | wc -l"" it prints 1174, so maybe Deepvariant recognize all reads and count them rightly , but why it doesn't output right indel, genotypes and other items? Why they are empyt?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/697:18,test,test,18,,https://github.com/google/deepvariant/issues/697,2,['test'],['test']
Testability,"Hi,. I'm trying to run make_examples with 64 shards using 64 cores (1 worker per shard default I assume) and the following settings:; docker: ""gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0""; zones: [""us-central1-c""] . I am trying to use essentially the same code as is listed in the documentation:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads /reads.bam \; --examples ""${sample_id}.examples.tfrecord${numShards}"" \; --gvcf ""${sample_id}.gvcf.tfrecord${numShards}"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo. ```; However, I am getting the following error for each of the shards: . `ValueError: ('Output is not sharded but shard > 0', 4)`. What does this mean? I assume it means I am not actually sharding though it seems I am. Any help appreciated, thanks. Another example with full stack trace:. ```; Traceback (most recent call last):; File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1071, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 335, in default_options; flags_obj.gvcf or ''); File ""/cromwell_root/tmp.97900f56/Bazel.runfiles_WhyWBe/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py"", line 212, in resolve_filespecs; raise ValueError('Output is not sharded but shard > 0', shard); Val",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/143:366,Log,Log,366,,https://github.com/google/deepvariant/issues/143,3,"['Log', 'log']","['Log', 'log']"
Testability,"Hi,. If I run the following command:; ```bash; dv_make_examples.py \; --cores 16 \; --sample ISO_349.bam \; --ref PlasmoDB-41_Pfalciparum3D7_Genome.fasta.gz \; --reads ISO_349.bam \; --regions PlasmoDB-41_Pfalciparum3D7_Genome_ISO_349.per-base.bed.gz \; --logdir logs \; --examples ISO_349_shardedExamples; ```. I get the following output/error:; ```bash; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s ; ETA: 0s Left: 16 AVG: 0.00s local:16/0/100%/0.0s 2019-06-13 12:05:15.630247: W third_party/nucleus/io/sam_reader.cc:537] Unrecognized SAM header type, ignoring: ; I0613 12:05:15.631112 47747793808000 genomics_reader.py:213] Reading ISO_349.bam with NativeSamReader; Traceback (most recent call last):; File ""/state/part",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/191:256,log,logdir,256,,https://github.com/google/deepvariant/issues/191,2,['log'],"['logdir', 'logs']"
Testability,"Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads bamlink \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; ```; And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:; 1) Why doesn't this work?; 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks!. Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options); But I can't find the equivalent just for the make_examples section",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/148:238,Log,Log,238,,https://github.com/google/deepvariant/issues/148,3,"['Log', 'log']","['Log', 'log']"
Testability,"Hi,. Since DeepVariant does not support multi-GPU training ([Can model_train be run on multiple GPUs?](https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md#can-model_train-be-run-on-multiple-gpus)), I am pretty curious about ""We have tested training with 1 and 2 GPUs and observed the following runtimes:"" mentioned in the [training case](https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md#start-train).; Specifically, how is the training with 2 GPUs tested?. Thank you!; Regards : )",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/885:243,test,tested,243,,https://github.com/google/deepvariant/issues/885,2,['test'],['tested']
Testability,"Hi,. There appears to be a regression in v1.6 compared to v1.5 for handling cram input. The reference is set in the `make_examples` call, but does not seem to propagate to nucleus. The [code](https://github.com/google/deepvariant/blob/764bad20cbfa178d757ae81bbe05860640f2d5d4/third_party/nucleus/io/clif_postproc.py#L141) that raises the error ""ValueError: DATA_LOSS: Failed to parse SAM record"" is 4 years old, so must be some intermediate flag not working. The log in v1.6 looks like; ```; make_examples_core.py:301] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; genomics_reader.py:222] Reading sample.cram with NativeSamReader. ```. while in v1.5 it has 2 extra lines for setting the CRAM reference.; ```; make_examples_core.py:257] Task 1/4: Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; third_party/nucleus/io/sam_reader.cc:736] Setting HTS_OPT_BLOCK_SIZE to 134217728; third_party/nucleus/io/sam_reader.cc:764] Setting CRAM reference path to 'reference.fa'; genomics_reader.py:222] Reading sample.cram with NativeSamReader; ```. the `nucleus/io/sam_reader.cc` file had changes in b8d6d11, but it still looks correct so not sure what is happening. For completeness, I tried converting the cram to bam and rerun with v1.6, and that did worked, so this is somehow localised to cram support. Best,; Alex",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/741:463,log,log,463,,https://github.com/google/deepvariant/issues/741,1,['log'],['log']
Testability,"Hi,. This is running v1.6 via singularity with a command like; ```; /opt/deepvariant/bin/postprocess_variants --ref ref.fa --infile call_variants_output@1.tfrecord.gz --outfile all.vcf.gz --gvcf_outfile all.g.vcf.gz --nonvariant_site_tfrecord_path gvcf.tfrecord@24.gz --novcf_stats_report --haploid_contigs ""X,Y"" --par_regions_bed ""PAR.bed"" --cpus 2; ```; where PAR.bed is; ```; X 133300518 139009144; Y 1 6822380; ```; Running this without the haploid contigs and bed args works fine (although seems to use a **lot** more memory compared to v1.5), but when I try with these flags, the log file just explodes with ; ```; 2023-12-08 16:13:45.044559: I deepvariant/postprocess_variants.cc:109] Total #entries in single_site_calls = 11203892; I1208 16:16:08.029405 47184308283200 postprocess_variants.py:1313] CVO sorting took 3.885532522201538 minutes; I1208 16:16:08.030001 47184308283200 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I1208 16:16:08.030090 47184308283200 postprocess_variants.py:1318] Using 4 CPUs for parallelization of variant transformation.; I1208 16:16:08.959253 47184308283200 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: sample; I1208 16:20:43.170422 47184308283200 genomics_reader.py:222] Reading PAR.bed with NativeBedReader; I1208 16:20:43.188045 47184308283200 genomics_reader.py:222] Reading PAR.bed with NativeBedReader; I1208 16:20:43.189731 47184308283200 genomics_reader.py:222] Reading PAR.bed with NativeBedReader; I1208 16:20:43.205978 47184308283200 genomics_reader.py:222] Reading PAR.bed with NativeBedReader; ... ; ```; Within a couple of minutes it had written that ""Reading PAR.bed with NativeBedReader"" nearly 2 million times and the log file was already 200 Mb after a couple of minutes. I tried using `--logging_level FATAL` and `--logtostderr --stderrthreshold fatal`, but I couldn't find a way to turn off that logging.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/748:586,log,log,586,,https://github.com/google/deepvariant/issues/748,4,['log'],"['log', 'logging', 'logtostderr']"
Testability,"Hi,. Why all the GT of the PASS variant site is 1/1? use deepsomatic. chr1	91748	.	C	T	10	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:10:61:48,13:0.213115:9,19,0. The command:. BIN_VERSION=""1.6.1""; docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepsomatic:""${BIN_VERSION}"" \; run_deepsomatic \; --model_type=PACBIO \; --ref=/input/chm13v2.0.fa \; --reads_normal=/input/hifi_normal.bam \; --reads_tumor=/input/hifi_tumor.bam \; --output_vcf=/output/somatic_output_vcf.gz \; --output_gvcf=/output/somatic_output_g.vcf.gz \; --sample_name_tumor=""tumor"" \; --sample_name_normal=""normal"" \; --num_shards=48 \; --logging_dir=/output/logs \; --intermediate_results_dir=/output/intermediate_results_dir. Best wishes.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/848:648,log,logs,648,,https://github.com/google/deepvariant/issues/848,1,['log'],['logs']
Testability,"Hi,; I am running into an error when I try to use make_examples with a training VCF. The error does not occur when I run make_examples in training mode, without a truth VCF, so I believe the error occurs there. I call make examples with the following:; `/opt/deepvariant/bin/make_examples --mode training --ref refs/${ref} --reads ${BAM} --examples ${base}.tfrecord --truth_variants ${TRUTH_VCF} --confident_regions refs/confidence.bed `. I am using a simple test VCF attached here and I get the following error:; `; ValueError: Invalid argument: Invalid interval: reference_name: ""NC_000962.3"" start: -1 end: 22; `; Could you point me towards how to debug this issue? Thank you! ; [test.vcf.gz](https://github.com/google/deepvariant/files/1985941/test.vcf.gz)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/71:459,test,test,459,,https://github.com/google/deepvariant/issues/71,3,['test'],['test']
Testability,"Hi,; I am trying to deepen my understanding of DeepVariant (pun unintended), similar to what is written here (https://github.com/google/deepvariant/issues/306).; If I understand the discussion above correctly, whenever there is an insertion, the insertion itself will not be present in the pileup image, and the preceding pixel will be zero'd out. So if we have a cigar 6M3I1M such as:; ref: ACGCCT T; alt: ACGCCTCCCT; this will actually be represented in the pileup as:; ref: ACGCCTT; alt: ACGCC0T. I visualized the examples you provided in testdata, and on the 3rd example [fig1] (chr20:10001436-10001436) there is a case in which an insertion occurs right after the variant's position. Following the above logic, the center column should be zero'd out. However, it seems to not be the case. What happens instead is that the last channels center column indicates a diversion from the reference genome (a pixel is 'lit' up). So it looks like the bam read is a basic SNP, even though it actually is a length 5 insertion (the bam is an indel and the variant in the vcf is an snp).; In fact, I haven't seen any case of zero'd out pixels which aren't deletions.; Some options are:. 1. Did I not understand the insertions logic correctly? Whas it changed since the above git issue in April?; 2. Am I possibly working with the wrong files? I use the NA12878_S1.chr20.10_10p1mb.bam/.bai bam file with the golden.training_examples.tfrecord for your examples.; Sorry for the long post, I hope I was clear but let me know if more information/explanations are needed.; ![fig1](https://user-images.githubusercontent.com/52149642/98434660-6ab2e380-20da-11eb-887c-4683bb759d1c.png)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/379:542,test,testdata,542,,https://github.com/google/deepvariant/issues/379,3,"['log', 'test']","['logic', 'testdata']"
Testability,"Hi,; I installed deepvariant from conda. However, when I run dv_make_examples.py all jobs fail, and in their log is reported as:. ```; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 38, in <module>; from deepvariant import make_examples_core; File ""/tmp/Bazel.runfiles_v62e2j9f/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 41, in <module>; from etils import epath; ModuleNotFoundError: No module named 'etils' . ```; Do you have any suggestions?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/669:109,log,log,109,,https://github.com/google/deepvariant/issues/669,1,['log'],['log']
Testability,"Hi,; I want to call variations on the Pacbio bam data to get gvcf files.; But something went wrong with the program, and I spent two days wondering what went wrong. How do you solve this problem? ; Please help!; (No root permission;Centos7 x86;Only the user directory has read and write permission); ```; dv_make_examples.py --cores 3 --sample QJ --ref QJref.fa --reads F0F_sorted.merged.addg.uniq.rmdup.bam --logdir . --examples EXAMPLES --gvcf GVCF; ```; ```; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 48 / 3. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 3 AVG: 0.00s local:3/0/100%/0.0s lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemented; lchmod (file attributes) error: Function not implemente",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/598:410,log,logdir,410,,https://github.com/google/deepvariant/issues/598,2,['log'],"['logdir', 'login']"
Testability,"Hi,; I'm trying to train the network on my local machine with long reads but I get this error when trying to make examples out of the reads:. ```; 2018-12-10 17:04:46.071140: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1210 17:04:46.071218 140037815584512 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; I1210 17:04:46.072298 140037815584512 make_examples.py:1024] Preparing inputs; 2018-12-10 17:04:46.072535: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1210 17:04:46.072572 140037815584512 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; I1210 17:04:46.072917 140037815584512 genomics_reader.py:174] Reading project-retraining/testdata/variants.vcf.gz with NativeVcfReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_1gjjersh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_1gjjersh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1110, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_1gjjersh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1025, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_1gjjersh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 945, in processing_regions_from_options; options.min_shared_contigs_basepairs); File ""/tm",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128:350,test,testdata,350,,https://github.com/google/deepvariant/issues/128,2,['test'],['testdata']
Testability,"Hi. DeepVariant requires more than 60 minutes running time. I am encountering a problem of a termination of googleCloud shell after 60 minutes. I am trying to find a way to extent the running time of an idle. So far I could not find any relevant solution. Please let me know hot to prevent the googleCloud termination in order to successfully execute DeepVariant? Thank you a lot. . I tried to use screen mode. When the screen mode was used, I got an error running following command (branch 0.7):; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; sudo docker run \; -v /home/${USER}:/home/${USER} \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {} \; ) >""${LOG_DIR}/make_examples.log"" 2>&1. I got an error: -bash: /make_examples.log: Permission denied. Then I went out of the screen mode and try to run same command again and the error was still there.; Thank you a lot for the help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/88:571,log,log,571,,https://github.com/google/deepvariant/issues/88,3,['log'],['log']
Testability,"Hi. I am trying to apply variant calling on CCS reads of 16 samples. I am getting the following 2 errors:. 1. `[E::hts_open_format] Failed to open file /scratch/041-Melon-Reseq/Pacbio_targeted/alignedBams/ARUM_R--ARUM_R_vsMelonv4.MD.bam`; 2. `/var/log/slurm/log_slurmd//job2563083/slurm_script: línea 45: --num_shards=4: command not found; `. Below I present the defined variables and the command. `INPUT_DIR=""/scratch/041-Melon-Reseq/Pacbio_targeted/alignedBams""`; `OUTPUT_DIR=""/scratch/041-Melon-Reseq/Pacbio_targeted/DeepVariant""`; `NCPUS=4`; `BIN_VERSION=""0.10.0""`. singularity run; -B /usr/lib/locale/:/usr/lib/locale/ \; 	-B /home/kalexiou/PacBio/ \; 	docker://google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type=PACBIO \; 	--ref=/home/kalexiou/PacBio/Melon_v4.0_PacBio.fasta \; 	--reads=""${INPUT_DIR}""/${base} \; 	--regions ""chr05:24760000-27020000"" \; 	--output_vcf=""${OUTPUT_DIR}""/${name}.vcf.gz \; 	--output_gvcf=""${OUTPUT_DIR}""/${name}.g.vcf.gz \ ; 	--num_shards=""${NCPUS}"". Any help will be appreciated!. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/309:248,log,log,248,,https://github.com/google/deepvariant/issues/309,1,['log'],['log']
Testability,"Hi. I have used the following script to run deepvariant (v1.6.0) on WGS samples. . ``` bash; singularity exec -H $(pwd) docker://google/deepvariant:1.6.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${6} \; --ref=./human_g1k_v37_decoy.fasta \; --reads=./${2}_md.recal.cram \; --output_vcf=./${2}_hg37.dv.vcf.gz \; --output_gvcf=./${2}_hg37.dv.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=32; ```; Of the 30 samples I have, only 4 have not completed. I believe this is due to the 32nd shard not being generated in the temporary directory. All of the four samples that have not completed have the same error in the .log regarding the 32nd shard. The error is as follows:. ``` bash; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/call_variants_output.tfrecord.gz"" --examples ""/scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0219 07:48:12.876999 139989302617920 call_variants.py:471] Total 1 writing processes started.; W0219 07:48:12.885284 139989302617920 call_variants.py:482] Unable to read any records from /scratch3/users/kngeri004/b37/deepvar/tmp/tmp6uy3ir10/make_examples.tfrecord@32.gz. Output will contain zero records.; I0219 07:48:12.885881 139989302617920 call_variants.py:623] Complete: call_variants.; ```.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/776:721,log,log,721,,https://github.com/google/deepvariant/issues/776,1,['log'],['log']
Testability,"Hi. I'm running a pipeline but get an error on ""make_examples"" stage. Could you help me to debug possible cause, please?. Here is a log:; ```; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:51.456603: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; WARNING: Logging before flag parsing goes to stderr.; I0814 12:36:53.581777 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:36:54.201131 140158089049856 make_examples.py:1024] Preparing inputs; [W::hts_idx_load2] The index file is older than the data file: /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam.bai; 2019-08-14 12:36:58.286794: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I0814 12:36:59.914532 140158089049856 genomics_reader.py:174] Reading /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam with NativeSamReader; I0814 12:45:36.568115 140158089049856 make_examples.py:946] Common contigs are [u'LKUA01000001.1', u'LKUA01000002.1', ...<ANOTHER 300k NAMES>..., u'LKUA01311038.1', u'LKUA01311039.1']; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --examples /mnt/google/.google/output/cannabis-3k-vcf/staging/SRS1107973_LKUA01/staging/examples/0/examples_output.tfrecord@512.gz --reads /mnt/google/.google/input/cannabis-3k-results/manual/merged_bam/SRS1107973_LKUA01.sorted.merged.bam --ref /mnt/google/.google/input/cannabis-3k/reference/LKUA01/LKUA01.fa --task 8; ```. For me it looks that the error message is `parallel: This job failed:` and failure doesn't relate to the warnings at the beginning of the file?. Regards,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/207:132,log,log,132,,https://github.com/google/deepvariant/issues/207,2,"['Log', 'log']","['Logging', 'log']"
Testability,"Hi~; I tried to run Deepvariant v1.0.0 by docker image.; But it returned error message when I run test dataset. Here is the code:. ```sh; # BIN_VERSION=""1.0.0""; # INPUT_DIR=""${PWD}/quickstart-testdata""; # OUTPUT_DIR=""${PWD}/quickstart-output"". # ls quickstart-testdata; NA12878_S1.chr20.10_10p1mb.bam ; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi ; ucsc.hg19.chr20.unittest.fasta.gz.fai; NA12878_S1.chr20.10_10p1mb.bam.bai ; ucsc.hg19.chr20.unittest.fasta ; ucsc.hg19.chr20.unittest.fasta.gz.gzi; test_nist.b37_chr20_100kbp_at_10mb.bed ; ucsc.hg19.chr20.unittest.fasta.fai; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz ; ucsc.hg19.chr20.unittest.fasta.gz. # /home/d008/data/covid19/deepvarient/test# sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1 \; ```. And here is the error message from docker. ```sh; ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0907 09:04:08.296450 140053878712064 run_deepvariant.py:273] Re-using the directory for intermediate results in /output/intermediate_results_dir; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unitt",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/345:98,test,test,98,,https://github.com/google/deepvariant/issues/345,4,['test'],"['test', 'testdata']"
Testability,"I am currently running DeepVariant on CCS data after installing via Docker on an AWS instance. The make_examples step of the pipeline is taking much longer than expected. I have performed this in the past with 30X Illumina Data, and it has taken a few hours. This has been running for about a week on 23X coverage PacBio HiFi with a 16 core machine (CPU optimized), and I was wondering if that was expected. Below, I have the command issued:. `sudo time seq 0 $((N_SHARDS-1)) | sudo parallel --eta --halt 2 --joblog ""${LOGDIR}""/log --res ""${LOGDIR}"" sudo docker run -v ${HOME}:${HOME} -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/make_examples --mode calling --ref=/input/ucsc.hg38.no_alts.fasta --reads=/input/hg00733_ccs_to_hg38.bam --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" --task {}`. All variables listed have been set as expected. When I ssh in to the node I can see that it is running python in parallel and writing to the proper output files, but it is just taking forever to process anything. Any help would be greatly appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/208:519,LOG,LOGDIR,519,,https://github.com/google/deepvariant/issues/208,3,"['LOG', 'log']","['LOGDIR', 'log']"
Testability,"I am following the [Building from sources](https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-build-test.md) tutorial. I cloned the repository, then started building and got the following errors:. ```; viniws@woese:~/Code/deepvariant$ ./build-prereq.sh ; ========== Load config settings. ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Install the runtime packages' starting ; ========== Load config settings. ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Misc setup' starting ; ========== [qua set 26 16:42:55 -03 2018] Stage 'Update package list' starting ; E: The repository 'http://apt.postgresql.org/pub/repos/apt YOUR_UBUNTU_VERSION_HERE-pgdg Release' does not have a Release file. ; E: The repository 'http://ppa.launchpad.net/gnome-terminator/ppa/ubuntu bionic Release' does not have a Release file ; ```. And after:. ```; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=0; ++ TF_ENABLE_XLA=0; ++ export TF_NEED_CUDA=0; ++ TF_NEED_CUDA=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=0; ++ TF_NEED_MKL=0; ++ export TF_NEED_MPI=0; ++ TF_NEED_MPI=0; ++ export TF_NEED_OPENCL=0; ++ TF_NEED_OPENCL=0; ++ export TF_NEED_OPENCL_SYCL=0; ++ TF_NEED_OPENCL_SYCL=0; ++ export TF_NEED_S3=0; ++ TF_NEED_S3=0; ++ export TF_NEED_VERBS=0; ++ TF_NEED_VERBS=0; ++ export TF_CUDA_VERSION=8.0; ++ TF_CUDA_VERSION=8.0; ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ export TF_CUDNN_VERSION=6; ++ TF_CUDNN_VERSION=6; ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ DV_BAZEL_VERSION=0.15.0; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/98:114,test,test,114,,https://github.com/google/deepvariant/issues/98,1,['test'],['test']
Testability,"I am following the instructions under:; https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-build-test.md. 1. start GCE image : Ubuntu 16.04 with 100GB. git clone https://github.com/google/deepvariant; cd deepvariant; ./build-prereq.sh; ./build_and_test.sh; ```; ...; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ DV_COPT_FLAGS='--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ export DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; ++ DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; (17:54:59) INFO: Current date is 2017-12-22; (17:55:18) ERROR: /home/<mypath>/0fcc5a420905d68918d80793ee59fab4/external/com_goo; glesource_code_re2/BUILD:96:1: First argument of 'load' must be a label and start; with either '//', ':', or '@'. Us; e --incompatible_load_argument_is_label=false to temporarily disable this check. ... (17:55:26) ERROR: Analysis of target '//deepvariant/testing:gunit_extras' failed; build aborted: Loading failed; (17:55:26) INFO: Elapsed time: 27.289s; (17:55:26) FAILED: Build did NOT complete successfully (50 packages loaded); (17:55:26) ERROR: Couldn't start the build. Unable to run tests; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/20:111,test,test,111,,https://github.com/google/deepvariant/issues/20,4,['test'],"['test', 'testing', 'tests']"
Testability,I am running deepvariant using singularity:. I already tested it on the provided test files. singularity run --bind $bam_PATH/:/bam --bind $REFPATH:/ref deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant --model_type=PACBIO --reads=/bam/HG002.SequelII.merged_15kb_20kb.pbmm2.GRCh38.haplotag.10x.bam --ref=/ref/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa \; --output_vcf variants_HG002.vcf.gz --logging_dir calls/deepvariant/GRCh38/NA12886_HG002/ --num_shards 10 &. I was wondering what is the expected time for deepvariant on CCS reads?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/903:55,test,tested,55,,https://github.com/google/deepvariant/issues/903,2,['test'],"['test', 'tested']"
Testability,"I am trying to build DeepVariant from source, and **trying to use a custom python installation rather than the standard one.** However, ```bazel test ``` fails because it tries to use the standard library python. The requisite python is accessible as ""python"" because it is in the PATH variable, but bazel seems to ignore that and looks for python in the standard location. I am not an expert in bazel by any means, so any help in how to get around this issue is greatly appreciated. Here is the command used for build (all necessary libraries have been compiled. I didn't use run-prereq.sh and build-prereq.sh, but I installed them manually). Command used (this was edited into build_and_test.sh, and build_and_test.sh was run after the edits); ```; bazel test --host_javabase=@local_jdk//:jdk -c opt --local_test_jobs=1 ${DV_COPT_FLAGS} ""$@"" \; deepvariant/...; ```. settings.sh was changed as follows:; ```; export DV_USE_PREINSTALLED_TF=""1""; export TF_NEED_GCP=0; export CUDNN_INSTALL_PATH=""/usr""; export DV_GPU_BUILD=""1""; export DV_INSTALL_GPU_DRIVERS=""0""; export PYTHON_BIN_PATH='/opt/at11.0/bin/python'; export PYTHON_LIB_PATH='/opt/at11.0/lib64/python3.6/site-packages'; export USE_DEFAULT_PYTHON_LIB_PATH=0; export DV_COPT_FLAGS=""--copt=-mcpu=native --copt=-Wno-sign-compare --copt=-Wno-write-strings --copt=-DNO_WARN_X86_INTRINSICS""; ```. Error trace:; ```; (15:44:57) ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/org_tensorflow/tensorflow/core/BUILD:2762:1: Executing genrule @org_tensorflow//tensorflow/core:version_info_gen failed (Exit 1): bash failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; CUDA_TOOLKIT_PATH=/usr/local/cuda-10.0 \; GCC_HOST_COMPILER_PATH=/opt/at11.0/bin/gcc \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/opt/at11.0/bin:/opt/at11.0/sbin:/usr/local/nvidia/bin:/usr/loca",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/356:145,test,test,145,,https://github.com/google/deepvariant/issues/356,2,['test'],['test']
Testability,I am using DeepVariant on a local machine with docker. When I start DeepVariant it uses all the CPU cores of my machine. Since I want to test DeepVariant without any time limitations and use the machine for other tasks I am curious whether it is possible to limit the number of threads or the number of CPUs used by DeepVariant.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/42:137,test,test,137,,https://github.com/google/deepvariant/issues/42,1,['test'],['test']
Testability,"I don't have much experience using containers so it's difficult for me to troubleshoot the error I'm getting back when I run the test DeepVariant command:. `/usr/bin/docker-current: Error response from daemon: invalid volume spec "":/input"": invalid volume specification: ':/input'.; See '/usr/bin/docker-current run --help'`. As it might be relevant, the OS i'm using is CentOS Linux 7. Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/262:129,test,test,129,,https://github.com/google/deepvariant/issues/262,1,['test'],['test']
Testability,"I have a sample with a 4bp deletion at pos 775 and 1bp del at 779. Looking at the raw reads they seem to always be separate, and I confirmed there are no 5bp deletions called in this region in the alignment (via analyzing cigarStrings). DeepVariant is calling the 4bp del at 40% AD and the 1bp del at 77% AD. How could these logically add up to > 100%? Wouldn't that imply instances of 5bp deletion, but that would be reported separately, no? If someone could help me interoperate this that would be great. <img width=""113"" alt=""ComplimentaryIndelsP01-24"" src=""https://user-images.githubusercontent.com/65191963/153235606-e34c7406-d339-4b20-8ba7-fb29b42c713d.PNG"">",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/516:325,log,logically,325,,https://github.com/google/deepvariant/issues/516,1,['log'],['logically']
Testability,"I have been trying to run the whole genome case study with --regions in make_example, with the following command. I have copied all command in the genome case study page and added the regions to the make example as follow.; ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --regions '""chr20:10,000,000-10,010,000""'\; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1 ; ```; I have also tried ; `` --regions ""chr20:10,000,000-10,010,000""\`` ; and ; `` --regions chr20:10,000,000-10,010,000\`` ; However, in all cases, it seems like the whole data set is used in make example instead of the specific region. I am not sure what the problem is.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/72:302,log,log,302,,https://github.com/google/deepvariant/issues/72,2,['log'],['log']
Testability,"I have encountered the following error in several PacBio HiFi samples while running the docker image of deepvariant 1.4.0:. > F deepvariant/allelecounter.cc:872] Check failed: left_padding + right_padding < counts_.size() (5000 vs. 4022); Fatal Python error: Aborted. Deepvariant was run while enabling read normalization:. docker run -v ""input_path"":/input -v ""output_path"":/output google/deepvariant:1.4.0 /opt/deepvariant/bin /run_deepvariant --model_type=PACBIO --make_examples_extra_args=""normalize_reads=true"" --ref=/input/reference.fa --reads=/input/sample.bam --output_vcf=/output/sample.vcf --output_gvcf=/output/sample.gvcf --num_shards=16 --logging_dir=/output/logs. I know it is discouraged to enable read normalization due to potential excessive computational times, but I need it to make sure that I am capturing INDELs on the same conditions as my Illumina left-aligned samples. Any ideas on how to solve this issue?; Thank you,; Eugenio",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/762:672,log,logs,672,,https://github.com/google/deepvariant/issues/762,1,['log'],['logs']
Testability,"I have run the following command for RNA seq data and the output vcf size is very less and important variants are missing; BIN_VERSION=""1.5.0""; ```dockerfile; docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; run_deepvariant \; --model_type=WES \; --customized_model=model/model.ckpt \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=test_data/Aligned.sortedByCoord.out.bam \; --output_vcf=output/output.vcf.gz \; --num_shards=30 \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --logging_dir=output/logs \; --intermediate_results_dir output/intermediate_results_dir; ``` ; Please let me know if any error in the command i ran",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/775:571,log,logs,571,,https://github.com/google/deepvariant/issues/775,1,['log'],['logs']
Testability,"I have the problem that I can't enable the GPU when I run the Docker. I am using a NVIDIA P100. nvidia-docker run --runtime=nvidia -e NVIDIA_VISIBLE_DEVICES=all -it ... This is how I call call_variant ; `(time /opt/deepvariant/bin/call_variants --outfile ""${CALL_VARIANTS_OUTPUT}"" --examples ""${EXAMPLES}"" --checkpoint ""${MODEL}"" --execution_hardware accelerator) >""${LOG_DIR}/call_variants.log"" 2>&1`. The output from the call_variant log: . > 2018-06-23 22:47:42.743518: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA; WARNING: Logging before flag parsing goes to stderr.; I0623 22:47:43.324297 140315677046528 call_variants.py:329] Initializing model from /dv2/models/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wes_standard/model.ckpt; INFO:tensorflow:Restoring parameters from /dv2/models/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wes_standard/model.ckpt; I0623 22:47:44.415543 140315677046528 tf_logging.py:82] Restoring parameters from /dv2/models/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wes_standard/model.ckpt; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dEDnzG/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 388, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_dEDnzG/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 379, in main; batch_size=FLAGS.batch_size); File ""/tmp/Bazel.runfiles_dEDnzG/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 335, in call_variants; 'execution_hardware is set to accelerator, but no accelerator '; __main__.ExecutionHardwareError: execution_hardware is set to accelerator, but no accelerator was found; real	0m6.241s; user	0m6.872s; sys	0m2.256s. When I run the docker and check for the GPU with nvidia-smi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/81:391,log,log,391,,https://github.com/google/deepvariant/issues/81,3,"['Log', 'log']","['Logging', 'log']"
Testability,"I have tried running deepvariant (v1.6.0) using the rnaseq models using the example data with the google docker container:; `run_deepvariant --num_shards=32 --model_type=WES --customized_model=/opt/models/rnaseq/model.ckpt --regions=test.bed --ref=genome.fa --reads=hg005_gm26107.mrna.grch38.bam --output_vcf=deepvariantrna.vcf --logging_dir=logs --make_examples_extra_args=""split_skip_reads=true,channels=''""`. But it seems to get stuck at the call_variant step. **Any additional context:**. <img width=""1304"" alt=""image"" src=""https://github.com/google/deepvariant/assets/7647927/0c01d955-c1fa-4ede-9579-3f942c262c3a"">. test.bed; chr1 13670 13966; chr1 14409 14501; chr1 15005 15038; chr1 15796 15947; chr1 16607 16765; chr1 16858 17055; chr1 17233 17368; chr1 17369 17436; chr1 17606 17742; chr1 17915 18061; chr1 18268 18366; chr1 24738 24891; chr1 29534 30039; chr1 30267 30667; chr1 30976 31109; chr1 34554 35174; chr1 35245 35481; chr1 35721 36081; chr1 52473 53312; chr1 57598 57653; chr1 58700 58856; chr1 62916 64116; chr1 65419 65433; chr1 65520 65573; chr1 69037 71585; chr1 89295 91629; chr1 92091 92240; chr1 110953 111357; chr1 112700 112804; chr1 120721 120932; chr1 129055 129223; chr1 131025 134836; chr1 135141 135895; chr1 137682 137965; chr1 139790 139847; chr1 140075 140339; chr1 141474 143011; chr1 146386 149707; chr1 155767 155831; chr1 157784 157887; chr1 160446 160690; chr1 161314 161525; chr1 164263 164791; chr1 165491 165942; chr1 167129 168165; chr1 168610 168767; chr1 169049 169264; chr1 172557 172688; chr1 173753 173862; chr1 182696 182746; chr1 183132 183216; chr1 183494 183571; chr1 183740 183901; chr1 183981 184174; chr1 185217 185350; chr1 185491 185559; chr1 186317 186469; chr1 187129 187287; chr1 187376 187577; chr1 187755 187890; chr1 187891 187958; chr1 188130 188266; chr1 188439 188584; chr1 188791 188902; chr1 195263 195411; chr1 257864 259025; chr1 261550 261634; chr1 263015 268655; chr1 268667 268816; chr1 289266 289370; chr1 297345 297502; chr1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/766:233,test,test,233,,https://github.com/google/deepvariant/issues/766,3,"['log', 'test']","['logs', 'test']"
Testability,"I just downloaded the docker image and ran the following test on Centos OS 7. Everything worked fine. . INPUT_DIR=""/test/DeepVariant/quickstart-testdata""; OUTPUT_DIR=""/test/DeepVariant/quickstart-output""; BIN_VERSION=""0.8.0"". docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. Then I modified the shell script to run my sample ; > I'm using the custom ref - included fa, fai, .gz. gzi files in the input dir. >RHA; CTGGG ..... > I aligned my reads to the ref and extracted only mapped paired-end reads. @HD VN:1.6 SO:coordinate; @SQ SN:RHA LN:911; @PG ID:bwa PN:bwa VN:0.7.17-r1194-dirty CL:bwa mem -M -t 10 /..... > now when I run the docker tool, I get the following error message. 2019-09-24 15:23:14.405094: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring:; I0924 15:23:14.405213 139913087186688 genomics_reader.py:218] Reading /input/test.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 316, in default_options; sample_name = extract_sample_name_from_sam_reader(sam_reader); File ""/tmp/Bazel.runfiles_iYr42Y/runfiles/com_google_deepvariant",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/222:57,test,test,57,,https://github.com/google/deepvariant/issues/222,4,['test'],"['test', 'testdata']"
Testability,"I just started using DeepVariant 0.7.0 and I have gotten it to complete on a few exome runs. Out of the 4 exome runs I tested, I used 64 shards for the make_examples step. For 3 of the exomes, the make_examples step seemed to take about 10-15 minutes per shard. For a 4th exome, the make_examples step for one of the shards was taking much longer than 10-15 minutes; after 14 hours it was still running and I manually killed it. Now I have moved onto a whole-genome sequencing run for testing and the same thing is happening; 63/64 shards complete in about 1hour, but a straggling job has been running for 22 hours now.; ; * Have you run into this problem before and do you have suggestions for debugging?; * It occurred to me that there could be some ultra-high coverage region in my BAM file that could be slowing things down. But, I am looking for a way to investigate this. Is there an easy way to determine which region of the genome is in a particular shard?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/105:119,test,tested,119,,https://github.com/google/deepvariant/issues/105,2,['test'],"['tested', 'testing']"
Testability,"I looked at https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-details-training-data.md, where it says ""and, more `dowsample_fraction` during training"". Looking through make_examples it seems `downsample_fraction` is a fixed number for any given run of make_examples. My assumption currently is that make_examples is run multiple times with different `downsample_fraction` settings to obtain a wider mix of coverages in the training data, before running training (please correct me if wrong). I also had questions of how the downsampled mix was selected. E.g., were coverages in [20, 60]x equally distributed, or were the lower coverages used sparingly (since, for example, 20x is not a very interesting case)? Also, training a single model on multiple coverages and testing (variant calling) on a single coverage could be problematic (but in DeepVariant it is not), because the data distribution is different between the training and test sets. If there is a blog or article with some of these details, that would also be great!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/230:781,test,testing,781,,https://github.com/google/deepvariant/issues/230,2,['test'],"['test', 'testing']"
Testability,"I pull the latest version of deepvariant and failed to call make_examples as follows:. ```; chungtsai_su@seqslab:~/src/deepvariant$ ./bazel-bin/deepvariant/make_examples \; > --mode calling \; > --ref ""${REF}"" \; > --reads ""${BAM}"" \; > --regions ""chr20:10,000,000-10,010,000"" \; > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; 2018-12-20 07:06:44.997696: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:06:44.997911 140034548377344 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:06:45.001199 140034548377344 make_examples.py:1080] Preparing inputs; 2018-12-20 07:06:45.001707: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:06:45.001820 140034548377344 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:06:45.002523 140034548377344 make_examples.py:996] Common contigs are [u'chr20']; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_d_6TBY/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>; tf.app.run(); File ""/home/chungtsai_su/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_d_6TBY/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_d_6TBY/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_d_6TBY/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1000, in processing_regions_from_options; raise ValueError('The regions to call is empty. Check your --regions and '; ValueError: The regions to call is empty. Check your --regions and --exclude_regions flags to",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131:544,test,testdata,544,,https://github.com/google/deepvariant/issues/131,2,['test'],['testdata']
Testability,"I ran DeepVariant twice based on ""https://github.com/google/deepvariant/blob/r1.1/docs/deepvariant-pacbio-model-case-study.md"". ; deepvariant1- whatshap phase- whatshap haplotag-deepvariant2; Now I also want to use DeepTrio.I used the haplotagged.bam(generated from whatshap haplotag) as input bam.When I ran DeepTrio,the output.vcf.gz was generated normally.However,the log file showed the following warning message:; ------------------------; I0926 14:26:35.659228 47028170803008 call_variants.py:336] Shape of input examples: [140, 221, 9]; W0926 14:26:35.665323 47028170803008 call_variants.py:353] The height of the input image is not 100 (standard in DeepVariant) or 300 (standard in DeepTrio). Please double-check that the model is trained with the same parameters and version of DeepVariant as you generated the examples with. An **error** will not appear when these are mismatched because of how InceptionV3 works. Note that if you set --pileup_image_height in DeepVariant, then you must use a model trained with that same parameter.; 2021-09-26 14:26:35.668419: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2021-09-26 14:26:35.669638: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; 2021-09-26 14:26:35.671197: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2600000000 Hz; WARNING:tensorflow:Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; W0926 14:26:35.690017 47028170803008 estimator.py:1846] Using temporary folder as model directory: /TMP_DIR/tmpbptqemkc; ------------------------; The version I used:; DeepVariant 1.1.0; glnexus v1.3.1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/488:371,log,log,371,,https://github.com/google/deepvariant/issues/488,1,['log'],['log']
Testability,"I ran deepvariant in docker on centos 7 but had some errors:; docker run -v \; > /db_students/genetic_map/dv_workarea/test \; > dajunluo/deepvariant:latest \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WGS \; > --ref=/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; > --reads=/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; > --regions ""chr20:10,000,000-10,010,000"" \; > --output_vcf=test_output.vcf.gz \; > --output_gvcf=test_output.g.vcf.gz \; > --num_shards=2. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}. 2019-10-15 11:18:39.819615: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; 2019-10-15 11:18:39.819621: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m23.020s; user	0m1.610s; sys	0m3.206s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 1 | parallel -k --line-buffer /opt/de",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/226:118,test,test,118,,https://github.com/google/deepvariant/issues/226,5,['test'],"['test', 'testdata']"
Testability,"I ran the benchmarking using RTGtool for BWA-MEM + deepvariant 1.5.0 with 35x Illumina WGS data of HG002 (used in pFDA challenge) and compared the results with pFDA v2 challenge submission (BSODP here https://doi.org/10.18434/mds2-2336). I observed that the deepvariant 1.5.0 that I run has improved precision but lower recall. . pFDA v2 submission: `Precision: 0.9940, Recall: 0.9982 and F-score:0.9961`; deepvariant v1.5.0: `Precison: 0.9991 , recall: 0.9934, and -score: 0.9962`. I am just wondering if the pFDA submission has any extra filtering used after running deepvariant. Is there any publicly available deepvaraint v1.5.0 vcf for HG002?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/626:10,benchmark,benchmarking,10,,https://github.com/google/deepvariant/issues/626,1,['benchmark'],['benchmarking']
Testability,"I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**; - Centos 7; - deepvariant 1.3.0; - Singularity run pulling from here: docker://google/deepvariant:""1.3.0""; - quickstart example. **Steps to reproduce:**; ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service ; mkdir -p /mnt/share/jasontest; cd /mnt/share/jasontest; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; BIN_VERSION=""1.3.0""; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530:954,test,testdata,954,,https://github.com/google/deepvariant/issues/530,1,['test'],['testdata']
Testability,"I rebuilt docker images from instruction on https://github.com/google/deepvariant/issues/99#issuecomment-428366972. ```; gcloud builds submit \; --project ""${PROJECT_ID}"" \; --config cloudbuild.yaml \; --substitutions TAG_NAME=""${VERSION_NUMBER}"" \; --timeout 2h .; ```. I see three images on GCP Container Registry:. 1. **deepvariant**; 1. **deepvariant_gpu**; 1. **deepvariant_runner**. After finishing make_examples, now I am running calll_variant, but seems my rebuilt deepvariant_gpu image doesn't have CUDA installed, seeing such error. ```; ImportError: libcublas.so.9.0: cannot open shared object file: No such file or directory; ```. The command I used:; ```; ( time nvidia-docker run -v /home/${USER}:/home/${USER} gcr.io/my_project/deepvariant_gpu:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) | tee ""${LOG_DIR}/call_variants.log"" 2>&1; ```. I confirmed this is NOT an issue with gcr.io/deepvariant-docker/deepvariant_gpu, which means that it's just my rebuilt image missing CUDA driver. How should I modify the build command to build an image with CUDA driver, please?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/102:943,log,log,943,,https://github.com/google/deepvariant/issues/102,1,['log'],['log']
Testability,I tried running DeepVariant-0.5.0+cl-183695032 on the quickstart dataset with the following command:; ```; python call_variants.zip --dataset_config_p; btxt a.pbtxt --checkpoint DeepVariant-inception_v3-0.5.0+cl-182548131.data-wgs_standard/model.ckpt.index; ```. However it exits with the following error:; ```; KeyError: 'InceptionV3/Logits/Conv2d_1c_1x1/weights; ```,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/51:335,Log,Logits,335,,https://github.com/google/deepvariant/issues/51,1,['Log'],['Logits']
Testability,"I understand that PRs are not performed on github. So, I just wanted to recommend/discuss some potential changes for the shuffle_tfrecords_beam.py script to enable running it with SparkRunner (PortableRunner). Without these changes the script works only in LOOPBACK mode (which is a testing mode where the actual work is performed on the submitting host).",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/365:283,test,testing,283,,https://github.com/google/deepvariant/pull/365,1,['test'],['testing']
Testability,"I used DeepVariant to call SNPs and Indels with the HG002 Pacbio Revio benchmark data download from https://human-pangenomics.s3.amazonaws.com/submissions/80d00e88-7a92-46d8-88c7-48f1486e11ed--HG002_PACBIO_REVIO/, while the hap.py result showed Indels have very low precision (0.653927) and recall (0.884985) and SNPs seemed to be normal having 0.998 precision and recall. Type	Filter	TRUTH.TOTAL	TRUTH.TP	TRUTH.FN	QUERY.TOTAL	QUERY.FP	QUERY.UNK	FP.gtMETRIC.Recall	METRIC.Precision	METRIC.Frac_NA	METRIC.F1_Score; INDEL	ALL	523034	462877	60157	1215487	252834	484908	16813	0.884985	0.653927	0.398941	0.75211; INDEL	PASS	523034	462877	60157	1215487	252834	484908	16813	0.884985	0.653927	0.398941	0.75211; SNP	ALL	3352818	3349190	3628	3960152	8899	597545	864	0.998918	0.997354	0.150889	0.998135; SNP	PASS	3352818	3349190	3628	3960152	8899	597545	864	0.998918	0.997354	0.150889	0.998135. I running the pipeline with aligner minimap2 v2.24 with parameters ""-L --MD -Y -a -x map-hifi --secondary=no"" and calling SNVs with deepVariant v1.3.0 with --model_type=PACBIO.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/641:71,benchmark,benchmark,71,,https://github.com/google/deepvariant/issues/641,1,['benchmark'],['benchmark']
Testability,"I was running the whole exome example from here:; https://cloud.google.com/genomics/docs/tutorials/deepvariant#calling_exome_regions_configuration. ```#!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=PROJECT_ID; OUTPUT_BUCKET=gs://OUTPUT_BUCKET; STAGING_FOLDER_NAME=wes_staging; OUTPUT_FILE_NAME=wes_output.vcf; # Model for calling exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; #; # Changing the number of chards changes the output for some reason; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://deepvariant/exome-case-study-testdata/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam \; --bai gs://deepvariant/exome-case-study-testdata/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai \; --ref gs://deepvariant/exome-case-study-testdata/hs37d5.fa.gz \; --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \; --shards 64 \; --make_examples_workers 8 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 100 \; --call_variants_workers 1 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --max_preemptible_tries 5 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west1 \; --",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/112:944,test,testdata,944,,https://github.com/google/deepvariant/issues/112,1,['test'],['testdata']
Testability,"I would like to training a new model with two different dataset. There are about 8,500,00 training examples. But I used the python script shuffle_tfrecords_beam.py to shuffling these examples. I get the error that the code uses too mach memory. It is out of memory of my machine. My machine have 376G memory. Here is my command and the error log.; Command:; ```javascript; EXAMPLES=""/home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/NA12878_BGISEQ_PE150_5.training.examples.tfrecord-?????-of-00038.gz""; echo ""${EXAMPLES}""; OUTPUT_DIR=${EXAMPLES%/*} ; time python ./shuffle_tfrecords_beam.py ; --input_pattern_list=""${EXAMPLES}""; --output_pattern_prefix=""${OUTPUT_DIR}/training_set.with_label.shuffled"" ; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/training_set.dataset_config.pbtxt"" ; --output_dataset_name=""HG001""; --runner=BundleBasedDirectRunner; ```; the error message:; ```; /home/suanfa/Documents/shishiming/WGS_trained_model/BGISEQ-500_4_and_5_model/sample.training.examples.tfrecord-?????-of-00076.gz; /home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/direct_runner.py:342: DeprecationWarning: options is deprecated since First stable release.. References to <pipeline>.options will not be supported; pipeline.replace_all(_get_transform_overrides(pipeline.options)); INFO:root:Running pipeline with DirectRunner.; WARNING:root:Couldn't find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.; ERROR:root:Exception at bundle <apache_beam.runners.direct.bundle_factory._Bundle object at 0x7f86daaa07e8>, due to an exception.; Traceback (most recent call last):; File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 341, in call; finish_state); File ""/home/suanfa/virtualenv_beam/local/lib/python2.7/site-packages/apache_beam/runners/direct/executor.py"", line 381, in attempt_call; result = evaluator.finish_bundle(); ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/91:342,log,log,342,,https://github.com/google/deepvariant/issues/91,1,['log'],['log']
Testability,"I'm attempting to write a bam file of the realigned reads, as I'm seeing ADs in the vcf that do not line up with what is present in the input bam file. I'm mainly concerned with two specific locations in the genome. The full genome is approx 11 Gbp, the input bam file is about 190 GB, and the drive I'm attempting to output to has more than 4 TB available. When using `-emit_realigned_reads` and using `-realigner_diagnostics` to provide an output directory, the log file tells me that deepvariant attempts to write more than seven million bam files, making it impossible to access the directory before crashing due to running out of disk space. Is there some way of getting around this? I'm thinking either a more storage efficient way of getting the entire bam file, or a way of getting the bam file of the specific regions I'm interested in. I'm using deepvariant 1.0 from docker on Ubuntu 18.04. The data is short read Illumina data. . The command I'm using to run this:. ```; seq 0 $((60-1)) |\; parallel --halt 2 --line-buffer /opt/deepvariant/bin/make_examples \; --mode calling --emit_realigned_reads --realigner_diagnostics=results/sample/deepvariant/realigned \; --ref data/genome/reference.fasta --reads results/sample/aligned/sample.bam \; --examples results/sample/deepvariant/tmp/make_examples/make_examples.tfrecord@60.gz \; --sample_name sample --task {} 2> results/sample/deepvariant/tmp/make_examples.log ; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/370:464,log,log,464,,https://github.com/google/deepvariant/issues/370,2,['log'],['log']
Testability,"I'm trying to train a deepvariant model with a very simple topology.; After a few thousands of training steps, the logged training loss starts to vibrate around a rather high value. However the performance of saved models still keeps improving on my validation data set.; Why does this happen?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/194:115,log,logged,115,,https://github.com/google/deepvariant/issues/194,1,['log'],['logged']
Testability,"I'm unfamiliar with the bazel build environment. After a successful build with tensorflow-gpu, all tests passed, but on attempting to run a binary from bazel-bin I see. 2018-02-05 11:14:37.628020: I tensorflow/core/platform/s3/aws_logging.cc:53] Initializing Curl library; Traceback (most recent call last):; File ""/home2/bradBuild/deepvariant/bazel-bin/deepvariant/make_examples.runfiles/genomics/deepvariant/make_examples.py"", line 1105, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 118, in run; argv = flags.FLAGS(_sys.argv if argv is None else argv, known_only=True); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/flags.py"", line 112, in __call__; return self.__dict__['__wrapped'].__call__(*args, **kwargs); TypeError: __call__() got an unexpected keyword argument 'known_only'",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/47:99,test,tests,99,,https://github.com/google/deepvariant/issues/47,1,['test'],['tests']
Testability,"I've been trying to replicate the runtime of a WES sample using the same BAMs as the ones specified in https://raw.githubusercontent.com/google/deepvariant/r1.6/scripts/inference_deepvariant.sh . - Operating system: google cloud vertex AI jupyer notebook ; n1-standard-64 - 64v CPUs - 240GB RAM; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): docker deepvariant ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); BAMs: https://storage.googleapis.com/deepvariant/exome-case-study-testdata/HG003.novaseq.wes_idt.100x.dedup.bam. - Command:; export BIN_VERSION=""1.5.0""; export INPUT_DIR=""/home/jupyter/input""; export REF=""GCA_000001405.15_GRCh38_no_alt_analysis_set.fna""; export BAM=""HG003.novaseq.wes_idt.100x.dedup.bam""; export OUTPUT_DIR=""/home/jupyter/output""; export OUTPUT_VCF=""HG003.deepvariant.vcf.gz""; export OUTPUT_GVCF=""HG003.deepvariant.g.vcf.gz"". docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}"":""/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=""/input/${REF}"" --reads=""/input/${BAM}"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=64. its taking 53 mins to finish running when it should take 8mins according to https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md; ; I've also tried to run a WES sample of 23GB with the same cloud configuration, but it takes close to 2hrs to complete. ; Is there a reason for the differences in runtime? ; ; Is there another way to decrease runtime and match the runtimes specified https://github.com/google/deepvariant/blob/r1.6/docs/metrics.md",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/735:580,test,testdata,580,,https://github.com/google/deepvariant/issues/735,1,['test'],['testdata']
Testability,"I've had success following the **Getting started guide** with both CPU and GPU on the example datasets and now I'm trying to run the CPU version on my own data, _C. elegans_, but am getting an error:. ## Submission script for example. ```; #!/bin/bash; #SBATCH --job-name=example_DV; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; sin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:475,log,logs,475,,https://github.com/google/deepvariant/issues/292,3,"['log', 'test']","['logs', 'testdata']"
Testability,"I've successfully run deepvariant with test data. But I keep getting the following error when extracting pileup images from my own provided BAM file. What could be the problem, please?. ```; I1003 20:27:32.183320 140083390310144 make_examples.py:825] Found 0 candidates in chr1:1-1000 [1000 bp] [1.62s elapsed]; I1003 20:27:32.185085 140083390310144 make_examples.py:825] Found 0 candidates in chr1:1001-2000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.186733 140083390310144 make_examples.py:825] Found 0 candidates in chr1:2001-3000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.188343 140083390310144 make_examples.py:825] Found 0 candidates in chr1:3001-4000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.189908 140083390310144 make_examples.py:825] Found 0 candidates in chr1:4001-5000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.191494 140083390310144 make_examples.py:825] Found 0 candidates in chr1:5001-6000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.193065 140083390310144 make_examples.py:825] Found 0 candidates in chr1:6001-7000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.194626 140083390310144 make_examples.py:825] Found 0 candidates in chr1:7001-8000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.196187 140083390310144 make_examples.py:825] Found 0 candidates in chr1:8001-9000 [1000 bp] [0.00s elapsed]; I1003 20:27:32.197738 140083390310144 make_examples.py:825] Found 0 candidates in chr1:9001-10000 [1000 bp] [0.00s elapsed]; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_8StCi1/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1188, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_8StCi1/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1178, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_8StCi1/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1090, in make_examples_runner; c",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/99:39,test,test,39,,https://github.com/google/deepvariant/issues/99,1,['test'],['test']
Testability,"ID; OUTPUT_BUCKET=gs://OUTPUT_BUCKET; STAGING_FOLDER_NAME=wes_staging; OUTPUT_FILE_NAME=wes_output.vcf; # Model for calling exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; #; # Changing the number of chards changes the output for some reason; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://deepvariant/exome-case-study-testdata/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam \; --bai gs://deepvariant/exome-case-study-testdata/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai \; --ref gs://deepvariant/exome-case-study-testdata/hs37d5.fa.gz \; --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \; --shards 64 \; --make_examples_workers 8 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 100 \; --call_variants_workers 1 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --max_preemptible_tries 5 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west1 \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. Changing `--shards` to `128` changes the output in the VCF file. Running a diff between the two outputs shows th",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/112:1242,test,testdata,1242,,https://github.com/google/deepvariant/issues/112,1,['test'],['testdata']
Testability,"IR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata ; # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add; # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container; # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif); singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \; /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \; --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \; --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed; --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \; --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \; --call_variants_extra_args=""use_openvino=true"" \; --num_shards=$(nproc) \; --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \; --dry_run=true; ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why the output files are not created?. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/515:2050,test,testdata,2050,,https://github.com/google/deepvariant/issues/515,1,['test'],['testdata']
Testability,"Is this issue related to TF version?; Any help to fix this issue? Thanks.; ```; (base) [tahmad@gcn35 tests]$ BIN_VERSION=""1.4.0""; (base) [tahmad@gcn35 tests]$ singularity run --nv -B /usr/lib/locale/ docker://google/deepvariant:${BIN_VERSION}-gpu /opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref reference/GRCh38_no_alt_analysis_set.fasta; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --output_vcf deepvariant_output/output.vcf.gz --num_shards $(nproc) --regions chr20; INFO: Using cached SIF image; 2022-08-20 12:59:52.389461: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/__init__.py"", line 444, in <module>; _ll.load_library(_main_dir); File ""/home/tahmad/.local/lib/python3.8/site-packages/tensorflow/python/framework/load_library.py"", line 154, in load_library; py_tf.TF_LoadLibrary(lib); tensorflow.python.framework.errors_impl.NotFoundError: /usr/local/lib/python3.8/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so: undefined symbol: _ZN10tensorflow14kernel_factory17OpKernelRegistrar12InitInternalEPKNS_9KernelDefEN4absl12lts_2021032411string_viewESt10unique_ptrINS0_15OpKernelFactoryESt14default_deleteIS9_EE; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/555:101,test,tests,101,,https://github.com/google/deepvariant/issues/555,2,['test'],['tests']
Testability,Issue testing custom model,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:6,test,testing,6,,https://github.com/google/deepvariant/issues/797,1,['test'],['testing']
Testability,Logged training loss does not decrease while performance improves,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/194:0,Log,Logged,0,,https://github.com/google/deepvariant/issues/194,1,['Log'],['Logged']
Testability,"M13.minimap2_asm20.primary_alignments.sorted.bam with NativeSamReader; W1023 11:00:14.858257 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:9; W1023 11:00:14.858535 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:30; W1023 11:00:14.858640 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:63; W1023 11:00:14.858738 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:107; W1023 11:00:14.858865 140022713169664 make_examples.py:1145] Could not create PileupImage for candidate at chr1:109; I1023 11:00:14.943413 140022713169664 make_examples.py:1363] Task 0: 26 candidates (21 examples) [0.51s elapsed]; I1023 11:00:34.047770 140022713169664 make_examples.py:1363] Task 0: 100 candidates (123 examples) [19.10s elapsed]; I1023 11:00:53.403723 140022713169664 make_examples.py:1363] Task 0: 200 candidates (255 examples) [19.36s elapsed]; [E::fai_retrieve] Failed to retrieve block: unexpected end of file; 2020-10-23 11:01:17.042772: F ./third_party/nucleus/vendor/statusor.h:231] Non-OK-status: status_ status: Invalid argument: Couldn't fetch bases for reference_name: ""chr1"" start: 26013000 end: 26014000; I1023 11:00:13.973792 139819486582528 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes . **Any additional context:** . Deepvariant was successful on grch37 and grch38 reference genomes. I assume that the PacBio CCS model was trained on grch37 and grch38 reference genome and might not be applicable to other reference genomes. Can someone recommend a tweak to run deepvariant on CHM13 draft genome?. Regards,; Sangjin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/367:4178,test,test,4178,,https://github.com/google/deepvariant/issues/367,2,['test'],['test']
Testability,"Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I get the following error: ; ```; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/13939489157244551677"" failed: executing pipeline: Execution failed: action 5: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); details:; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116:1411,log,log,1411,,https://github.com/google/deepvariant/issues/116,1,['log'],['log']
Testability,"On Ubuntu 16.04 TLS, I built it with Python 2.7 and tensorflow 1.4.1. It failed all tests that import random (e.g. `deepvariant/deepvariant/core/cloud_utils_test.py`). It turned out that in `random.py`, it imports `math`, and it mistakingly imports the `math.py` in `/deepvariant/deepvariant/core/`, instead of from the standard library. Is there a fix?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/32:84,test,tests,84,,https://github.com/google/deepvariant/issues/32,1,['test'],['tests']
Testability,Pacbio data test failed,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/631:12,test,test,12,,https://github.com/google/deepvariant/issues/631,1,['test'],['test']
Testability,"Per: https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md#how-much-gpu-memory-is-needed-for-the-keras-models. 16GB. In our test, we observe the model occupying 16GB GPU memory. I have a GPU NVIDIA RTX A2000 with 6GB memory, is there a way I can use it to do the call variants step with no problems?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/759:130,test,test,130,,https://github.com/google/deepvariant/issues/759,1,['test'],['test']
Testability,"R_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.0`; 3. `[u'Error in job call-varia--root--180321-233157-28 - code 9: Quota CPUS exceeded in region us-central1']`. The `...-stderr.log` file written to `staging-folder` also begins with the errors; ```; /tmp/ggp-896952821: line 16: type: gsutil: not found; debconf: delaying package configuration, since apt-utils is not installed; debconf: delaying package configuration, since apt-utils is not installed; W: GPG error: http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY 3746C208A7317B0F; W: The repository 'http://packages.cloud.google.com/apt cloud-sdk-xenial InRelease' is not signed.; debconf: delaying package configuration, since apt-utils is not installed; % Total % Received % Xferd Average Speed Time Time Time Current; Dload Upload Total Spent Left Speed. 0 0 0 0 0 0 0 0 --:--:-- --:--:-- --:--:-- 0; 100 663 100 663 0 0 5012 0 --:--:-- --:--:-- --:--:-- 5022; debconf: delaying package configuration, since apt-utils is not installed; WARNING: Logging before flag parsing goes to stderr.; ```. But I then see many messages about candidate variants it's found. . The directory `staging-folder/examples/0/` also includes 8 `.gz` files like `examples_output.tfrecord-00007-of-00008.gz`. . Can you help me figure out what I'm doing wrong?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:3840,Log,Logging,3840,,https://github.com/google/deepvariant/issues/60,1,['Log'],['Logging']
Testability,Run OpenVINO processing in separate thread which let's to use common logging (https://github.com/google/deepvariant/pull/363/commits/3cfa6c563824bddc84e36373f65f2620160d6eb5 + https://github.com/google/deepvariant/pull/363/commits/9e69c4096fac8ddb788c3d29e4405fc50e85d1e3) . Please ignore test scripts from `.github/workflows` - they are not a part of patch but just used for validation.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/pull/393:69,log,logging,69,,https://github.com/google/deepvariant/pull/393,2,"['log', 'test']","['logging', 'test']"
Testability,Run a interactive mode of docker container (different than deepvariant container) and plan to build the deepvariant from source:. run the build-prereq.sh and run build_and_test.sh. Then how to test deepvariant function?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/448:193,test,test,193,,https://github.com/google/deepvariant/issues/448,1,['test'],['test']
Testability,"Running make_examples locally from the Docker container. It runs without errors on the NA12878_S1.chr20.10_10p1mb.bam dataset, but fails with my BAM file. See the stack trace below. My BAM file is too large to attach here. What do you recommend? Also what are the allowed values for --logging_level? Please advise. ```; # ./opt/deepvariant/bin/make_examples --logging_level DEBUG --mode calling --ref /dv2/reference/CFSAN000189.fasta --reads /dv2/samples/CFSAN000211/reads.sorted.bam --examples output.examples.tfrecord; WARNING: Logging before flag parsing goes to stderr.; I1228 21:10:23.407845 140668049200896 client.py:1004] Timeout attempting to reach GCE metadata service.; W1228 21:10:23.408325 140668049200896 htslib_gcp_oauth.py:88] GCP credentials not found; only local files and public gs:// URIs will be accessible from htslib; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 1015, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 48, in run; _sys.exit(main(_sys.argv[:1] + flags_passthrough)); File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 969, in main; options = default_options(add_flags=True, flags=FLAGS); File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 207, in default_options; sample_name = extract_sample_name_from_reads(flags.reads); File ""/tmp/Bazel.runfiles_9tjOWl/runfiles/genomics/deepvariant/make_examples.py"", line 406, in extract_sample_name_from_reads; raise ValueError('Expected a single sample, found {}'.format(samples)); ValueError: Expected a single sample, found set([]). ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/28:530,Log,Logging,530,,https://github.com/google/deepvariant/issues/28,1,['Log'],['Logging']
Testability,"S-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent2.tfrecord.gz"" --outfile ""/out_dir/199718.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent2.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent2.log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.705964 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; real	0m3.173s; user	0m3.003s; sys	0m3.160s; real	0m3.194s; user	0m3.299s; sys	0m4.216s; real	0m3.254s; user	0m3.024s; sys	0m2.808s; post_process returns: [0, 0, 0]; real	2008m37.771s; user	78330m54.158s; sys	730m9.042s; ```. **Does the quick start test work on your system?** Yes.; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Yes, see below:; ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=/input/GRCh38_no_alt_analysis_set.fasta \; --reads_child=/input/HG002.chr20.10_10p1mb.bam \; --reads_parent1=/input/HG003.chr20.10_10p1mb.bam \; --reads_parent2=/input/HG004.chr20.10_10p1mb.bam \; --output_vcf_child /output/HG002.output.vcf.gz \; --output_vcf_parent1 /output/HG003.output.vcf.gz \; --output_vcf_parent2 /output/HG004.output.vcf.gz \; --sample_name_child 'HG002' \; --sample_name_parent1 'HG003' \; --sample_name_parent2 'HG004' \; --num_shards $(nproc) \; --regions ""chr20:10,000,000-10,010,000"" \; --intermediate_results_dir /output/intermediate_results_dir \; ``",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/429:3933,test,test,3933,,https://github.com/google/deepvariant/issues/429,1,['test'],['test']
Testability,"SORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; (08:09:38) INFO: Current date is 2017-12-08; (08:09:38) WARNING: /home/huangl/.cache/bazel/_bazel_huangl/008c6ca154d923f28d39cff9fad40a7f/external/org_tensorflow/tensorflow/core/BUILD:1806:1: in includes attribute of cc_library rule @org_tensorflow//tensorflow/core:framework_headers_lib: '../../../../external/nsync/public' resolves to 'external/nsync/public' not below the relative path of its package 'external/org_tensorflow/tensorflow/core'. This will be an error in the future. Since this rule was created by the macro 'cc_header_only_library', the error might have been caused by the macro implementation in /home/huangl/.cache/bazel/_bazel_huangl/008c6ca154d923f28d39cff9fad40a7f/external/org_tensorflow/tensorflow/tensorflow.bzl:1100:30; (08:09:38) INFO: Analysed 241 targets (0 packages loaded).; (08:09:38) INFO: Found 185 targets and 56 test targets...; (08:09:38) ERROR: missing input file '@clif//:clif/bin/pyclif_proto'; (08:09:38) ERROR: /home/huangl/biotools/deepvariant/deepvariant/core/protos/BUILD:32:1: //deepvariant/core/protos:core_pyclif_clif_rule: missing input file '@clif//:clif/bin/pyclif_proto'; (08:09:38) ERROR: /home/huangl/biotools/deepvariant/deepvariant/core/protos/BUILD:32:1 1 input file(s) do not exist; (08:09:38) INFO: Elapsed time: 0.334s, Critical Path: 0.00s; (08:09:38) FAILED: Build did NOT complete successfully; //deepvariant:allelecounter_test NO STATUS; //deepvariant:call_variants_test NO STATUS; //deepvariant:data_providers_test NO STATUS; //deepvariant:make_examples_test NO STATUS; //deepvariant:model_eval_test NO STATUS; //deepvariant:model_train_test NO STATUS; //deepvariant:modeling_test NO STATUS; //deepvariant:pileup_image_test NO STATUS; //deepvariant:postprocess_variants_lib_test NO STATUS; //deepvariant:postprocess_variants_test NO STATUS; //deepvariant:tf_utils_test",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6:3223,test,test,3223,,https://github.com/google/deepvariant/issues/6,1,['test'],['test']
Testability,"Sorry to bother, but could you please provide me with the original path for downloading the data in the exome case study? I am interested in exploring some related data. Thank you very much. . Here's the code related to the data in case study:; ```; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/842:318,test,testdata,318,,https://github.com/google/deepvariant/issues/842,1,['test'],['testdata']
Testability,"TY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/sheina.sim/condaenvs; softwarepath=/project/ag100pest/sheina.sim/software; slurmpath=/project/ag100pest/sheina.sim/slurm_scripts. module load apptainer. apptainer exec deepvariant_1.6.0.sif /opt/deepvariant/bin/train \; --config=/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/dv_config.py:base \; --config.train_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_set.pbtxt"" \; --config.tune_dataset_pbtxt=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/validation_set.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/deepvariant_output/training_dir_test2"" \; --strategy=mirrored \; --config.batch_size=512 ; `. **Code to test the custom model:** . `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --ntasks-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=atlas # standard node(s); #SBATCH --job-name=""deepvariant_modeltest""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH --output=""deepvariant_modeltest-%j-%N.out"" # job standard output file (%j replaced by job id); #SBATCH --error=""deepvariant_modeltest-%j-%N.err"" # job standard error file (%j replaced by job id); #SBATCH --account=ag100pest. LOAD MODULES, INSERT CODE, AND RUN YOUR PROGRAMS HERE. export PATH=$PATH:/project/ag100pest/sratoolkit/sratoolkit.2.10.9-centos_linux64/bin; export PATH=$PATH:/project/ag100pest/sheina.sim/software/miniconda3/bin. export SINGULARITY_CACHEDIR=$TMPDIR ; export SINGULARITY_TMPDIR=$TMPDIR. condapath=/project/ag100pest/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/797:3261,test,test,3261,,https://github.com/google/deepvariant/issues/797,1,['test'],['test']
Testability,"Thanks for the wonderful tool! It's exciting to know that DeepVariant supports ONT Duplex Data.; I wonder what tools you use to generate bam files for testing. Assuming you use minimap2, what parameters did you use to generate optimal input for DeepVariant? Given the high accuracy of Duplex data, the default setting should not work well anymore because that's for old ONT data.; Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/617:151,test,testing,151,,https://github.com/google/deepvariant/issues/617,1,['test'],['testing']
Testability,"The code in [Download binaries, models, and test data](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md#download-binaries-models-and-test-data) the of the DV quick start guide ran successfully. However, running [make_examples](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md#make_examples) using the quickstart-testdata failed with the following error:; ```; Traceback (most recent call last):; attila-ThinkS:~/tools/deepvariant$ python bin/make_examples.zip \; > --mode calling \; > --ref ""${REF}"" \; > --reads ""${BAM}"" \; > --regions ""chr20:10,000,000-10,010,000"" \; > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; File ""/tmp/Bazel.runfiles_pbJgd2/runfiles/genomics/deepvariant/make_examples.py"", line 45, in <module>; from deepvariant import variant_caller; File ""/tmp/Bazel.runfiles_pbJgd2/runfiles/genomics/deepvariant/variant_caller.py"", line 50, in <module>; from deepvariant.python import variant_calling; ImportError: libcrypto.so.1.0.0: cannot open shared object file: No such file or directory; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41:44,test,test,44,,https://github.com/google/deepvariant/issues/41,3,['test'],"['test', 'test-data', 'testdata']"
Testability,"The errors (part of them). + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; (00:49:22) INFO: Current date is 2018-01-27; (00:49:22) Loading:; (00:49:22) Loading: 0 packages loaded; (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:96:; 1: First argument of 'load' must be a label and start with either '//', ':', or '@'. Use --incompatible_load_argument_is_label=fals; e to temporarily disable this check.; (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:98:; 1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:100; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:102; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:104; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:106; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:108; :1: name 're2_test' is not defined (did you mean 'ios_test'?); (00:49:22) ERROR: /home/<user>/.cache/bazel/_bazel_ravi/74e2f34442216df8489f404815744088/external/com_googlesource_code_re2/BUILD:110; :1: name 're2_test' is not defined (did you mean 'ios_test'?)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/43:51,test,test,51,,https://github.com/google/deepvariant/issues/43,1,['test'],['test']
Testability,This is a test.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/426:10,test,test,10,,https://github.com/google/deepvariant/issues/426,1,['test'],['test']
Testability,"Traceback (most recent call last):; File ""get-pip.py"", line 32992, in <module>; main(); File ""get-pip.py"", line 135, in main; bootstrap(tmpdir=tmpdir); File ""get-pip.py"", line 111, in bootstrap; monkeypatch_for_cert(tmpdir); File ""get-pip.py"", line 92, in monkeypatch_for_cert; from pip._internal.commands.install import InstallCommand; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/commands/__init__.py"", line 9, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/base_command.py"", line 15, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/cmdoptions.py"", line 24, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/cli/parser.py"", line 12, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/configuration.py"", line 26, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/logging.py"", line 29, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/utils/misc.py"", line 44, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/__init__.py"", line 66, in <module>; File ""<frozen zipimport>"", line 259, in load_module; File ""/tmp/tmpwq54m3sg/pip.zip/pip/_internal/locations/_distutils.py"", line 20, in <module>; ModuleNotFoundError: No module named 'distutils.cmd'",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/730:1140,log,logging,1140,,https://github.com/google/deepvariant/issues/730,1,['log'],['logging']
Testability,Using PAR region flag seems to log NativeBedReader endlessly,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/748:31,log,log,31,,https://github.com/google/deepvariant/issues/748,1,['log'],['log']
Testability,"VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different computers, and the error was the same.; There is a previous issue in this forum (https://github.com/google/deepvariant/issues/181) where the user did not set BIN_VERSION variable correctly, and **IT IS NOT MY CASE**!!!!. I tested if the volumes were mounted correctly, according to the script:; OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; sudo docker run \; -i \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:0.8.0 \; find /input. And the result was:; /input/NA12878_S1.chr20.10_10p1mb.bam; /input/NA12878_S1.chr20.10_10p1mb.bam.bai; /input/test_nist.b37_chr20_100kbp_at_10mb.bed; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; /input/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; /input/ucsc.hg19.chr20.unittest.fasta; /input/ucsc.hg19.chr20.unittest.fasta.fai; /input/ucsc.hg19.chr20.unittest.fasta.gz; /input/ucsc.hg19.chr20.unittest.fasta.gz.fai; /input/ucsc.hg19.chr20.unittest.fasta.gz.gzi. It means that all files are in the mounted Docker volume /input . Thanks so much for any help,; Rogério",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/223:2372,test,tested,2372,,https://github.com/google/deepvariant/issues/223,2,['test'],"['testdata', 'tested']"
Testability,Version Build Channel; _libgcc_mutex 0.1 main ; _openmp_mutex 5.1 1_gnu ; absl-py 2.1.0 pypi_0 pypi; argparse 1.4.0 pypi_0 pypi; blas 1.0 mkl ; bzip2 1.0.8 h5eee18b_6 ; ca-certificates 2024.7.2 h06a4308_0 ; chex 0.1.86 pypi_0 pypi; clu 0.0.9 pypi_0 pypi; contextlib2 21.6.0 pypi_0 pypi; cython 3.0.10 pypi_0 pypi; enum34 1.1.8 pypi_0 pypi; etils 1.7.0 pypi_0 pypi; flax 0.8.5 pypi_0 pypi; fsspec 2024.6.1 pypi_0 pypi; importlib-resources 6.4.0 pypi_0 pypi; intel-openmp 2023.1.0 hdb19cb5_46306 ; intervaltree 3.0.2 pypi_0 pypi; jax 0.4.31 pypi_0 pypi; jaxlib 0.4.31 pypi_0 pypi; ld_impl_linux-64 2.38 h1181459_1 ; libffi 3.4.4 h6a678d5_1 ; libgcc-ng 11.2.0 h1234567_1 ; libgomp 11.2.0 h1234567_1 ; libstdcxx-ng 11.2.0 h1234567_1 ; libuuid 1.41.5 h5eee18b_0 ; markdown-it-py 3.0.0 pypi_0 pypi; mdurl 0.1.2 pypi_0 pypi; mkl 2023.1.0 h213fc3f_46344 ; mkl-service 2.4.0 py310h5eee18b_1 ; mkl_fft 1.3.8 py310h5eee18b_0 ; mkl_random 1.2.4 py310hdb19cb5_0 ; ml-collections 0.1.1 pypi_0 pypi; ml-dtypes 0.4.0 pypi_0 pypi; mock 5.1.0 pypi_0 pypi; msgpack 1.0.8 pypi_0 pypi; ncurses 6.4 h6a678d5_0 ; nest-asyncio 1.6.0 pypi_0 pypi; numpy 1.24.3 py310h5f9d8c6_1 ; numpy-base 1.24.3 py310hb5e798b_1 ; openssl 3.0.14 h5eee18b_0 ; opt-einsum 3.3.0 pypi_0 pypi; optax 0.2.3 pypi_0 pypi; orbax-checkpoint 0.5.23 pypi_0 pypi; packaging 24.1 pypi_0 pypi; pip 24.0 py310h06a4308_0 ; protobuf 3.13.0 pypi_0 pypi; pygments 2.18.0 pypi_0 pypi; python 3.10.14 h955ad1f_1 ; pyyaml 6.0.1 pypi_0 pypi; readline 8.2 h5eee18b_0 ; rich 13.7.1 pypi_0 pypi; scipy 1.14.0 pypi_0 pypi; setuptools 69.5.1 py310h06a4308_0 ; six 1.16.0 pypi_0 pypi; sortedcontainers 2.1.0 pypi_0 pypi; sqlite 3.45.3 h5eee18b_0 ; tbb 2021.8.0 hdb19cb5_0 ; tensorstore 0.1.64 pypi_0 pypi; tf-slim 1.1.0 pypi_0 pypi; tk 8.6.14 h39e8969_0 ; toolz 0.12.1 pypi_0 pypi; typing-extensions 4.12.2 pypi_0 pypi; tzdata 2024a h04d1e81_0 ; wheel 0.43.0 py310h06a4308_0 ; wrapt 1.16.0 pypi_0 pypi; xz 5.4.6 h5eee18b_1 ; zipp 3.19.2 pypi_0 pypi; zlib 1.2.13 h5eee18b_1,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/859:12726,mock,mock,12726,,https://github.com/google/deepvariant/issues/859,1,['mock'],['mock']
Testability,"When I run build_and_test.sh, I get the following isssues; ```. Extracting Bazel installation...; .............................; (12:58:42) INFO: Current date is 2018-03-20; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:02) ERROR: /home/vinay/deepvariant/deepvariant/BUILD:564:1: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream and referenced by '//deepvariant:modeling'; (13:01:03) ERROR: Analysis of target '//deepvariant:binaries' failed; build aborted: no such package '@org_tensorflow_slim//': Unexpected end of ZLIB input stream; (13:01:03) INFO: Elapsed time: 146.946s; (13:01:03) FAILED: Build did NOT complete successfully (60 packages loaded); Fetching https://mirror.bazel.build/ufpr.dl.sourceforge.net/project/giflib/giflib-5.1.4.tar.gz; 26,415b 43s; Fetching https://mirror.bazel.build/github.com/libjpeg-turbo/libjpeg-turbo/archive/1.5.1.tar.gz; 32,588b 42s; Fetching https://mirror.bazel.build/www.kurims.kyoto-u.ac.jp/~ooura/fft.tgz; 20,092b 40s; (13:01:03) ERROR: Couldn't start the build. Unable to run tests. ```. Please Help",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/59:1390,test,tests,1390,,https://github.com/google/deepvariant/issues/59,1,['test'],['tests']
Testability,"When executing ""docker run google/deepvariant:1.4.0"" I receive the following error message:; `The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.; /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@""`. **Setup**; - Operating system: Ubuntu 20.04.4 LTS (Focal Fossa); - DeepVariant version: 1.4.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: docker run google/deepvariant:1.4.0; - Error trace: The TensorFlow library was compiled to use SSE4.1 instructions, but these aren't available on your machine.; /opt/deepvariant/bin/run_deepvariant: line 2: 7 Aborted (core dumped) python3 -u /opt/deepvariant/bin/run_deepvariant.py ""$@"". **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; CPU information from /proc/cpuinfo; product: Common KVM processor; vendor: Intel Corp.; physical id: 2; bus info: cpu@1; width: 64 bits; capabilities: fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx x86-64 constant_tsc nopl xtopology cpuid tsc_known_freq pni cx16 x2apic hypervisor lahf_lm cpuid_fault pti",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/552:948,test,test,948,,https://github.com/google/deepvariant/issues/552,2,['test'],['test']
Testability,"With `DV_CPP_TENSORFLOW_TAG=master`, I believe deepvariant will not build from source. TF master does not build with Bazel=0.15.0:. ```; + bazel test -c opt --local_test_jobs=1 --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; [bazel INFO src/main/cpp/option_processor.cc:235] Looking for master bazelrcs in the following three paths: /root/deepvariant/tools/bazel.rc, , /etc/bazel.bazelrc; [bazel INFO src/main/cpp/option_processor.cc:165] User provided no rc file.; [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/tools/bazel.rc; [bazel INFO src/main/cpp/rc_file.cc:53] Parsing the RcFile /root/deepvariant/../tensorflow/tools/bazel.rc; [bazel FATAL src/main/cpp/blaze.cc:1263] Unexpected error reading .blazerc file '/root/deepvariant/../tensorflow/tools/bazel.rc'; ```. And deepvariant does not build with bazel=0.19.0, see #134. ___. Linux xxx 4.15.0-1031-aws #33-Ubuntu SMP Fri Dec 7 09:32:27 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux. ```; # Copyright 2017 Google LLC.; #; # Redistribution and use in source and binary forms, with or without; # modification, are permitted provided that the following conditions; # are met:; #; # 1. Redistributions of source code must retain the above copyright notice,; # this list of conditions and the following disclaimer.; #; # 2. Redistributions in binary form must reproduce the above copyright; # notice, this list of conditions and the following disclaimer in the; # documentation and/or other materials provided with the distribution.; #; # 3. Neither the name of the copyright holder nor the names of its; # contributors may be used to endorse or promote products derived from this; # software without specific prior written permission.; #; # THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""; # AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE; # IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE; # ARE DI",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/145:145,test,test,145,,https://github.com/google/deepvariant/issues/145,1,['test'],['test']
Testability,\; --gvcf \; --phased_output \; --ont; ```; Relevant part of the log file (which is over 200MB):. ```; run_pepper_margin_deepvariant call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -s 6061-SL-0029 -o /cromwell_root/pepper_output -p T708322218_ONT.10_14-p.deepvariant_pepper --gvcf --phased_output --ont; [11-03-2021 13:40:40] INFO: VARIANT CALLING MODULE SELECTED; [11-03-2021 13:40:40] INFO: [1/9] RUNNING THE FOLLOWING COMMAND; -------; mkdir -p /cromwell_root/pepper_output; ; mkdir -p /cromwell_root/pepper_output/logs; ; mkdir -p /cromwell_root/pepper_output/intermediate_files;; -------; [11-03-2021 13:40:40] INFO: [2/9] RUNNING THE FOLLOWING COMMAND; -------; time pepper_snp call_variant -b /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_SNP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_snp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/1_pepper_snp.log; -------; [11-03-2021 13:40:41] INFO: CALL VARIANT MODULE SELECTED.; [11-03-2021 13:40:41] INFO: ONT PROFILE SET FOR VARIANT CALLING.; [11-03-2021 13:40:41] INFO: RUN-ID: 11032021_134041; [11-03-2021 13:40:41] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_snp/images_11032021_134041/; [11-03-2021,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:1744,log,logs,1744,,https://github.com/google/deepvariant/issues/491,1,['log'],['logs']
Testability,"] /input/gvcf.tfrecord-00000-of-00030.gz; No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1143, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1122, in main; vcf_writer, gvcf_writer); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 953, in merge_and_write_variants_and_nonvariants; nonvariant = next_or_none(nonvariant_iterable); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 948, in next_or_none; return next(iterable); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/third_party/nucleus/io/tfrecord.py"", line 139, in read_shard_sorted_tfrecords; protos = Reader(path, proto, compression_type=compression_type).iterate(); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/third_party/nucleus/io/tfrecord.py"", line 60, in Reader; path, proto, compression_type=compression_type); File ""/tmp/Bazel.runfiles_88m2azo2/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 174, in __init__; 'Error trying to open %s for reading' % input_path); OSError: [Errno 5] Error trying to open /input/gvcf.tfrecord-00000-of-00030.gz for reading. real 95m15.549s; user 89m30.039s; sys 5m49.495s. **Does the quick start test work on your system?**; yes. **Any additional context:**. Appreciate any help. Cheers.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/413:4415,test,test,4415,,https://github.com/google/deepvariant/issues/413,1,['test'],['test']
Testability,"] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.681643 140029649073920 make_examples.py:1080] Preparing inputs; 2018-12-20 07:17:31.682071: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.682173 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.682885 140029649073920 make_examples.py:996] Common contigs are [u'chr20']; I1220 07:17:31.684022 140029649073920 make_examples.py:1086] Writing examples to /home/chungtsai_su/quickstart-output/examples.tfrecord.gz; 2018-12-20 07:17:31.684869: I third_party/nucleus/io/sam_reader.cc:561] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2018-12-20 07:17:31.688126: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.688252 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.884236 140029649073920 make_examples.py:1119] Task 0: 6 candidates (6 examples) [0.20s elapsed]; I1220 07:17:33.209975 140029649073920 make_examples.py:1134] Writing MakeExamplesRunInfo to /home/chungtsai_su/quickstart-output/examples.tfrecord.gz.run_info.pbtxt; I1220 07:17:33.241107 140029649073920 make_examples.py:1137] Found 76 candidate variants; I1220 07:17:33.241497 140029649073920 make_examples.py:1138] Created 82 examples; ```. Also, the problem can be detected in test case. ```; //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s; //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s; //deepvariant/vendor:timer_test PASSED in 0.8s; //deepvariant:make_examples_test FAILED in 2 out of 2 in 1.7s; Stats over 2 runs: max = 1.7s, min = 1.6s, avg = 1.7s, dev = 0.1s; /home/chungtsai_su/.cache/bazel/_bazel_chungtsai_su/959496e1d4e585c03b8886e389170de9/execroot/com_google_deepvariant/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131:4822,test,testdata,4822,,https://github.com/google/deepvariant/issues/131,1,['test'],['testdata']
Testability,"_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ls -1 ${INPUT_DIR}; mkdir -p ${OUTPUT_DIR}; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Here is the complete error msg:; #############################################; **Any additional context:**; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; LANGUAGE = (unset),; LC_ALL = (unset),; LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; 2023-07-13 21:50:44.574140: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU i",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678:2341,test,test,2341,,https://github.com/google/deepvariant/issues/678,2,['test'],['test']
Testability,"_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --reads \""/input-gcsfused-{}/${BAM}\"" --ref \""${INPUT_REF}\"" --task {} --gvcf \""${GVCF}\""/gvcf_output.tfrecord@\""${SHARDS}\"".gz\"""": exit status 127: bash: gcsfuse: command not found. Is it possible to identify the problem/typo?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/214:2074,test,test,2074,,https://github.com/google/deepvariant/issues/214,1,['test'],['test']
Testability,_WHL_FILENAME=tensorflow-1.9.0.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ export GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_CURL_PATH=https://storage.googleapis.com/deepvariant/packages/tensorflow; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; ++ DV_COPT_FLAGS='--copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings'; + bazel; build_and_test.sh: line 39: bazel: command not found; + PATH=/home/solokopi/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-march=corei7 --copt=-Wno-sign-compare --copt=-Wno-write-strings deepvariant/...; Unexpected error reading .blazerc file '/home/solokopi/Desktop/deepvariant-r0.7/../tensorflow/tools/bazel.rc'; solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ . solokopi@solokopi-All-Series:~/Desktop/deepvariant-r0.7$ sudo bash build-prereq.sh; [sudo] password for solokopi: ; ========== Load config settings.; ========== [2018年 08月 24日 星期五 19:30:03 CST] Stage 'Install the runtime packages' starting; ========== Load config settings.; ========== [2018年 08月 24日 星期五 19:30:03 CST] Stage 'Misc setup' starting; Hit:1 http://mirrors.aliyun.com/ubuntu xenial InRelease; Hit:2 http://mirrors.aliyun.com/ubuntu xenial-updates InRelease ; Get:3 http://mirrors.aliyun.com/ubuntu xenial-backports InRelease [107 kB] ; Ign:4 http://dl.google.com/linux/chrome/deb stable InRelease ; Hit:5 http://ppa.launchpad.net/webupd8team/java/ubuntu xenial InRelease ; Hit:6 http:/,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:3431,test,test,3431,,https://github.com/google/deepvariant/issues/89,1,['test'],['test']
Testability,_core.py:257] Task 21/32: Preparing inputs; I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']; I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']; I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs; I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']; I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs; I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Tas,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:10502,test,testdata,10502,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,_core.py:257] Task 8/32: Preparing inputs; I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']; I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs; I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']; I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs; I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs; **********; I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op.; INFO:tensorflow:Reloading EMA...; I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA...; INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt; I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt; I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]; I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:11619,test,testdata,11619,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Launcher: Job 6 completed in 0 seconds.; Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); Launcher: Job 7 completed in 0 seconds.; FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory; Launcher: Job 8 completed in 0 seconds.; Launcher: Task 0 done. Exiting.; Launcher: Task 1 done. Exiting.; I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test; I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to run deepvariant for multiple bam files parallelly in a slurm based HPC cluster",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:3548,test,test,3548,,https://github.com/google/deepvariant/issues/717,5,['test'],['test']
Testability,"_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds.; Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Launcher: Job 6 completed in 0 seconds.; Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:2482,test,test,2482,,https://github.com/google/deepvariant/issues/717,1,['test'],['test']
Testability,"_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. This script made errors [(please see this file)](https://github.com/google/deepvariant/files/2708579/shuffle.train.0_id_masked.log). However, the codes which used chromosome-divided tfrecords (chr1-10 and chr11-chr19) worked fine as follows:. INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 10`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. and. INPUT_PATTERN_LIST=examples.train/sample_id/11/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 12 19`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. I cannot figure out the reason why this divided approach succeeded. Could you tell me any suggestions of points to check?; Because I'm using personal data under strict data management, I cannot provide or share our tfrecords. I used apache-beam v2.9.0 in python 2.7.5 (not in docker of DeepVariant v0.7.1).; If some version dependencies exist, I'd like to get some dockerfile like your environment. Best regards,. Masaru",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/133:1875,log,log,1875,,https://github.com/google/deepvariant/issues/133,3,['log'],['log']
Testability,"_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} )' returned non-zero exit status 250. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; no; **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/471:3084,test,testdata,3084,,https://github.com/google/deepvariant/issues/471,4,['test'],"['test', 'testdata']"
Testability,"_run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/_0_Диссертация/Exp/seq1/seq1.fa"" --reads ""/home/platon/_0_Диссертация/Exp/seq1/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Second attempt. This time with paths consisting only of latin characters.; `sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/platon/test/seq1.fa --reads=/home/platon/test/seq1.bam --output_vcf=/home/platon/test/seq1.vcf`. ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}. [E::hts_open_format] Failed to open file /home/platon/test/seq1.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.re",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/219:3168,test,test,3168,,https://github.com/google/deepvariant/issues/219,1,['test'],['test']
Testability,"``. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? ; I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs...; Using shell: /usr/bin/bash; Provided cores: 64; Rules claiming more threads will be scaled down.; Select jobs to execute... > [Wed Jan 4 18:30:51 2023]; > rule deepvariant:; > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta; > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz; > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log; > jobid: 0; > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2; > threads: 64; > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323; > ; > Activating singularity image singularity/deepvariant_1.4.0.sif; > INFO: Convert SIF file to sandbox...; > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp; > ; > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****; > ; > ; > ***** Running the command:*****; > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_cou",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:2936,log,log,2936,,https://github.com/google/deepvariant/issues/602,2,['log'],"['log', 'logs']"
Testability,"```(bash; BIN_VERSION=""1.1.0""; INPUT_DIR=""${PWD}/lecture8_data"". ls -1 ${INPUT_DIR}. OUTPUT_DIR=""${PWD}/AO6-output""; mkdir -p ""${OUTPUT_DIR}"". docker run \; 	-v ""${INPUT_DIR}"":""/input"" \; 	-v ""${OUTPUT_DIR}"":""/output"" \; 	google/deepvariant:""${BIN_VERSION}"" \; 	/opt/deepvariant/bin/run_deepvariant \; 	--model_type=WGS \; 	--ref=/input/ref.fa \; 	--reads=/input/sample.bam \; 	--output_vcf=/output/OUTPUT_VCF.vfc \; 	--output_gvcf=/output/OUTPUT_GVCF.vfc \; 	--call_variants_extra_args=""use_openvino=true"" \; 	--num_shards=$(nproc) \; 	--logging_dir=/output/logs. ```. ```{bash}; ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpgv2oy1s_/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpgv2oy1s_/make_examples.tfrecord@8.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 502, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 323, in call_variants; first_example = tf_utils.get_one_example_from_examples_path(examples_filename); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/tf_utils.py"", line 205, in get_one_example_from_examples_path; 'Cannot find matching files with the pattern ""{}""'.format(source))",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/435:559,log,logs,559,,https://github.com/google/deepvariant/issues/435,3,['log'],"['log', 'logs']"
Testability,a/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs; I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']; I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']; I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs; I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']; I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.234176 139704511100736 make_examples_core.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:10253,test,testdata,10253,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"ad apptainer/1.2.5; module load clusterbasics; module load samtools; module load bedtools. OUTPUT_DIR=./output/$sID. mkdir -p $OUTPUT_DIR; mkdir -p ./tmp; export TMPDIR=`realpath ./tmp`. if [ ! -f $sBAM.bai ]; then; echo producing bai index for $sBAM; samtools index $sBAM; fi. if [ ! -f ""${OUTPUT_DIR}/dv.log"" ];then; bedtools coverage -g genome.file -sorted -d -a genome.bed -b ""$sBAM"" | awk '{if ($5>=3) print $1""\t""($4-1)""\t""$4""\t""$5}' | bedtools merge -d 1 -c 4 -o mean -i - > ${OUTPUT_DIR}/cov3x.bed; fi. apptainer run -B /public:/public,/public3:/public3,/public2:/public2,/fast3:/fast3,/public4:/public4 \; /public4/software/deepvariant/1.6.1/cpuver/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --make_examples_extra_args=""normalize_reads=true"" \; --model_type=WES \; --ref=$REF \; --reads=""$sBAM"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --regions=""${OUTPUT_DIR}/cov3x.bed"" \; --num_shards=$CPU > ${OUTPUT_DIR}/dv.log 2>&1. `. Inspecting the tail of the log, it appears that the program gets stuck at the make_examples step, with many threads reporting finding 0 examples:; 'I0812 17:25:00.705988 139682501986112 make_examples_core.py:301] Task 14/32: Overhead for preparing inputs: 270 seconds; I0812 17:25:00.763086 139682501986112 make_examples_core.py:301] Task 14/32: 0 candidates (0 examples) [0.06s elapsed]; I0812 17:25:01.273164 139627217889088 genomics_reader.py:222] Reading ../mapped/SRR18493715.RNA-Seq.Camellia_sp._multipetala.leaf/Aligned.sortedByCoord.out.bam with NativeSamReader; I0812 17:25:01.308415 139627217889088 make_examples_core.py:301] Task 18/32: Writing gvcf records to /public4/courses/ec3121/shareddata/Camellia_Sect_Chrysantha/star_hapbetter/deepvariant/tmp/tmp0n4wz07d/gvcf.tfrecord-00018-of-00032.gz; I0812 17:25:01.325705 139627217889088 make_examples_core.py:301] Task 18/32: Writing examples to /public4/courses/ec3121/shareddata/Camellia_Sect_Chrysantha/star_hapbetter/deepvariant/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/867:1641,log,log,1641,,https://github.com/google/deepvariant/issues/867,1,['log'],['log']
Testability,ader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']; I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs; I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']; I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs; I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']; I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs; I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] T,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:10988,test,testdata,10988,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"ages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/_0_Диссертация/Exp/seq1/seq1.fa"" --reads ""/home/platon/_0_Диссертация/Exp/seq1/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Second attempt. This time with paths consisting only of latin characters.; `sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/platon/test/seq1.fa --reads=/home/platon/test/seq1.bam --output_vcf=/home/platon/test/seq1.vcf`. ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}. [E::hts_open_format] Failed to open file /home/platon/test/seq1.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_opt",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/219:3128,test,test,3128,,https://github.com/google/deepvariant/issues/219,1,['test'],['test']
Testability,"ake_examples_runner(options); File ""/tmp/Bazel.runfiles_1j54j0yh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1025, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_1j54j0yh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 953, in processing_regions_from_options; options.exclude_calling_regions),; File ""/tmp/Bazel.runfiles_1j54j0yh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 549, in build_calling_regions; regions = ranges.RangeSet.from_contigs(contigs); File ""/tmp/Bazel.runfiles_1j54j0yh/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 126, in from_contigs; return cls(make_range(contig.name, 0, contig.n_bases) for contig in contigs); File ""/tmp/Bazel.runfiles_1j54j0yh/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 86, in __init__; self._by_chr[range_.reference_name].addi(range_.start, range_.end, None); File ""/usr/local/lib/python2.7/dist-packages/intervaltree/intervaltree.py"", line 330, in addi; return self.add(Interval(begin, end, data)); File ""/usr/local/lib/python2.7/dist-packages/intervaltree/intervaltree.py"", line 313, in add; "" {0}"".format(interval); ValueError: IntervalTree: Null Interval objects not allowed in IntervalTree: Interval(0, 0); parallel: This job failed:; python bin/make_examples.zip --mode calling --ref ../Falcon_Unzip/all_p_ctg.fa --reads ../Falcon_Unzip/out.bam --sample_name FalconSet --examples quickstart-output/examples.tfrecord@40.gz --task 1; seq 0 $((N_SHARDS-1)) 0,00s user 0,00s system 0% cpu 0,003 total; parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python 2,40s user 0,92s system 109% cpu 3,034 total. ```. Is there something wrong with the bam file? My reads were mapped using bwa mem ; `bwa mem -p -t 48 all_p_ctg.fa ../ARC_MA_sequencing/94_Fastq_files_GC047403/GC047403.170925_ARC_.fq.gz|samtools sort -@ 48 -m 5G -o out.bam -`. thanks for any help",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/77:6740,LOG,LOGDIR,6740,,https://github.com/google/deepvariant/issues/77,3,"['LOG', 'log']","['LOGDIR', 'log']"
Testability,"aligner/realigner.py"", line 806 in realign_reads; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples_core.py"", line 1881 in realign_reads; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples_core.py"", line 1908 in <listcomp>; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples_core.py"", line 1907 in realign_reads_per_sample_multisample; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples_core.py"", line 1709 in process; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples_core.py"", line 2838 in make_examples_runner; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples.py"", line 224 in main; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_v_91gifn/runfiles/_main/deepvariant/make_examples.py"", line 234 in <module>; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/ucsc.hg19.chr20.unittest.fasta --reads /input/NA12878_S1.chr20.10_10p1mb.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --channels insert_size --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real 0m5.584s; user 0m13.426s; sys 0m0.563s; ```; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; I am running this on an AWS graviton4 machine (aarch64 architecture). The Dockerfile does not work for me directly. I had to run `build-prereq.sh` and resolve all the errors manually within a docker container. But I did run build_and_test.sh to make sure all tests passed and the binaries were also built successfully.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879:5097,test,test,5097,,https://github.com/google/deepvariant/issues/879,3,['test'],"['test', 'tests']"
Testability,"all last):; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768, in main; call_variants(; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 598, in call_variants; model_example_shape = dv_utils.get_shape_and_channels_from_json(; File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/dv_utils.py"", line 367, in get_shape_and_channels_from_json; example_info = json.load(f); File ""/usr/lib/python3.8/json/__init__.py"", line 293, in load; return loads(fp.read(),; File ""/usr/lib/python3.8/json/__init__.py"", line 357, in loads; return _default_decoder.decode(s); File ""/usr/lib/python3.8/json/decoder.py"", line 337, in decode; obj, end = self.raw_decode(s, idx=_w(s, 0).end()); File ""/usr/lib/python3.8/json/decoder.py"", line 355, in raw_decode; raise JSONDecodeError(""Expecting value"", s, err.value) from None; json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0). Process ForkProcess-1:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/tmp/Bazel.runfiles_5v5s5_vp/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 454, in post_processing; item = output_queue.get(timeout=180); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 108, in get; raise Empty; _queue.Empty. real 3m2.335s; user 0m7.450s; sys 0m4.274s`. **Does the quick start test work on your system?**; Yes",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/869:7696,test,test,7696,,https://github.com/google/deepvariant/issues/869,1,['test'],['test']
Testability,"all_variants.py"", line 494, in main; call_variants(; File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 363, in call_variants; raise ValueError('The number of channels in examples and checkpoint '; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 7 channels while the examples have 6. My command line looks like this:. export HOME=/root && N_SHARDS=32 && GVCF_TFRECORDS=""./gvcf.tfrecord@${N_SHARDS}.gz"" && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --gvcf ./gvcf.tfrecord@${N_SHARDS}.gz --add_hp_channel --noadd_hp_channel --downsample_fraction 0 --reads /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/NA12878_S1.chr20.10_10p1mb.bam --regions /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/test_nist.b37_chr20_100kbp_at_10mb.bed --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/ucsc.hg19.chr20.unittest.fasta --sample_name NA12878_S1 --vsc_min_count_indels 2 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/wgs/model.ckpt --use_openvino --batch_size 512 --include_debug_info --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/ucsc.hg19.chr20.unittest.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./NA12878_S1.vcf --nonvariant_site_tfrecord_path ""${GVCF_TFRECORDS}"" --gvcf_outfile ./NA12878_S1.gvcf.gz ) > ./postprocess_variants.log 2>&1. Is there something I am missing?. Thanks,; Raisa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/625:2719,log,log,2719,,https://github.com/google/deepvariant/issues/625,3,['log'],['log']
Testability,"alling_exome_regions_configuration. ```#!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=PROJECT_ID; OUTPUT_BUCKET=gs://OUTPUT_BUCKET; STAGING_FOLDER_NAME=wes_staging; OUTPUT_FILE_NAME=wes_output.vcf; # Model for calling exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; #; # Changing the number of chards changes the output for some reason; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://deepvariant/exome-case-study-testdata/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam \; --bai gs://deepvariant/exome-case-study-testdata/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai \; --ref gs://deepvariant/exome-case-study-testdata/hs37d5.fa.gz \; --regions gs://deepvariant/exome-case-study-testdata/refseq.coding_exons.b37.extended50.bed \; --shards 64 \; --make_examples_workers 8 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 100 \; --call_variants_workers 1 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --max_preemptible_tries 5 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west1 \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/112:1093,test,testdata,1093,,https://github.com/google/deepvariant/issues/112,1,['test'],['testdata']
Testability,"ant_files; OUTPUT_FILE_NAME=TLE_a_001_deep_variant.vcf; # Model for calling whole exome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.0/DeepVariant-inception_v3-0.7.0+data-wes_standard; IMAGE_VERSION=0.7.0; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west1-b \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://canis/CNR-data/CDS-canonical.bed \; --bam gs://canis/CNR-data/TLE_a_001.bam \; --bai gs://canis/CNR-data/TLE_a_001.bam.bai \; --ref gs://genomics-public-data/references/GRCh38_Verily/GRCh38_Verily_v1.genome.fa \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones us-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}""; ```. I get the following error: ; ```; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 862, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 845, in run; _run_make_examples(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 340, in _run_make_examples; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 352, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/valis-194104/operations/13939489157244551677"" failed: executing pipeline: Execution failed: action 5: unexpected exit status 1 wa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/116:1327,log,logging,1327,,https://github.com/google/deepvariant/issues/116,1,['log'],['logging']
Testability,"apan.; I also asked in https://github.com/google/deepvariant/issues/127 ; but this is a different type of issue. I also want to train DeepVariant model from the WGS data in our laboratory using our standalone computational resources.; I referred and followed https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md . First, I successfully made tfrecord file using DeepVariant v0.7.1 (docker). Then I tried shuffling tfrecords as follows:. #!/bin/sh; ; INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 19`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done; ; /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. This script made errors [(please see this file)](https://github.com/google/deepvariant/files/2708579/shuffle.train.0_id_masked.log). However, the codes which used chromosome-divided tfrecords (chr1-10 and chr11-chr19) worked fine as follows:. INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 10`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. and. INPUT_PATTERN_LIST=examples.train/sample_id/11/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 12 19`; do; INPUT_PATTERN_L",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/133:1048,log,log,1048,,https://github.com/google/deepvariant/issues/133,1,['log'],['log']
Testability,"at . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0; I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt; W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:1508,log,log,1508,,https://github.com/google/deepvariant/issues/172,1,['log'],['log']
Testability,atchNorm/moving_variance|InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6b/Branch_2/Conv2d_0c_1x7/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/weights/RMSProp|InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/BatchNorm/moving_variance|InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean|InceptionV3/Logits/Conv2d_1c_1x1/biases|InceptionV3/Conv2d_1a_3x3/weights/RMSProp_1|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_7c/Branch_2/Conv2d_0d_3x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/weights/RMSProp_1|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean|InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights/RMSProp_1|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights/RMSProp|InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean|InceptionV3/Mixed_6d/Branch_1/Conv2d_0c_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Bra,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:26279,Log,Logits,26279,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,"ation for your free program. However, as I understand it, I have some high-level questions / concerns:. **1)** You mention scoring high on the PrecisionFDA calling, but I don't see the term ""DeepVariant"" or ""Google"" any where. I apologize for missing something that a lot of people might know, but would you mind explaining how the results in the [PrecisionFDA results table](https://precision.fda.gov/challenges/truth/results) match DeepVariant?. **2)** While it has been hard for me explain myself precisely, I have been concerned that there was somehow over-fitting for some datasets reporting extremely high accuracy. In the PrecisionFDA challenge, I think it should probably be mentioned that there are multiple programs with high percentages, including different workflows that actually use the same variant caller (like GATK) and no strategy was ""best"" for all the criteria defined. **3)** While I don't know the precise way in which you could have a lack of independence for training versus test datasets, I think there are other benchmarks that better match what I would expect for more typical results. For example, [in this paper](https://www.nature.com/articles/s41587-019-0054-x), it says ""_In the high-confidence regions, when comparing these pipelines to each other (https://precision.fda.gov/jobs/job-FJpqBP80F3YyfJG02bQzPJBj, link immediately accessible by requesting an account), they agreed on 99.7% of SNVs and 98.7% of indels. Outside the high-confidence regions (https://precision.fda.gov/jobs/job-FJpqJF80F3YyXqz6Kv8Q1BQK), they agreed with each other on only 76.5% of SNVs and 78.7% of indels_."". **4)** You mention ""_No filtering is needed beyond setting your preferred minimum quality threshold_."". While I don't currently have much first-hand experience (although that is on my long-term ""to-do"" list), I didn't think this was true. For example, unless I am missing something, this table reports a very high ""Failed Filters"" count for DeepVariant versus GATK:. https://www.n",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/165:1074,test,test,1074,,https://github.com/google/deepvariant/issues/165,2,"['benchmark', 'test']","['benchmarks', 'test']"
Testability,"azel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open /home/platon/test/seq1.bam. real	0m1.571s; user	0m1.481s; sys	0m0.786s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Source data:; [test_seq_data.tar.gz](https://github.com/google/deepvariant/files/3611565/test_seq_data.tar.gz)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/219:5558,test,test,5558,,https://github.com/google/deepvariant/issues/219,2,['test'],['test']
Testability,b 6 18:18 test.examples.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 te,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1550,test,test,1550,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"bam \; --output_vcf ""deepvariant/output.vcf.gz"" \; --num_shards 24 -v 2; ```; - Error trace: (if applicable); ```bash; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples /tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 2. real 0m35.091s; user 0m1.452s; sys 0m1.237s; I0205 10:26:31.374659 47922265431040 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 23 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ref.fa --reads reads.bam --examples ""/tmp/tmp7rsj5zvh/make_examples.tfrecord@24.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --noparse_sam_aux_fields --norealign_reads --nosort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 252.; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; - I have limited access to docker in general, and no access to docker on this machine. I've been running v1.0.0 on singularity on different input types (human WGS) regularly without errors. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/419:2318,test,test,2318,,https://github.com/google/deepvariant/issues/419,2,['test'],['test']
Testability,"bash; set -euo pipefail; # Set common settings.; PROJECT_ID=udndv-197518 #changed; OUTPUT_BUCKET=gs://udnXXXXXX #changed; STAGING_FOLDER_NAME=staging-folder #changed; OUTPUT_FILE_NAME=output.vcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-182548131.data-wgs_standard; # Model for calling exome sequencing data.; # MODEL=gs://deepvariant/models/DeepVariant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-181413382.data-wes_standard; IMAGE_VERSION=0.5.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; DOCKER_IMAGE_GPU=gcr.io/deepvariant-docker/deepvariant_gpu:""${IMAGE_VERSION}"". # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --pipeline-file deepvariant_pipeline.yaml \; --logging ""${OUTPUT_BUCKET}""/runner_logs \; --zones us-west1-b \; --inputs `echo \; PROJECT_ID=""${PROJECT_ID}"", \; OUTPUT_BUCKET=""${OUTPUT_BUCKET}"", \; MODEL=""${MODEL}"", \; DOCKER_IMAGE=""${DOCKER_IMAGE}"", \; DOCKER_IMAGE_GPU=""${DOCKER_IMAGE_GPU}"", \; STAGING_FOLDER_NAME=""${STAGING_FOLDER_NAME}"", \; OUTPUT_FILE_NAME=""${OUTPUT_FILE_NAME}"" \; | tr -d '[:space:]'`; ```. I execute `./runner.sh`, and a few minutes later I can tell with `gcloud alpha genomics operations describe` that it's failed. That output is [attached](https://github.com/google/deepvariant/files/1835589/describe.out.txt). . I can see in it several distinct potential errors: . 1. `11: Docker run failed: command failed: [03/21/2018 23:29:54 INFO gcp_deepvariant_runner.py] Running make_examples...`; 2. ` [03/21/2018 23:29:54 WARNING __init__.py] file_cache is unavailable when using oauth2client >= 4.0.0`; 3. `[u'Error in job call-varia--root--180321-233157-28 - code 9: Quota CPUS exceeded in region us-central1']`. The `...-stderr.log` file written to `staging-folder` also begins with the errors; ```; /tmp/ggp-896952821: line 16: type: gsutil: not found; debconf: delaying package configuration, since a",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:1899,log,logging,1899,,https://github.com/google/deepvariant/issues/60,1,['log'],['logging']
Testability,"bed_file} \; --output_vcf ${PWD}/${params.outdir}/${sample_id}_raw.vcf.gz \; --output_gvcf ${PWD}/${sample_id}_raw.gvcf.gz \; --num_shards ${task.cpus}; --intermediate_results_dir ${PWD}/tmp > deepvariant_log.txt 2>&1. """"""; }. ############# Error ###################. N E X T F L O W ~ version 24.04.4. Launching `dip.nf` [deadly_pike] DSL2 - revision: e075b1fba0. executor > local (2); [a6/9c6b79] process > deepvar (Germline Variant on SRR26512959) [ 0%] 0 of 2; ERROR ~ Error executing process > 'deepvar (Germline Variant on SRR26512958)'. executor > local (2); [a6/9c6b79] process > deepvar (Germline Variant on SRR26512959) [100%] 1 of 1, failed: 1; ERROR ~ Error executing process > 'deepvar (Germline Variant on SRR26512958)'. Caused by:; Process `deepvar (Germline Variant on SRR26512958)` terminated with an error exit status (127). Command executed:. sudo docker run -v ""/home/ubuntu/dd/nextflow2"":""/home/ubuntu/dd/nextflow2"" google/deepvariant:1.6.1 /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /home/ubuntu/dd/nextflow2/reference/Homo_sapiens_assembly38.fasta --reads /home/ubuntu/dd/nextflow2/output/4.markDuplicate/SRR26512958_sorted_md.bam --regions /home/ubuntu/dd/nextflow2/reference/hg38_exome.bed --output_vcf /home/ubuntu/dd/nextflow2/output/5.snvS/SRR26512958_raw.vcf.gz --output_gvcf /home/ubuntu/dd/nextflow2/SRR26512958_raw.gvcf.gz --num_shards 16; --intermediate_results_dir /home/ubuntu/dd/nextflow2/tmp > deepvariant_log.txt 2>&1. Command exit status:; 127. Command output:; (empty). Command error:; docker: Error response from daemon: open /var/lib/docker/overlay2/fe3663cd03e849890d83be14603f217249f3f43f9585b554df599d0318909f21/.tmp-committed2046174062: no such file or directory.; See 'docker run --help'. Work dir:; /home/ubuntu/dd/nextflow2/work/ea/9ecd306270fe3f00d9b73f8261fe89. Tip: you can try to figure out what's wrong by changing to the process work dir and showing the script file named `.command.sh`. -- Check '.nextflow.log' file for details",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/883:3205,log,log,3205,,https://github.com/google/deepvariant/issues/883,1,['log'],['log']
Testability,beta; prev_var_name: Unchanged; I0415 07:34:38.044384 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_1/Conv2d_0c_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.047698 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.048196 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.048751 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.049181 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.049621 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/biases; prev_var_name: Unchanged; I0415 07:34:38.050132 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050539 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/weights; prev_var_name: Unchanged; I0415 07:34:38.050942 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights; prev_var_name: Unchanged; I0415 07:34:38.051342 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.051747 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights; prev_var_name: Unchanged; I0415 07:34:38.052149 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5b/B,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:115213,Log,Logits,115213,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,"bove command successfully ran for _make_example_ module and failed at _call_varaint_keras.py_ with the following error message.. -------------. _Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 399, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 312, in run_; _run_main(main, args); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 387, in main; call_variants(; File ""/tmp/Bazel.runfiles_004ga1wn/runfiles/com_google_deepvariant/deepvariant/call_variants_keras.py"", line 344, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py"", line 1047, in assert_consumed; raise AssertionError(; AssertionError: Some objects had attributes which were not restored:; <tf.Variable 'classification/kernel:0' shape=(2048, 3) dtype=float32, numpy=_. ------------; Please note that I used the same checkpoint path as I was using while running _run_deepvariant.py_.; Could you please help me to resolve the issue and suggest if I am missing anything while using _call_varaint_keras.py_?. Thanks,; Saurabh",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/636:2104,Assert,AssertionError,2104,,https://github.com/google/deepvariant/issues/636,2,['Assert'],['AssertionError']
Testability,"c.): Singularity (v3.10.0); - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Input BAM was downsampled 10-fold to 30X. **Steps to reproduce:**; - Command:; ```; singularity run \; -B /usr/lib/locale/:/usr/lib/locale/ \; -B /paedyl01/disk1/louisshe/ref/GIAB/HG005/hs37d5/novoalign_bam/:/input_reads \; -B /paedyl01/disk1/louisshe/out/GIAB/HG005/heterozygous_deletions/heterozygous_sites/:/output \; -B /tmp:/tmp \; -B /paedyl01/disk1/louisshe/ref/hs37d5:/ref/hs37d5 \; -B /paedyl01/disk1/louisshe/ref/hg19:/ref/hg19 \; --home /paedyl01/disk1/louisshe/ref/GIAB/HG005/hs37d5/ \; --contain \; /paedyl01/disk1/louisshe/tools/DeepVariant/deepvariant_1.6.1.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/ref/hs37d5/hs37d5.fa \; --reads=/input_reads/HG005.hs37d5.30x.bam \; --output_vcf=/output/HG005.dv.vcf.gz \; --output_gvcf=/output/HG005.dv.g.vcf.gz \; --num_shards=10 \; --intermediate_results_dir=/tmp \; --logging_dir=/output/log \; --dry_run=false \; --par_regions_bed=/ref/hg19/ucsc.hg19.par.bed \; --haploid_contigs=""chrX,chrY""; ```; - Error trace:; Error trace below is from `HG005_deppvariant.log`. No error prompts prior to this step.; ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/call_variants_output.tfrecord.gz"" --examples ""/tmp/make_examp. /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Ker. For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; I0619 14:57:56.059498 47403021002560 call_variants.py:563] Total 1 writing processes started.; I0619 14:57:56.063244 47403021002560",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:1507,log,log,1507,,https://github.com/google/deepvariant/issues/833,1,['log'],['log']
Testability,"cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o; neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs; I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']; I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs; I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']; I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Ta",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:9385,test,testdata,9385,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']; I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs; I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']; I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']; I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs; I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']; I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:10016,test,testdata,10016,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,ckages; ++ export DV_GPU_BUILD=0; ++ DV_GPU_BUILD=0; ++ export DV_USE_GCP_OPTIMIZED_TF_WHL=1; ++ DV_USE_GCP_OPTIMIZED_TF_WHL=1; ++ export GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ GCP_OPTIMIZED_TF_WHL_FILENAME=tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl; ++ export GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ GCP_OPTIMIZED_TF_WHL_PATH=gs://deepvariant/packages/tensorflow; ++ export DV_TF_NIGHTLY_BUILD=0; ++ DV_TF_NIGHTLY_BUILD=0; ++ export DV_INSTALL_GPU_DRIVERS=0; ++ DV_INSTALL_GPU_DRIVERS=0; +++ which python; ++ export PYTHON_BIN_PATH=/usr/bin/python; ++ PYTHON_BIN_PATH=/usr/bin/python; ++ export USE_DEFAULT_PYTHON_LIB_PATH=1; ++ USE_DEFAULT_PYTHON_LIB_PATH=1; ++ export 'DV_COPT_FLAGS=--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ DV_COPT_FLAGS='--copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3'; ++ export DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; ++ DV_TENSORFLOW_GIT_SHA=ab0fcaceda001825654424bf18e8a8e0f8d39df2; + [[ 0 = \1 ]]; + bazel test -c opt --copt=-msse4.1 --copt=-msse4.2 --copt=-mavx --copt=-O3 deepvariant/...; ..................; (09:27:04) INFO: Current date is 2017-12-21; (09:27:04) Loading: ; (09:27:04) Loading: 0 packages loaded; (09:27:05) Loading: 0 packages loaded; (09:27:06) Loading: 7 packages loaded; currently loading: deepvariant/core/genomics ... (6 packages); (09:27:07) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:08) Loading: 10 packages loaded; currently loading: deepvariant/core/genomics ... (3 packages); (09:27:09) Analyzing: 242 targets (15 packages loaded); (09:27:11) Analyzing: 242 targets (16 packages loaded); (09:27:12) Analyzing: 242 targets (18 packages loaded); (09:27:14) Analyzing: 242 targets (31 packages loaded); (09:27:14) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:96:1: Fir,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:3176,test,test,3176,,https://github.com/google/deepvariant/issues/19,1,['test'],['test']
Testability,"com/google/deepvariant/blob/r1.5/docs/FAQ.md**: YES. **Describe the issue:**; (A clear and concise description of what the issue is.); CANNOT RUN EXAMPLE DATA USING A SINGULARITY CONTAINER - GETTING AN ERROR: RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem. **Setup**; - Operating system: Ubuntu 18.04 (bionic); - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): SINGULARITY sif made as follows:; BIN_VERSION=""1.5.0""; singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}""; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); EXAMPLE DATA PROVIDED. **Steps to reproduce:**; - Command:. INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ \; /fh/fast/furlan_s/grp/sifs/deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz. - Error trace: (if applicable) SEE BELOW. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. YES THIS IS WITH THE QUICK START EXAMPLE. **Any additional context:**. Message:. 2023-05-02 14:40:43.757041: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the followin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640:1034,test,testdata,1034,,https://github.com/google/deepvariant/issues/640,1,['test'],['testdata']
Testability,com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_groups.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/walker-inl.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/flags.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/logging.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mix.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/mutex.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/rune.cc' contains an error and its package is in error and referenced by '@com_goo,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:16685,log,logging,16685,,https://github.com/google/deepvariant/issues/19,1,['log'],['logging']
Testability,com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:util/util.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/filtered_re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/re2.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/deepvariant/deepvariant/testing/BUILD:19:1: Target '@com_googlesource_code_re2//:re2' contains an error and its package is in error and referenced by '//deepvariant/testing:gunit_extras'; (09:27:18) ERROR: Analysis of target '//deepvariant/testing:gunit_extras_test' failed; build aborted: Loading failed; (09:27:18) INFO: Elapsed time: 14.618s; (09:27:18) FAILED: Build did NOT complete successfully (48 packages loaded); (09:27:18) ERROR: Couldn't start the build. Unable to run tests; ```; Could anyone shed some light on this issue? Interestingly this was working a few days ago but possibly on a different host. Could it be hardware dependent?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:20543,test,testing,20543,,https://github.com/google/deepvariant/issues/19,4,['test'],"['testing', 'tests']"
Testability,"containers in /home/chungtsai_su/.local/lib/python2.7/site-packages (from intervaltree==2.1.0) (2.1.0); Installing collected packages: intervaltree; Successfully installed intervaltree-2.1.0; ```; Then the problem is solved. ; ```; chungtsai_su@seqslab:~/src/deepvariant$ ./bazel-bin/deepvariant/make_examples --mode calling --ref ""${REF}"" --reads ""${BAM}"" --regions ""chr20:10,000,000-10,010,000"" --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; 2018-12-20 07:17:31.678190: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.678396 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.681643 140029649073920 make_examples.py:1080] Preparing inputs; 2018-12-20 07:17:31.682071: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.682173 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.682885 140029649073920 make_examples.py:996] Common contigs are [u'chr20']; I1220 07:17:31.684022 140029649073920 make_examples.py:1086] Writing examples to /home/chungtsai_su/quickstart-output/examples.tfrecord.gz; 2018-12-20 07:17:31.684869: I third_party/nucleus/io/sam_reader.cc:561] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2018-12-20 07:17:31.688126: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.688252 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.884236 140029649073920 make_examples.py:1119] Task 0: 6 candidates (6 examples) [0.20s elapsed]; I1220 07:17:33.209975 140029649073920 make_examples.py:1134] Writing MakeExamplesRunInfo to /home/chungtsai_su/quickstart-output/examples.tfrecord.gz.run_info.pbtxt; I1220 07:17:33.241107 140029",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131:4203,test,testdata,4203,,https://github.com/google/deepvariant/issues/131,1,['test'],['testdata']
Testability,core.py:257] Task 8/32: Common contigs are ['chr20']; I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs; I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']; I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs; I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs; **********; I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op.; INFO:tensorflow:Reloading EMA...; I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA...; INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt; I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt; I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]; I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]; I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s; user 0m37.643s; sys 0m6.426s. ***** Running the command:*****; time /opt/deepvariant/bin/post,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:11857,test,testdata,11857,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"cratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Launcher: Job 6 completed in 0 seconds.; Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); Launcher: Job 7 completed in 0 seconds.; FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory; Launcher: Job 8 completed in 0 seconds.; Launcher: Task 0 done. Exiting.; Launcher: Task 1 done. Exiting.; I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test; I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /sc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:3378,test,test,3378,,https://github.com/google/deepvariant/issues/717,1,['test'],['test']
Testability,"cromwell_root/pepper_output/pepper_snp/; ; echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; ; zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq; -------; CONTIGS FOUND IN PEPPER SNP VCF:; chr10; chr14; [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND; -------; time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;; mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; ; samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; -------; Running OpenMP with 64 threads.; > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json; > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL; > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks; > Ordering chunks by estimated depth; > Setup complete, beginning run; > Polishing 3% complete (46/1342). Estimated time remaining: unknown; > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s; > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s; > Polishing 14% complete (191/1342). Estimated time remaining: 2m 39s; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:6587,log,log,6587,,https://github.com/google/deepvariant/issues/491,1,['log'],['log']
Testability,"cvo_paths_and_first_record; raise ValueError(; ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; ???. **Any additional context:**; Yes. I can change the parameter ""--infile"" of the postprocess_variants.py call from ""./call_variants_output.tfrecord.gz"" to ""./call_variants_output@1.tfrecord.gz"" and it works. Anyway, the call of postprocess_variants.py is auto-generated by ""/opt/deepvariant/bin/run_deepvariant"". The error does not occur for every sample ... directory content of intermediate_results_dir after the error occured:; call_variants.log; call_variants_output-00000-of-00001.tfrecord.gz; gvcf.tfrecord-00000-of-00008.gz; gvcf.tfrecord-00001-of-00008.gz; gvcf.tfrecord-00002-of-00008.gz; gvcf.tfrecord-00003-of-00008.gz; gvcf.tfrecord-00004-of-00008.gz; gvcf.tfrecord-00005-of-00008.gz; gvcf.tfrecord-00006-of-00008.gz; gvcf.tfrecord-00007-of-00008.gz; make_examples.log; make_examples.tfrecord-00000-of-00008.gz; make_examples.tfrecord-00000-of-00008.gz.example_info.json; make_examples.tfrecord-00001-of-00008.gz; make_examples.tfrecord-00001-of-00008.gz.example_info.json; make_examples.tfrecord-00002-of-00008.gz; make_examples.tfrecord-00002-of-00008.gz.example_info.json; make_examples.tfrecord-00003-of-00008.gz; make_examples.tfrecord-00003-of-00008.gz.example_info.json; make_examples.tfrecord-00004-of-00008.gz; make_examples.tfrecord-00004-of-00008.gz.example_info.json; make_examples.tfrecord-00005-of-00008.gz; make_examples.tfrecord-00005-of-00008.gz.example_info.json; make_examples.tfrecord-00006-of-00008.gz; make_examples.tfrecord-00006-of-00008.gz.example_info.json; make_examples.tfrecord-00007-of-00008.gz; make_examples.tfrecord-00007-of-00008.gz.example_info.json; postprocess_variants.log",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818:2555,log,log,2555,,https://github.com/google/deepvariant/issues/818,2,['log'],['log']
Testability,"d and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**; - Centos 7; - deepvariant 1.3.0; - Singularity run pulling from here: docker://google/deepvariant:""1.3.0""; - quickstart example. **Steps to reproduce:**; ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service ; mkdir -p /mnt/share/jasontest; cd /mnt/share/jasontest; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; BIN_VERSION=""1.3.0""; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530:1034,test,testdata,1034,,https://github.com/google/deepvariant/issues/530,1,['test'],['testdata']
Testability,"d request same parameters when using --cluster so:; ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```; rule deepvariant:; input:; bam=rules.apply_bqsr.output.bam,; ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'; output:; vcf=""results/deepvariant/{sample}.vcf.gz""; params:; model=""WES""; threads: ; 64; resources:; mem_mb=163840; log:; ""logs/deepvariant/{sample}/stdout.log""; singularity:; ""singularity/deepvariant_1.4.0.sif""; # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU; shell:; """"""; /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'; """"""; ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? ; I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs...; Using shell: /usr/bin/bash; Provided cores: 64; Rules claiming more threads will be scaled down.; Select jobs to execute... > [Wed Jan 4 18:30:51 2023]; > rule deepvariant:; > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta; > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz; > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:2006,log,log,2006,,https://github.com/google/deepvariant/issues/602,1,['log'],['log']
Testability,d/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights/RMSProp|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Logits/Conv2d_1c_1x1/weights|InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Conv2d_2b_3x3/BatchNorm/moving_mean|InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7a/Branch_1/Conv2d_0c_7x1/BatchNorm/beta|InceptionV3/Mixed_6e/Branch_1/Co,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:43576,Log,Logits,43576,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Conv2d_2a_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights/RMSProp|InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/weights/RMSProp_1|InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_5b/Branch_2/Conv2d_0c_3x3/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/weights/RMSProp|InceptionV3/Logits/Conv2d_1c_1x1/weights/RMSProp|InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp|InceptionV3/Mixed_7b/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta|InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights|InceptionV3/Conv2d_2b_3x3/weights/RMSProp|InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights/RMSProp_1|InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_7b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:64412,Log,Logits,64412,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,"deepvariant image on singularity and running it on a cluster but this error happens on many machines.; I don't know what causes this error. I0624 02:14:00.095050 47429297437696 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0624 02:14:01.826225 47429297437696 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0624 02:14:01.954994 47429297437696 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Ru",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465:1039,log,logs,1039,,https://github.com/google/deepvariant/issues/465,1,['log'],['logs']
Testability,"deepvariant_runner \; --project ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam file? The .fai file was created from the .fa file using samtools index command. . ValueError: Reference contigs span 3270284521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""CM000663.2"" is 248956422 bp and IS MISSING, ""KI270706.1"" is 175055 bp and IS MISSING, ""KI270707.1"" is 32032 bp and IS MISSING, ""KI270708.1"" is 127682 bp and IS MISSING, ""KI270709.1"" is 66860 bp and IS MISSING, ""KI270710.1"" is 40176 bp and IS MISSING... Any feedback would be appreciated. Thanks, -Matt. [staging_folder1_logs_make_examples_7.txt](https://github.com/google/deepvariant/files/3700231/staging_folder1_l",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/225:1689,log,log,1689,,https://github.com/google/deepvariant/issues/225,1,['log'],['log']
Testability,"downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-08-28 10:40:42.588215: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; I0828 10:40:42.589054 140318776715072 train.py:92] Running with debug=False; I0828 10:40:42.589343 140318776715072 train.py:100] Use TPU at local; I0828 10:40:42.589422 14",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:2493,Log,Log,2493,,https://github.com/google/deepvariant/issues/876,1,['Log'],['Log']
Testability,"ds /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds.; Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Launcher: Job 6 completed in 0 seconds.; Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:2312,test,test,2312,,https://github.com/google/deepvariant/issues/717,1,['test'],['test']
Testability,"e lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? ; I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs...; Using shell: /usr/bin/bash; Provided cores: 64; Rules claiming more threads will be scaled down.; Select jobs to execute... > [Wed Jan 4 18:30:51 2023]; > rule deepvariant:; > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta; > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz; > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log; > jobid: 0; > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2; > threads: 64; > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323; > ; > Activating singularity image singularity/deepvariant_1.4.0.sif; > INFO: Convert SIF file to sandbox...; > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp; > ; > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****; > ; > ; > ***** Running the command:*****; > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *; > *; > *; > I0104 18:49:24.340415 140179943589696 make_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:3000,log,log,3000,,https://github.com/google/deepvariant/issues/602,1,['log'],['log']
Testability,"ealign script. Same script worked fine when I ran pre-compiled binary from the Docker container 🤷‍♂️. ```; input=""./quickstart-testdata"". ./bazel-bin/deepvariant/make_examples \; --ref=$input/ucsc.hg19.chr20.unittest.fasta \; --reads=$input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --examples examples.tfrecord@1.gz \; --mode calling \; --logging_every_n_candidates 10 \; --realign_reads; ```. ```; ./make_examples_demo.sh ; 2019-07-16 17:49:02.877175: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:02.877284 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.117142 139897470359360 make_examples.py:1110] Preparing inputs; 2019-07-16 17:49:03.117644: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.117749 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.118745 139897470359360 make_examples.py:1034] Common contigs are [u'chr20']; I0716 17:49:03.120177 139897470359360 make_examples.py:1116] Writing examples to examples.tfrecord-00000-of-00001.gz; 2019-07-16 17:49:03.121118: I third_party/nucleus/io/sam_reader.cc:600] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-07-16 17:49:03.124279: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.124422 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/home/nyakovenko/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_de",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199:1221,test,testdata,1221,,https://github.com/google/deepvariant/issues/199,1,['test'],['testdata']
Testability,"ecision, recall, f1) for either het or homalt. Or more specifically they are all 0.0. Eval stats are reported for homref though. I have now tried running the training several times with different hyperparameters but so far still no change at the het or homalt eval stats. . My first, very simple question is thus, are these eval stats truly 0 (i.e. the model is very bad) or is 0.0 some starting value and there are not enough data to calculate them initially? I am warmstarting from the 1.6.1 wgs model so I cant imagine the model is really that bad at calling variants initially, even if in a fish. . **Setup**; Running on a university computing cluster (https://hpc-unibe-ch.github.io/) ; OS: Rocky 9.3 Blue Onyx; GPU: rtx4090 ; Installation: Running from Docker image via singularity; DV version: 1.6.1. **Data**; I am training on examples from 5 individuals, data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:1238,test,testing,1238,,https://github.com/google/deepvariant/issues/876,1,['test'],['testing']
Testability,"ecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```; ## Run `call_variants`; echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log.""; ( time sudo dock",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:2698,Log,Log,2698,,https://github.com/google/deepvariant/issues/151,1,['Log'],['Log']
Testability,"eepvaraint on a wgs file. ‘[E::vcf_parse_format] Incorrect number of FORMAT fields at 1:19355\n.’ I have googled it but I am not sure what is causing this issue and how to resolve it. I will appreciate your help. ; I have attached the full error log, bash and yaml files. . error:; code: 10; message: |-; 11: Docker run failed: ASS""\ncalls {\n info {\n key: ""AD""\n value {\n values {\n int_value: 8\n }\n values {\n int_value: 7\n }\n }\n }\n info {\n key: ""DP""\n value {\n values {\n int_value: 15\n }\n }\n }\n info {\n key: ""GQ""\n value {\n values {\n int_value: 12\n }\n }\n }\n…….; ……….; ………..Reading /mnt/data/output/gs/gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/UDN631726_deepVariant_v0.6.1.g.vcf with NativeVcfReader\nI0806 01:37:07.984452 140434439055104 postprocess_variants.py:593] Writing output to VCF file: /mnt/data/output/gs/gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/UDN631726_deepVariant_v0.6.1.g.vcf\nI0806 01:37:08.052251 140434439055104 genomics_writer.py:118] Writing /mnt/data/output/gs/gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/UDN631726_deepVariant_v0.6.1.g.vcf with NativeVcfWriter\n**[E::vcf_parse_format] Incorrect number of FORMAT fields at 1:19355\n.** See logs at gs://gbsc-gcp-project-udn-dev-deep-variant/UDN631726/gvcf/deepvariant_staging_folder/logs/']]. Operation ID: ENqznd7QLBi1jMjtufCo3ckBIK2c48-5AyoPcHJvZHVjdGlvblF1ZXVl. Best,; Shruti. Shruti Marwaha, PhD.; Research Engineer,; Stanford Center for Undiagnosed Diseases; Stanford University. [UDN631726_gvcf_error_08052018.log](https://github.com/google/deepvariant/files/2263671/UDN631726_gvcf_error_08052018.log); Since GIT does not allow me to attach .sh or .yaml files, I am saving them as text files and attaching.; [deepvariant_v0.6.1_UDN631726.yaml.txt](https://github.com/google/deepvariant/files/2263680/deepvariant_v0.6.1_UDN631726.yaml.txt); [deepvariant_v0.6.1_UDN631726.sh.txt](https://github.com/google/deepvariant/files/2263674/deepvariant_v0.6.1_UDN631726.sh.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/86:1280,log,logs,1280,,https://github.com/google/deepvariant/issues/86,4,['log'],"['log', 'logs']"
Testability,"eepvariant/issues/127 ; but this is a different type of issue. I also want to train DeepVariant model from the WGS data in our laboratory using our standalone computational resources.; I referred and followed https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md . First, I successfully made tfrecord file using DeepVariant v0.7.1 (docker). Then I tried shuffling tfrecords as follows:. #!/bin/sh; ; INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 19`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done; ; /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. This script made errors [(please see this file)](https://github.com/google/deepvariant/files/2708579/shuffle.train.0_id_masked.log). However, the codes which used chromosome-divided tfrecords (chr1-10 and chr11-chr19) worked fine as follows:. INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 10`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. and. INPUT_PATTERN_LIST=examples.train/sample_id/11/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 12 19`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/133:1066,log,log,1066,,https://github.com/google/deepvariant/issues/133,1,['log'],['log']
Testability,"eference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out?. **Setup**; - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters); - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.):; - ``` BIN_VERSION=""1.5.0""; docker pull; ; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`; . **Steps to reproduce:**; - Command:; - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1; - Error trace: (if applicable); - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader; I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs; [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_c22i4j8u/runfiles/com_google_deepvariant/deepvariant/make_examples.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:1546,log,logx,1546,,https://github.com/google/deepvariant/issues/653,1,['log'],['logx']
Testability,"el_train, every process was working fine. When I try to run model_train and model_eval in my computer, model_train seemed to work but model_eval returned ValueError: Must specify steps > 0, given: 0. The error came from estimator.py file in tensorflow. In Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0; I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt; W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:1146,log,log,1146,,https://github.com/google/deepvariant/issues/172,1,['log'],['log']
Testability,"elapsed]; I0325 17:32:48.146358 47041007318848 make_examples_core.py:301] Task 44/48: 4691 candidates (4897 examples) [21.00s elapsed]; I0325 17:32:48.127754 47600061708096 make_examples_core.py:301] Task 36/48: 4081 candidates (4253 examples) [19.43s elapsed]; Fatal Python error: Segmentation fault. Current thread 0x00002b8260148740 (most recent call first):; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 882 in align_to_haplotype; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2250 in align_to_all_haplotypes; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2322 in <listcomp>; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2321 in create_pileup_examples; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1566 in writes_examples_in_region; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2847 in make_examples_runner; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 224 in main; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_30v6ynlb/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234 in <module>; I0325 17:32:49.865826 47092596426560 make_examples_core.py:301] Task 3/48: 6125 candidates (6410 examples) [17.20s elapsed]; parallel: This job failed:; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/794:9764,test,test,9764,,https://github.com/google/deepvariant/issues/794,2,['test'],['test']
Testability,"ent in the de novo call for the proband than when DeepVariant is run in singleton mode on the proband. In our case, comparing the output VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepTrio? Is there a reason why the HG001/NA12891/NA12892 trios were used as training for the child model but not the parent model?. <br>. Many thanks,; Macabe. <br>. ![image](https://user-images.githubusercontent.com/37773554/128098808-740a1ab0-a6af-452f-8bed-d1f4ba0ceb80.png); Current DeepTrio training info (likely typo for Ashkenazim trio, cf. HG002/HG00**3**/HG004). [1] https://acade",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/475:1363,benchmark,benchmark,1363,,https://github.com/google/deepvariant/issues/475,1,['benchmark'],['benchmark']
Testability,"ep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.2rc', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; [12/12/2018 13:33:54 ERROR gcp_deepvariant_runner.py] For more information, consult the worker log at gs://ms_bam/deep_output/stage/logs/call_variants/0; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 908, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 895, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 491, in _run_call_variants; _run_call_variants_with_pipelines_api(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 483, in _run_call_variants_with_pipelines_api; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 369, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/23049213423"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). 5. The script is run from ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:8306,log,log,8306,,https://github.com/google/deepvariant/issues/129,2,['log'],"['log', 'logs']"
Testability,"epvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --reads \""/input-gcsfused-{}/${BAM}\"" --ref \""${INPUT_REF}\"" --task {} --gvcf \""${GVCF}\""/gvcf_output.tf",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/214:1481,log,logging,1481,,https://github.com/google/deepvariant/issues/214,1,['log'],['logging']
Testability,"error pops out when I switch to my BAM. ```; [05/03/2018 23:30:18 INFO discovery.py] URL being requested: GET https://genomics.googleapis.com/v1alpha2/operations?filter=projectId+%3D+isb-cgc-06-0004+AND+labels.job-id+%3D+call-varia--root--180503-233007-45&alt=json&pageSize=256; call-varia--root--180503-233007-45: FAILURE; [u""Error in job call-varia--root--180503-233007-45 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]; [05/03/2018 23:30:18 ERROR gcp_deepvariant_runner.py] Job failed with error [[u""Error in job call-varia--root--180503-233007-45 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]]. Job args: ['--project', 'isb-cgc-XX-XXXX', '--logging', 'gs://XXXXX/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0', '--input-recursive', 'EXAMPLES=gs://XXXXX/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://XXXXX/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/70:1099,log,logging,1099,,https://github.com/google/deepvariant/issues/70,1,['log'],['logging']
Testability,"ervice,so i hardly can try to tell the Administrator to update some tools because there are other users and any update to key tools may cause them some troublesome.AS i know,many people work on bioinformation use cluster service and do not have permission to do sudo update or maybe not have a docker in service,but conda can do.; so i try search conda deepvariant,and i try conda install -c bioconda deepvariant=1.0.0(on python3,and i also try other version on python2),and i find dv_make_examples.py, and i see many other guys also try conda.(https://github.com/google/deepvariant/issues/9).; when i run dv_make_examples.py on python3,i get ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and i know it is wrong with GLIBC and the solution is to update GLIBC to GLIBC_2.23 ,but i can not . i ask my Administrator and he say the glibc is too important and update it on cluster service may cause other users bug. . so is there any chance i can use deepvatiant ? and again,i can not install from source(no permission to sudo ) or docker(don't have docker on cluster service ),and i can't update glibc .; And the info are like this:; ImportError: /lib64/libm.so.6: version `GLIBC_2.23' not found (required by /var/tmp/Bazel.runfiles_okfco2gt/runfiles/com_google_protobuf/python/google/protobuf/pyext/_message.so); and strings /lib64/libc.so.6 |grep GLIBC:; GLIBC_2.2.5; GLIBC_2.2.6; GLIBC_2.3; GLIBC_2.3.2; GLIBC_2.3.3; GLIBC_2.3.4; GLIBC_2.4; GLIBC_2.5; GLIBC_2.6; GLIBC_2.7; GLIBC_2.8; GLIBC_2.9; GLIBC_2.10; GLIBC_2.11; GLIBC_2.12; GLIBC_2.13; GLIBC_2.14; GLIBC_2.15; GLIBC_2.16; GLIBC_2.17; GLIBC_PRIVATE; and the other information is :; Linux version 3.10.0-1127.18.2.el7.x86_64 (mockbuild@kbuilder.bsys.centos.org) (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39) (GCC) ) #1 SMP Sun Jul 26 15:27:06 UTC 2020; conda 4.9.2; Python 3.7.6. i really hope you can help me.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/391:1947,mock,mockbuild,1947,,https://github.com/google/deepvariant/issues/391,1,['mock'],['mockbuild']
Testability,"es, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-08-28 10:40:42.588215: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination; I0828 10:40:42.589054 140318776715072 train.py:92] Running with debug=False; I0828 10:40:42.589343 140318776715072 train.py:100] Use TPU at local; I0828 10:40:42.589422 140318776715072 train.py:103] experiment_dir: /home/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:2528,log,log,2528,,https://github.com/google/deepvariant/issues/876,1,['log'],['log']
Testability,"es/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374, in call_variants; raise ValueError(f'Shape mismatch in {example_info_json} and '; ValueError: Shape mismatch in ./examples.tfrecord-00000-of-00032.gz.example_info.json and /opt/models/pacbio/model.ckpt.example_info.json.; ```. My command line looks like this:; `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --add_hp_channel --add_hp_channel --alt_aligned_pileup diff_channels --downsample_fraction 0 --reads /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/HG002.merged.bam --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/GRCh38ERCC.ensembl.fasta --realign_reads --regions 20 --sample_name HG002 --split_skip_reads --vsc_min_count_indels 2 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --batch_size 512 --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/628:2527,LOG,LOGDIR,2527,,https://github.com/google/deepvariant/issues/628,6,"['LOG', 'log']","['LOGDIR', 'log', 'logs']"
Testability,es_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']; I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs; I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']; I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs; I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']; I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs; I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs; **********; I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op.; INFO:tensorf,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:11131,test,testdata,11131,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512""; 13:33:48 Started running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Stopped running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Execution failed: action 4: unexpected exit status 1 was not ignored; 13:33:51 Worker released; ""run"": operation ""projects/ms-deepvariant/operations/234234234234"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.2rc', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; [12/12/2018 13:33:54 ERROR gcp_de",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:7375,log,logs,7375,,https://github.com/google/deepvariant/issues/129,1,['log'],['logs']
Testability,examples.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz; ...; -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 te,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1388,test,test,1388,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/f68a72a7-7df6-41e6-a9cf-96fe9f05de7f/deepvariant/Bazel.runfiles_yms6j76p/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 363, in call_variants; raise ValueError('The number of channels in examples and checkpoint '; ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 7 channels while the examples have 6. My command line looks like this:. export HOME=/root && N_SHARDS=32 && GVCF_TFRECORDS=""./gvcf.tfrecord@${N_SHARDS}.gz"" && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --gvcf ./gvcf.tfrecord@${N_SHARDS}.gz --add_hp_channel --noadd_hp_channel --downsample_fraction 0 --reads /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/NA12878_S1.chr20.10_10p1mb.bam --regions /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/test_nist.b37_chr20_100kbp_at_10mb.bed --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/ucsc.hg19.chr20.unittest.fasta --sample_name NA12878_S1 --vsc_min_count_indels 2 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/wgs/model.ckpt --use_openvino --batch_size 512 --include_debug_info --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/b7b5cc2c-7194-40e7-",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/625:2002,LOG,LOGDIR,2002,,https://github.com/google/deepvariant/issues/625,6,"['LOG', 'log']","['LOGDIR', 'log', 'logs']"
Testability,"fa.gz \; --reads=/input/1115492_23181_0_0.cram \; --regions ""chr3:10,049,322-10,156,156"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=5 ; ```; - Error trace:; ; > parallel: This job failed:; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 166, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 128, in default_options; samples_in_order, sample_role_to_train = one_sample_from_flags(; File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags; sample_name = make_examples_core.assign_sample_name(; File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 134, in assign_sample_name; with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:; File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_ir3xkizo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__; self._reader = sam_reader.SamReader.from_file(; ValueError: NOT_FOUND: Could not open /input/1115492_23181_0_0.cram; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /input/GCF_000001405.26_GRCh38_genomic1.fa.gz --reads /input/1115492_23181_0_0.cram --examples /tmp/tmpf8z5m2q; 0/make_examples.tfrecord@5.gz --channels insert_size --gvcf /tmp/tmpf8z5m2q0/gvcf.tfrecord@5.gz --regions chr3:10,049,322-10,156,156 --task 4. I didn't try the quick start test.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/588:2650,test,test,2650,,https://github.com/google/deepvariant/issues/588,1,['test'],['test']
Testability,files and parameters not found when running test-data,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/558:44,test,test-data,44,,https://github.com/google/deepvariant/issues/558,1,['test'],['test-data']
Testability,frecord-00000-of-00064.gz; -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz; ...; -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 te,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1226,test,test,1226,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"ged.bcf; Date=Tue Feb 15 12:15:20 2022; ```. ### Variant line; ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	father	mother	proband; X	48684399	X_48684399_C_A	C	A	61	.	AF=0.5;AQ=61	GT:DP:AD:GQ:PL:RNC	0/0:22:22,0:50:0,75,749:..	0/1:37:19,18:54:54,0,64:..	1/1:18:0,18:52:61,55,0:..; ```. # DeepTrio . Now, with the DeepTrio -> GVCF -> GLNexus pipeline:; Pipeline; ```; # Load singularity; module load singularity; BIN_VERSION=""1.1.0"". # Load env for bcftools; ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/; source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh; conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped; export SINGULARITY_CACHEDIR=$PWD; singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Number of threads; NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered); cd $SLURM_SUBMIT_DIR; WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis/. ## Set working space; mkdir -p $WORKING_DIR; cd $WORKING_DIR. #### GRCh38 #### ; echo ""GRCh38 genome""; GENOME=GRCh38; FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/; FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS; BAM_DIR=$WORKING_DIR; Case_ID=Case1; FAMILY_ID=$Case_ID; PROBAND_ID=${Case_ID}_proband; MOTHER_ID=${Case_ID}_mother; FATHER_ID=${Case_ID}_father. PROBAND_BAM=${PROBAND_ID}.sorted.bam; FATHER_BAM=${FATHER_ID}.sorted.bam; MOTHER_BAM=${MOTHER_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz; FATHER_VCF=${FATHER_ID}.vcf.gz; MOTHER_VCF=${MOTHER_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz; FATHER_GVCF=${FATHER_ID}.gvcf.gz; MOTHER_GVCF=${MOTHER_ID}.gvcf.gz. # Run singularity; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; 	-B ""${BAM_DIR}"":""/bamdir"" \; 	-B ""${FASTA_DIR}"":""/genomedir"" \; 	-B ""${OUTPUT_DIR}"":""/output"" \; 	docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518:7556,test,test,7556,,https://github.com/google/deepvariant/issues/518,1,['test'],['test']
Testability,"get_cvo_paths_and_first_record(); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record; raise ValueError(; ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; ???. **Any additional context:**; Yes. I can change the parameter ""--infile"" of the postprocess_variants.py call from ""./call_variants_output.tfrecord.gz"" to ""./call_variants_output@1.tfrecord.gz"" and it works. Anyway, the call of postprocess_variants.py is auto-generated by ""/opt/deepvariant/bin/run_deepvariant"". The error does not occur for every sample ... directory content of intermediate_results_dir after the error occured:; call_variants.log; call_variants_output-00000-of-00001.tfrecord.gz; gvcf.tfrecord-00000-of-00008.gz; gvcf.tfrecord-00001-of-00008.gz; gvcf.tfrecord-00002-of-00008.gz; gvcf.tfrecord-00003-of-00008.gz; gvcf.tfrecord-00004-of-00008.gz; gvcf.tfrecord-00005-of-00008.gz; gvcf.tfrecord-00006-of-00008.gz; gvcf.tfrecord-00007-of-00008.gz; make_examples.log; make_examples.tfrecord-00000-of-00008.gz; make_examples.tfrecord-00000-of-00008.gz.example_info.json; make_examples.tfrecord-00001-of-00008.gz; make_examples.tfrecord-00001-of-00008.gz.example_info.json; make_examples.tfrecord-00002-of-00008.gz; make_examples.tfrecord-00002-of-00008.gz.example_info.json; make_examples.tfrecord-00003-of-00008.gz; make_examples.tfrecord-00003-of-00008.gz.example_info.json; make_examples.tfrecord-00004-of-00008.gz; make_examples.tfrecord-00004-of-00008.gz.example_info.json; make_examples.tfrecord-00005-of-00008.gz; make_examples.tfrecord-00005-of-00008.gz.example_info.json; make_examples.tfrecord-00006-of-00008.gz; make_examples.tfrecord-00006-o",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818:2223,log,log,2223,,https://github.com/google/deepvariant/issues/818,1,['log'],['log']
Testability,"github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:; Yes. Same error msgs were observed. But I was lunching deepvariant with singularity; **Describe the issue:**; (A clear and concise description of what the issue is.); The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity.; **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable); module load singularity; BIN_VERSION=""1.5.0""; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools""; INPUT_DIR=""${LABASE}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; OUTPUT_DIR=""${LABASE}/quickstart-output""; mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ls -1 ${INPUT_DIR}; mkdir -p ${OUTPUT_DIR}; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678:986,test,testdata,986,,https://github.com/google/deepvariant/issues/678,1,['test'],['testdata']
Testability,"gle/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md), and I got an error regarding numpy as below. Could you help me resolve this issue? I used deepvariant_1.6.0 image. ```; 2023-12-02 23:23:35.126320: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I1202 23:23:41.449015 46912500266816 run_deepvariant.py:519] Re-using the directory for intermediate results in /flashscratch/kimkw/tmp/tmppin2lwy5. ***** Intermediate results will be written to /flashscratch/kimkw/tmp/tmppin2lwy5 in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""./quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/flashscratch/kimkw/tmp/tmppin2lwy5/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/flashscratch/kimkw/tmp/tmppin2lwy5/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I1202 23:23:46.123890 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.133658 46912500266816 make_examples_core.py:301] Preparing inputs; I1202 23:23:46.139615 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.140348 46912500266816 make_examples_core.py:301] Common contigs are ['chr20']; I1202 23:23:46.141555 46912500266816 make_examples_core.py:301] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; I1202 23:23:46.150200 46912500266816 genomics_reader",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/746:1147,test,testdata,1147,,https://github.com/google/deepvariant/issues/746,1,['test'],['testdata']
Testability,hNorm/beta|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights|InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_6a/Branch_1/Conv2d_0b_3x3/weights|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_1/Conv2d_0b_5x5/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/weights|InceptionV3/Mixed_5d/Branch_0/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights/RMSProp_1|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_1x3/BatchNorm/beta|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights|InceptionV3/Conv2d_4a_3x3/weights/RMSProp_1|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_variance|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Logits/Conv2d_1c_1x1/weights/RMSProp_1|InceptionV3/Mixed_6a/Branch_0/Conv2d_1a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance|InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean|InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights|InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights|InceptionV3/Mixed_5d/Branch_2/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Conv2d_3b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Conv2d_4a_3x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:88743,Log,Logits,88743,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,hap.py benchmarking,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/896:7,benchmark,benchmarking,7,,https://github.com/google/deepvariant/issues/896,1,['benchmark'],['benchmarking']
Testability,"he terminal. . Please help! . **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: 0.8; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace:; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 42, in <module>; import tensorflow as tf; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/__init__.py"", line 28, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/__init__.py"", line 52, in <module>; from tensorflow.core.framework.graph_pb2 import *; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/graph_pb2.py"", line 17, in <module>; from tensorflow.core.framework import function_pb2 as tensorflow_dot_core_dot_framework_dot_function__pb2; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/function_pb2.py"", line 18, in <module>; from tensorflow.core.framework import op_def_pb2 as tensorflow_dot_core_dot_framework_dot_op__def__pb2; File ""/usr/local/lib/python2.7/dist-packages/tensorflow/core/framework/op_def_pb2.py"", line 210, in <module>; serialized_options=None, file=DESCRIPTOR),; File ""/tmp/Bazel.runfiles_B0iKHl/runfiles/protobuf_archive/python/google/protobuf/descriptor.py"", line 534, in __new__; return _message.default_pool.FindFieldByName(full_name); KeyError: ""Couldn't find field tensorflow.OpDef.control_output"". **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/342:2588,test,test,2588,,https://github.com/google/deepvariant/issues/342,2,['test'],['test']
Testability,"hecked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: CentOS Linux release 7.9.2009; - DeepVariant version: deepvariant:0.9.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); - Illumina, HG38, standard capture panel. **Steps to reproduce:**; - Command: Snakemake command:; - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12; - actual command (XXXXX = removed for security purposes) ; - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/550:1610,test,test,1610,,https://github.com/google/deepvariant/issues/550,2,['test'],['test']
Testability,"hello, ; I tested PacBio data on version 1.4 of the deepTrio image. But I received an error message after more than 100 minutes. parallel: This job failed:; /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref hs37d5.fasta --reads_parent1 HG003.haplotagged.bam --reads_parent2 HG004.haplotagged.bam --reads HG002.haplotagged.bam --examples intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz) --sample_name HG002 --sample_name_parent1 HG003 --sample_name_parent2 HG004 --add_hp_channel --alt_aligned_pileup diff_channels --gvcf intermediate_results_dir/[gvcf.tfrecord@32.gz](mailto:gvcf.tfrecord@32.gz) --parse_sam_aux_fields --pileup_image_height_child 60 --pileup_image_height_parent 40 --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 0. real 113m56.944s; user 112m32.542s; sys 0m37.407s; I1020 05:16:02.013775 140329939375936 run_deeptrio.py:674] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 31 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""hs37d5.fasta"" --reads_parent1 ""HG003.haplotagged.bam"" --reads_parent2 ""HG004.haplotagged.bam"" --reads ""HG002.haplotagged.bam"" --examples ""intermediate_results_dir/[make_examples.tfrecord@32.gz](mailto:make_examples.tfrecord@32.gz)"" --sample_name ""HG002"" --sample_name_parent1 ""HG003"" --sample_name_parent2 ""HG00",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/720:11,test,tested,11,,https://github.com/google/deepvariant/issues/720,1,['test'],['tested']
Testability,"hello,; I am testing RNA-seq data with deepvarian1.4. ; I have some questions about：; 1. Is GATK SplitNCigarReads necessary for input bam?; 2. Could you explain in detail what the parameter ""--make_examples_extra_args=""split_skip_reads=true, channels=''"" dose?; 3. Does the RNAseq model only apply to the CDS region?; Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/635:13,test,testing,13,,https://github.com/google/deepvariant/issues/635,1,['test'],['testing']
Testability,"hello,; I tested a NGS sample on DV1.4. An error occurred in calling a variant at a specific locus.The VCF results show that the genotype at this locus is 1/1, but the first-generation sequencing results did not reveal a homozygous mutation. I checked the BAM file, and I couldn't draw a conclusion about the homozygous mutation either. How was the 1/1 result determined? Can you explain the reasons and methods to avoid the error from happening?; ![image](https://github.com/google/deepvariant/assets/70870741/1c0710d4-f9a4-40ef-a2d7-c982e42eac1b); ![image](https://github.com/google/deepvariant/assets/70870741/4665a415-6236-46a2-ad0a-958ddf4ca2bf); ![image](https://github.com/google/deepvariant/assets/70870741/5433bce6-1b83-4d8a-88d3-8173708ac8ed); Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/655:10,test,tested,10,,https://github.com/google/deepvariant/issues/655,1,['test'],['tested']
Testability,"hello,; When analyzing PacBio data, I encountered some problems.; I tested the example data provided by DeepConsensus and aligned it using pbmm2(1.12.0). When I analyze with DV1.5, I got an error.; Data: gs://brain-genomics-public/research/deepconsensus/quickstart/v1.2/n1000.subreads.bam; My cmd:; 1. pbmm2 align hs37d5.fasta fa.fofn n1000.subreads_to_ccs_aligned.bam --sort --rg '@RG\tID:test1\tSM:test1'; 2. /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref hs37d5.fasta \; --reads n1000.subreads_to_ccs_aligned.bam \; --output_vcf n1000.depv.vcf.gz \; --output_gvcf n1000.depv.g.vcf.gz \; --num_shards 32 \; --intermediate_results_dir intermediate_results_dir; Error:; ![1688017046760](https://github.com/google/deepvariant/assets/70870741/e5973e9a-c44b-4a12-9179-f4657f63f4bb). Could you help me to solve the problem?; Looking forward to your reply. Thanks.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/672:68,test,tested,68,,https://github.com/google/deepvariant/issues/672,1,['test'],['tested']
Testability,"his prefix.; I0208 09:29:54.405941 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:55.469674 139859027293952 session_manager.py:491] Running local_init_op.; I0208 09:29:55.510524 139859027293952 session_manager.py:493] Done running local_init_op.; I0208 09:29:55.864006 139859027293952 modeling.py:410] Reloading EMA...; I0208 09:29:55.864634 139859027293952 saver.py:1270] Restoring parameters from /opt/models/wes/model.ckpt; I0208 09:29:59.699455 139859027293952 call_variants.py:399] Processed 1 examples in 1 batches [827.229 sec per 100]; ```; top. Looks like there is a lot of under utilized compute resources; ```; top - 00:16:22 up 1 day, 23:24, 1 user, load average: 16.03, 16.02, 16.00; Tasks: 621 total, 1 running, 620 sleeping, 0 stopped, 0 zombie; %Cpu(s): 25.0 us, 0.0 sy, 0.0 ni, 75.0 id, 0.0 wa, 0.0 hi, 0.0 si, 0.0 st; KiB Mem : 52262400+total, 48358937+free, 8878616 used, 30156016 buff/cache; KiB Swap: 0 total, 0 free, 0 used. 51216832+avail Mem . PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND ; 23437 root 20 0 27.319g 3.709g 136440 S 800.0 0.7 18646:29 python ; 23944 root 20 0 27.259g 3.687g 137396 S 799.7 0.7 18613:17 python ; 1 root 20 0 119604 5788 4112 S 0.0 0.0 0:03.37 systemd ; 2 root 20 0 0 0 0 S 0.0 0.0 0:00.01 kthreadd ; 3 root 20 0 0 0 0 S 0.0 0.0 0:00.06 ksoftirqd/0 ; 4 root 20 0 0 0 0 S 0.0 0.0 0:00.00 kworker/0:0 ; ```; search for recently modified files; ```; ubuntu@ip-172-31-21-181:/deepTmp$ sudo find . -type f -printf '%T@ %p\n' | sort -n | tail -1 | cut -f2- -d"" ""; ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; ubuntu@ip-172-31-21-181:/deepTmp$ ls -l ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; -rw-r--r-- 1 root 0 Feb 8 09:29 ./deepvariant_tmp_output/call_variants_output.tfrecord.gz; ubuntu@ip-172-31-21-181:/deepTmp$ date; Mon Feb 10 00:22:46 UTC 2020; ubuntu@ip-172-31-21-181:/deepTmp$ ; ```. I also noticed there are several deprecation warnings in the log file",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/269:5104,log,log,5104,,https://github.com/google/deepvariant/issues/269,1,['log'],['log']
Testability,"hub.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```; $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; mkdir: cannot create directory ‘/mnt/cdw-genome’: Permission denied; 0.00user 0.00system 0:00.00elapsed 0%CPU (0avgtext+0avgdata 1692maxresident)k; 0inputs+0outputs (0major+73minor)pagefaults 0swaps; Academic tradition requires you to cite works you base your article on.; When using programs that use GNU Parallel to process data for publication; please cite:. O. Tange (2011): GNU Parallel - The Command-Line Power Tool,; ;login: The USENIX Magazine, February 2011:42-47. This helps funding further development; AND IT WON'T COST YOU A CENT.; If you pay 10000 EUR you should feel free to use GNU Parallel without citing. To silence this citation notice: run 'parallel --citation'. Computers / CPU cores / Max jobs to run; 1:local / 8 / 8. Computer:jobs running/jobs completed/%of started jobs/Average seconds to complete; ETA: 0s Left: 8 AVG: 0.00s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.ba",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:5720,log,login,5720,,https://github.com/google/deepvariant/issues/171,1,['log'],['login']
Testability,"i have used this command to run deep variant and generate VCF file. sudo docker run -v `pwd`:`pwd` -w `pwd` google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=${base_path}/${ref_file_name}.fasta --reads=${base_path}/base_recalib/$i\_vqsr.bam --output_vcf=deep_variant_results/$i\.vcf.gz --output_gvcf=deep_variant_results/$i\.g.vcf.gz. I want to run post-process variants but cannot get it from the above command. 1) Is there some way to add parameters to above command?. 2) I found two links related to it:-; a) [https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-gvcf-support.md); GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`. There is no make_examples.zip in the bin directory and what should be supplied to this parameter --examples. Can you please give more details about variables?. b) [https://github.com/google/deepvariant/issues/103](https://github.com/google/deepvariant/issues/103); sudo docker run -v ${HOME}:${HOME} gcr.io/deepvariant-docker/deepvariant:0.7.0 /opt/deepvariant/bin/postprocess_variants \; --ref ${OUTDIR}/data/hg19.fa \; --infile ${OUTDIR}/output/cvo.tfrecord.gz \; --outfile ${OUTDIR}/output/output.vcf.gz \; --nonvariant_site_tfrecord_path ${OUTDIR}/output/gvcf.tfrecord@8.gz \; --gvcf_outfile ${OUTDIR}/output/output.gvcf.gz. This is another way and parameters are also different. how to define the infile here?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/319:866,log,log,866,,https://github.com/google/deepvariant/issues/319,2,['log'],['log']
Testability,"iants.py:461] Done calling variants from a total of 9497443 examples.; real	507m54.839s; user	17892m22.565s; sys	172m54.026s; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_child.tfrecord.gz"" --outfile ""/out_dir/199713.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_child.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_child.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent1.tfrecord.gz"" --outfile ""/out_dir/199710.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent1.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent1.log; ***** Starting the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/ref_dir/ARS-UCD1.2_Btau5.0.1Y.fa"" --infile ""/out_dir/199713-199710-199718/call_variants_output_parent2.tfrecord.gz"" --outfile ""/out_dir/199718.output.vcf.gz"" --nonvariant_site_tfrecord_path ""/out_dir/199713-199710-199718/gvcf_parent2.tfrecord@56.gz"" 2>&1 | tee /out_dir/199713-199710-199718//postprocess_variants_parent2.log; E0307 04:23:51.666978 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.667161 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; E0307 04:23:51.705964 46912496319168 errors.py:61] gVCF creation requires both nonvariant_site_tfrecord_path and gvcf_outfile flags to be set.; real	0m3.173s; user	0m3.003s; sys	0m3.160s; real	0m3.194s; user	0m3.299s; sys	0m4.216s; real	0m3.254s; user	0m3.024s; sys	0m2.808s; post_process returns: [0, 0, 0]; real	2008m37.771s; user	78330m54.158s; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/429:2833,log,log,2833,,https://github.com/google/deepvariant/issues/429,1,['log'],['log']
Testability,"ign(; File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 1817, in region_reads_norealign; reads = reservoir_sample_reads(; File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads; return utils.reservoir_sample(iterable_of_reads, k, random); File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample; for i, item in enumerate(iterable):; File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next; not_done = self._cc_iterable.PythonNext(record); RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0; ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```; singularity exec DeepVariant_1.6.1.sif bash; pip install --user google-nucleu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812:3785,test,testdata,3785,,https://github.com/google/deepvariant/issues/812,1,['test'],['testdata']
Testability,"iled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:01:58.210216 140172092593984 saving_utils.py:359] Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.; W1025 22:02:31.766536 140172092593984 save.py:271] Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets; WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter; W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.a",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:2291,log,logs,2291,,https://github.com/google/deepvariant/issues/722,1,['log'],['logs']
Testability,"image](https://github.com/google/deepvariant/assets/30355684/4c6fdc69-f881-44aa-8935-8c627a591f6f). Example of reads that were moved:; Original alignment; NB501857:464:HH7FWBGXV:2:23210:26812:14806 99 X 140993994 50 79M = 140994064 149 CCAGATTCCTGTGAGCCGCTCCTTCTCCTCCACTTTAGTGAGTCTTTTCCAGAGTTCCCCTGAGAGAACTCAGAGTACT AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEE<EEEEEEEEEEEEAEEEEE XA:Z:X,+140993784,79M,2; PG:Z:MarkDuplicates AS:i:74 XS:i:69 MD:Z:17C61 NM:i:1 RG:Z:DM_23_2198; NB501857:464:HH7FWBGXV:2:23210:26812:14806 147 X 140994064 57 79M = 140993994 -149 CAGAGTACTTTTGAGGGTTTTCCCCAGTCTCCTCTCCAGATTCCTGTGAGCTCCTCCTCCTCCTCCACTTTATTGAGTC AAEEEEEEEEEEEEEE<66EEEEEEEEEE/EAEEEEEEEE6EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAAAAA XA:Z:X,-140994589,50M3D29M,4; PG:Z:MarkDuplicates AS:i:79 XS:i:67 MD:Z:79 NM:i:0 RG:Z:DM_23_2198; Local realignment; X:140993145-140994144/ X_140993145_140994144realigned_reads.bam X_140993145_140994144realigned_reads.bam.bai; frmascla@frt:DeepV-TEST$ samtools view Local/X_140993145_140994144realigned_reads.bam | grep NB501857:464:HH7FWBGXV:2:23210:26812:14806; NB501857:464:HH7FWBGXV:2:23210:26812:14806 99 X 140993784 50 79M = 140994064 149 CCAGATTCCTGTGAGCCGCTCCTTCTCCTCCACTTTAGTGAGTCTTTTCCAGAGTTCCCCTGAGAGAACTCAGAGTACT AAAAAEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAEE<EEEEEEEEEEEEAEEEEE; NB501857:464:HH7FWBGXV:2:23210:26812:14806 19 X 140993854 57 79M = 140993994 -149 CAGAGTACTTTTGAGGGTTTTCCCCAGTCTCCTCTCCAGATTCCTGTGAGCTCCTCCTCCTCCTCCACTTTATTGAGTC AAEEEEEEEEEEEEEE<66EEEEEEEEEE/EAEEEEEEEE6EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEAAAAA. Original alignment, bam file; https://www.dropbox.com/scl/fi/c9tc01sdtf2sroxj3u3bj/original_alignment.bam?rlkey=jgxnyhyse2ekcu6t1s3l3lnnl&dl=0; Local realignment, bam file; https://www.dropbox.com/scl/fi/oqhny0s7h9hu3zcyrprig/X_140993145_140994144realigned_reads.bam?rlkey=zmbon72t19vjlcdht1zt6m5xg&dl=0. **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:; Yes. **Setup**; - Op",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/763:1914,TEST,TEST,1914,,https://github.com/google/deepvariant/issues/763,1,['TEST'],['TEST']
Testability,"in 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o; neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs; I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']; I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs; I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] T",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:8754,test,testdata,8754,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"in this event every single job fails. I could of course run all my samples on a single interactive session, keep checking the log file and restart the run every time it fails but I guess that's less than optimal plus this way I can really only run one sample at the time. For the interactive sessions I request 180G and 64cpus (in my case it's: ```srsh --mem=180G --cpus-per-task=64 --partition=long```). . I would request same parameters when using --cluster so:; ```snakemake --cluster ""sbatch --mem=180G cpus-per-task=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```; rule deepvariant:; input:; bam=rules.apply_bqsr.output.bam,; ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'; output:; vcf=""results/deepvariant/{sample}.vcf.gz""; params:; model=""WES""; threads: ; 64; resources:; mem_mb=163840; log:; ""logs/deepvariant/{sample}/stdout.log""; singularity:; ""singularity/deepvariant_1.4.0.sif""; # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU; shell:; """"""; /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'; """"""; ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? ; I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs...; Using shell: /usr/bin/bash; Provided co",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:1555,log,log,1555,,https://github.com/google/deepvariant/issues/602,1,['log'],['log']
Testability,"int; saver.restore(sess, checkpoint_filename_with_path); File ""/root/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py"", line 1538, in restore; + compat.as_text(save_path)); ValueError: The passed save_path is not a valid checkpoint: gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard//model.ckpt; 13:33:48 Unexpected exit status 1 while running ""-c /opt/deepvariant/bin/call_variants --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --outfile \""${CALLED_VARIANTS}\""/call_variants_output.tfrecord-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARD_INDEX}\"")\""-of-\""$(printf \""%05d\"" \""${CALL_VARIANTS_SHARDS}\"")\"".gz --checkpoint \""${MODEL}\""/model.ckpt --batch_size 512""; 13:33:48 Started running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Stopped running ""-c gsutil -q cp /google/logs/output gs://ms_bam/deep_output/stage/logs/call_variants/0""; 13:33:50 Execution failed: action 4: unexpected exit status 1 was not ignored; 13:33:51 Worker released; ""run"": operation ""projects/ms-deepvariant/operations/234234234234"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.2rc', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:6592,log,logs,6592,,https://github.com/google/deepvariant/issues/129,4,['log'],['logs']
Testability,"kages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>; import numpy as np; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import core; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 47, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the multiarray numpy extension module failed. Most; likely you are trying to import a failed build of numpy.; Here is how to proceed:; - If you're working with a numpy git repository, try `git clean -xdf`; (removes all files not under version control) and rebuild numpy.; - If you are simply trying to use the numpy version that you have installed:; your installation is broken - please reinstall numpy.; - If you have already reinstalled and that did not fix the problem, then:; 1. Check that you are using the Python you expect (you're using /usr/bin/python),; and that you have no directories in your PATH or PYTHONPATH that can; interfere with the Python and numpy versions you're trying to use.; 2. If (1) looks fine, you can open a new issue at; https://github.com/numpy/numpy/issues. Please include details on:; - how you installed Python; - how you installed numpy; - your operating system; - whether or not you have multiple versions of Python installed; - if you built from source, your compiler versions and ideally a build log. Note: this error has many possible causes, so please don't comment on; an existing issue about this - open a new one instead. Original error was: libopenblas.so.0: cannot open shared object file: No such file or directory; ```. I need to run deepvariant as a non-root user via singulairty on the HPC platform. The non-GPU version works just fine.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/243:2177,log,log,2177,,https://github.com/google/deepvariant/issues/243,1,['log'],['log']
Testability,"ker://google/deepvariant:{config['DEEPVARIANT_VERSION']}""; params:; vsc_min_fraction_indels=""0.12"",; pileup_image_width=199,; shard='{shard}',; examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=25000 \; --max_reads_per_partition=600 \; --phase_reads \; --pileup_image_width {params.pileup_image_width} \; --norealign_reads \; --sort_by_haplotypes \; --track_ref_reads \; --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \; --mode calling \; --ref {input.reference} \; --reads {input.bam} \; --examples {params.examples} \; --gvcf {params.gvcf} \; --task {params.shard}) > {log} 2>&1; """""". ```; Error:. ```; 2023-07-12 15:19:55.926661: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI De>; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-07-12 15:19:59.038994: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 SO:>; 2023-07-12 15:19:59.039047: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag BC: in RG line, ignoring: @RG ID:d98f52ac>; 2023-07-12 15:19:59.039065: W third_party/nucleus/io/sam_reader.cc:174] Unknown tag CM: in RG line, ignoring: @RG ID:d98f52ac>; I0712 15:19:59.039340 140472429422400 genomics_reader.py:222] Reading batches/Test349/D18757/aligned/D18757.hs37d5.bam with NativeS>; I0712 15:19:59.044630 140472429422400 make_examples_core.py:257] Task 32/96: Preparing inputs; 2023-07-12 15:19:59.049074: W third_party/nucleus/io/sam_reader.cc:131] Unknown tag pb: in header line, ignoring: @HD VN:1.6 ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:2388,log,log,2388,,https://github.com/google/deepvariant/issues/677,1,['log'],['log']
Testability,"king fine. When I try to run model_train and model_eval in my computer, model_train seemed to work but model_eval returned ValueError: Must specify steps > 0, given: 0. The error came from estimator.py file in tensorflow. In Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0; I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt; W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:1177,log,log,1177,,https://github.com/google/deepvariant/issues/172,1,['log'],['log']
Testability,"l be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089; W0828 10:40:49.488072 140318776715072 deprecation.py:350] From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be rem>; Instructions for updating:; Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089; 2024-08-28 10:40:49.893797: W tensorflow/core/framework/dataset.cc:769] Input of GeneratorDatasetOp::Dataset will not be optimized because the dataset does not implement the AsGraphDefInternal() method needed to apply optimizations.; I0828 10:40:49.947995 140318776715072 train.py:316]; ```. And here is an excerpt of from a later portion of the log file including some training and tuning steps, where you can see the 0.0 for het and homalt eval stats. . ```; I0829 07:13:59.098341 140305134778112 logging_writer.py:48] [13700] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.552009105682373, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1_; weighted=1.0, train/false_negatives=0.0, train/false_positives=0.0, train/learning_rate=9.999999747378752e-05, train/loss=0.5520098805427551, train/precision=1.0, train/precision_het=0.0, train/precision_homalt=0.0, train/precision_homref=1.0, train/recall=1.0, train/recall_het=0.0,; train/recall_homalt=0.0, train/recall_homref=1.0, train/true_negatives=12800.0, train/true_positives=6400.0; I0829 07:18:37.609363 140318776715072 local.py:41] Setting work unit notes: 0.3 steps/s, 60.6% (13778/22724), ETA: 8h52m; I0829 07:18:37.611700 140305134778112 logging_writer.py:48] [13778] steps_per_sec=0.280118; I0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:6308,log,log,6308,,https://github.com/google/deepvariant/issues/876,1,['log'],['log']
Testability,"l first):; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 500 in _import_graph_de; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/importer.py"", line 414 in import_graph_def; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/function_def_to_graph.py"", line 87 in func; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/function_deserialization.py"", line 416 i; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 154 in __init__; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 958 in load_partial; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/saved_model/load.py"", line 828 in load; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 596 in call_; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768 in main; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/absl_py/absl/app.py"", line 258 in _run_main; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/absl_py/absl/app.py"", line 312 in run; File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789 in <modu. real 0m5.038s; user 0m3.921s; sys 0m1.122s; Process ForkProcess-1:; Traceback (most recent call last):; File ""/usr/lib/python3.8/multiprocessing/process.py"", line 315, in _bootstrap; self.run(); File ""/usr/lib/python3.8/multiprocessing/process.py"", line 108, in run; self._target(*self._args, **self._kwargs); File ""/tmp/Bazel.runfiles_vitt1d55/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 454, in post; File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 108, in get; raise Empty; _queue.Empty; INFO: Cleaning up image...; ```. Please let me know if I could provide the input BAM for testing/debugging. Thank you for your time. Best regards,; Louis",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/833:4886,test,testing,4886,,https://github.com/google/deepvariant/issues/833,1,['test'],['testing']
Testability,"la/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ...; and processing is performed on CPUs. . Also, these details are put in the log hudreds of times:; 2024-07-03 18:27:31.862526: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-03 18:27:31.862557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 8308a7bb3067; 2024-07-03 18:27:31.862563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 8308a7bb3067; 2024-07-03 18:27:31.862607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 555.42.6; 2024-07-03 18:27:31.862621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 555.42.6; 2024-07-03 18:27:31.862626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 555.42.6; (then repeated with every call). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; L40S card is CUDA CC = 8.9; supported since CUDA >= 11.8 currently 12.4 and 12.5; https://developer.nvidia.com/cuda-11-8-0-download-archive - installed, still doesn't work with containerized 11.3.1. tensorRT is used; https://docs.nvidia.com/deeplearning/tensorrt/support-matrix/index.html ; currently 10.1 ; version 8 in CUDA 11.8; python >= 3.8. lspci | grep -i nvidia; 0a:00.0 3D controller: NVIDIA Corporation AD102GL [L40S] (rev a1); ae:00.0 3D controller: NVIDIA Corporation AD102GL [L40S] (rev a1). nvidia-container-cli is installed (supersedes nvidia-docker - https://github.com/NVIDIA/nvidia-docker )",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:4309,test,test,4309,,https://github.com/google/deepvariant/issues/844,2,['test'],['test']
Testability,"le>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/_0_Диссертация/Exp/seq1/seq1.fa"" --reads ""/home/platon/_0_Диссертация/Exp/seq1/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Second attempt. This time with paths consisting only of latin characters.; `sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/platon/test/seq1.fa --reads=/home/platon/test/seq1.bam --output_vcf=/home/platon/test/seq1.vcf`. ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}. [E::hts_open_format] Failed to open file /home/platon/test/seq1.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvari",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/219:3094,test,test,3094,,https://github.com/google/deepvariant/issues/219,1,['test'],['test']
Testability,"led_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 94). These functions will not be directly callable after loading.; INFO:tensorflow:Assets written to: /home/train_new/checkpoints/ckpt-150/assets; I1025 22:02:39.405452 140172092593984 builder_impl.py:797] Assets written to: /home/train_new/checkpoints/ckpt-150/assets; WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; W1025 22:02:44.960290 140172092593984 checkpoint.py:205] Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter; W1025 22:02:44.960591 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.iter; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay; W1025 22:02:44.960684 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.decay; WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.awg_optimizer.momentum; W1025 22:02:44.960754 140172092593984 checkpoint.py:214] Value in checkpoint could not be found in the restored objec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/722:2709,log,logs,2709,,https://github.com/google/deepvariant/issues/722,1,['log'],['logs']
Testability,"les runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/S-001737188.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/S-001737188.g.vcf.gz"" ) 2>&1 | tee /output/logs/postprocess_variants.log. real	10m14.442s; user	0m1.472s; sys	0m1.164s",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465:1976,log,logs,1976,,https://github.com/google/deepvariant/issues/465,4,['log'],"['log', 'logs']"
Testability,les.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz; ...; -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 te,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1307,test,test,1307,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"les_o79jsi96/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/sbgenomics/workspaces/aa109ba2-5d46-4def-a398-4a7e1ee8806e/tasks/2555e949-a7d7-40a9-9a24-b002adf182c2/deepvariant-1-0-0/Bazel.runfiles_o79jsi96/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 351, in call_variants; num_channels_in_checkpoint_model, example_shape[2])); **ValueError: The number of channels in examples and checkpoint should match, but the checkpoint has 9 channels while the examples have 8.**`. My command line looks like this:. `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --alt_aligned_pileup diff_channels --reads /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/HG002.merged.bam --ref Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --norealign_reads --regions 20 --sample_name HG002 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --use_openvino --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/aa109ba2-5d46-4def-a398-4a7e1ee8806e/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./HG002.vcf ) > ./postprocess_variants.log 2>&1`. When I try and run with HYBRID model, everything goes smoothly. Do you have some input on this?. Thanks,; Ajsa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/458:2466,log,log,2466,,https://github.com/google/deepvariant/issues/458,3,['log'],['log']
Testability,"les_runner(options); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 2003, in make_examples_runner; candidates, examples, gvcfs = region_processor.process(region); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1472, in process; self.in_memory_sam_reader.replace_reads(self.region_reads(region)); File ""/tmp/Bazel.runfiles_n9h1txbv/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1534, in region_reads; error_message + '\nFailed to parse BAM/CRAM file. '; ValueError: Data loss: Failed to parse SAM record; Failed to parse BAM/CRAM file. This is often caused by:; (1) When using a CRAM file, and setting --use_ref_for_cram to false (which means you want to use the embedded ref instead of a ref file), this error could be because of inability to find the embedded ref file.; (2) Your BAM/CRAM file could be corrupted. Please check its md5. **Does the quick start test work on your system?**; it work. **Any additional context:**; I also run the mistaken command directly inside docker container, . [gosadmin@node3 trio]$ docker run -it -v ""${DIR}"":""/data"" google/deepvariant:${VERSION} /bin/bash. root@d2911291b750:/data# ls -alh; total 42G; drwxrwxr-x 2 1000 1000 4.0K May 8 03:39 .; drwxr-xr-x 1 root root 29 May 8 08:21 ..; -rw-rw-r-- 1 1000 1000 6.9M Apr 30 08:19 HG002.bai; -rw-rw-r-- 1 1000 1000 9.7G Apr 30 08:19 HG002.bam; -rw-rw-r-- 1 1000 1000 9.6M Apr 30 09:01 HG002_truth.bed; -rw-rw-r-- 1 1000 1000 147M Apr 30 09:00 HG002_truth.vcf.gz; -rw-rw-r-- 1 1000 1000 1.5M Apr 30 09:00 HG002_truth.vcf.gz.tbi; -rw-rw-r-- 1 1000 1000 6.8M Apr 30 08:33 HG003.bai; -rw-rw-r-- 1 1000 1000 8.4G Apr 30 08:33 HG003.bam; -rw-rw-r-- 1 1000 1000 9.2M Apr 30 09:03 HG003_truth.bed; -rw-rw-r-- 1 1000 1000 129M Apr 30 09:02 HG003_truth.vcf.gz; -rw-rw-r-- 1 1000 1000 1.5M Apr 30 09:02 HG003_truth.vcf.gz.tbi; -rw-rw-r-- 1 1000 1000 7.0M May 7 07:42 HG004.bai; -rw-rw-r-- 1 100",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/455:3010,test,test,3010,,https://github.com/google/deepvariant/issues/455,1,['test'],['test']
Testability,"licable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in chec",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/471:1945,test,testdata,1945,,https://github.com/google/deepvariant/issues/471,1,['test'],['testdata']
Testability,"ll be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```; ## Run `call_variants`; echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log.""; ( time sudo docker run \; -v ""${BASE}"":""${BASE}"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) 2>&1 | tee ""${LOG_DIR}/call_variants.log""; echo ""Done.""; echo; ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard and then somehow merging all the ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:3702,log,log,3702,,https://github.com/google/deepvariant/issues/151,1,['log'],['log']
Testability,"lledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/_0_Диссертация/Exp/seq1/seq1.fa"" --reads ""/home/platon/_0_Диссертация/Exp/seq1/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Second attempt. This time with paths consisting only of latin characters.; `sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/platon/test/seq1.fa --reads=/home/platon/test/seq1.bam --output_vcf=/home/platon/test/seq1.vcf`. ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}. [E::hts_open_format] Failed to open file /home/platon/test/seq1.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; r",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/219:3518,test,test,3518,,https://github.com/google/deepvariant/issues/219,1,['test'],['test']
Testability,"ller for BGISEQ-500 data, it says that . > At the same time, start model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0; I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt; W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-???",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:1478,log,log,1478,,https://github.com/google/deepvariant/issues/172,1,['log'],['log']
Testability,"llo, . Previously I had an issue where the parameters I was using were not producing checkpoints in the model training step. I know that choosing parameters has a component of guesswork and iteration, and I was wondering if there are recommendations anywhere on how to choose a starting point for model training parameters, or if there are descriptions somewhere of what changing a particular parameter is likely to do. In the run described below, I am attempting to train the model on an individual using a second individual for the training data and a third individual for the validation data, but my goal is to use multiple individuals for both the training and validation sets, akin to the project described [here](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). . Secondly, I've been having an issue lately where I submit scripts to my computing cluster, and though they are granted resources and produce a log file, the log file is empty after several days of the code running indicating no progress has been made or that the program has even initialized. I am also asking my cluster resources about this, as I suspect it is more likely an issue with resource allocation, but I would also very much appreciate if someone could take a glance at the code I am submitting to make sure there are no obvious causes for this in the deepvariant commands that I'm just completely missing. . Thank you very much! . Best, . Haley. Here is the code: ; `#!/bin/bash. #SBATCH -p atlas ; #SBATCH --time=5-48:00:00 # walltime limit (HH:MM:SS); #SBATCH --nodes=1 # number of nodes; #SBATCH --gpus-per-node=1 # 20 processor core(s) per node X 2 threads per core; #SBATCH --partition=gpu-a100 # standard node(s); #SBATCH --ntasks=1; #SBATCH --job-name=""deepvariant_modeltraining""; #SBATCH --mail-user=haley.arnold@usda.gov # email address; #SBATCH --mail-type=BEGIN; #SBATCH --mail-type=END; #SBATCH --mail-type=FAIL; #SBATCH -",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/840:1000,log,log,1000,,https://github.com/google/deepvariant/issues/840,2,['log'],['log']
Testability,"llready have an open Issue on the Workflow but we are at the Point that we think its ether Nucleus or Tensorflow that produces the error PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: 1.5.0; - Tensorflow 2.11.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: PacBio HIFI reads. **Steps to reproduce:**; ```; rule deepvariant_make_examples:; input:; bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",; bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",; reference=config[""ref""][""fasta""],; output:; tfrecord=temp(; f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz""; ),; nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>; log:; f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",; benchmark:; f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv""; container:; f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}""; params:; vsc_min_fraction_indels=""0.12"",; pileup_image_width=199,; shard='{shard}',; examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=25000 \; --max_reads_per_partition=600 \; --phase_reads \; --pileup_image_width {params.pileup_image_width} \; --norealign_reads \; --sort_by_haplotypes \; --track_ref_rea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:1160,log,log,1160,,https://github.com/google/deepvariant/issues/677,2,['log'],"['log', 'logs']"
Testability,"local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py:1473: NameBasedSaverStatus.__init__ (from tensorflow.python.checkpoint.checkpoint) is deprecated and will be removed in a future version.; Instructions for updating:; Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 789, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 768, in main; call_variants(; File ""/tmp/Bazel.runfiles_rw0m5gar/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 640, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py"", line 1047, in assert_consumed; raise AssertionError(; AssertionError: Some objects had attributes which were not restored: ; ```; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/857:3457,Assert,AssertionError,3457,,https://github.com/google/deepvariant/issues/857,4,"['Assert', 'test']","['AssertionError', 'test']"
Testability,ls_test NO STATUS; //deepvariant/core:errors_test NO STATUS; //deepvariant/core:genomics_io_gcs_test NO STATUS; //deepvariant/core:genomics_io_noplugin_test NO STATUS; //deepvariant/core:genomics_io_test NO STATUS; //deepvariant/core:hts_test NO STATUS; //deepvariant/core:hts_verbose_test NO STATUS; //deepvariant/core:io_utils_test NO STATUS; //deepvariant/core:py_math_test NO STATUS; //deepvariant/core:py_utils_test NO STATUS; //deepvariant/core:ranges_test NO STATUS; //deepvariant/core:reader_base_test NO STATUS; //deepvariant/core:reference_fai_test NO STATUS; //deepvariant/core:sam_reader_test NO STATUS; //deepvariant/core:samplers_test NO STATUS; //deepvariant/core:variantutils_test NO STATUS; //deepvariant/core:vcf_reader_test NO STATUS; //deepvariant/core:vcf_writer_test NO STATUS; //deepvariant/core/python:hts_verbose_test NO STATUS; //deepvariant/core/python:math_wrap_test NO STATUS; //deepvariant/core/python:reference_wrap_test NO STATUS; //deepvariant/core/python:sam_reader_wrap_test NO STATUS; //deepvariant/core/python:vcf_reader_wrap_test NO STATUS; //deepvariant/core/python:vcf_writer_wrap_test NO STATUS; //deepvariant/environment_tests:env_smoke_test NO STATUS; //deepvariant/environment_tests:protobuf_implementation_test NO STATUS; //deepvariant/python:allelecounter_wrap_test NO STATUS; //deepvariant/python:variant_calling_wrap_test NO STATUS; //deepvariant/realigner:aligner_test NO STATUS; //deepvariant/realigner:realigner_test NO STATUS; //deepvariant/realigner:ssw_test NO STATUS; //deepvariant/realigner:window_selector_test NO STATUS; //deepvariant/realigner/python:debruijn_graph_wrap_test NO STATUS; //deepvariant/realigner/python:ssw_misc_test NO STATUS; //deepvariant/realigner/python:ssw_wrap_test NO STATUS; //deepvariant/testing:gunit_extras_test NO STATUS; //deepvariant/vendor:statusor_test NO STATUS; //deepvariant/vendor:timer_test NO STATUS; //deepvariant/vendor/python:statusor_examples_test NO STATUS; `; how to fix it ? I am not a root users.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/6:6276,test,testing,6276,,https://github.com/google/deepvariant/issues/6,1,['test'],['testing']
Testability,"lwy5/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I1202 23:23:46.123890 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.133658 46912500266816 make_examples_core.py:301] Preparing inputs; I1202 23:23:46.139615 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.140348 46912500266816 make_examples_core.py:301] Common contigs are ['chr20']; I1202 23:23:46.141555 46912500266816 make_examples_core.py:301] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; I1202 23:23:46.150200 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.240882 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.241135 46912500266816 make_examples_core.py:301] Writing gvcf records to /flashscratch/kimkw/tmp/tmppin2lwy5/gvcf.tfrecord-00000-of-00001.gz; I1202 23:23:46.248160 46912500266816 make_examples_core.py:301] Writing examples to /flashscratch/kimkw/tmp/tmppin2lwy5/make_examples.tfrecord-00000-of-00001.gz; I1202 23:23:46.248263 46912500266816 make_examples_core.py:301] Overhead for preparing inputs: 0 seconds; RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem .; Traceback (most recent call last):; File ""/flashscratch/kimkw/tmp/Bazel.runfiles_hubgarxp/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 234, in <module>; app.run(main); File ""/flashscra",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/746:2325,test,testdata,2325,,https://github.com/google/deepvariant/issues/746,1,['test'],['testdata']
Testability,"m"" --examples ""/flashscratch/kimkw/tmp/tmppin2lwy5/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/flashscratch/kimkw/tmp/tmppin2lwy5/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I1202 23:23:46.123890 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.133658 46912500266816 make_examples_core.py:301] Preparing inputs; I1202 23:23:46.139615 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.140348 46912500266816 make_examples_core.py:301] Common contigs are ['chr20']; I1202 23:23:46.141555 46912500266816 make_examples_core.py:301] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; I1202 23:23:46.150200 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.240882 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.241135 46912500266816 make_examples_core.py:301] Writing gvcf records to /flashscratch/kimkw/tmp/tmppin2lwy5/gvcf.tfrecord-00000-of-00001.gz; I1202 23:23:46.248160 46912500266816 make_examples_core.py:301] Writing examples to /flashscratch/kimkw/tmp/tmppin2lwy5/make_examples.tfrecord-00000-of-00001.gz; I1202 23:23:46.248263 46912500266816 make_examples_core.py:301] Overhead for preparing inputs: 0 seconds; RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem .; Traceback (most recent call last):; File ""/flashscratch/kimkw/t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/746:2181,test,testdata,2181,,https://github.com/google/deepvariant/issues/746,1,['test'],['testdata']
Testability,"m/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Slurm based ; - DeepVariant version: deepvariant_1.5.0.sif; - Installation method : singularity image ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**; - Command:. projDir=/home1/***/***/deepvaraint/; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds.; Launcher: Task 2 running job 3 on c304-01",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:976,test,test,976,,https://github.com/google/deepvariant/issues/717,2,['test'],['test']
Testability,"m54191_180528_182730/29557366/26432_30218: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437453: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/34281_36602: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437552: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/6879_10765: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437662: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/30289_34208: Not found: Could not read base quality scores; 2020-02-10 20:52:23.437766: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29557366/18650_22440: Not found: Could not read base quality scores; 2020-02-10 20:52:23.438311: W third_party/nucleus/io/sam_reader.cc:474] Could not read base quality scores m54191_180528_182730/29163785/119_9105: Not found: Could not read base quality scores; 2020-02-10 20:52:23.440373: F deepvariant/allelecounter.cc:103] Check failed: offset + len <= read.aligned_quality_size() (4265 vs. 0). The packages work with the test Illumina files. After testing several options, I notice what I got as raw data is either the bam file (without quality scores, only ""!"") or a fasta files. However, it seems that using those reads during the mapping wuth BWA (or with pbmm2) produces bam files without mapping scores which seems to be the problem. . This is how I mapped reads:. bwa mem -x pacbio -t 8 -R $ReadGroup $1 $2 | java -jar /sw/apps/bioinfo/picard/2.20.4/rackham/picard.jar SortSam \; INPUT=/dev/stdin \; OUTPUT=""$sample.bwa.picardSort.bam"" \; SORT_ORDER=coordinate. I feel this is a very basic question, but it is actually possible to run DeepVariant maping PB reads but in fasta format? Am I missing somthing to get the mapping scores in the bam file?. Thanks a lot,. Cheers,. /Sergio",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/270:2235,test,test,2235,,https://github.com/google/deepvariant/issues/270,2,['test'],"['test', 'testing']"
Testability,"mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different computers, and the error was the same.; There is a previous issue in this forum (https://github.com/google/deepvariant/issues/181) where the user did not set BIN_VERSION variable correc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/223:1350,test,testdata,1350,,https://github.com/google/deepvariant/issues/223,1,['test'],['testdata']
Testability,"me/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?; [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]; ```. This is the script that I am running DeepVariant:. ```; OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant; REF=/mnt/efs-genome/Ref/hg19.gatk.fasta; BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam; MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs; N_SHARDS=4. #mkdir -p ""${LOGDIR}""; #time seq 0 $((N_SHARDS-1)) | \; # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \; # sudo docker run \; # -v /mnt/efs-genome:/mnt/efs-genome \; # gcr.io/deepvariant-docker/deepvariant \; # /opt/deepvariant/bin/make_examples \; # --mode calling \; # --ref ""${REF}"" \; # --reads ""${BAM}"" \; # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \; -v /mnt/efs-genome:/mnt/efs-genome \; gcr.io/deepvariant-docker/deepvariant \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""${MODEL}""; ```. Can you please help me troubleshoot?. I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very mu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:13039,LOG,LOGDIR,13039,,https://github.com/google/deepvariant/issues/166,2,"['LOG', 'log']","['LOGDIR', 'logs']"
Testability,"ment, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**; - Command:. projDir=/home1/***/***/deepvaraint/; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds.; Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarke",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:1337,test,test,1337,,https://github.com/google/deepvariant/issues/717,1,['test'],['test']
Testability,"model_eval on CPUs. Since I don't have a TPU, so the following is the code I used and attempt to run model_train and model_eval on CPU simultaneously. The following is the code I used:. `(time python /home/bin/model_train.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.dataset_config.pbtxt \. --train_dir=/data/output/trained_model \. --model_name=""inception_v3"" \. --number_of_steps=10 \. --save_interval_secs=3000 \. --batch_size=32 \. --learning_rate=0.008 \. --start_from_checkpoint=/home/models/model.ckpt) >/data/output/log/model_training/model_train.log 2>&1\. & (time python2 /home/bin/model_eval.zip \; --dataset_config_pbtxt=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt \. --checkpoint_dir=/data/output/trained_model \. --number_of_steps=10 \. --batch_size=32) >/data/output/log/model_training/model_eval.log 2>&1`. The following is the message from model_eval log :. > I0415 07:34:19.493486 140713377441536 model_eval.py:141] Set KMP_BLOCKTIME to 0; I0415 07:34:19.495834 140713377441536 model_eval.py:177] Running fixed eval for: /data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.dataset_config.pbtxt; W0415 07:34:19.536698 140713377441536 deprecation.py:323] From /tmp/Bazel.runfiles_tELT0A/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.584646 140713377441536 model_eval.py:190] Running evaluations on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/validation_set_with_label_shuffled/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=8, mode=eval with model DeepVariantMod",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:1564,log,log,1564,,https://github.com/google/deepvariant/issues/172,1,['log'],['log']
Testability,"n I run ""make_examples"" for testing dataset:. python /opt/deepvariant/bin/make_examples.zip \; --mode calling \; --ref /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --examples /usit/abel/u1/senz/DeepVariant0.7.2/quickstart_output/examples.tfrecord.gz. 2019-02-17 16:15:42.409205: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: ; I0217 16:15:42.409567 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0217 16:15:42.417557 139914391602944 make_examples.py:1080] Preparing inputs; 2019-02-17 16:15:42.422480: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring: ; I0217 16:15:42.422776 139914391602944 genomics_reader.py:213] Reading /usit/abel/u1/senz/DeepVariant0.7.2/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0217 16:15:42.424446 139914391602944 make_examples.py:996] Common contigs are [u'chr20']; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1205, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1195, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1081, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 998, in processing_regions_from_options; options.exclude_calling_regions); File ""/tmp/Bazel.runfiles_DVDplM/runfiles/com_google",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/155:1052,test,testdata,1052,,https://github.com/google/deepvariant/issues/155,1,['test'],['testdata']
Testability,"n Singularity"" section in quick start test (https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md), and I got an error regarding numpy as below. Could you help me resolve this issue? I used deepvariant_1.6.0 image. ```; 2023-12-02 23:23:35.126320: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I1202 23:23:41.449015 46912500266816 run_deepvariant.py:519] Re-using the directory for intermediate results in /flashscratch/kimkw/tmp/tmppin2lwy5. ***** Intermediate results will be written to /flashscratch/kimkw/tmp/tmppin2lwy5 in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""./quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/flashscratch/kimkw/tmp/tmppin2lwy5/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/flashscratch/kimkw/tmp/tmppin2lwy5/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I1202 23:23:46.123890 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.133658 46912500266816 make_examples_core.py:301] Preparing inputs; I1202 23:23:46.139615 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.140348 46912500266816 make_examples_core.py:301] Common contigs are ['chr20']; I1202 23:23:46.141555 46912500266816 make_examples_core.py:301] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed i",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/746:1084,test,testdata,1084,,https://github.com/google/deepvariant/issues/746,1,['test'],['testdata']
Testability,nce/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean|InceptionV3/Mixed_6d/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta|InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights/RMSProp|InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta|InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_2/Conv2d_0b_7x1/weights|InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta/RMSProp_1|InceptionV3/Logits/Conv2d_1c_1x1/biases/RMSProp_1|InceptionV3/Mixed_7c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/weights/RMSProp|InceptionV3/Mixed_5c/Branch_2/Conv2d_0c_3x3/weights/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/weights|InceptionV3/Mixed_7c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean|InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/weights|InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/BatchNorm/moving_variance|InceptionV3/Mixed_7b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance|InceptionV3/Mixed_6c/Branc,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:59566,Log,Logits,59566,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,"nd from call_variants_outputs for variant reference_bases: ""C""; alternate_bases: ""A""; calls {; info {; key: ""AD""; value {; values {; int_value: 17; }; values {; int_value: 4; }; }; }; info {; key: ""DP""; value {; values {; int_value: 21; }; }; }; info {; key: ""VAF""; value {; values {; number_value: 0.190476190476; }; }; }; genotype: -1; genotype: -1; call_set_name: ""XY406-1""; }; end: 10147; reference_name: ""1""; start: 10146; is [[0], [0], [0]], which is invalid.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 874, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 851, in main; header=header); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 595, in write_variants_to_vcf; for variant in variant_generator:; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 110, in _group_overlapping_variants; for variant in sorted_variants:; File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 631, in _transform_call_variants_output_to_variants; outputs, multi_allelic_qual_filter); File ""/tmp/Bazel.runfiles_4jh3iyl1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 559, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check. **Does the quick start test work on your system?**; YES",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/485:3151,test,test,3151,,https://github.com/google/deepvariant/issues/485,1,['test'],['test']
Testability,"neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I1202 23:23:41.449015 46912500266816 run_deepvariant.py:519] Re-using the directory for intermediate results in /flashscratch/kimkw/tmp/tmppin2lwy5. ***** Intermediate results will be written to /flashscratch/kimkw/tmp/tmppin2lwy5 in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""./quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/flashscratch/kimkw/tmp/tmppin2lwy5/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/flashscratch/kimkw/tmp/tmppin2lwy5/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I1202 23:23:46.123890 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.133658 46912500266816 make_examples_core.py:301] Preparing inputs; I1202 23:23:46.139615 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.140348 46912500266816 make_examples_core.py:301] Common contigs are ['chr20']; I1202 23:23:46.141555 46912500266816 make_examples_core.py:301] Starting from v0.9.0, --use_ref_for_cram is default to true. If you are using CRAM input, note that we will decode CRAM using the reference you passed in with --ref; I1202 23:23:46.150200 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.240882 46912500266816 genomics_reader.py:222] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1202 23:23:46.241135 46912500266816 make_examples_core.py:301] Writing gvcf records to /flashscratch/ki",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/746:1487,test,testdata,1487,,https://github.com/google/deepvariant/issues/746,1,['test'],['testdata']
Testability,"nfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 323, in call_variants; first_example = tf_utils.get_one_example_from_examples_path(examples_filename); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/tf_utils.py"", line 205, in get_one_example_from_examples_path; 'Cannot find matching files with the pattern ""{}""'.format(source)); ValueError: Cannot find matching files with the pattern ""/tmp/tmpgv2oy1s_/make_examples.tfrecord@8.gz"". real	0m2.022s; user	0m2.036s; sys	0m0.413s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ref.fa"" --infile ""/tmp/tmpgv2oy1s_/call_variants_output.tfrecord.gz"" --outfile ""/output/OUTPUT_VCF.vfc"" --nonvariant_site_tfrecord_path ""/tmp/tmpgv2oy1s_/gvcf.tfrecord@8.gz"" --gvcf_outfile ""/output/OUTPUT_GVCF.vfc"" ) 2>&1 | tee /output/logs/postprocess_variants.log. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1184, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1107, in main; sample_name = get_sample_name(); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1053, in get_sample_name; _, record = get_cvo_paths_and_first_record(); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_googl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/435:2483,log,logs,2483,,https://github.com/google/deepvariant/issues/435,1,['log'],['logs']
Testability,"ning \; --use_ref_for_cram=true \; --ref=${reference} \; --examples ${accession}.ds0.with_labels.examples \; --sample_name ${accession} \; --reads ${cram} \; --truth_variants=${accession}.vcf.gz \; --confident_regions=${accession}.bed \; --regions CM0XXXXXX.1 \; --write_run_info. ```. Shuffling code is similar to this, but repeated multiple time:; ```; raw_dataset = tf.data.TFRecordDataset(inputs); for raw_record in itershuffle(raw_dataset, 2000):; example = tf.train.Example(); example.ParseFromString(raw_record.numpy()); writer = random.choice(out_fhs); writer.write(example.SerializeToString()); ```. And training code is like this:; ```; /opt/deepvariant/bin/model_train \; --dataset_config_pbtxt=${config_path} \; --batch_size=256 \; --train_dir=${training_dir} \; --model_name=""inception_v3"" \; --learning_rate=0.008 \; --start_from_checkpoint=/opt/models/wgs/model.ckpt \; --number_of_steps=50000 \; --save_interval_secs 300; ```. Here is the run info for just one sample's examples set (only a single chromosome, for testing purposes, from the .run_info.pbtxt file):. ```; labeling_metrics {; n_truth_variant_sites: 3469; n_truth_variant_alleles: 3474; n_candidate_variant_sites: 9778; n_candidate_variant_alleles: 9943; n_non_confident_candidate_variant_sites: 2219; n_true_positive_sites: 3468; n_true_positive_alleles: 3845; n_false_negative_sites: 1; n_false_negative_alleles: 1; n_false_positive_sites: 6309; n_false_positive_alleles: 6469; n_inexact_position_matches: 1; n_exact_position_matches: 3469; n_exact_position_and_allele_matches: 3443; n_exact_position_and_allele_and_genotype_matches: 3443; }; ```. Training runs just fine, with loss starting at ~1.2 and dropping to 0.04. Batch size is relatively small (memory error on the GPU with any larger). Is it simply my patience or is something else going on? I can provide tensorboard stats as well, but taking any model and performing make_examples(calling) -> postprocess results in only refcalls. Thanks, and let me know wh",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/251:1540,test,testing,1540,,https://github.com/google/deepvariant/issues/251,1,['test'],['testing']
Testability,"non_variants: false}, {orig_names: [GQ], name: GQ, description: ""##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=\""Genotype Quality\"">"", type: int, number: basic, default_type: missing, count: 1, combi_method: min, ignore_non_variants: true}, {orig_names: [PL], name: PL, description: ""##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\""Phred-scaled genotype Likelihoods\"">"", type: int, number: genotype, default_type: missing, count: 0, combi_method: missing, ignore_non_variants: true}]}}; ##INFO=<ID=AF,Number=A,Type=Float,Description=""Allele Frequency estimate for each alternate allele"">; ##INFO=<ID=AQ,Number=A,Type=Integer,Description=""Allele Quality score reflecting evidence for each alternate allele (Phred scale)"">; ##INFO=<ID=AC,Number=A,Type=Integer,Description=""Allele count in genotypes"">; ##INFO=<ID=AN,Number=1,Type=Integer,Description=""Total number of alleles in called genotypes"">; ##FILTER=<ID=MONOALLELIC,Description=""Site represents one ALT allele in a region with multiple variants that could not be unified into non-overlapping multi-allelic sites"">; ##FORMAT=<ID=GT,Number=1,Type=String,Description=""Genotype"">; ##FORMAT=<ID=RNC,Number=2,Type=Character,Description=""Reason for No Call in GT: . = n/a, M = Missing data, P = Partial data, I = gVCF input site is non-called, D = insufficient Depth of coverage, - = unrepresentable overlapping deletion, L = Lost/unrepresentable allele (other than deletion), U = multiple Unphased variants present, O = multiple Overlapping variants present, 1 = site is Monoallelic, no assertion about presence of REF or ALT allele"">; ##FORMAT=<ID=DP,Number=1,Type=Integer,Description=""Approximate read depth (reads with MQ=255 or with bad mates are filtered)"">; ##FORMAT=<ID=AD,Number=R,Type=Integer,Description=""Allelic depths for the ref and alt alleles in the order listed"">; ##FORMAT=<ID=GQ,Number=1,Type=Integer,Description=""Genotype Quality"">; ##FORMAT=<ID=PL,Number=G,Type=Integer,Description=""Phred-scaled genotype Likelihoods"">; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/633:4883,assert,assertion,4883,,https://github.com/google/deepvariant/issues/633,1,['assert'],['assertion']
Testability,"nt/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**; (A clear and concise description of what the issue is.). A variant with VAF value 1 is called as heterozygous. The IGV visualisation of the .bam file shows that it should clearly be a homozygous variant. ![igv_panel_chr2_24146804](https://user-images.githubusercontent.com/84016709/204764192-9bd69aa6-7f23-4490-9391-7af95e909e3f.png). Here the line from .vcf file:; ```; chr2	24146804	.	C	T	29.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:3:162:0,162:1:26,0,0; ```; .gvcf file:; ```; chr2	24146804	.	C	T,<*>	29.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:3:162:0,162,0:1,0:26,0,0,990,990,990; ```. We also asked our collaborators to run the same sample and in their results, a homozygous variant is called:; ```; chr2	24146804	.	C	T	30.8	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:5:161:0,161:1:28,2,0; ```; ```; chr2	24146804	.	C	T,<*>	30.8	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:5:161:0,161,0:1,0:28,2,0,990,990,990; ```. What could cause this discrepancy, if the DeepVariant versions and commands are the same?. **Setup**; - Operating system: Ubuntu16.04; - DeepVariant version: 1.2.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); NovaSeq 6000 using Twist Comprehensive Exome with mtDNA add-in, GRCh38. **Steps to reproduce:**; - Command:; ```; docker run \; -v ${MOUNT_DIR}:${MOUNT_DIR} \; google/deepvariant:1.2.0-rc0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""${REFERENCE}"" \; --reads=""${INPUT}"" \; --regions=""${CAPTURE_KIT}"" \; --output_vcf=${OUTPUT_VCF} \; --output_gvcf=${OUTPUT_GVCF} \; --num_shards=64 \; --postprocess_variants_extra_args=""only_keep_pass=true""; ```; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; No.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/592:1864,test,test,1864,,https://github.com/google/deepvariant/issues/592,2,['test'],['test']
Testability,"nt/deepvariant/postprocess_variants.py"", line 1249, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1205, in main; write_variants_to_vcf(; File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 778, in write_variants_to_vcf; for variant in variant_iterable:; File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 87, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 106, in _group_overlapping_variants; for variant in sorted_variants:; File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 853, in _transform_call_variants_output_to_variants; canonical_variant, predictions = merge_predictions(; File ""/tmp/Bazel.runfiles_ohe4bkg1/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 717, in merge_predictions; raise ValueError('`call_variants_outputs` did not pass sanity check.'); ValueError: `call_variants_outputs` did not pass sanity check.; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? ; No, the quick start and also chr22 from the same sample ran through. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/517:5715,test,test,5715,,https://github.com/google/deepvariant/issues/517,2,['test'],['test']
Testability,"oject/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" : No such file or directory; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 196, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 182, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 133, in default_options; samples_in_order, sample_role_to_train = one_sample_from_flags(; File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 88, in one_sample_from_flags; sample_name = make_examples_core.assign_sample_name(; File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 147, in assign_sample_name; with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:; File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_u72sdm6v/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__; self._reader = sam_reader.SamReader.from_file(; ValueError: NOT_FOUND: Could not open /N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam; parallel: This job failed:",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678:5491,test,testdata,5491,,https://github.com/google/deepvariant/issues/678,1,['test'],['testdata']
Testability,"omwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; ; rm -rf /cromwell_root/pepper_output/pepper_snp/; ; echo ""CONTIGS FOUND IN PEPPER SNP VCF:""; ; zcat /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz | grep -v '#' | cut -f1 | uniq; -------; CONTIGS FOUND IN PEPPER SNP VCF:; chr10; chr14; [11-03-2021 13:54:07] INFO: [4/9] RUNNING THE FOLLOWING COMMAND; -------; time margin phase /cromwell_root/fc-1aea7e86-3760-4d8f-9f98-d199e815e8e2/7a319de0-a99a-4429-84a6-20c8f2b9373f/ONTWholeGenome/977d19ea-5082-4605-8595-803df94ec9dc/call-CallVariants/CallVariants/2ab0b7ef-d657-4d70-9d3c-3b9b74720a00/call-size_balanced_scatter/shard-2/cacheCopy/T708322218_ONT.10_14-p.bam /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz /opt/margin_dir/params/misc/allParams.ont_haplotag.json -t 64 -V -o /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN 2>&1 | tee /cromwell_root/pepper_output/logs/2_margin_haplotag.log;; mv /cromwell_root/pepper_output/*.bam /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; ; samtools index -@64 /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam; -------; Running OpenMP with 64 threads.; > Parsing model parameters from file: /opt/margin_dir/params/misc/allParams.ont_haplotag.json; > Parsed 346237 HET VCF entries from /cromwell_root/pepper_output/PEPPER_SNP_OUPUT.vcf.gz; skipped 0 for region, 0 for not being PASS, 115453 for being homozygous, 0 for being INDEL; > Set up bam chunker in 20s with chunk size 100000 and overlap 10000 (for region=chr10,chr14), resulting in 1342 total chunks; > Ordering chunks by estimated depth; > Setup complete, beginning run; > Polishing 3% complete (46/1342). Estimated time remaining: unknown; > Polishing 5% complete (80/1342). Estimated time remaining: 3m 10s; > Polishing 9% complete (131/1342). Estimated time remaining: 2m 51s; > Polishing",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:6564,log,logs,6564,,https://github.com/google/deepvariant/issues/491,1,['log'],['logs']
Testability,"on3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 621, in predict; features, input_hooks = self._get_features_from_input_fn(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/estimator.py"", line 1019, in _get_features_from_input_fn; result, _, hooks = estimator_util.parse_input_fn_result(result); File ""/usr/local/lib/python3.8/dist-packages/tensorflow_estimator/python/estimator/util.py"", line 60, in parse_input_fn_result; result = iterator.get_next(); File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/data/ops/iterator_ops.py"", line 444, in get_next; flat_ret = gen_dataset_ops.iterator_get_next(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/ops/gen_dataset_ops.py"", line 2865, in iterator_get_next; _, _, _op, _outputs = _op_def_library._apply_op_helper(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/op_def_library.py"", line 744, in _apply_op_helper; op = g._create_op_internal(op_type_name, inputs, dtypes=None,; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 3697, in _create_op_internal; ret = Operation(; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/ops.py"", line 2101, in __init__; self._traceback = tf_stack.extract_stack_for_node(self._c_op). real	41m45.880s; user	1063m44.358s; sys	25m21.900s; INFO: Cleaning up image...; ERROR: failed to delete container image tempDir /tmp/pbs.1173981.omics/rootfs-2853380811: unlinkat /tmp/pbs.1173981.omics/rootfs-2853380811/tmp-rootfs-1307439201/opt/traps/lib/libmodule64.so: permission denied; singularity/3.10.0 is unloaded . - `. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; No; **Any additional context:**; Input BAM file seems valid. Checked with samtools quickcheck -v command.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:17439,test,test,17439,,https://github.com/google/deepvariant/issues/564,2,['test'],['test']
Testability,"ont_1121_none/model.ckpt-30200 --ref=/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa --reads=/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam --output_vcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.vcf.gz --output_gvcf=/cromwell_root/pepper_output/T708322218_ONT.10_14-p.deepvariant_pepper.g.vcf.gz --sample_name=""6061-SL-0029"" --intermediate_results_dir=/cromwell_root/pepper_output/dv_intermediate_outputs/ --num_shards=64 --make_examples_extra_args=""alt_aligned_pileup=none,realign_reads=false,min_mapping_quality=1,min_base_quality=1,sort_by_haplotypes=true,parse_sam_aux_fields=true,add_hp_channel=false,variant_caller=vcf_candidate_importer,proposed_variants=/cromwell_root/pepper_output/PEPPER_HP_OUPUT.vcf.gz"" --postprocess_variants_extra_args=""use_multiallelic_model=True"" 2>&1 | tee /cromwell_root/pepper_output/logs/4_DeepVariant.log; -------; STARTING DEEPVARIANT; I1103 14:39:53.527210 140335058065216 run_deepvariant.py:317] Re-using the directory for intermediate results in /cromwell_root/pepper_output/dv_intermediate_outputs/; I1103 14:39:53.527496 140335058065216 run_deepvariant.py:327] You set --customized_model. Instead of using the default model for WGS, `call_variants` step will load /opt/dv_models/ont_1121_none/model.ckpt-30200 instead. ***** Intermediate results will be written to /cromwell_root/pepper_output/dv_intermediate_outputs/ in docker. ****. ***** Running the command:*****; ( time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa"" --reads ""/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam"" --examples ""/cromwell_root/pepper_output/dv_intermediate_outputs/make_examples.tfrecord@64.gz"" --noadd_hp_channel --alt_aligned_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:22608,log,log,22608,,https://github.com/google/deepvariant/issues/491,1,['log'],['log']
Testability,"oot 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_exampl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1879,test,test,1879,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"oot root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${samp",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1960,test,test,1960,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"ope-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}"". 1. I have quoted #set -euo pipefail out as it returns an error.; 2. The bed file is located in a public bucket #119 ; 3. I have tried with docker image 0.7.1 which returns following error:. [12/12/2018 14:14:08 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 14:37:23 ERROR gcp_deepvariant_runner.py] Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/5187520767668161022"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-west1-*', '--name', 'call_variants', '--vm-labels', 'dv-job-name=call_variants', '--output', 'gs://ms_bam/deep_output/stage/logs/call_variants/0', '--image', 'gcr.io/deepvariant-docker/deepvariant:0.7.1', '--inputs', 'EXAMPLES=gs://ms_bam/deep_output/stage/examples/0/*', '--outputs', 'CALLED_VARIANTS=gs://ms_bam/deep_output/stage/called_variants/*', '--machine-type', 'custom-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; Traceback (most recent call last):;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:2577,log,logs,2577,,https://github.com/google/deepvariant/issues/129,1,['log'],['logs']
Testability,"or PacificBiosciences/HiFiTargetEnrichment#4 , since i cant find what the error is and how to fix it i opend the Issue. Many thanks in advance. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: 1.5.0; - Tensorflow 2.11.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: PacBio HIFI reads. **Steps to reproduce:**; ```; rule deepvariant_make_examples:; input:; bam=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam"",; bai=f""batches/{batch}/{{sample}}/aligned/{{sample}}.{ref}.bam.bai"",; reference=config[""ref""][""fasta""],; output:; tfrecord=temp(; f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord-{{shard}}-of-{config['N_SHARDS']:05}.gz""; ),; nonvariant_site_tfrecord=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord-{{shard}}-of-{config['N_SHARDS']:0>; log:; f""batches/{batch}/logs/deepvariant/make_examples/{{sample}}.{ref}.{{shard}}-of-{config['N_SHARDS']:05}.log"",; benchmark:; f""batches/{batch}/benchmarks/deepvariant/{{sample}}.{{shard}}.dv_make_examples.tsv""; container:; f""docker://google/deepvariant:{config['DEEPVARIANT_VERSION']}""; params:; vsc_min_fraction_indels=""0.12"",; pileup_image_width=199,; shard='{shard}',; examples=f""batches/{batch}/{{sample}}/deepvariant/examples/examples.tfrecord@{config['N_SHARDS']}.gz"",; gvcf=f""batches/{batch}/{{sample}}/deepvariant/examples/gvcf.tfrecord@{config['N_SHARDS']}.gz"",; message:; ""DeepVariant make_examples {wildcards.shard} for {input.bam}.""; shell:; """"""; sleep 180; (/opt/deepvariant/bin/make_examples \; --add_hp_channel \; --alt_aligned_pileup=diff_channels \; --min_mapping_quality=1 \; --parse_sam_aux_fields \; --partition_size=25000 \; --max_reads_per_partition=600 \; --phase_reads \; --pileup_image_width {params.pileup_image_width} \; --norealign_reads \; --sort_by_haplotypes \; --track_ref_reads \; --vsc_min_fraction_indels {params.vsc_min_fraction_indels} \; --mode calling \; --ref {input.reference} \; --reads {input.bam} \;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/677:1269,log,log,1269,,https://github.com/google/deepvariant/issues/677,3,"['benchmark', 'log']","['benchmark', 'benchmarks', 'log']"
Testability,"or. I0624 02:14:00.095050 47429297437696 run_deepvariant.py:313] Creating a directory for intermediate results in /output/intermediate_results_dir; I0624 02:14:01.826225 47429297437696 run_deepvariant.py:405] Creating a directory for logs in /output/logs; I0624 02:14:01.954994 47429297437696 run_deepvariant.py:227] Creating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --infi",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465:1197,log,logs,1197,,https://github.com/google/deepvariant/issues/465,1,['log'],['logs']
Testability,"or/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7; 2020-09-24 03:47:45.652085: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] **Could not create cudnn handle:** CUDNN_STATUS_INTERNAL_ERROR; 2020-09-24 03:47:45.654628: E tensorflow/stream_executor/cuda/cuda_dnn.cc:329] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR; Traceback (most recent call last):; File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1365, in _do_call; return fn(*args); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1350, in _run_fn; target_list, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1443, in _call_tf_sessionrun; run_metadata); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[{{node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D}}]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_dgqnmzud/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:12476,log,log,12476,,https://github.com/google/deepvariant/issues/358,1,['log'],['log']
Testability,ore.py:257] Task 10/32: Preparing inputs; I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']; I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs; I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']; I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']; I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs; I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:9873,test,testdata,9873,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"paedyl01/disk1/yangyxt/test_tmp/singularity_run_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/Bazel.runfiles_dtfjmfk6/runfiles/absl_py/absl/app.py"", line 300 in run; File ""/paedyl01/disk1/yangyxt/test_tmp/singularity_run_aRMiURV3spzCOiBE1ojvo_fj2J05oPk1/Bazel.runfiles_dtfjmfk6/runfiles/com_google_deepvariant/deeptrio/make_examples.py"", line 312 in <module>; parallel: This job failed:; /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref /paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta --reads_parent1 /paedyl01/disk1/yangyxt/wesplus/50_samples_20220304/aligned_results/A210126.deduped.bam --reads /paedyl01/disk1/yangyxt/wesplus/50_sampl. real 1154m27.406s; user 287m31.460s; sys 2m20.121s; I0509 05:42:42.322269 47221860157248 run_deeptrio.py:674] None. Traceback (most recent call last):; File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 688, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/deeptrio/run_deeptrio.py"", line 672, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.8/subprocess.py"", line 364, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 5 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/deeptrio/make_examples --mode calling --ref ""/paedyl01/disk1/yangyxt/indexed_genome/ucsc.hg19.fasta"" --reads_parent1 ""/paedyl01/disk1/yangyxt/wesplus/50_samples_; Line 770: In function call_deeptrio_per_pair: Tue May 9 05:42:52 HKT 2023: Failed on running singularity DeepTrio. Quit with error; `. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/646:4831,test,test,4831,,https://github.com/google/deepvariant/issues/646,2,['test'],['test']
Testability,parallel: This job failed reproducing the test example,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/325:42,test,test,42,,https://github.com/google/deepvariant/issues/325,1,['test'],['test']
Testability,"ples_runner; regions, calling_regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 2019, in processing_regions_from_options; ref_contigs = fasta.IndexedFastaReader(; File ""/tmp/Bazel.runfiles_nkfcw9hw/runfiles/com_google_deepvariant/third_party/nucleus/io/fasta.py"", line 106, in __init__; self._reader = reference.IndexedFastaReader.from_file(; ValueError: NOT_FOUND: could not load fasta and/or fai for fasta /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa --reads /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam --examples /tmp/tmpfab4tpv7/make_examples.tfrecord@32.gz --channels insert_size --gvcf /tmp/tmpfab4tpv7/gvcf.tfrecord@32.gz --task 18. - ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Yes, the quick test run as normal.; ```. 3. reference index does; ```$ ls /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa*; -rw-rw-r-- 1 zhoujianglin zhoujianglin 3189750467 Apr 26 14:53 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6274909010 Apr 26 15:23 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.0123; -rw-rw-r-- 1 zhoujianglin zhoujianglin 106669 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.amb; -rw-rw-r-- 1 zhoujianglin zhoujianglin 6924 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.ann; -rw-rw-r-- 1 zhoujianglin zhoujianglin 10196727247 Apr 26 15:42 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.bwt.2bit.64; -rw-rw-r-- 1 zhoujianglin zhoujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:7033,test,test,7033,,https://github.com/google/deepvariant/issues/653,2,['test'],['test']
Testability,"python/client/session.py"", line 956, in run; run_metadata_ptr); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run; feed_dict_tensor, options, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run; run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 300, in run; _run_main(main, args); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/absl_py/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"",",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:16095,log,log,16095,,https://github.com/google/deepvariant/issues/358,1,['log'],['log']
Testability,p|InceptionV3/Mixed_7b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6c/Branch_1/Conv2d_0b_1x7/weights/RMSProp|InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7a/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance|InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_2/Conv2d_0d_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/weights|InceptionV3/Mixed_6e/Branch_1/Conv2d_0c_7x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6a/Branch_1/Conv2d_0a_1x1/weights/RMSProp_1|InceptionV3/Logits/Conv2d_1c_1x1/biases/RMSProp|InceptionV3/Mixed_5d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_6c/Branch_2/Conv2d_0c_1x7/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/weights|InceptionV3/Mixed_6b/Branch_1/Conv2d_0b_1x7/weights/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights/RMSProp_1|InceptionV3/Mixed_6c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_2/Conv2d_0e_1x7/weights/RMSProp_1|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weigh,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:52227,Log,Logits,52227,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,"quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs; **********; I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op.; INFO:tensorflow:Reloading EMA...; I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA...; INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt; I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters from /opt/models/wgs/model.ckpt; I0519 16:22:48.504039 139665862911808 call_variants.py:462] Processed 1 examples in 1 batches [632.440 sec per 100]; I0519 16:22:48.735930 139665862911808 call_variants.py:468] Processed 305 examples in 1 batches [2.084 sec per 100]; I0519 16:22:48.736088 139665862911808 call_variants.py:471] Done calling variants from a total of 305 examples. real 0m8.934s; user 0m37.643s; sys 0m6.426s. ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/tmp/tmp6gzkras0/call_variants_output.tfrecord.gz"" --outfile ""singularity-output/output.vcf.gz"" --nonvariant_site_tfrecord_path ""/tmp/tmp6gzkras0/gvcf.tfrecord@32.gz"" --gvcf_outfile ""singularity-output/output.g.vcf.gz"". 2023-05-19 16:22:49.638487: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0519 16:22:51.170790 140640470185792 postprocess_variants.py:972] Using sample name from call_variants output. Sample name: NA12878; 2023-05-19 16:22:51.171487: I deepvariant/postprocess_variants.cc:88] Read from: /tmp/tmp6gzkras0/call_variants_output.tfrecord.gz; 2023-05-19 16:22:51.173038: I deepvariant/postprocess_vari",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:12889,test,testdata,12889,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 test.gvcf.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 5831798 Feb 6 18:18 test.gvcf.tfrecord-00063-of-00064.gz. ```. Surprisingly, this was generated using the following command:. ```; ## Run `make_examples`; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; e",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:2122,test,test,2122,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"realign_reads --sort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 7. real 133m6.986s; user 230m53.643s; sys 1m59.690s; I0425 04:55:14.754644 140234455082752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 19 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/cromwell_root/gcp-public-data--broad-references/hg19/v0/Homo_sapiens_assembly19.fasta"" --reads ""/cromwell_root/fc-13e1404e-623c-489f-956c-b388fa9fb975/bams/HG001.SequelII.pbmm2.hs37d5.whatshap.haplotag.RTG.trio.bam"" --examples ""/cromwell_root/tmp.e4eeba80/tmpphthddeo/make_examples.tfrecord@20.gz"" --add_hp_channel --alt_aligned_pileup ""diff_channels"" --parse_sam_aux_fields --norealign_reads --sort_by_haplotypes --vsc_min_fraction_indels ""0.12"" --task {} )' returned non-zero exit status 247.; 2021/04/25 04:55:23 Starting delocalization.; ```; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; `Yes. I tested the PacBio case study on chr20 on my system. It ran through without any problems`; **Any additional context:**; `The total error log is quite lengthy but mostly just logging make_examples.py. I can share with you the Terra workflow which you can reproduce. To do that, I probably need your email address. `",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/446:3649,test,test,3649,,https://github.com/google/deepvariant/issues/446,5,"['log', 'test']","['log', 'logging', 'test', 'tested']"
Testability,"reating a make_examples runtime by region directory in /output/logs/make_examples_runtime_by_region. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --reads ""/input/S-001737188.markdup.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --runtime_by_region ""/output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --task {} ) 2>&1 | tee /output/logs/make_examples.log. parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --reads /input/S-001737188.markdup.bam --examples /output/intermediate_results_dir/make_examples.tfrecord@1.gz --runtime_by_region /output/logs/make_examples_runtime_by_region/make_examples_runtime@1.tsv --gvcf /output/intermediate_results_dir/gvcf.tfrecord@1.gz --task 0. real	14m5.230s; user	0m1.869s; sys	0m3.689s. ***** Running the command:*****; ( time /opt/deepvariant/bin/call_variants --outfile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --use_openvino ) 2>&1 | tee /output/logs/call_variants.log. real	6m35.370s; user	0m1.385s; sys	0m1.152s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa"" --infile ""/output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/output/S-001737188.vcf.gz"" --nonvariant_site_tfrecord_path ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --gvcf_outfile ""/output/S-001737188.g.vcf.gz"" ) 2>&1 | tee /output/logs/postprocess_variants.log. real	10m14.442s; user	0m",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/465:1492,log,logs,1492,,https://github.com/google/deepvariant/issues/465,1,['log'],['logs']
Testability,"regions_from_options; options.exclude_calling_regions); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1065, in build_calling_regions; ranges.RangeSet.from_regions(regions_to_include, contig_dict)); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 161, in from_regions; return cls(ranges=from_regions(regions, contig_map=contig_map)); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 113, in __init__; for i, range_ in enumerate(ranges):; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 493, in from_regions; for elt in reader(region):; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/util/ranges.py"", line 458, in bed_parser; with bed.BedReader(filename) as fin:; File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 127, in _native_reader; return NativeBedReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_qccapoh4/runfiles/com_google_deepvariant/third_party/nucleus/io/bed.py"", line 104, in __init__; self._reader = bed_reader.BedReader.from_file(bed_path, options); ValueError: Not found: Could not open /opt/command/test_dir/part_0.bed; **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; My part_0.bed has 5 columns and goes error; chrm start end name (option); 1 0 1005000 0 5000; But when I change my part_0.bed to 4 columns , it works; chrm start end name ; 1 0 1005000 0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/374:3942,test,test,3942,,https://github.com/google/deepvariant/issues/374,2,['test'],['test']
Testability,"remove and 1 not upgraded.; N: Ignoring file 'google-chrome.list.1' in directory '/etc/apt/sources.list.d/' as it has an invalid filename extension; Requirement already up-to-date: pip in /usr/local/lib/python2.7/dist-packages (18.0); ========== [2018年 08月 24日 星期五 19:54:09 CST] Stage 'Install python packages' starting; Requirement already satisfied: contextlib2 in /usr/local/lib/python2.7/dist-packages (0.5.5); Requirement already satisfied: enum34 in /usr/local/lib/python2.7/dist-packages (1.1.6); Requirement already satisfied: sortedcontainers==1.5.3 in /usr/local/lib/python2.7/dist-packages (1.5.3); Requirement already satisfied: intervaltree in /usr/local/lib/python2.7/dist-packages (2.1.0); Requirement already satisfied: sortedcontainers in /usr/local/lib/python2.7/dist-packages (from intervaltree) (1.5.3); Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (2.0.0); Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (4.2.0); Requirement already satisfied: funcsigs>=1; python_version < ""3.3"" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.0.2); Requirement already satisfied: six>=1.9 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0) (1.11.0); Requirement already satisfied: numpy==1.14 in /usr/local/lib/python2.7/dist-packages (1.14.0); Requirement already satisfied: requests>=2.18 in /usr/local/lib/python2.7/dist-packages (2.19.1); Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2018.8.13); Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (3.0.4); Requirement already satisfied: urllib3<1.24,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (1.23); Requirement already satisfied: idna<2.8,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests>=2.18) (2.7); Requirement already satisfied: scip",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:14010,mock,mock,14010,,https://github.com/google/deepvariant/issues/89,1,['mock'],['mock']
Testability,rev_var_name: Unchanged; I0415 07:34:38.032978 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0d_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.033324 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.033725 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034166 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_7a/Branch_1/Conv2d_1a_3x3/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.034531 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_0/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.034941 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Logits/Conv2d_1c_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035353 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6a/Branch_1/Conv2d_1a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.035823 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6d/Branch_2/Conv2d_0c_1x7/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036331 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.036871 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6b/Branch_2/Conv2d_0b_7x1/BatchNorm/beta; prev_var_name: Unchanged; I0415 07:34:38.037236 140368878327552 warm_starting_util.py:466] Warm-starting variable: InceptionV3/Mixed_6e/Branch_2/Conv2d_0a_1x1/weights; prev_var_name: Unchanged; I0415 07:34:38.037589 140368878327552 warm_starting_util.py:466] Warm-starting variable: Inceptio,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:110444,Log,Logits,110444,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,"roject ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam file? The .fai file was created from the .fa file using samtools index command. . ValueError: Reference contigs span 3270284521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""CM000663.2"" is 248956422 bp and IS MISSING, ""KI270706.1"" is 175055 bp and IS MISSING, ""KI270707.1"" is 32032 bp and IS MISSING, ""KI270708.1"" is 127682 bp and IS MISSING, ""KI270709.1"" is 66860 bp and IS MISSING, ""KI270710.1"" is 40176 bp and IS MISSING... Any feedback would be appreciated. Thanks, -Matt. [staging_folder1_logs_make_examples_7.txt](https://github.com/google/deepvariant/files/3700231/staging_folder1_logs_make_examples_7.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/225:1819,log,log,1819,,https://github.com/google/deepvariant/issues/225,1,['log'],['log']
Testability,"ry using our standalone computational resources.; I referred and followed https://github.com/google/deepvariant/blob/r0.7/docs/deepvariant-tpu-training-case-study.md . First, I successfully made tfrecord file using DeepVariant v0.7.1 (docker). Then I tried shuffling tfrecords as follows:. #!/bin/sh; ; INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 19`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done; ; /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. This script made errors [(please see this file)](https://github.com/google/deepvariant/files/2708579/shuffle.train.0_id_masked.log). However, the codes which used chromosome-divided tfrecords (chr1-10 and chr11-chr19) worked fine as follows:. INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 10`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. and. INPUT_PATTERN_LIST=examples.train/sample_id/11/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 12 19`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --inpu",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/133:1203,log,log,1203,,https://github.com/google/deepvariant/issues/133,1,['log'],['log']
Testability,"s local:8/0/100%/0.0s docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled""; ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):; ```; docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant; See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md.; ```; I do have the `gcloud alpha genomics pipelines` example working, so this isn’t absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google C",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:7139,test,test,7139,,https://github.com/google/deepvariant/issues/171,2,['test'],['test']
Testability,s to reproduce:**; - Command:. projDir=/home1/***/***/deepvaraint/; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds.; Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:1423,test,test,1423,,https://github.com/google/deepvariant/issues/717,1,['test'],['test']
Testability,"s(; File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 976, in reservoir_sample_reads; return utils.reservoir_sample(iterable_of_reads, k, random); File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/util/utils.py"", line 117, in reservoir_sample; for i, item in enumerate(iterable):; File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 95, in __next__; record, not_done = self._raw_next(); File ""/tmp/Bazel.runfiles_rrr7jrkj/runfiles/com_google_deepvariant/third_party/nucleus/io/clif_postproc.py"", line 154, in _raw_next; not_done = self._cc_iterable.PythonNext(record); RuntimeError: PythonNext() argument read is not valid: Dynamic cast failed; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /work/cjm124/SWFst/DeepVariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /work/cjm124/SWFst/DeepVariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/make_examples.tfrecord@12.gz --channels insert_size --gvcf /work/cjm124/SWFst/DeepVariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@12.gz --regions chr20:10,000,000-10,010,000 --task 0; ```. **Does the quick start test work on your system?** No. Is there any way to reproduce the issue by using the quick start? . I first observed this issue when trying to use my own data, but have the same issue with quickstart and above command. I found a prior issue (#559) and tried the suggested solution of explicitly installing nucleus. The commands and error from that is below:. commands:. ```; singularity exec DeepVariant_1.6.1.sif bash; pip install --user google-nucleus; run_deepvariant --model_type=WGS \; 	--ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; 	--reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; 	--regions ""chr20:10,00",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/812:3875,test,testdata,3875,,https://github.com/google/deepvariant/issues/812,1,['test'],['testdata']
Testability,"s/tensorflow_core/python/training/monitored_session.py"", line 1418, in run; run_metadata=run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py"", line 1176, in run; return self._sess.run(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 956, in run; run_metadata_ptr); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1180, in _run; feed_dict_tensor, options, run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1359, in _do_run; run_metadata); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py"", line 1384, in _do_call; raise type(e)(node_def, op, message); tensorflow.python.framework.errors_impl.UnknownError: 2 root error(s) found.; (0) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; (1) Unknown: Failed to get convolution algorithm. This is probably because cuDNN failed to initialize, so try looking to see if a warning log message was printed above.; 	 [[node InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D (defined at usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1751) ]]; 	 [[softmax_tensor_1/_3035]]; 0 successful operations.; 0 derived errors ignored. Original stack trace for 'InceptionV3/InceptionV3/Conv2d_1a_3x3/Conv2D':; File ""tmp/Bazel.runfiles_dgqnmzud/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 491, in <module>; tf.compat.v1.app.run(); File ""usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolera",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/358:15771,log,log,15771,,https://github.com/google/deepvariant/issues/358,1,['log'],['log']
Testability,s_core.py:257] Task 21/32: Common contigs are ['chr20']; I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']; I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs; I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']; I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs; I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']; I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.289134 140638923466560 make_examples_cor,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:10739,test,testdata,10739,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,s_core.py:257] Task 6/32: Common contigs are ['chr20']; I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.279789 140552972691264 make_examples_core.py:257] Task 8/32: Preparing inputs; I0519 16:22:23.281702 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.282378 140552972691264 make_examples_core.py:257] Task 8/32: Common contigs are ['chr20']; I0519 16:22:23.271428 140087439025984 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192873 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.234176 139704511100736 make_examples_core.py:257] Task 12/32: Preparing inputs; I0519 16:22:23.236589 139704511100736 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.237330 139704511100736 make_examples_core.py:257] Task 12/32: Common contigs are ['chr20']; I0519 16:22:23.237694 140638923466560 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.289134 140638923466560 make_examples_core.py:257] Task 29/32: Preparing inputs; I0519 16:22:23.215111 139798323177280 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.265897 139798323177280 make_examples_core.py:257] Task 9/32: Preparing inputs; **********; I0519 16:22:46.781010 139665862911808 session_manager.py:529] Done running local_init_op.; INFO:tensorflow:Reloading EMA...; I0519 16:22:47.119282 139665862911808 modeling.py:418] Reloading EMA...; INFO:tensorflow:Restoring parameters from /opt/models/wgs/model.ckpt; I0519 16:22:47.119841 139665862911808 saver.py:1410] Restoring parameters,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:11369,test,testdata,11369,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"s_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1419, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1300, in main; sample_name = get_sample_name(); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1203, in get_sample_name; _, record = get_cvo_paths_and_first_record(); File ""/tmp/Bazel.runfiles_t3t5ek8u/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1179, in get_cvo_paths_and_first_record; raise ValueError(; ValueError: ('Found multiple file patterns in input filename space: ', './call_variants_output.tfrecord.gz'). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; ???. **Any additional context:**; Yes. I can change the parameter ""--infile"" of the postprocess_variants.py call from ""./call_variants_output.tfrecord.gz"" to ""./call_variants_output@1.tfrecord.gz"" and it works. Anyway, the call of postprocess_variants.py is auto-generated by ""/opt/deepvariant/bin/run_deepvariant"". The error does not occur for every sample ... directory content of intermediate_results_dir after the error occured:; call_variants.log; call_variants_output-00000-of-00001.tfrecord.gz; gvcf.tfrecord-00000-of-00008.gz; gvcf.tfrecord-00001-of-00008.gz; gvcf.tfrecord-00002-of-00008.gz; gvcf.tfrecord-00003-of-00008.gz; gvcf.tfrecord-00004-of-00008.gz; gvcf.tfrecord-00005-of-00008.gz; gvcf.tfrecord-00006-of-00008.gz; gvcf.tfrecord-00007-of-00008.gz; make_examples.log; make_examples.tfrecord-00000-of-00008.g",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/818:1580,test,test,1580,,https://github.com/google/deepvariant/issues/818,2,['test'],['test']
Testability,"scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Launcher: Job 6 completed in 0 seconds.; Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); Launcher: Job 7 completed in 0 seconds.; FATAL: could not open image /opt/deepvariant/bin/run_deepvariant: failed to retrieve path for **/opt/deepvariant/bin/run_deepvariant: lstat /opt/deepvariant:** no such file or directory; Launcher: Job 8 completed in 0 seconds.; Launcher: Task 0 done. Exiting.; Launcher: Task 1 done. Exiting.; I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test; I1014 18:52:08.193618 23054196500288 run_deepvariant.py:364] Re-using the directory for intermediate results in /scratch/***/***/deepvariant_test/test/output_test. **Any additional context:**. How to",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:3464,test,test,3464,,https://github.com/google/deepvariant/issues/717,1,['test'],['test']
Testability,"sfully through the make_example step with 64 shards, and have produced 64 examples and gvcf files like so:; ```; -rw-r--r-- 1 root root 14394035 Feb 6 18:18 test.examples.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 16089657 Feb 6 18:18 test.examples.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 14238866 Feb 6 18:18 test.examples.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 14484530 Feb 6 18:19 test.examples.tfrecord-00003-of-00064.gz; ...; -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1055,test,test,1055,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,"sk=64"" --jobs 64 --profie profile/ ```(where profile holds singularity args etc.). Singularity image is deepvariant_1.4.0.sif. my Snakemake rule:. ```; rule deepvariant:; input:; bam=rules.apply_bqsr.output.bam,; ref='/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta'; output:; vcf=""results/deepvariant/{sample}.vcf.gz""; params:; model=""WES""; threads: ; 64; resources:; mem_mb=163840; log:; ""logs/deepvariant/{sample}/stdout.log""; singularity:; ""singularity/deepvariant_1.4.0.sif""; # ""singularity/deepvariant_1.4.0-gpu.sif"" # for GPU; shell:; """"""; /opt/deepvariant/bin/run_deepvariant --model_type {params.model} --ref {input.ref} --reads {input.bam} --output_vcf {output.vcf} --num_shards {threads} --make_examples_extra_args='vsc_min_count_snps=3,vsc_min_fraction_snps=0.2,vsc_min_count_indels=3,vsc_min_fraction_indels=0.10'; """"""; ```. Below is the begening and end of the log file. I am happy to include the entire log file but there is nothing out of the ordinary between those lines below (same output as for jobs that finished successfully). Could you please advise on what parameters to change to successfully run DeepVariant by submitting it to the SLURM scheduler? ; I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs...; Using shell: /usr/bin/bash; Provided cores: 64; Rules claiming more threads will be scaled down.; Select jobs to execute... > [Wed Jan 4 18:30:51 2023]; > rule deepvariant:; > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta; > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz; > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log; > jobid: 0; > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2; > threads: 64; > resourc",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:2049,log,log,2049,,https://github.com/google/deepvariant/issues/602,1,['log'],['log']
Testability,"sl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time /opt/deepvariant/bin/call_variants --outfile ""/tmp/deepvariant_tmp_output/call_variants_output.tfrecord.gz"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@20.gz"" --checkpoint ""/input/mosquito_model/model.ckpt""' returned non-zero exit status 1; ```; However, the directory mosquito_model does exist and contains. `model.ckpt-97700.data-00000-of-00001 model.ckpt-97700.index model.ckpt-97700.meta`. The model file was providen to me by your colleague Andrew Carroll. Is there a way to check the files are ""correct""? . EDIT: here is the script . ```; #!/usr/bin/zsh; OUTPUT_DIR=""${PWD}/ARCcestor_mosquito_model""; mkdir -p ""${OUTPUT_DIR}""; INPUT_DIR=""${PWD}""; BIN_VERSION=""0.9.0""; N_SHARDS=20; LOG_DIR=""${OUTPUT_DIR}/logs"" ; mkdir -p ""${LOG_DIR}"" ; #declare -a decade=(ARCcestor D2A1 D2B3 D3A1 D4A3 D5B3 H2A3 H2C3 H4A4 H4C2 H5A3); #for SAMPLE in ""${decade[@]}""; #do; # BAM=${SAMPLE}.sorted.bam. #OUTPUT_VCF=${SAMPLE}.vcf.gz; #OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time (docker run -v ""${INPUT_DIR}"":""/input"" -v ""${OUTPUT_DIR}:/output"" google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/input/shasta_final.fa --reads=""/input/ARCcestor.sorted.bam"" --regions=""/input/ARCcestor.bed"" --output_vcf=""/output/${OUTPUT_VCF}"" --output_gvcf=""/output/${OUTPUT_GVCF}"" --num_shards=""${N_SHARDS}"" --customized_model=""/input/mosquito_model/model.ckpt"" ) 2>&1 | tee -a ""${LOG_DIR}/make_examples.log""; #done; ```. _Originally posted by @aderzelle in https://github.com/google/deepvariant/issues/268#issuecomment-586584341_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/273:3900,log,logs,3900,,https://github.com/google/deepvariant/issues/273,2,['log'],"['log', 'logs']"
Testability,"sr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open /home/platon/test/seq1.bam. real	0m1.571s; user	0m1.481s; sys	0m0.786s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/219:4780,test,test,4780,,https://github.com/google/deepvariant/issues/219,1,['test'],['test']
Testability,"ss.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/_0_Диссертация/Exp/seq1/seq1.fa"" --reads ""/home/platon/_0_Диссертация/Exp/seq1/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}' returned non-zero exit status 1; ```. Second attempt. This time with paths consisting only of latin characters.; `sudo docker run gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/home/platon/test/seq1.fa --reads=/home/platon/test/seq1.bam --output_vcf=/home/platon/test/seq1.vcf`. ```; ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/home/platon/test/seq1.fa"" --reads ""/home/platon/test/seq1.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --task {}. [E::hts_open_format] Failed to open file /home/platon/test/seq1.bam; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1186, in main; options = default_options(add_flags=True, flags_obj=FLAGS); File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 315, in default_options; with sam.SamReader(flags_obj.reads) as sam_reader:; File ""/tmp/Bazel.runfiles_AruJXP/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 216, in __init__; self._reader = self._native_reader(input_path, **k",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/219:3369,test,test,3369,,https://github.com/google/deepvariant/issues/219,1,['test'],['test']
Testability,"ssw_init; | ; --0.92%--qP_byte. 28.27% , 0.04% ,python ,libssw.so ,[.] ssw_align; | ; --28.23%--ssw_align; | ; |--14.88%--sw_sse2_word; | ; |--8.45%--sw_sse2_byte; | ; |--2.89%--banded_sw; | ; --1.19%--__memcpy_sse2_unaligned. 14.88% , 14.86% ,python ,libssw.so ,[.] sw_sse2_word; | ; --14.86%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --14.86%--ssw_align; sw_sse2_word. 8.45% , 8.43% ,python ,libssw.so ,[.] sw_sse2_byte; | ; --8.43%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --8.43%--ssw_align; sw_sse2_byte. 4.72% , 0.00% ,python ,[unknown] ,[.] 0x00000000009063e0; |; ---0x9063e0; | ; --3.94%--PyEval_EvalFrameEx; | ; --3.57%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | ; --3.32%--learning::genomics::deepvariant::DeBruijnGraph::Build; | ; --3.02%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | ; --2.63%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | ; --1.89%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | ; --1.60%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | ; --0.56%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. To do this properly would require that the tests be performed on different datasets, and different CPUs on the same Cloud environment - with different distributed scenarios - which would be cost-prohibitive for me. Hope it helps and have a great weekend!; Paul",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:12308,test,tests,12308,,https://github.com/google/deepvariant/issues/50,1,['test'],['tests']
Testability,"start-output/examples.tfrecord.gz; 2018-12-20 07:17:31.684869: I third_party/nucleus/io/sam_reader.cc:561] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2018-12-20 07:17:31.688126: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.688252 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.884236 140029649073920 make_examples.py:1119] Task 0: 6 candidates (6 examples) [0.20s elapsed]; I1220 07:17:33.209975 140029649073920 make_examples.py:1134] Writing MakeExamplesRunInfo to /home/chungtsai_su/quickstart-output/examples.tfrecord.gz.run_info.pbtxt; I1220 07:17:33.241107 140029649073920 make_examples.py:1137] Found 76 candidate variants; I1220 07:17:33.241497 140029649073920 make_examples.py:1138] Created 82 examples; ```. Also, the problem can be detected in test case. ```; //deepvariant/realigner/python:ssw_misc_test PASSED in 0.3s; //deepvariant/realigner/python:ssw_wrap_test PASSED in 0.3s; //deepvariant/vendor:timer_test PASSED in 0.8s; //deepvariant:make_examples_test FAILED in 2 out of 2 in 1.7s; Stats over 2 runs: max = 1.7s, min = 1.6s, avg = 1.7s, dev = 0.1s; /home/chungtsai_su/.cache/bazel/_bazel_chungtsai_su/959496e1d4e585c03b8886e389170de9/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_2_of_2/test.log; /home/chungtsai_su/.cache/bazel/_bazel_chungtsai_su/959496e1d4e585c03b8886e389170de9/execroot/com_google_deepvariant/bazel-out/k8-opt/testlogs/deepvariant/make_examples_test/shard_1_of_2/test.log; //deepvariant:model_eval_test PASSED in 49.7s; Stats over 10 runs: max = 49.7s, min = 2.6s, avg = 9.1s, dev = 13.6s; //deepvariant:model_train_test PASSED in 127.0s; Stats over 10 runs: max = 127.0s, min = 2.7s, avg = 42.5s, dev = 47.3s. Executed 38 out of 38 tests: 37 tests pass and 1 fails locally.; (06:34:35) INFO: Build completed, 1 test FAILED, 2471 total actions; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131:5826,test,testlogs,5826,,https://github.com/google/deepvariant/issues/131,9,"['log', 'test']","['log', 'test', 'testlogs', 'tests']"
Testability,"stats are reported for homref though. I have now tried running the training several times with different hyperparameters but so far still no change at the het or homalt eval stats. . My first, very simple question is thus, are these eval stats truly 0 (i.e. the model is very bad) or is 0.0 some starting value and there are not enough data to calculate them initially? I am warmstarting from the 1.6.1 wgs model so I cant imagine the model is really that bad at calling variants initially, even if in a fish. . **Setup**; Running on a university computing cluster (https://hpc-unibe-ch.github.io/) ; OS: Rocky 9.3 Blue Onyx; GPU: rtx4090 ; Installation: Running from Docker image via singularity; DV version: 1.6.1. **Data**; I am training on examples from 5 individuals, data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had hig",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:1292,test,test,1292,,https://github.com/google/deepvariant/issues/876,1,['test'],['test']
Testability,"su/.local/lib/python2.7/site-packages/intervaltree-3.0.2.dist-info/*; /home/chungtsai_su/.local/lib/python2.7/site-packages/intervaltree/*; Proceed (y/n)? Y; Successfully uninstalled intervaltree-3.0.2; chungtsai_su@seqslab:~/src/deepvariant$ pip install --user 'intervaltree==2.1.0'; Collecting intervaltree==2.1.0; Requirement already satisfied: sortedcontainers in /home/chungtsai_su/.local/lib/python2.7/site-packages (from intervaltree==2.1.0) (2.1.0); Installing collected packages: intervaltree; Successfully installed intervaltree-2.1.0; ```; Then the problem is solved. ; ```; chungtsai_su@seqslab:~/src/deepvariant$ ./bazel-bin/deepvariant/make_examples --mode calling --ref ""${REF}"" --reads ""${BAM}"" --regions ""chr20:10,000,000-10,010,000"" --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; 2018-12-20 07:17:31.678190: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.678396 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.681643 140029649073920 make_examples.py:1080] Preparing inputs; 2018-12-20 07:17:31.682071: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.682173 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I1220 07:17:31.682885 140029649073920 make_examples.py:996] Common contigs are [u'chr20']; I1220 07:17:31.684022 140029649073920 make_examples.py:1086] Writing examples to /home/chungtsai_su/quickstart-output/examples.tfrecord.gz; 2018-12-20 07:17:31.684869: I third_party/nucleus/io/sam_reader.cc:561] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2018-12-20 07:17:31.688126: W third_party/nucleus/io/sam_reader.cc:525] Unrecognized SAM header type, ignoring:; I1220 07:17:31.688252 140029649073920 genomics_reader.py:213] Reading /home/chungtsai_su/quickstart-testdata/NA128",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/131:3849,test,testdata,3849,,https://github.com/google/deepvariant/issues/131,1,['test'],['testdata']
Testability,"sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 180, in <module>; app.run(main); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 160, in main; proto_utils.uses_fast_cpp_protos_or_die(); File ""/scratch/SlurmTMP/sukmb352.4618444/Bazel.runfiles_ipgylakz/runfiles/com_google_deepvariant/third_party/nucleus/util/proto_utils.py"", line 41, in uses_fast_cpp_protos_or_die; raise ValueError('Expected to be using C++ protobuf implementation '; ValueError: Expected to be using C++ protobuf implementation (api_implementation.Type() == ""cpp"") but it is python; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 14; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref Homo_sapiens_GRCh38_no_alts.fa.gz --reads Indiv_I33975_Sample_I33975-L2.dedup.bam --examples /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/make_examples.tfrecord@16.gz --gvcf /scratch/SlurmTMP/sukmb352.4618444/tmpdh6mqoql/gvcf.tfrecord@16.gz --regions xgen-exome-research-panel-targets-v2.bed --task 7. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/499:5477,test,test,5477,,https://github.com/google/deepvariant/issues/499,2,['test'],['test']
Testability,"sys.exit(main(argv)); File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 494, in main; call_variants(; File ""/workspaces/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/tasks/499fbb3f-82af-4cc5-825b-d0c6b15c72bd/deepvariant/Bazel.runfiles_ncuuffv4/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 374, in call_variants; raise ValueError(f'Shape mismatch in {example_info_json} and '; ValueError: Shape mismatch in ./examples.tfrecord-00000-of-00032.gz.example_info.json and /opt/models/pacbio/model.ckpt.example_info.json.; ```. My command line looks like this:; `export HOME=/root && N_SHARDS=32 && LOGDIR=/opt/deepvariant/logs/ && mkdir -p ""${LOGDIR}"" && ( /usr/bin/time seq 0 $((N_SHARDS-1)) | parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" python /opt/deepvariant/bin/make_examples.zip --mode calling --task {} --examples ""./examples.tfrecord@${N_SHARDS}.gz"" --add_hp_channel --add_hp_channel --alt_aligned_pileup diff_channels --downsample_fraction 0 --reads /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/HG002.merged.bam --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/GRCh38ERCC.ensembl.fasta --realign_reads --regions 20 --sample_name HG002 --split_skip_reads --vsc_min_count_indels 2 ) > ./make_examples.log 2>&1 && ( python /opt/deepvariant/bin/call_variants.zip --outfile ./call_variants_output.tfrecord.gz --examples ./examples.tfrecord@${N_SHARDS}.gz --checkpoint /opt/models/pacbio/model.ckpt --batch_size 512 --num_readers 32 ) > ./call_variants.log 2>&1 && ( python /opt/deepvariant/bin/postprocess_variants.zip --ref /Projects/b7b5cc2c-7194-40e7-8598-aeb7f670ad77/GRCh38ERCC.ensembl.fasta --infile ./call_variants_output.tfrecord.gz --outfile ./HG002.vcf ) > ./postprocess_variants.log 2>&1`. When I try and run with HYBRID model, everything goes ok. Do you have some input on this?. Thanks,; Raisa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/628:3166,log,log,3166,,https://github.com/google/deepvariant/issues/628,3,['log'],['log']
Testability,"t data . **Steps to reproduce:**; - Command: sudo docker run --platform linux/amd64 google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/quickstart-output/output.vcf.gz --output_gvcf=/quickstart-output/output.g.vcf.gz --intermediate_results_dir /quickstart-output/intermediate_results_dir --num_shards=1; - Error trace: (if applicable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; T",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/471:1360,test,testdata,1360,,https://github.com/google/deepvariant/issues/471,1,['test'],['testdata']
Testability,"t-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --examples /quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz --gvcf /quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz --regions chr20:10,000,000-10,010,000 --task 0. real	0m3.353s; user	0m3.542s; sys	0m0.718s; I0712 04:14:21.282448 274906666752 run_deepvariant.py:416] None; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 421, in <module>; app.run(main); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 299, in run; _run_main(main, args); File ""/usr/local/lib/python3.6/dist-packages/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 414, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python3.6/subprocess.py"", line 311, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command '( time seq 0 0 | parallel -q --halt 2 -",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/471:2005,test,testdata,2005,,https://github.com/google/deepvariant/issues/471,1,['test'],['testdata']
Testability,"t. Is it possible to have instruction for building deepvariant on Centos 7. . CLIF building error - I get the following error during installation using ./INSTALL.sh. . Scanning dependencies of target clif-matcher; [100%] Building CXX object clif/backend/CMakeFiles/clif-matcher.dir/matcher_main.cc.o; [100%] Linking CXX executable clif-matcher; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE[_ZTIN4llvm2cl4listINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEbNS0_6parserIS7_EEEE]+0x18): undefined reference to typeinfo for llvm::cl::Option' CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyIbEE[_ZTIN4llvm2cl15OptionValueCopyIbEE]+0x10): undefined reference to typeinfo for llvm:🆑:GenericOptionValue'; CMakeFiles/clif-matcher.dir/matcher_main.cc.o:(.data.rel.ro._ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE[_ZTIN4llvm2cl15OptionValueCopyINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEE]+0x10): undefined reference to typeinfo for llvm::cl::GenericOptionValue' libclifMatcher.a(ast.cc.o):(.data.rel.ro._ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE[_ZTIN4clif18TranslationUnitAST24ConversionFunctionFinderE]+0x10): undefined reference to typeinfo for clang::ast_matchers::MatchFinder::MatchCallback'. **Setup**; - Operating system: Centos 7; - DeepVariant version: Latest github version; - Installation method (Docker, built from source, etc.): building from source; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/380:2180,test,test,2180,,https://github.com/google/deepvariant/issues/380,2,['test'],['test']
Testability,"t/deepvariant/call_variants.py"", line 492, in main; use_tpu=FLAGS.use_tpu,; File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 323, in call_variants; first_example = tf_utils.get_one_example_from_examples_path(examples_filename); File ""/tmp/Bazel.runfiles_7of_f3z_/runfiles/com_google_deepvariant/deepvariant/tf_utils.py"", line 205, in get_one_example_from_examples_path; 'Cannot find matching files with the pattern ""{}""'.format(source)); ValueError: Cannot find matching files with the pattern ""/tmp/tmpgv2oy1s_/make_examples.tfrecord@8.gz"". real	0m2.022s; user	0m2.036s; sys	0m0.413s. ***** Running the command:*****; ( time /opt/deepvariant/bin/postprocess_variants --ref ""/input/ref.fa"" --infile ""/tmp/tmpgv2oy1s_/call_variants_output.tfrecord.gz"" --outfile ""/output/OUTPUT_VCF.vfc"" --nonvariant_site_tfrecord_path ""/tmp/tmpgv2oy1s_/gvcf.tfrecord@8.gz"" --gvcf_outfile ""/output/OUTPUT_GVCF.vfc"" ) 2>&1 | tee /output/logs/postprocess_variants.log. Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1184, in <module>; tf.compat.v1.app.run(); File ""/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py"", line 40, in run; _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/absl_py/absl/app.py"", line 299, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/absl_py/absl/app.py"", line 250, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1107, in main; sample_name = get_sample_name(); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1053, in get_sample_name; _, record = get_cvo_paths_and_first_record(); File ""/tmp/Bazel.runfiles_vdgo7zab/runfiles/com_google_deepvariant/deepvariant/po",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/435:2509,log,log,2509,,https://github.com/google/deepvariant/issues/435,1,['log'],['log']
Testability,"t_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds.; Launcher: Task 2 running job 3 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1); 2023-10-14 18:52:03.562000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; Launcher: Job 6 completed in 0 seconds.; Launcher: Task 0 running job 7 on c304-012.ls6.tacc.utexas.edu (apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/D",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:2398,test,test,2398,,https://github.com/google/deepvariant/issues/717,1,['test'],['test']
Testability,"tart running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads reads.bam \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; echo ""Done.""; echo; ```. Which was based on this example: https://github.com/google/deepvariant/blob/r0.7/scripts/run_wgs_case_study_docker.sh. I would have expected the naming scheme to match the pattern I specified instead of the 000*-of-00064... strange. Now I am trying to move on to the next step, but again having trouble figuring out how to deal with these multiple example files /sharding when passing them as inputs to the call_variants step. . In the example, it recommends:. ```; ## Run `call_variants`; echo ""Start running call_variants...Log will be in the terminal and also to ${LOG_DIR}/call_variants.log.""; ( time sudo docker run \; -v ""${BASE}"":""${BASE}"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${EXAMPLES}"" \; --checkpoint ""${MODEL}""; ) 2>&1 | tee ""${LOG_DIR}/call_variants.log""; echo ""Done.""; echo; ```. Is there some magic pattern recognition that knows to look for files of the format 000*-of-00064? Confused as to how I should do this; should I run call_variants on 64 separate machines, with each machine running a job on one of the sharded make_examples outputs? When I try incorporating the code recommended in the example workflow, I get the following error:. `ValueError: Cannot find matching files with the pattern ""test.examples.tfrecord@64.gz""`. So obviously not working out of the box as specified. But I'm not sure whether call_variants is intelligent to handle sharded examples or if I should be explicitly only running it once on each shard",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:3637,Log,Log,3637,,https://github.com/google/deepvariant/issues/151,1,['Log'],['Log']
Testability,"tern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. This script made errors [(please see this file)](https://github.com/google/deepvariant/files/2708579/shuffle.train.0_id_masked.log). However, the codes which used chromosome-divided tfrecords (chr1-10 and chr11-chr19) worked fine as follows:. INPUT_PATTERN_LIST=examples.train/sample_id/1/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 2 10`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. and. INPUT_PATTERN_LIST=examples.train/sample_id/11/sample_id.tfrecord-?????-of-00056.gz; for CHROM in `seq 12 19`; do; INPUT_PATTERN_LIST=""$INPUT_PATTERN_LIST,examples.train/sample_id/$CHROM/sample_id.tfrecord-?????-of-00056.gz""; done. /usr/bin/python ../../git/deepvariant-r0.7/tools/shuffle_tfrecords_beam.py \; --input_pattern_list=$INPUT_PATTERN_LIST \; --output_pattern_prefix=training_set.with_label.shuffled \; --output_dataset_config_pbtxt=training_set.dataset_config.pbtxt \; --output_dataset_name=sample_id \; --runner=DirectRunner > log/shuffle.train.log 2>&1. I cannot figure out the reason why this divided approach succeeded. Could you tell me any suggestions of points to check?; Because I'm using personal data under strict data management, I cannot provide or share our tfrecords. I used apache-beam v2.9.0 in python 2.7.5 (not in docker of DeepVariant v0.7.1).; If some version dependencies exist, I'd like to get some docker",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/133:1857,log,log,1857,,https://github.com/google/deepvariant/issues/133,1,['log'],['log']
Testability,test,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/426:0,test,test,0,,https://github.com/google/deepvariant/issues/426,1,['test'],['test']
Testability,test issue,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/1:0,test,test,0,,https://github.com/google/deepvariant/issues/1,1,['test'],['test']
Testability,test.examples.tfrecord-00003-of-00064.gz; ...; -rw-r--r-- 1 root root 15225527 Feb 6 18:18 test.examples.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 14663343 Feb 6 18:19 test.examples.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 14571664 Feb 6 18:19 test.examples.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 13704439 Feb 6 18:19 test.examples.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 14383355 Feb 6 18:18 test.examples.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 13559255 Feb 6 18:19 test.examples.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 16376740 Feb 6 18:19 test.examples.tfrecord-00062-of-00064.gz; -rw-r--r-- 1 root root 15276769 Feb 6 18:18 test.examples.tfrecord-00063-of-00064.gz; -rw-r--r-- 1 root root 5842718 Feb 6 18:18 test.gvcf.tfrecord-00000-of-00064.gz; -rw-r--r-- 1 root root 5860574 Feb 6 18:18 test.gvcf.tfrecord-00001-of-00064.gz; -rw-r--r-- 1 root root 5852289 Feb 6 18:18 test.gvcf.tfrecord-00002-of-00064.gz; -rw-r--r-- 1 root root 5845856 Feb 6 18:19 test.gvcf.tfrecord-00003-of-00064.gz; -rw-r--r-- 1 root root 5834861 Feb 6 18:18 test.gvcf.tfrecord-00004-of-00064.gz; -rw-r--r-- 1 root root 5812744 Feb 6 18:18 test.gvcf.tfrecord-00005-of-00064.gz; -rw-r--r-- 1 root root 5856643 Feb 6 18:19 test.gvcf.tfrecord-00006-of-00064.gz; ...; -rw-r--r-- 1 root root 5893279 Feb 6 18:19 test.gvcf.tfrecord-00054-of-00064.gz; -rw-r--r-- 1 root root 5850799 Feb 6 18:19 test.gvcf.tfrecord-00055-of-00064.gz; -rw-r--r-- 1 root root 5844041 Feb 6 18:18 test.gvcf.tfrecord-00056-of-00064.gz; -rw-r--r-- 1 root root 5816735 Feb 6 18:19 test.gvcf.tfrecord-00057-of-00064.gz; -rw-r--r-- 1 root root 5852875 Feb 6 18:19 test.gvcf.tfrecord-00058-of-00064.gz; -rw-r--r-- 1 root root 5820441 Feb 6 18:19 test.gvcf.tfrecord-00059-of-00064.gz; -rw-r--r-- 1 root root 5797526 Feb 6 18:18 test.gvcf.tfrecord-00060-of-00064.gz; -rw-r--r-- 1 root root 5893496 Feb 6 18:19 test.gvcf.tfrecord-00061-of-00064.gz; -rw-r--r-- 1 root root 5818504 Feb 6 18:19 te,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/151:1469,test,test,1469,,https://github.com/google/deepvariant/issues/151,1,['test'],['test']
Testability,test_make_labeler_ref fail due to mock data,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/154:34,mock,mock,34,,https://github.com/google/deepvariant/issues/154,1,['mock'],['mock']
Testability,"ticed. . Note, this is a simulated pathogenic variant from bamsurgeon, but the VCF representation is the focus of this problem. . Let's start with an IGV snapshot of the variant:; ![image](https://user-images.githubusercontent.com/16579982/154755554-3642728e-03c3-4c87-ba89-d66f0ecd6982.png). Now, I'll go into the representation from the DeepVariant --> GVCF --> GLnexus pipeline:. ## DeepVariant Pipeline:; ```; # Load singularity; module load singularity; BIN_VERSION=""1.1.0"". # Load env for bcftools; ANNOTATEVARIANTS_INSTALL=/mnt/common/WASSERMAN_SOFTWARE/AnnotateVariants/; source $ANNOTATEVARIANTS_INSTALL/opt/miniconda3/etc/profile.d/conda.sh; conda activate $ANNOTATEVARIANTS_INSTALL/opt/AnnotateVariantsEnvironment. # Pull latest version, if you already have it, this will be skipped; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Number of threads; NSLOTS=$SLURM_CPUS_PER_TASK. # Go to the submission directory (where the sbatch was entered); cd $SLURM_SUBMIT_DIR; WORKING_DIR=/mnt/scratch/Public/TRAINING/GenomeAnalysisModule/StudentSpaces/Old/test/CaseAnalysis. ## Set working space; mkdir -p $WORKING_DIR; cd $WORKING_DIR. #### GRCh38 #### ; echo ""GRCh38 genome""; GENOME=GRCh38; FASTA_DIR=/mnt/common/DATABASES/REFERENCES/GRCh38/GENOME/; FASTA_FILE=GRCh38-lite.fa. SEQ_TYPE=WGS; BAM_DIR=$WORKING_DIR; FAMILY_ID=Case1; PROBAND_ID=Case1_proband; MOTHER_ID=Case1_mother; FATHER_ID=Case1_father; SIBLING_ID=.; PED=$FAMILY_ID.ped. MOTHER_PRESENT=true; FATHER_PRESENT=true; SIBLING_PRESENT=false. PROBAND_BAM=${PROBAND_ID}.sorted.bam; FATHER_BAM=${FATHER_ID}.sorted.bam; MOTHER_BAM=${MOTHER_ID}.sorted.bam; SIBLING_BAM=${SIBLING_ID}.sorted.bam. PROBAND_VCF=${PROBAND_ID}.vcf.gz; FATHER_VCF=${FATHER_ID}.vcf.gz; MOTHER_VCF=${MOTHER_ID}.vcf.gz; SIBLING_VCF=${SIBLING_ID}.vcf.gz. PROBAND_GVCF=${PROBAND_ID}.gvcf.gz; FATHER_GVCF=${FATHER_ID}.gvcf.gz; MOTHER_GVCF=${MOTHER_ID}.gvcf.gz; SIBLING_GVCF=${SIBLING_ID}.gvcf.gz. # Now use the booleans to choose whether or not you run ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/518:1302,test,test,1302,,https://github.com/google/deepvariant/issues/518,1,['test'],['test']
Testability,tionV3/Mixed_5d/Branch_2/Conv2d_0c_3x3/weights|InceptionV3/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights/RMSProp|InceptionV3/Mixed_6c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_7b/Branch_1/Conv2d_0b_3x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta/RMSProp|InceptionV3/Mixed_6e/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0b_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6d/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6e/Branch_2/Conv2d_0e_1x7/BatchNorm/moving_variance|InceptionV3/Mixed_5c/Branch_1/Conv_1_0c_5x5/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights|InceptionV3/Mixed_6d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta|InceptionV3/Logits/Conv2d_1c_1x1/biases/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_1/Conv2d_0b_5x5/weights/RMSProp_1|InceptionV3/Mixed_6d/Branch_2/Conv2d_0b_7x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean|InceptionV3/Mixed_5c/Branch_1/Conv2d_0b_1x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_7c/Branch_1/Conv2d_0c_3x1/BatchNorm/moving_mean/ExponentialMovingAverage|InceptionV3/Mixed_6b/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/RMSProp_1|InceptionV3/Mixed_7c/Branch_2/Conv2d_0c_1x3/BatchNorm/beta|InceptionV3/Mixed_6b/Branch_3/Conv2d_0b_1x1/weights/RMSProp|InceptionV3/Mixed_7a/Branch_1/Conv2d_0b_1x7/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d_0d_7x1/weights/ExponentialMovingAverage|InceptionV3/Mixed_6c/Branch_2/Conv2d,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:42445,Log,Logits,42445,,https://github.com/google/deepvariant/issues/172,1,['Log'],['Logits']
Testability,"tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/third_party/nucleus/io/tfrecord.py"", line 190, in write_tfrecords; for proto in protos:; File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 91, in maybe_resolve_conflicting_variants; for overlapping_candidates in _group_overlapping_variants(sorted_variants):; File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/haplotypes.py"", line 111, in _group_overlapping_variants; for variant in sorted_variants:; File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1062, in _transform_call_variants_output_to_variants; yield _transform_call_variant_group_to_output_variant(**cvo_group_kwargs); File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 1036, in _transform_call_variant_group_to_output_variant; return add_call_to_variant(; File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 434, in add_call_to_variant; gq, variant.quality = compute_quals(predictions, index); File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/deepvariant/postprocess_variants.py"", line 469, in compute_quals; genomics_math.ptrue_to_bounded_phred(predictions[prediction_index]); File ""/tmp/Bazel.runfiles_in6znu90/runfiles/com_google_deepvariant/third_party/nucleus/util/genomics_math.py"", line 143, in ptrue_to_bounded_phred; raise ValueError('ptrue must be between zero and one: {}'.format(ptrue)); ValueError: ptrue must be between zero and one: nan; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. I'm running this code on an H100 GPU running nvidia driver - `535.183.06` and CUDA version is `12.2`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/901:5558,test,test,5558,,https://github.com/google/deepvariant/issues/901,2,['test'],['test']
Testability,"tps://genomics.googleapis.com/v1alpha2/operations?filter=projectId+%3D+isb-cgc-06-0004+AND+labels.job-id+%3D+call-varia--root--180503-233007-45&alt=json&pageSize=256; call-varia--root--180503-233007-45: FAILURE; [u""Error in job call-varia--root--180503-233007-45 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]; [05/03/2018 23:30:18 ERROR gcp_deepvariant_runner.py] Job failed with error [[u""Error in job call-varia--root--180503-233007-45 - code 2: failed to insert instance: googleapi: Error 400: Invalid value for field 'resource.guestAccelerators[0].acceleratorCount': '1'. At most 8 vCPUs can be used along with 1 accelerator cards in an instance., invalid""]]. Job args: ['--project', 'isb-cgc-XX-XXXX', '--logging', 'gs://XXXXX/wliang_deepvariant/XXXXX.stage/logs', '--boot-disk-size', '50', '--zones', 'us-west1-b', 'us-east1-d', '--wait', '--name', 'call_variants', '--image', 'gcr.io/deepvariant-docker/deepvariant_gpu:0.6.0', '--input-recursive', 'EXAMPLES=gs://XXXXX/wliang_deepvariant/XXXXX.stage/examples/15', 'MODEL=gs://deepvariant/models/DeepVariant/0.6.0/DeepVariant-inception_v3-0.6.0+cl-191676894.data-wgs_standard', '--output-recursive', 'CALLED_VARIANTS=gs://XXXXX/wliang_deepvariant/XXXXX.stage/called_variants', '--min-cores', '8', '--min-ram', '60', '--disk-size', '50', '--env', 'SHARDS=512', '--env', 'SHARD_START_INDEX=480', '--env', 'SHARD_END_INDEX=511', '--env', 'CONCURRENT_JOBS=1', '--command', '\ncd /opt/deepvariant/bin/ && \\\nseq -f ""%05g"" ""${SHARD_START_INDEX}"" ""${SHARD_END_INDEX}"" | \\\nparallel --jobs ""${CONCURRENT_JOBS}"" --halt 2 \\\n./call_variants \\\n --examples ""${EXAMPLES}""/examples_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-{}-of-""$(printf ""%05d"" ""${SHARDS}"")"".gz \\\n --checkpoint ""${MODEL}""/mod",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/70:1152,log,logs,1152,,https://github.com/google/deepvariant/issues/70,1,['log'],['logs']
Testability,"tput VCFs from these two different runs we saw a reduction in the GQ score assigned to the variant from 56 when using DeepVariant in singleton mode, to only 10 when using DeepTrio.; 2. GLnexus filtering, according to the `DeepVariantWGS` configuration we were using, removing our variant of interest in the case of DeepTrio due to the low likelihood assigned to the call. To partly overcome this we are looking to switch the GLnexus configuration to `DeepVariant_unfiltered` as mentioned in https://github.com/google/deepvariant/issues/440. However we would like to further evaluate this change on a known truth set to determine the increase in false-positive calls (similar to [1] with DV-GLN-NOMOD vs DV-GLN-OPT, but for DeepTrio instead... because from what I understand that paper evaluated DeepVariant). I have seen that all three GIAB/NIST benchmark trios have been used as training data for DeepTrio so would like to ask:. 1. Were all chromosomes from these trios used to train the DeepTrio models? I believe the DeepVariant WGS training data excluded chr20-22, and the deeptrio test data uses HG001 Chr20 [2], so I assume chr20-22 were excluded from the Deeptrio models for each of the trios too and would be suitable for testing? Or any alternative suggestions for this?; 2. I understand that the DeepTrio docs aren't officially released yet, but would it be possible please to provide an overview of the workings and differences between the Child and Parent models for DeepTrio? Is there a reason why the HG001/NA12891/NA12892 trios were used as training for the child model but not the parent model?. <br>. Many thanks,; Macabe. <br>. ![image](https://user-images.githubusercontent.com/37773554/128098808-740a1ab0-a6af-452f-8bed-d1f4ba0ceb80.png); Current DeepTrio training info (likely typo for Ashkenazim trio, cf. HG002/HG00**3**/HG004). [1] https://academic.oup.com/bioinformatics/article/36/24/5582/6064144 ; [2] https://github.com/google/deepvariant/tree/r1.2/deeptrio/testdata/input",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/475:1603,test,test,1603,,https://github.com/google/deepvariant/issues/475,3,['test'],"['test', 'testdata', 'testing']"
Testability,"ts from 1342 chunks.; > Merging took 7s; > Merge cleanup took 0s; Separated reads with divisions: H1 475116, H2 453908, and H0 159194; > Wrote haplotyped bams in 1m 43s; > Finished phasing in 18m 46s. real	18m47.373s; user	245m51.236s; sys	2m8.903s; mv: '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' and '/cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam' are the same file; [11-03-2021 14:13:04] INFO: [5/9] RUNNING THE FOLLOWING COMMAND; -------; time pepper_hp call_variant -b /cromwell_root/pepper_output/MARGIN_PHASED.PEPPER_SNP_MARGIN.haplotagged.bam -f /cromwell_root/broad-dsde-methods-long-reads/resources/references/grch38_noalt/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa -t 64 -m /opt/pepper_models/PEPPER_HP_R941_ONT_V4.pkl -o /cromwell_root/pepper_output/pepper_hp/ -s 6061-SL-0029 -w 4 -bs 64 --ont 2>&1 | tee /cromwell_root/pepper_output/logs/3_pepper_hp.log; -------; [11-03-2021 14:13:05] INFO: CALL VARIANT MODULE SELECTED; [11-03-2021 14:13:05] INFO: ONT VARIANT CALLING PROFILE SET.; [11-03-2021 14:13:05] INFO: RUN-ID: 11032021_141305; [11-03-2021 14:13:05] INFO: IMAGE OUTPUT: /cromwell_root/pepper_output/pepper_hp/images_11032021_141305/; [11-03-2021 14:13:05] STEP 1: GENERATING IMAGES:; [11-03-2021 14:13:05] INFO: COMMON CONTIGS FOUND: ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr8', 'chr9', 'chr10', 'chr11', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrM', 'chrX', 'chrY']; [11-03-2021 14:13:05] INFO: TOTAL CONTIGS: 25 TOTAL INTERVALS: 30895 TOTAL BASES: 3094460376; [11-03-2021 14:13:05] STARTING THREAD: 0 FOR 483 INTERVALS; [11-03-2021 14:13:05] INFO: [THREAD 00] 10/483 COMPLETE (2%) [ELAPSED TIME: 0 Min 0 Sec]; [11-03-2021 14:13:06] INFO: [THREAD 00] 20/483 COMPLETE (4%) [ELAPSED TIME: 0 Min 0 Sec]; [11-03-2021 14:13:06] INFO: [THREAD 00] 30/483 COMPLETE (6%) [ELAPSED TIME: 0 Min 0 Sec]; [11-03-2021 14:13:06] INFO: [THREA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/491:9764,log,log,9764,,https://github.com/google/deepvariant/issues/491,1,['log'],['log']
Testability,"tup**; - Operating system: Running inside docker image - `google/deepvariant:1.6.0-gpu`; - DeepVariant version: `1.6.0`; - Installation method (Docker, built from source, etc.): Docker image - `google/deepvariant:1.6.0-gpu`; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: Running the quickstart cmd --; ```; /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/opt/deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/opt/deepvariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/opt/deepvariant/quickstart-output/output.vcf.gz --output_gvcf=/opt/deepvariant/quickstart-output/output.g.vcf.gz --intermediate_results_dir /opt/deepvariant/quickstart-output/intermediate_results_dir --num_shards=1 --verbosity=2; ```. - Error trace: (if applicable) In the `postprocess_variants` step; ```; ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/opt/deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/opt/deepvariant/quickstart-output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/opt/deepvariant/quickstart-output/output.vcf.gz"" --cpus ""1"" --gvcf_outfile ""/opt/deepvariant/quickstart-output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/opt/deepvariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"". 2024-10-31 20:36:34.101345: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64; 2024-10-31 20:36:34.101375: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make s",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/901:1342,test,testdata,1342,,https://github.com/google/deepvariant/issues/901,1,['test'],['testdata']
Testability,"uf package. I tried protobuf version 3.20.3 and 4.21.9, but the error message is the same. In order to run DeepVariant successfully, what additional packages (version) should I install besides cloning the singularity image?. **Setup**; - Operating system: ; - DeepVariant version: 1.4.0; - Installation method: singularity; - Type of data: quick start test; ; **Steps to reproduce:**; ; ```; SINGULARITY_TMPDIR=/scratch/midway3/weilu1/tmp SINGULARITY_CACHEDIR=/scratch/midway3/weilu1/cache singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_1.4.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1. INFO: Converting SIF file to temporary sandbox...; WARNING: underlay of /usr/bin/nvidia-smi required more than 50 (469) bind mounts; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 33, in <module>; from tensorflow.core.framework import function_pb2; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/core/framework/function_pb2.py"", line 7, in <module>; from google.protobuf import descriptor as _descriptor; File ""/home/weilu1/.local/lib/python3.8/site-packages/google/protobuf/descriptor.py"", line 40, in <module>; from google.protobuf.internal",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/580:1372,sandbox,sandbox,1372,,https://github.com/google/deepvariant/issues/580,1,['sandbox'],['sandbox']
Testability,uild TensorFlow with the appropriate compiler flags.; I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs; I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']; I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs; I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']; I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.227729 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.280361 140555533080384 make_examples_core.py:257] Task 6/32: Preparing inputs; I0519 16:22:23.282453 140555533080384 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.283533 140555533080384 make_examples_core.py:257] Task 6/32: Common contigs are ['chr20']; I0519 16:22:23.228248 140552972691264 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.279789 140552972691264 make_examples_core,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:9623,test,testdata,9623,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"ujianglin 2813 May 19 17:15 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai; -rw-rw-r-- 1 zhoujianglin zhoujianglin 784363628 Apr 26 15:21 /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.pac. ```. **************; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2023-05-19 16:22:21.555857: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (o; neDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0519 16:22:23.193474 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.256151 139896863770432 make_examples_core.py:257] Task 10/32: Preparing inputs; I0519 16:22:23.258605 139896863770432 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.259495 139896863770432 make_examples_core.py:257] Task 10/32: Common contigs are ['chr20']; I0519 16:22:23.239336 140148036429632 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.192739 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.235120 140421750466368 make_examples_core.py:257] Task 21/32: Preparing inputs; I0519 16:22:23.239059 140421750466368 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.240968 140421750466368 make_examples_core.py:257] Task 21/32: Common contigs are ['chr20']; I0519 16:22:23.242177 140053689509696 genomics_reader.py:222] Reading quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0519 16:22:23.227729 140555533080384 genomics_reader.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:8992,test,testdata,8992,,https://github.com/google/deepvariant/issues/653,1,['test'],['testdata']
Testability,"umina_dv/Ilumina/output ; export INPUT_DIR=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata ; # the important part is to export the variables of paths used in the execution of the singularity command (OUTPUT_DIR and INPUT_DIR) and then add; # -B ${TMPDIR}:${TMPDIR} which mounts the $TMPDIR path defined by SLURM in the same place inside the container so you can use /scratch correctly and it exists inside the container; # This is where we run the container, and instead of ""docker run"" we use ""singularity run"" I just removed the docker part as we already have the container image (deepvariant_1.2.0.sif); singularity run -B /usr/lib/locale/:/usr/lib/locale/ -B ${TMPDIR}:${TMPDIR} \; /export/soft/singularity-containers/deepvariant/deepvariant_1.2.0.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/GRCh38_no_alt_analysis_set.fasta \; --reads=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/sample_1_recal.bam \; --regions=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/quickstart-testdata/Twist_ComprehensiveExome_targets_hg38.bed; --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz \; --output_gvcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.gvcf.gz \; --call_variants_extra_args=""use_openvino=true"" \; --num_shards=$(nproc) \; --intermediate_results_dir=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/intermediate_results_dir \; --dry_run=true; ```. After reading the line, where the interval bed file is given as an input, it gives an error that output.vcf is not found. Then I get the below error as well:. `/var/lib/slurm/slurmd/job31271228/slurm_script: line 29: --output_vcf=/scicore/home/cichon/thirun0000/Illumina_dv/Ilumina/output/sample_1.output.vcf.gz: No such file or directory`. I am confused, does the interval file step, require vcf file. Why t",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/515:1946,test,testdata,1946,,https://github.com/google/deepvariant/issues/515,1,['test'],['testdata']
Testability,"unfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles_3accq8qt/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 430, in call_variants; output_queue = multiprocessing.Queue(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 103, in Queue; return Queue(maxsize, ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/queues.py"", line 42, in __init__; self._rlock = ctx.Lock(); File ""/usr/lib/python3.8/multiprocessing/context.py"", line 68, in Lock; return Lock(ctx=self.get_context()); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 162, in __init__; SemLock.__init__(self, SEMAPHORE, 1, 1, ctx=ctx); File ""/usr/lib/python3.8/multiprocessing/synchronize.py"", line 57, in __init__; sl = self._semlock = _multiprocessing.SemLock(; FileNotFoundError: [Errno 2] No such file or directory. real 0m41.958s; user 0m6.224s; sys 0m3.683s. ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, the error happens with the quick start. . **Any additional context:**. Files generated with intermediate_results_dir. ```; gvcf.tfrecord-00000-of-00016.gz make_examples.tfrecord-00000-of-00016.gz make_examples.tfrecord-00008-of-00016.gz; gvcf.tfrecord-00001-of-00016.gz make_examples.tfrecord-00000-of-00016.gz.example_info.json make_examples.tfrecord-00008-of-00016.gz.example_info.json; gvcf.tfrecord-00002-of-00016.gz make_examples.tfrecord-00001-of-00016.gz make_examples.tfrecord-00009-of-00016.gz; gvcf.tfrecord-00003-of-00016.gz make_examples.tfrecord-00001-of-00016.gz.example_info.json make_examples.tfrecord-00009-of-00016.gz.example_info.json; gvcf.tfrecord-00004-of-00016.gz make_examples.tfrecord-00002-of-00016.gz make_examples.tfrecord-00010-of-00016.gz; gvcf.tfrecord-00005-of-00016.gz make_examples.tf",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733:3449,test,test,3449,,https://github.com/google/deepvariant/issues/733,2,['test'],['test']
Testability,"unfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 85, in one_sample_from_flags; sample_name = make_examples_core.assign_sample_name(; File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/deepvariant/make_examples_core.py"", line 131, in assign_sample_name; with sam.SamReader(reads_filenames.split(',')[0]) as sam_reader:; File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/genomics_reader.py"", line 221, in __init__; self._reader = self._native_reader(input_path, **kwargs); File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 260, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/home/rrautsa/Bazel.runfiles_bpldxvlm/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 227, in __init__; self._reader = sam_reader.SamReader.from_file(; ValueError: Not found: Could not open input/HG003.GRCh38.chr20.pFDA_truthv2.bam. ...REPEAT ABOVE ERROR {NPROC} TIMES... parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref reference/GRCh38_no_alt_analysis_set.fasta --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam --examples ./tmpkj84jstw/make_examples.tfrecord@16.gz --add_hp_channel --alt_aligned_pileup diff_channels --noparse_sam_aux_fields --pileup_image_width 199 --norealign_reads --regions chr20 --nosort_by_haplotypes --vsc_min_fraction_indels 0.12 --task 12. real	0m4.843s; user	0m3.036s; sys	0m0.866s; ```. **Setup**; - Operating system: CentOS Linux release 8.2.2004 (Core); - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity/Docker; - Type of data: [Tutorial Data]((https://github.com/google/deepvariant/blob/r1.3/docs/deepvariant-pacbio-model-case-study.md)). **Does the quick start test work on your system?**; The same error occurs in the quick start test with `Error in tempfile() using template...` as above.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/533:5610,test,test,5610,,https://github.com/google/deepvariant/issues/533,2,['test'],['test']
Testability,"urope-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --gvcf_outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --shards 512 \; --make_examples_workers 32 \; --make_examples_cores_per_worker 16 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 32 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 50 \; --postprocess_variants_disk_gb 200 \; --gcsfuse ""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions europe-west1 \; --docker-image gcr.io/cloud-genomics-pipelines/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". And i get the following error:. 07:03:22 Stopped running ""-c timeout=10; elapsed=0; seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""mkdir -p ./input-gcsfused-{} && gcsfuse --implicit-dirs \""${GCS_BUCKET}\"" /input-gcsfused-{}\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""until mountpoint -q /input-gcsfused-{}; do test \""${elapsed}\"" -lt \""${timeout}\"" || fail \""Time out waiting for gcsfuse mount points\""; sleep 1; elapsed=$((elapsed+1)); done\"" && seq \""${SHARD_START_INDEX}\"" \""${SHARD_END_INDEX}\"" | parallel --halt 2 \""/opt/deepvariant/bin/make_examples --mode calling --examples \""${EXAMPLES}\""/examples_output.tfrecord@\""${SHARDS}\"".gz --reads \""/input-gcsfused-{}/${BAM}\"" --ref \""${INPUT_REF}\"" --task {} --gvcf \""${GVCF}\""/gvcf_output.tfrecord@\""${SHARDS}\"".gz\"""": exit status 127: bash: gcsfuse: command not found. Is it",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/214:1565,log,log,1565,,https://github.com/google/deepvariant/issues/214,1,['log'],['log']
Testability,"ut of the reads:. ```; 2018-12-10 17:04:46.071140: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1210 17:04:46.071218 140037815584512 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; I1210 17:04:46.072298 140037815584512 make_examples.py:1024] Preparing inputs; 2018-12-10 17:04:46.072535: W third_party/nucleus/io/sam_reader.cc:531] Unrecognized SAM header type, ignoring: ; I1210 17:04:46.072572 140037815584512 genomics_reader.py:174] Reading project-retraining/testdata/aligned_reads.bam with NativeSamReader; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; [W::bcf_hdr_register_hrec] An INFO field has no Type defined. Assuming String; [W::bcf_hdr_register_hrec] An INFO field has no Number defined. Assuming '.'; I1210 17:04:46.072917 140037815584512 genomics_reader.py:174] Reading project-retraining/testdata/variants.vcf.gz with NativeVcfReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_1gjjersh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1120, in <module>; tf.app.run(); File ""/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_1gjjersh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1110, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_1gjjersh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1025, in make_examples_runner; regions = processing_regions_from_options(options); File ""/tmp/Bazel.runfiles_1gjjersh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 945, in processing_regions_from_options; options.min_shared_contigs_basepairs); File ""/tmp/Bazel.runfiles_1gjjersh/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 455, in _ensure_consistent_con",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/128:1133,test,testdata,1133,,https://github.com/google/deepvariant/issues/128,1,['test'],['testdata']
Testability,"vcf=test_output.vcf.gz \; > --output_gvcf=test_output.g.vcf.gz \; > --num_shards=2. ***** Running the command:*****; time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}. 2019-10-15 11:18:39.819615: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; 2019-10-15 11:18:39.819621: F tensorflow/core/platform/cpu_feature_guard.cc:37] The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m23.020s; user	0m1.610s; sys	0m3.206s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 235, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 215, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 1 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/opt/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/opt/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@2.gz"" --regions ""chr20:10,000,000-10,010,000"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@2.gz"" --task {}' returned non-zero exit status 2. Do anyone know how to handle it?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/226:2067,test,testdata,2067,,https://github.com/google/deepvariant/issues/226,2,['test'],['testdata']
Testability,"ver.py"", line 1477, in _import_meta_graph_with_return_elements; **kwargs)); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/meta_graph.py"", line 809, in import_scoped_meta_graph_with_return_elements; return_elements=return_elements); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py"", line 507, in new_func; return func(*args, **kwargs); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 405, in import_graph_def; producer_op_list=producer_op_list); File ""/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/importer.py"", line 501, in _import_graph_def_internal; graph._c_graph, serialized, options) # pylint: disable=protected-access; tensorflow.python.framework.errors_impl.NotFoundError: Op type not registered 'LegacyParallelInterleaveDatasetV2' in binary running on bbfd0038f901. Make sure the Op and Kernel are registered in the binary running in this process. Note that if you are loading a saved graph which used ops from tf.contrib, accessing (e.g.) `tf.contrib.resampler` should be done before importing the graph, as contrib ops are lazily registered when the module is first accessed`. **Does the quick start test work on your system?** ; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?; Quick start works on my system -- I can perform make_examples, call_variants and post_processing. **Any additional context:**; (e.g. Tensorflow version, cuDNN version, NVIDIA Driver information from running `nvidia-smi`). Code snippet :; `import tensorflow as tf. meta_path = '/opt/models/wgs/model.ckpt.meta'. ckpt_folder = '/opt/models/wgs'. with tf.compat.v1.Session() as sess:. saver = tf.compat.v1.train.import_meta_graph(meta_path). print(""\n**Import Sucessful\n**""). saver.restore(sess,tf.compat.v1.train.latest_checkpoint(ckpt_folder)); `",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/339:2828,test,test,2828,,https://github.com/google/deepvariant/issues/339,2,['test'],['test']
Testability,"vidia/lib:/usr/local/nvidia/lib64; 2024-07-03 17:21:58.247080: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.; Runs all 3 steps to go from input DNA reads to output VCF/gVCF files.; (... then -- the list of all options follows). If run on a proper BAM file with all options provided, all TF-TRT warning messages are periodically repeated as well as ; CUDA Version 11.3.1; 2024-07-02 22:47:07.493311: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-02 22:47:12.386498: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; ...; and processing is performed on CPUs. . Also, these details are put in the log hudreds of times:; 2024-07-03 18:27:31.862526: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected; 2024-07-03 18:27:31.862557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 8308a7bb3067; 2024-07-03 18:27:31.862563: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: 8308a7bb3067; 2024-07-03 18:27:31.862607: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 555.42.6; 2024-07-03 18:27:31.862621: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 555.42.6; 2024-07-03 18:27:31.862626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 555.42.6; (then repeated with every call). **Does the quick start test work on your system?**; Please test",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:3345,log,log,3345,,https://github.com/google/deepvariant/issues/844,1,['log'],['log']
Testability,"xamples) [6.77s elapsed]; I0706 12:06:34.020470 140496845215552 make_examples_core.py:301] Task 39/40: 20480581 candidates (20541819 examples) [9.70s elapsed]; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref Reference/Human_genome/CHM13/chm13v2.0.fa --reads HG00438.hap48.cram --examples tmp/tmp_HG00438.hap48/make_examples.tfrecord@40.gz --channels insert_size --gvcf tmp/tmp_HG00438.hap48/gvcf.tfrecord@40.gz --keep_legacy_allele_counter_behavior --min_mapping_quality 1 --normalize_reads --task 24. real 1500m24.832s; user 1478m47.340s; sys 6m51.817s. ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Deepvariant previously work well with data aligned to linear reference genome. So, I think it could be some issue with pangenome-aligned data. I can share the cram (e.g., chr22) if you would like to test. **Any additional context:**; Code I used to align to a haplotype sampled pangenome graph:; ```; # vg giraffe to align reads to graph; rg=""ID:1\tLB:lib_${sample}\tSM:${sample}\tPL:illumina\tPU:lib_${sample}.1""; time \; vg giraffe \; --progress \; --read-group $rg \; --sample ${sample} \; --output-format gam \; -f ${fqpath}/${sample%.hap*}_1_paired.fq.gz \; -f ${fqpath}/${sample%.hap*}_2_paired.fq.gz \; -Z ${gbzpath}/${sample}.gbz \; -t $threads > ${outpath}/${sample}.gam. # vg surject to convert gam to bam; time \; vg surject \; -F $path_list \; -x ${gbzpath}/${sample}.gbz \; -t $(($threads - 10)) \; --sam-output \; --read-group '1' \; --sample ${sample} \; --prune-low-cplx \; --interleaved \; --max-frag-len 3000 \; ${outpath}/${sample}.gam | \; sed 's/CHM13#0#//g' - | \; samtools view -hb - | \; samtools reheader ${outpath}/${sample}.tmp.header.sam - | \; samtools sort --write-index -@ 10 -m 4G -O CRAM --reference $ref -T ${outpath}/${sample}.cram -o ${outpath}/${samp",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/847:2371,test,test,2371,,https://github.com/google/deepvariant/issues/847,1,['test'],['test']
Testability,"xome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/; IMAGE_VERSION=0.7.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; COMMAND=""/opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ${PROJECT_ID} \; --zones europe-west1-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --regions gs://public_bed/CHR20.bed \; --bam gs://ms_bam/NoDup_FB4.bam \; --bai gs://ms_bam/NoDup_FB4.bam.bai \; --ref gs://ms_bam/Homo_sapiens_assembly38.fasta \; --ref_fai gs://ms_bam/Homo_sapiens_assembly38.fasta.fai \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --zones europe-west1-b \; --docker-image gcr.io/deepvariant-docker/deepvariant_runner:""${IMAGE_VERSION}"" \; --command-line ""${COMMAND}"". 1. I have quoted #set -euo pipefail out as it returns an error.; 2. The bed file is located in a public bucket #119 ; 3. I have tried with docker image 0.7.1 which returns following error:. [12/12/2018 14:14:08 INFO gcp_deepvariant_runner.py] Running make_examples...; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] make_examples is done!; [12/12/2018 14:34:47 INFO gcp_deepvariant_runner.py] Running call_variants...; [12/12/2018 14:37:23 ERROR gcp_deepvariant_runner.py] Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/5187520767668161022"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION); . Job args: ['pipelines', '--project', 'ms-deepvariant', 'run', '--attempts', '2', '--pvm-attempts', '0', '--boot-disk-size', '50', '--output-interval', '60s', '--zones', 'europe-",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:1426,log,log,1426,,https://github.com/google/deepvariant/issues/129,1,['log'],['log']
Testability,"y run DeepVariant by submitting it to the SLURM scheduler? ; I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task 13/64: Created 2879 examples. Building DAG of jobs...; Using shell: /usr/bin/bash; Provided cores: 64; Rules claiming more threads will be scaled down.; Select jobs to execute... > [Wed Jan 4 18:30:51 2023]; > rule deepvariant:; > input: results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam, /mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta; > output: results/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2.vcf.gz; > log: logs/deepvariant/s534_EKDN210017195-1A_HTTJ3DSX2_L2/stdout.log; > jobid: 0; > wildcards: sample=s534_EKDN210017195-1A_HTTJ3DSX2_L2; > threads: 64; > resources: mem_mb=163840, disk_mb=16401, tmpdir=/tmp/kmarians_4189323; > ; > Activating singularity image singularity/deepvariant_1.4.0.sif; > INFO: Convert SIF file to sandbox...; > I0104 18:31:03.183642 139718628308800 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/kmarians_4189323/tmpxrz5rqbp; > ; > ***** Intermediate results will be written to /tmp/kmarians_4189323/tmpxrz5rqbp in docker. ****; > ; > ; > ***** Running the command:*****; > time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/mnt/shared/scratch/kmarians/private/dyslexia_gatk/workflow/resources/genome.fasta"" --reads ""results/recal/s534_EKDN210017195-1A_HTTJ3DSX2_L2.bam"" --examples ""/tmp/kmarians_4189323/tmpxrz5rqbp/make_examples.tfrecord@64.gz"" --channels ""insert_size"" --vsc_min_count_indels ""3"" --vsc_min_count_snps ""3"" --vsc_min_fraction_indels ""0.10"" --vsc_min_fraction_snps ""0.2"" --task {}. > *; > *; > *; > I0104 18:49:24.340415 140179943589696 make_examples_core.py:243] Task 13/64: Found 2793 candidate variants; > I0104 18:49:24.340478 140179943589696 make_examples_core.py:243] Task",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/602:3259,sandbox,sandbox,3259,,https://github.com/google/deepvariant/issues/602,1,['sandbox'],['sandbox']
Testability,"y:301] Task 1/4: Found 0 candidate variants; I0729 14:44:37.899752 140710547908416 make_examples_core.py:301] Task 1/4: Created 0 examples; I0729 14:44:37.893192 139779121772352 make_examples_core.py:301] Task 2/4: Writing example info to /tmp/tmpkcjcf0p_/make_examples.tfrecord-00002-of-00004.gz.example_info.json; I0729 14:44:37.893293 139779121772352 make_examples_core.py:2958] example_shape = None; I0729 14:44:37.893665 139779121772352 make_examples_core.py:2959] example_channels = [1, 2, 3, 4, 5, 6, 19]; I0729 14:44:37.894033 139779121772352 make_examples_core.py:301] Task 2/4: Found 0 candidate variants; I0729 14:44:37.894105 139779121772352 make_examples_core.py:301] Task 2/4: Created 0 examples. real	0m4.791s; user	0m11.503s; sys	0m2.085s. ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpkcjcf0p_/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpkcjcf0p_/make_examples.tfrecord@4.gz"" --checkpoint ""/opt/models/wes"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: . TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). . For more information see: https://github.com/tensorflow/addons/issues/2807 . warnings.warn(; I0729 14:44:41.088234 139722246891328 call_variants.py:471] Total 1 writing processes started.; W0729 14:44:41.090612 139722246891328 call_variants.py:482] Unable to read any records from /tmp/tmpkcjcf0p_/make_examples.tfrecord@4.gz. Output will contain zero records.; I0729 14:44:41.091079 139722246891328 call_variants.py:623] Complete: call_variants. **Does the quick start test work on your system?**; yes. **Any additional context:**; Some samples work fine, some very similar samples keep running",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/855:12496,test,test,12496,,https://github.com/google/deepvariant/issues/855,1,['test'],['test']
Testability,"zelrc:; #16 1489.8 Inherited 'common' options: --experimental_repo_remote_exec; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.bazelrc:; #16 1489.8 'build' options: --define framework_shared_object=true --java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --host_java_toolchain=@tf_toolchains//toolchains/java:tf_java_toolchain --define=use_fast_cpp_protos=true --define=allow_oversize_protos=true --spawn_strategy=standalone -c opt --announce_rc --define=grpc_no_ares=true --noincompatible_remove_legacy_whole_archive --enable_platform_specific_config --define=with_xla_support=true --config=short_logs --config=v2 --define=no_aws_support=true --define=no_hdfs_support=true --deleted_packages=tensorflow/compiler/mlir/tfrt,tensorflow/compiler/mlir/tfrt/benchmarks,tensorflow/compiler/mlir/tfrt/jit/python_binding,tensorflow/compiler/mlir/tfrt/jit/transforms,tensorflow/compiler/mlir/tfrt/python_tests,tensorflow/compiler/mlir/tfrt/tests,tensorflow/compiler/mlir/tfrt/tests/saved_model,tensorflow/compiler/mlir/tfrt/transforms/lhlo_gpu_to_tfrt_gpu,tensorflow/core/runtime_fallback,tensorflow/core/runtime_fallback/conversion,tensorflow/core/runtime_fallback/kernel,tensorflow/core/runtime_fallback/opdefs,tensorflow/core/runtime_fallback/runtime,tensorflow/core/runtime_fallback/util,tensorflow/core/tfrt/common,tensorflow/core/tfrt/eager,tensorflow/core/tfrt/eager/backends/cpu,tensorflow/core/tfrt/eager/backends/gpu,tensorflow/core/tfrt/eager/core_runtime,tensorflow/core/tfrt/eager/cpp_tests/core_runtime,tensorflow/core/tfrt/fallback,tensorflow/core/tfrt/gpu,tensorflow/core/tfrt/run_handler_thread_pool,tensorflow/core/tfrt/runtime,tensorflow/core/tfrt/saved_model,tensorflow/core/tfrt/saved_model/tests,tensorflow/core/tfrt/tpu,tensorflow/core/tfrt/utils; #16 1489.8 (21:51:01) INFO: Reading rc options for 'build' from /opt/tensorflow/.tf_configure.bazelrc:; #16 1489.8 'build' options: --action_env PYTHON_BIN_PATH=/usr/local/bin/python3 --act",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/608:1834,benchmark,benchmarks,1834,,https://github.com/google/deepvariant/issues/608,4,"['benchmark', 'test']","['benchmarks', 'tests']"
Testability,"{outpath}/${sample}.g.vcf.gz \; --num_shards=${threads} \; --intermediate_results_dir ${TMPDIR} \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"". ```; - Error trace: (if applicable); ```; I0706 12:06:34.396180 140036894541632 make_examples_core.py:301] Task 16/40: 20600183 candidates (20662757 examples) [6.77s elapsed]; I0706 12:06:34.020470 140496845215552 make_examples_core.py:301] Task 39/40: 20480581 candidates (20541819 examples) [9.70s elapsed]; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref Reference/Human_genome/CHM13/chm13v2.0.fa --reads HG00438.hap48.cram --examples tmp/tmp_HG00438.hap48/make_examples.tfrecord@40.gz --channels insert_size --gvcf tmp/tmp_HG00438.hap48/gvcf.tfrecord@40.gz --keep_legacy_allele_counter_behavior --min_mapping_quality 1 --normalize_reads --task 24. real 1500m24.832s; user 1478m47.340s; sys 6m51.817s. ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Deepvariant previously work well with data aligned to linear reference genome. So, I think it could be some issue with pangenome-aligned data. I can share the cram (e.g., chr22) if you would like to test. **Any additional context:**; Code I used to align to a haplotype sampled pangenome graph:; ```; # vg giraffe to align reads to graph; rg=""ID:1\tLB:lib_${sample}\tSM:${sample}\tPL:illumina\tPU:lib_${sample}.1""; time \; vg giraffe \; --progress \; --read-group $rg \; --sample ${sample} \; --output-format gam \; -f ${fqpath}/${sample%.hap*}_1_paired.fq.gz \; -f ${fqpath}/${sample%.hap*}_2_paired.fq.gz \; -Z ${gbzpath}/${sample}.gbz \; -t $threads > ${outpath}/${sample}.gam. # vg surject to convert gam to bam; time \; vg surject \; -F $path_list \; -x ${gbzpath}/${sample}.gbz \; -t $(($threads - 10)) \; --sam-output",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/847:1977,test,test,1977,,https://github.com/google/deepvariant/issues/847,2,['test'],['test']
Testability,"}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; BIN_VERSION=""1.3.0""; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" --num_shards=1; . stack trace:. I0317 09:40:21.184321 140398386173760 run_deepvariant.py:341] Creating a directory for intermediate results in /mnt/share/jasontest/quickstart-output/intermediate_results_dir; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 493, in <module>; app.run(main); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 312, in run; _run_main(main, args); File ""/usr/local/lib/python3.8/dist-packages/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 460, in main; intermediate_results_dir = check_or_create_intermediate_results_dir(; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 343, in check_or_create_intermediate_results_dir; os.makedirs(intermediate_results_dir); File ""/usr/lib/python3.8/os.py"", line 213, in makedirs; makedirs(head, exist_ok=exist_ok); File ""/usr/lib/python3.8/os.py"", line 213, in makedirs; makedirs(head, exist_ok=exist_ok); File ""/usr/lib/python3.8/os.py"", line 213, in makedirs; makedirs(head, exist_ok=exist_ok); [Previous line repeated 1 more time]; File ""/usr/lib/python3.8/os.py"", line 223, in makedirs; mkdir(name, mode); OSError: [Errno 30] Read-only file system: '/mnt/share'. **Does the quick start test work on your system?**; The quick test works on my system as long as my data is not in the /mnt/ folder. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530:3668,test,test,3668,,https://github.com/google/deepvariant/issues/530,2,['test'],['test']
Testability,"}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi. I have installed the DeepVariant image according to: . BIN_VERSION=""0.8.0""; sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"". When I run the script test: . OUTPUT_DIR=""${PWD}/quickstart-output""; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ""${OUTPUT_DIR}"". BIN_VERSION=""0.8.0""; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}""; \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ ; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=1. The following error happens:. FATAL Flags parsing error: flag --ref=None: Flag --ref must have a value other than None.; Pass --helpshort or --helpfull to see help on flags.; ./run_deepvariant.sh: line 12: --ref=/input/ucsc.hg19.chr20.unittest.fasta: No such file or directory. I tried it on three different computers, and the error was the same.; There is a previous issue in this forum (https://github.com/google/deepvarian",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/223:1274,test,test,1274,,https://github.com/google/deepvariant/issues/223,1,['test'],['test']
Usability," ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/__init__.py"", line 24, in <module>; from tensorflow.python import pywrap_tensorflow # pylint: disable=unused-import; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/tensorflow/python/__init__.py"", line 47, in <module>; import numpy as np; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/__init__.py"", line 142, in <module>; from . import core; File ""/home/josephguhlin/.local/lib/python2.7/site-packages/numpy/core/__init__.py"", line 47, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the multiarray numpy extension module failed. Most; likely you are trying to import a failed build of numpy.; Here is how to proceed:; - If you're working with a numpy git repository, try `git clean -xdf`; (removes all files not under version control) and rebuild numpy.; - If you are simply trying to use the numpy version that you have installed:; your installation is broken - please reinstall numpy.; - If you have already reinstalled and that did not fix the problem, then:; 1. Check that you are using the Python you expect (you're using /usr/bin/python),; and that you have no directories in your PATH or PYTHONPATH that can; interfere with the Python and numpy versions you're trying to use.; 2. If (1) looks fine, you can open a new issue at; https://github.com/numpy/numpy/issues. Please include details on:; - how you installed Python; - how you installed numpy; - your operating system; - whether or not you have multiple versions of Python installed; - if you built from source, your compiler versions and ideally a build log. Note: this error has many possible causes, so please don't comment on; an existing issue about this - open a new one instead. Original error was: libopenblas.so.0: cannot open shared object file: No such file or directory; ```. I need to run deepvariant as a non-root user via singulairty on the H",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/243:1427,simpl,simply,1427,,https://github.com/google/deepvariant/issues/243,1,['simpl'],['simply']
Usability," `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""; `; - Error trace: (if applicable); ```; ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-10 12:07:21.275077: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0710 12:07:24.889796 139944337696576 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: sample1; I0710 12:09:25.874185 139944337696576 postprocess_variants.py:1313] CVO sorting took 2.0161957065264384 minutes; I0710 12:09:25.874843 139944337696576 postprocess_variants.py:1316] Transforming call_variants_output to variants.; I0710 12:09:25.874915 139944337696576 postprocess_variants.py:1318] Using 19 CPUs for parallelization of variant transformation.; I0710 12:09:45.096508",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849:1784,learn,learning-container-license,1784,,https://github.com/google/deepvariant/issues/849,1,['learn'],['learning-container-license']
Usability," was not used. These are the output (part of the output were removed due to the limit of the characters of this post):. ```; ➜ t7 apptainer run --nv \; -B input:/input \; -B output_apptainer_gpu:/output \; deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf=output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz \; --output_gvcf=output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz \; --num_shards=$(nproc) \; --customized_model=input/weights-51-0.995354.ckpt; INFO: /usr/local/etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-02-17 23:31:25.687399: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-02-17 23:31:39.809521: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:2632,Learn,Learning,2632,,https://github.com/google/deepvariant/issues/774,1,['Learn'],['Learning']
Usability,"![DeepTrio_QUAL](https://user-images.githubusercontent.com/22089494/114759224-e5d49180-9d2b-11eb-9c5e-cb33c9979d2d.png); Hello, . I am running DeepTrio for a dataset with known true-positive SNPs and indels. I followed guidelines for DeepTrio but had to change --config DeepVariantWGS to DeepVariant_unfiltered at the glnexus_cli step as a default QUAL threshold of 10 removed a lot of my TP calls.; I have compared distributions of QUAL score in the TP subset and all calls found by DeepTrio. Please see an attached histogram of all DeepTrio calls vs TP calls. Could you please tell me if it is expected that QUAL of TP calls is between 0 and30, while there are calls with QUAL of up to 100? If not, what I could do wrong?; Thank you!. Best regards,; Maria. **Setup**; - Operating system: Linux; - DeepVariant version: deepvariant_deeptrio-1.1.0.sif; - Installation method (Docker, built from source, etc.): singularity/3.6.4; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) HiSeq X Ten, hg38, WGS",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/440:219,guid,guidelines,219,,https://github.com/google/deepvariant/issues/440,1,['guid'],['guidelines']
Usability,"""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 30, in <module>; import numpy as np; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 140, in <module>; from . import core; File ""/home/asherrar/.local/lib/python3.8/site-packages/numpy/core/__init__.py"", line 49, in <module>; raise ImportError(msg); ImportError:. IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!. Importing the numpy C-extensions failed. This error can happen for; many reasons, often due to issues with your setup or how NumPy was; installed. We have compiled some common reasons and troubleshooting tips at:. https://numpy.org/devdocs/user/troubleshooting-importerror.html. Please note and check the following:. * The Python version is: Python3.8 from ""/usr/bin/python3""; * The NumPy version is: ""1.23.0"". and make sure that they are the versions you expect.; Please carefully study the documentation linked above for further help. Original error was: libflexiblas.so.3: cannot open shared object file: No such file or directory; ```. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start? Any attempt to execute via `singularity run` leads to an error. **Any additional context:**; As far as I can tell, my environment meets the requirements for both Python and NumPy - though at the same time when I `singularity shell` into the SIF file, its versioning seems semi-independent of my main environment (Python 3.8.10 regardless of my environment's version, but using the NumPy 1.23.0 provided by my computing cluster). I feel like I'm missing something really simple, but I've tried the NumPy troubleshooting page and can't seem to crack this error. If it helps, I'm attempting this with `singularity` version 3.8.4, which is the newest version available to me in my computing cluster.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/610:3404,simpl,simple,3404,,https://github.com/google/deepvariant/issues/610,1,['simpl'],['simple']
Usability,"(sequencing instrument, reference genome, anything special that is unlike the case studies?) Illumina WGS, GCA_000001405.15_GRCh38_no_alt_analysis_set. **Steps to reproduce:**; - Command: ; `podman run -it --rm --security-opt=label=disable --hooks-dir=/usr/share/containers/oci/hooks.d/ --gpus 1 -v /data:/data --device nvidia.com/gpu=all google/deepvariant:1.6.1-gpu /opt/deepvariant/bin/postprocess_variants --ref ""/data/references/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz"" --infile ""/data/variants/sample1.intermediate/call_variants_output.tfrecord.gz"" --outfile ""/data/variants/sample1.vcf.gz"" --cpus ""19"" --gvcf_outfile ""/data/variants/sample1.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/data/variants/sample1.intermediate/gvcf.tfrecord@19.gz""; `; - Error trace: (if applicable); ```; ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-10 12:07:21.275077: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; I0710 12:07:24.889796 139944337696576 postprocess_variants.py:1211] Using sample name from call_variants output. Sample name: sample1; I0710 12:09:25.874185 139944337696576 postprocess_variants.py:1313] CVO sorting took 2.0161957065264384 minutes; I0710 12:09:25.874843 139944337696576 postprocess_variants.py:1316] Transform",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/849:1620,Learn,Learning,1620,,https://github.com/google/deepvariant/issues/849,1,['Learn'],['Learning']
Usability,") and use that checkpoint in the future.; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 633, in <module>; app.run(main); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 618, in main; call_variants(; File ""/tmp/Bazel.runfiles__zgkztyv/runfiles/com_google_deepvariant/deepvariant/call_variants.py"", line 558, in call_variants; model.load_weights(checkpoint_path).expect_partial(); File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 70, in error_handler; raise e.with_traceback(filtered_tb) from None; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/checkpoint/checkpoint.py"", line 1047, in assert_consumed; raise AssertionError(; AssertionError: Some objects had attributes which were not restored: ; <tf.Variable 'conv2d/kernel:0' shape=(3, 3, 7, 32) dtype=float32, numpy=; ; My knowledge in deep learning models is not the best, so if you could please tell me how to overcome this error, as the RNA model seems to have very promising results for RNA variant calling and i want to use it. **Setup**; - Operating system: Ubuntu 20.0; - DeepVariant version: Latest version 1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: GIAB benchmark data used in the deepvariant-rnaseq-case-study.md but not restricted to chr20. **Steps to reproduce:**; - Command: ; docker run -v ""$(pwd):$(pwd)"" -w $(pwd) google/deepvariant:latest run_deepvariant --model_type=WES --customized_model=model/model.ckpt --ref=GCA_000001405.15_GRCh38_no_alt_analysis_set.fasta --reads=STAR/Mapping/marked_split.bam --output_vcf=STAR/Mapping/deepvariant.rna.vcf --num_shards=$(nproc)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/845:2855,learn,learning,2855,,https://github.com/google/deepvariant/issues/845,1,['learn'],['learning']
Usability,"**Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: Ubuntu; - DeepVariant version: 0.9.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: sample: CHM13, instrument: PacBio Sequel CCS sequencing, reference genome: CHM13 draft genome from T2T project. **Steps to reproduce:**; - Command:. ```; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=/input/chm13.draft_v1.0.fasta \; --reads=/input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam \; --output_vcf=/output/CHM13.CHM13.minimap2_asm20.deepvariant_0.9.0.vcf.gz \; --output_gvcf=/output/CHM13.CHM13.minimap2_asm20.deepvariant_0.9.0.g.vcf.gz \; --num_shards=29; ```. - Error trace: (if applicable). ```; ***** Running the command:*****; time seq 0 28 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/chm13.draft_v1.0.fasta"" --reads ""/input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp. I1023 11:00:14.182121 140022713169664 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1023 11:00:14.268690 140022713169664 genomics_reader.py:223] Reading /input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam with NativeSamReader; I1023 11:00:14.297683 140022713169664 make_examples.py:1324] Preparing inputs; I1023 11:00:14.382807 140022713169664 genomics_reader.py:223] Reading /input/CHM13.CHM13.minimap2_asm20.primary_alignments.sorted.bam with NativeSamReader; I1023 11:00:14.425673 140022713169664 make_examples.py:1248] Common contigs are [u'chr1', u'chr2', u'chr3', u'chr4', u'chr5', u'chr6', u'chr7', u'chr8', u'chr9', u'chr10', u'chr11', u'chr12', u'chr13', u'chr14', u'chr15', u'chr16', u'ch; I1023 11:00:14.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/367:28,clear,clear,28,,https://github.com/google/deepvariant/issues/367,1,['clear'],['clear']
Usability,"**Describe the issue:**; (A clear and concise description of what the issue is.); I have no ""sudo"" authority; I think deepvariant tool have to run ""sudo"" authority ; and I did this command; `sudo docker pull google/deepvariant:""0.10.0""`. as far as I understand, this next step is ; `sudo docker run \; -v ""${INPUT_DIR}"":""/3.Sort"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${ref_fasta}"" \; --reads=""/3.Sort/$1_Markdup_sort.bam"" \; --output_vcf=/output/$1_Deepvariant.output.vcf.gz \; --output_gvcf=/output/$1_Deepvariant.output.g.vcf.gz \; --num_shards=${N_SHARDS}; `; is that right?; But I can't use ""sudo"" every time. how can I run 'deepvariant' without 'sudo' ?? . **Setup**; - Operating system: Ubuntu; - DeepVariant version:0.10.0",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/335:28,clear,clear,28,,https://github.com/google/deepvariant/issues/335,1,['clear'],['clear']
Usability,"**Describe the issue:**; Apparently DeepVariant will not call variants on certain regions, irrespective of the ""calling intervals"" I pass it via BED file. . First of all, I only found this out after googling it and coming across a closed issue. This seems like it is ""important"" information. I spent a fair amount of time trying to figure out why my calls were missing MT information... Secondly, while I ""get"" that the results may not be highly reliable, MT variant calling is still useful (and commonly done) for some applications; so if I pass the Mitochondrion as a calling target, I would expect to get MT variant calls. This is a bit of an odd behavior, I think. . Solutions: Clearly document this on github (sorry if I didn't see it, if it is already there). And maybe allow users to overwrite this through their BED file targets - maybe with a warning (unless MT variants are never trained so the algorithm is simply unable to call them). . **Setup**; Any. **Steps to reproduce:**; N/A",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/333:682,Clear,Clearly,682,,https://github.com/google/deepvariant/issues/333,2,"['Clear', 'simpl']","['Clearly', 'simply']"
Usability,"**Describe the issue:**; Hi! I am trying to use deep-trio to call variants of drosophila (PACBIO data). I have noticed you have provide guides for training CNN model of deep variant, but I have no idea of training model of deep trio. Can I train a drosophila model of deep trio?. **Setup**; - Operating system: Cent OS; - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) pacbio sequencing data",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/532:136,guid,guides,136,,https://github.com/google/deepvariant/issues/532,1,['guid'],['guides']
Usability,"**Describe the issue:**; I follow the quick start guidelines, and meet this error. **Setup**; - Operating system: MacBook Air (M1, 2020); - DeepVariant version: 19.03.14; - Installation method (Docker, built from source, etc.): Docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) quick start data . **Steps to reproduce:**; - Command: sudo docker run --platform linux/amd64 google/deepvariant /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/quickstart-output/output.vcf.gz --output_gvcf=/quickstart-output/output.g.vcf.gz --intermediate_results_dir /quickstart-output/intermediate_results_dir --num_shards=1; - Error trace: (if applicable) I0712 04:14:17.889120 274906666752 run_deepvariant.py:313] Creating a directory for intermediate results in /quickstart-output/intermediate_results_dir. ***** Intermediate results will be written to /quickstart-output/intermediate_results_dir in docker. ****. ***** Running the command:*****; ( time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/quickstart-output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --gvcf ""/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {} ). 2021-07-12 04:14:21.223394: F tensorflow/core/lib/monitoring/collection_registry.cc:70] Check failed: collection_function Requires collection_function to contain an implementation.; qemu: uncaught target signal 6 (Aborted) - core dumped; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads /quickst",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/471:50,guid,guidelines,50,,https://github.com/google/deepvariant/issues/471,1,['guid'],['guidelines']
Usability,"**Describe the issue:**; I ran DeepVariant step by step using Illumina reads. I have a simple question : is it unable to run `make_examples` using `cram` file when running them in parallel? . I generated my alignment file in CRAM format to reduce the file size. However, when I attempted to run the `make_examples` command in parallel, it failed with the error message `/dev/tty: No such device or address`. Below is what I tried : ; 1. non-parallel + bam ✅; 2. non-parallel + cram ✅ ; 3. parallel + bam ✅ ; 4. non-parallel + cram 🔴 . I can run it using `BAM` file instead, but i'm just curious if this is the cause of this error. . **Setup**; - Operating system: Linux/4.18.0-513.18.1.el8_9.x86_64; - DeepVariant version: v1.6.0; - Installation method (Docker, built from source, etc.): HPC, sorry I don't know; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); Not special, I used common toy data. **Steps to reproduce:**; - Command: ; ```; seq 0 $((N_SHARDS-1)) \; | parallel -P ${SLURM_CPUS_PER_TASK} --halt 2 \; --joblog ""$wd/logs-parallel-$SLURM_JOB_ID/log"" --res ""$wd/logs-parallel-$SLURM_JOB_ID"" \; make_examples --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --regions ""chr20:10,000,000-10,010,000"" \; --examples output/examples.tfrecord@${N_SHARDS}.gz\; --channels insert_size \; --task {} \; || exit 1; ```; - Error trace: (if applicable); ```; META: 0s Left: 48 AVG: 0.00s local:48/0/100%/0.0s ESC[Ksh: /dev/tty: No such device or address; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/786:87,simpl,simple,87,,https://github.com/google/deepvariant/issues/786,1,['simpl'],['simple']
Usability,"**Describe the issue:**; Since I couldn't run DeepVariant with Docker, I thought I'd try the prebuilt binaries version, but I couldn't find a guide on how to use the prebuilt binaries DeepVariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/590:142,guid,guide,142,,https://github.com/google/deepvariant/issues/590,1,['guid'],['guide']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/459:120,clear,clear,120,,https://github.com/google/deepvariant/issues/459,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.1/docs/FAQ.md**:; Yes. **Describe the issue:**; Launching an Ubuntu 20.04 server t2 micro EC2 on AWS, installed docker using snap, downloaded data from quickstart guide verbatim https://github.com/google/deepvariant/blob/r1.0/docs/deepvariant-quick-start.md. **Setup**; - Operating system: Ubuntu 20.04 server t2 micro EC2 on AWS; - DeepVariant version: BIN_VERSION=""1.1.0""; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Quick start data. **Steps to reproduce:**; - Command:; ```; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.1.0"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir; ```. - Error trace: (if applicable); ```; Unable to find image 'google/deepvariant:1.1.0' locally; 1.1.0: Pulling from google/deepvariant; be8ec4e48d7f: Pull complete ; 33b8b485aff0: Pull complete ; d887158cc58c: Pull complete ; 05895bb28c18: Pull complete ; 35be0878dcf6: Pull complete ; 03fb656082b2: Pull complete ; 1d3e393af6d8: Pull complete ; 9663085972fa: Pull complete ; 10ac03989960: Pull complete ; 401f11974a9b: Pull complete ; 67f12673f7e4: Pull complete ; 99116330e4f4: Pull complete ; 6fbbce8e3587: Pull complete ; c223e83ce2e3: Pull complete ; c02ebb3220a1: Pull complete ; 0c7a427ce17a: Pull complete ; ec9cd66333fe: Pull complete ; 9d57046ae5b9: Pull complete ; 0f54",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/462:237,guid,guide,237,,https://github.com/google/deepvariant/issues/462,1,['guid'],['guide']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.2/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; can deepvariant detect multiallelic positions, for example, Ref is A, and Alt is C, G. And the GT is denoted as 1/2",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/480:120,clear,clear,120,,https://github.com/google/deepvariant/issues/480,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**: Yes. **Describe the issue:**; (A clear and concise description of what the issue is.); Running singularity on HPC returns this error, our HPC does not have docker so I assumed singularity would work: . **Setup**; - Operating system: Linux HPC; - DeepVariant version: 1.3.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: WES. **Steps to reproduce:**; ```; #!/bin/bash --login; #SBATCH -J AmyHouseman_deepvariant; #SBATCH -o %x.stdout.%J.%N; #SBATCH -e %x.stderr.%J.%N; #SBATCH --ntasks=1; #SBATCH --ntasks-per-node=1; #SBATCH -p c_compute_wgp; #SBATCH --account=scw1581; #SBATCH --mail-type=ALL # Mail events (NONE, BEGIN, END, FAIL, ALL); #SBATCH --mail-user=HousemanA@cardiff.ac.uk # Where to send mail; #SBATCH --array=1-33; #SBATCH --time=02:00:00; #SBATCH --time=072:00:00; #SBATCH --mem-per-cpu=32GB. module purge; module load singularity; module load parallel. set -eu. cd /scratch/c.c21087028/; BIN_VERSION=""1.3.0"". singularity pull docker://google/deepvariant:""${BIN_VERSION}"". sed -n ""${SLURM_ARRAY_TASK_ID}p"" Polyposis_Exome_Analysis/fastp/All_fastp_input/List_of_33_exome_IDs | parallel -j 1 ""singularity run singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; -ref=Polyposis_Exome_Analysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna \; --reads=Polyposis_Exome_Analysis/samtools/index/indexed_picardbamfiles/{}PE_markedduplicates.bam \; --output_vcf=Polyposis_Exome_Analysis/deepvariant/vcf/{}PE_output.vcf.gz \; --output_gvcf=Polyposis_Exome_Analysis/deepvariant/gvcf/{}PE_output.vcf.gz \; --intermediate_results_dir=Polyposis_Exome_Analysis/deepvariant/intermediateresults/{}PE_output_intermediate""; ```. **Error::**. ``FATAL: While making image from oci registry: error fetching image to cache: failed to get checksum for docker://google/d",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/522:124,clear,clear,124,,https://github.com/google/deepvariant/issues/522,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/525:120,clear,clear,120,,https://github.com/google/deepvariant/issues/525,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.3/docs/FAQ.md**:; YES. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: ubuntu **16.04**; - DeepVariant version: **1.1.0**; - Installation method (Docker, built from source, etc.): **built from source**; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) **WGS**. **Here is the problem:** I was trying to count reference supporting reads by the class ""**ReadSupportsAlt**"" defined in pileup_image_native.cc. To make sure it was correct, I also printed out the first value of Allele Depth (""**AD**"") for reference supporting reads. However, it turned out that there was an inconsistent number of reads counted by these two ways. To be more specific, there were more reference supporting reads counted by ""**ReadSupportAlt**"" than “**AD**“ did in general. At the very beginning, I thought it was non-alternate-allele reads that made this kind of inconsistent, then I viewed log files. Unfortunately, I found that there were at least 2 more reference supporting reads counted by ""**ReadSupportAlt**"" than “**AD**“ did (SNP, min_counts_snps = 2). So I am confused with the result. I would appreciate it if someone help me with this issue.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/529:125,clear,clear,125,,https://github.com/google/deepvariant/issues/529,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**: Yes. **Describe the issue:**; (A clear and concise description of what the issue is.). A variant with VAF value 1 is called as heterozygous. The IGV visualisation of the .bam file shows that it should clearly be a homozygous variant. ![igv_panel_chr2_24146804](https://user-images.githubusercontent.com/84016709/204764192-9bd69aa6-7f23-4490-9391-7af95e909e3f.png). Here the line from .vcf file:; ```; chr2	24146804	.	C	T	29.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:3:162:0,162:1:26,0,0; ```; .gvcf file:; ```; chr2	24146804	.	C	T,<*>	29.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:3:162:0,162,0:1,0:26,0,0,990,990,990; ```. We also asked our collaborators to run the same sample and in their results, a homozygous variant is called:; ```; chr2	24146804	.	C	T	30.8	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:5:161:0,161:1:28,2,0; ```; ```; chr2	24146804	.	C	T,<*>	30.8	PASS	.	GT:GQ:DP:AD:VAF:PL	1/1:5:161:0,161,0:1,0:28,2,0,990,990,990; ```. What could cause this discrepancy, if the DeepVariant versions and commands are the same?. **Setup**; - Operating system: Ubuntu16.04; - DeepVariant version: 1.2.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); NovaSeq 6000 using Twist Comprehensive Exome with mtDNA add-in, GRCh38. **Steps to reproduce:**; - Command:; ```; docker run \; -v ${MOUNT_DIR}:${MOUNT_DIR} \; google/deepvariant:1.2.0-rc0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""${REFERENCE}"" \; --reads=""${INPUT}"" \; --regions=""${CAPTURE_KIT}"" \; --output_vcf=${OUTPUT_VCF} \; --output_gvcf=${OUTPUT_GVCF} \; --num_shards=64 \; --postprocess_variants_extra_args=""only_keep_pass=true""; ```; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/592:124,clear,clear,124,,https://github.com/google/deepvariant/issues/592,2,['clear'],"['clear', 'clearly']"
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:** After running the code in the deepvariant docker container (quick start), the output vcf files have not been generated.; (A clear and concise description of what the issue is.). **Setup**; - Operating system:Mac OS ; - DeepVariant version: Latest; - Installation method (Docker, built from source, etc.): Docker; - Type of data: Test files(sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: sudoa docker run \-v ""${INPUT_DIR}"":""/input"" \-v ""${INPUT_DIR}"":""/output"" \google/deepvariant:""${BIN_VERSION}"" \/opt/deepvariant/bin/run_deepvariant \--model_type=WES \--ref=/input/ucsc.hg19.chr20.unittest.fasta \--reads=/input/NA12878_S1.chr20.10_10p1mb.bam \--regions ""chr20:10,000,000-10,010,000"" \--output_vcf=/output/output.vcf.gz \--output_gvcf=/output/output.g.vcf.gz \--num_shards=1 \--dry_run=true; - Error trace: No error.(if applicable)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/561:240,clear,clear,240,,https://github.com/google/deepvariant/issues/561,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system: CentOS Linux release 7.9.2009; - DeepVariant version: deepvariant:0.9.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); - Illumina, HG38, standard capture panel. **Steps to reproduce:**; - Command: Snakemake command:; - docker --rm -v {params.input_dir}/:/input -v {params.output_dir}/{params.sample}_DeepVariant:/output -v /data:/data -v {params.bed_dir}:/bed --user $CURRENT_UID google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/{params.sample}.bam --regions=/bed/{params.primary_bed} --output_vcf=/output/{params.sample}_DeepVariant.vcf.gz --output_gvcf=/output/{params.sample}_DeepVariant.gvcf.gz --num_shards=12; - actual command (XXXXX = removed for security purposes) ; - docker --rm -v XXXXXXXXX/gatk_align_metrics_t/:/input -v XXXXXXXXX/deep_variant2/xGENIDTn2_DeepVariant:/output -v /XXXXXXXXX/deepvariant/data:/data -v XXXXXXXXX/bed:/bed google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WES --ref=/data/hg38.fa.gz --reads=/input/xGENIDTn2.bam --regions=/bed/xgen-exome-hyb-panel-v2-targets-hg38.bed --output_vcf=/output/xGENIDTn2_DeepVariant.vcf.gz --output_gvcf=/output/xGENIDTn2_DeepVariant.gvcf.gz --num_shards=12; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, the quickstart creates files as root. As it's a high performance computing cluster, I am no longer able to delete these files. How do I stop it from creating files as root?. **Any additional",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/550:120,clear,clear,120,,https://github.com/google/deepvariant/issues/550,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.); Issue encountered during running with Docker, thinking it is possibly due to tf not supported by m1 chip, here is the issue. ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine.; qemu: uncaught target signal 6 (Aborted) - core dumped. **Setup**; - Operating system: MacOs (Mac mini/ m1 chip); - DeepVariant version:1.4.0; - Installation method (Docker, built from source, etc.): Docker; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); The test data from GitHub; **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/545:120,clear,clear,120,,https://github.com/google/deepvariant/issues/545,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:; Yes. **Describe the issue:**; I was following the quick start guide for running singularity on a gpu node. Initially, I encounter the dynamic cast failed error similar to #559 . After installing the google-nucleus package, I encountered this new error about protobuf package. I tried protobuf version 3.20.3 and 4.21.9, but the error message is the same. In order to run DeepVariant successfully, what additional packages (version) should I install besides cloning the singularity image?. **Setup**; - Operating system: ; - DeepVariant version: 1.4.0; - Installation method: singularity; - Type of data: quick start test; ; **Steps to reproduce:**; ; ```; SINGULARITY_TMPDIR=/scratch/midway3/weilu1/tmp SINGULARITY_CACHEDIR=/scratch/midway3/weilu1/cache singularity run --nv -B /usr/lib/locale/:/usr/lib/locale/ \; deepvariant_1.4.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \; --num_shards=1. INFO: Converting SIF file to temporary sandbox...; WARNING: underlay of /usr/bin/nvidia-smi required more than 50 (469) bind mounts; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 48, in <module>; import tensorflow as tf; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/__init__.py"", line 41, in <module>; from tensorflow.python.tools import module_util as _module_util; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/__init__.py"", line 41, in <module>; from tensorflow.python.eager import context; File ""/usr/local/lib/python3.8/dist-packages/tensorflow/python/eager/context.py"", line 33, in <module>;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/580:154,guid,guide,154,,https://github.com/google/deepvariant/issues/580,1,['guid'],['guide']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:; Yes; **Describe the issue:**; A potential denovo variant is filtered out due to mendelian violation. While found the deletion in the same sample via GATK and IGV (both raw BAM file and realigned BAM file from GATK HaplotypeCaller). Wonder how to loosen the criteria to increase the recall of denovo variants.; (A clear and concise description of what the issue is.). **Setup**; - Operating system: CentOS7; - DeepVariant version: 1.4; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); - 150bp paired-end Illumina data. **Steps to reproduce:**; - Command: ; `/opt/deepvariant/bin/run_deepvariant \; --model_type=${model_type} \; --ref=""${ref_genome}"" \; --reads=""${bam_file}"" \; --make_examples_extra_args=""normalize_reads=true"" \; ${region_arg} \; --output_vcf=""${output_vcf}"" \; --output_gvcf=""${output_gvcf}"" \; --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp/${singularity_inter}"" \; --num_shards=${threads}`; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**; Here is the IGV screenshot of the position where DeepVariant failed to identify one bp deletion (Upper panel illustrates the alignment from raw BAM file, lower panel illustrates the alignment from the realigned BAM file from GATK HaplotypeCaller):; ![image](https://user-images.githubusercontent.com/40780228/218404096-273ed999-6443-43c2-83b9-108661d738d4.png). P.S. Please consider granting a parameter of DeepVariant to let users generate the realigned BAM file from DeepVariant. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/612:405,clear,clear,405,,https://github.com/google/deepvariant/issues/612,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.4/docs/FAQ.md**:; Yes; **Describe the issue:**; At the call_variants.py step, running into error that tensorflow.python.framework.errors_impl.DataLossError: truncated record at 19179998357' failed with EOF reached; (A clear and concise description of what the issue is.). **Setup**; - Operating system:CentOS7 ; - DeepVariant version:1.4.0; - Installation method (Docker, built from source, etc.):singularity run with SIF image pulled from docker://google/deepvariant:""${BIN_VERSION}""; - Type of data: (sequencing instrument: BGI, reference genome: hg19, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: ; - `singularity run \; -B ""/paedyl01/disk1/yangyxt,/usr/lib/locale:/usr/lib/locale,/tmp:/paedyl01/disk1/yangyxt/test_tmp"" \; --workdir /paedyl01/disk1/yangyxt \; ${image} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=${model_type} \; --ref=""${ref_genome}"" \; --reads=""${bam_file}"" \; ${region_arg} \; --output_vcf=""${output_vcf}"" \; --output_gvcf=""${output_gvcf}"" \; --intermediate_results_dir ""/paedyl01/disk1/yangyxt/test_tmp"" \; --num_shards=${threads} && \; ls -lh ${output_vcf} && \; ls -lh ${output_gvcf}`; - Error trace: (if applicable); - ; - `***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/paedyl01/disk1/yangyxt/test_tmp/call_variants_output.tfrecord.gz"" --examples ""/paedyl01/disk1/yangyxt/test_tmp/make_examples.tfrecord@14.gz"" --checkpoint ""/opt/models/wgs/model.ckpt"" --openvino_model_dir ""/paedyl01/disk1/yangyxt/test_tmp"". I0826 20:44:28.894064 47737984214848 call_variants.py:317] From /paedyl01/disk1/yangyxt/test_tmp/make_examples.tfrecord-00000-of-00014.gz.example_info.json: Shape of input examples: [100, 221, 7], Channels of input examples: [1, 2, 3, 4, 5, 6, 19].; I0826 20:44:28.898550 47737984214848 call_variants.py:317] From /opt/models/wgs/model.ckpt.example_info.json: Shape of input examples: [100, 221, ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/564:292,clear,clear,292,,https://github.com/google/deepvariant/issues/564,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**: YES. **Describe the issue:**; (A clear and concise description of what the issue is.); CANNOT RUN EXAMPLE DATA USING A SINGULARITY CONTAINER - GETTING AN ERROR: RuntimeError: module compiled against API version 0x10 but this version of numpy is 0xf . Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem. **Setup**; - Operating system: Ubuntu 18.04 (bionic); - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): SINGULARITY sif made as follows:; BIN_VERSION=""1.5.0""; singularity pull deepvariant.sif docker://google/deepvariant:""${BIN_VERSION}""; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); EXAMPLE DATA PROVIDED. **Steps to reproduce:**; - Command:. INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". singularity exec --bind ""${INPUT_DIR}"":""/input"",""${OUTPUT_DIR}"":""/output"",/usr/lib/locale/:/usr/lib/locale/ \; /fh/fast/furlan_s/grp/sifs/deepvariant.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz. - Error trace: (if applicable) SEE BELOW. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. YES THIS IS WITH THE QUICK START EXAMPLE. **Any additional context:**. Message:. 2023-05-02 14:40:43.757041: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural N",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/640:124,clear,clear,124,,https://github.com/google/deepvariant/issues/640,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/651:120,clear,clear,120,,https://github.com/google/deepvariant/issues/651,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Slurm based ; - DeepVariant version: deepvariant_1.5.0.sif; - Installation method : singularity image ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) . **Steps to reproduce:**; - Command:. projDir=/home1/***/***/deepvaraint/; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL2.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL2.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1; apptainer exec --bind $projDir /home1/***/***/deepvaraint/deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant --model_type WES --ref /work/***/***/data/common/human/b37/human_g1k_v37_decoy.fasta --reads /scratch/***/***/deepvariant_test/test/DupMarkedBams/FPL3.DupsMarked.bam --output_vcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.vcf.gz --output_gvcf /scratch/***/***/deepvariant_test/test/output_test/FPL3.output.g.vcf.gz --logging_dir /scratch/***/***/deepvariant_test/test/output_test --intermediate_results_dir /scratch/***/***/deepvariant_test/test/output_test --num_shards 16 2>&1. - Error trace: (if applicable). Launcher: Task 2 running job 1 on c304-012.ls6.tacc.utexas.edu (#!/bin/bash); Launcher: Job 1 completed in 0 seconds.; Launcher: Task 2 running job 2 on c304-012.ls6.tacc.utexas.edu (projDir=/home1/***/***/deepvaraint/); Launcher: Job 2 completed in 0 seconds",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/717:120,clear,clear,120,,https://github.com/google/deepvariant/issues/717,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). I want to use singularity to install software **DeepVariant**, but it generates an error, is there some suggestion.thanks. **Setup**; - Operating system: linux（Centos）; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.):singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: **/projects/liming/Software/mambaforge-pypy3/envs/singularity/bin/singularity pull /projects/liming/Software/deepvariant/deepvariant.sif docker://google/deepvariant:""1.5.0""**; - Error trace: (if applicable); <img width=""953"" alt=""image"" src=""https://github.com/google/deepvariant/assets/26595839/035ed38c-3a15-45e8-8bb3-dc0e0cfc3200"">. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/668:120,clear,clear,120,,https://github.com/google/deepvariant/issues/668,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.); Hi developers,; I'd like to run `DeepVariant` for my `WGS` sequencing data. My sequencing data were from `BGI` platform and were preprocessed by `fastp, bwa+Hs37d5, MarkDuplicatesSpark`. I tried to use the 'sorted and deduplicated bam' file as input for `DeepVariant` in `singularity` mode. However, I always encountered the 'reference index' `not found` error. But my `reference` fasta file and `reference index` fai file does exist. Could you please help me figure it out?. **Setup**; - Operating system: Linux version 3.10.0-1127.el7.x86_64 (gcc version 4.8.5 20150623 (Red Hat 4.8.5-39), Computation Node (one node of Clusters); - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.):; - ``` BIN_VERSION=""1.5.0""; docker pull; ; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; singularity build --fakeroot deepvariant.sif docker://google/deepvariant:1.5.0```; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); `BGI platform, WGS data, Hs37d5 reference, fastp QC, bwa-mem2 mapping, MarkDuplicatesSpark sort & dedup`; . **Steps to reproduce:**; - Command:; - 1. singularity run /lustre/Data/toolsDB//deepvariant.sif /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=$ref_idx --reads=$dedupbam --output_vcf=$vcfout --output_gvcf=$gvcfout --num_shards=32 >$logx 2>&1; - Error trace: (if applicable); - ```I0522 08:40:36.823651 140633630893888 genomics_reader.py:222] Reading /lustre/home/zhoujianglin/datasets/2304GQS_FSZ_SNP/mappinged_bams/2-13A_bwa2Hs37d5_sorted_dedup.bam with NativeSamReader; I0522 08:40:36.846348 140633630893888 make_examples_core.py:257] Task 27/32: Preparing inputs; [E::fai_load3_core] Failed to open FASTA index /lustre/Data/toolsDB/HostRefs/Human_hs37d5/hs37d5.fa.fai: No such file or",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/653:120,clear,clear,120,,https://github.com/google/deepvariant/issues/653,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:; Yes. Same error msgs were observed. But I was lunching deepvariant with singularity; **Describe the issue:**; (A clear and concise description of what the issue is.); The same error msgs were observed just like described in FAQ. But this time I was lunching deepvariant and testing dataset with singularity.; **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable); module load singularity; BIN_VERSION=""1.5.0""; singularity pull docker://google/deepvariant:""${BIN_VERSION}""; LABASE=""/N/project/Walker_lab/PacBio_Revio_WGS/Human_HiFi_0623/tools""; INPUT_DIR=""${LABASE}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; OUTPUT_DIR=""${LABASE}/quickstart-output""; mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ls -1 ${INPUT_DIR}; mkdir -p ${OUTPUT_DIR}; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvar",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/678:205,clear,clear,205,,https://github.com/google/deepvariant/issues/678,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.5/docs/FAQ.md**:; Yes.; **Describe the issue:**; (A clear and concise description of what the issue is.); I have several human samples of PacBio HIFI reads with on average 20X depth. I was trying to call out small variants using deepvariant. However, it's been three days and the program is still at 'make_examples' stage.; **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); Operating system:; Redhat enterprise v7.9, x86_64; **Steps to reproduce:**; - Command:; BIN_VERSION=""1.5.0""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ --bind ${INPUT_DIR} --bind ${OUTPUT_DIR} \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=PACBIO \; --ref=""${INPUT_DIR}""/HG38.fa \; --reads=""${INPUT_DIR}""/0661-349-4156123_PDX_m15.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=4 \; --intermediate_results_dir=""${OUTPUT_DIR}""/tmp_dir \; --make_examples_extra_args=""vsc_min_fraction_snps=0.2,vsc_min_fraction_indels=0.2""; I allocated 4 cores and 70GBs to run that program.; I added the VAF thresholds for SNPs and Indels because I read the reported issues:; https://github.com/google/deepvariant/issues/578; - Error trace: (if applicable); And here are some most recent results I got from stdout:; I0720 09:27:03.965433 47167827691328 make_examples_core.py:257] 7300984 candidates (8293381 examples) [14.77s elapsed]; I0720 09:27:18.676311 47167827691328 make_examples_core.py:257] 7302320 candidates (8294814 examples) [14.71s elapsed]; I0720 09:28:15.982849 47167827691328 make_examples_core.py:257] 7304006 candidates (8296543 examples) [57.31s elapsed]; I0720 09:30:09.747373 47167827691328 make_examples_core.py:257] 7306537 candidates (8299",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/683:126,clear,clear,126,,https://github.com/google/deepvariant/issues/683,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**: Yes. **Describe the issue:**; (A clear and concise description of what the issue is.). Fatal Python error: Segmentation fault when make_examples. **Setup**; - Operating system: Cent; - DeepVariant version: 1.6.0; - Installation method (Docker, built from source, etc.): singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?); PacBio HiFi data, but the quality was added by `seqtk -X 5` with one fasta. It worked with 30 samples, but one chromosome of one sample cannot finished with this error. **Steps to reproduce:**; - Command:; ```bash; #!/bin/bash; sample=$1; threads=$2. chr=$3; indir=""01.mapping""; outdir=""02.snps""; sif=""dv-1.6.0.sif"". singularity exec -B ${indir}:/input -B ${outdir}:/output ${sif} /bin/bash -c ""/opt/deepvariant/bin/run_deepvariant --model_type PACBIO --ref /input/ref.fa --reads /input/${sample}.sorted.bam --regions chr${chr} --output_vcf=/output/${sample}.chr${chr}.vcf.gz --output_gvcf=/output/${sample}.chr${chr}.g.vcf.gz --intermediate_results_dir=/output/${sample}_chr${chr} --num_shards=${threads} --sample_name=${sample}""; rm -rf ${outdir}/${sample}_chr${chr}; ```; - Error trace: (if applicable); ```bash; Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]; Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]; Warning: The alignment path of one pair of sequences may miss a small part. [ssw.c ssw_align]; I0325 17:32:25.437496 47491250571072 make_examples_core.py:301] Task 0/48: 3061 candidates (3283 examples) [15.51s elapsed]; I0325 17:32:25.481451 47092596426560 make_examples_core.py:301] Task 3/48: 3479 candidates (3686 examples) [15.88s elapsed]; I0325 17:32:25.287480 47393598515008 make_examples_core.py:301] Task 1/48: 2217 candidates (2340 examples) [4.86s elapsed]; I0325 17:32:27.143459 47041007318848 mak",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/794:126,clear,clear,126,,https://github.com/google/deepvariant/issues/794,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). Hello All,. I have been testing ONT datasets on the HPC cluster to benchmark and optimize them. While using the mapped ONT BAM files from the HG002 and HG003 datasets from the UCSC studies, I observed that DeepVariant gets stuck at the make_examples stage. Even after 24 hours, it remains in the same stage which is unsual. I would appreciate your input on this issue. **Setup**; - Operating system: Linux, HPC cluster; - DeepVariant version: 1.5.0; - Installation method (Docker, built from source, etc.): Singularity; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) ; -ONT : https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/UCSC_Ultralong_OxfordNanopore_Promethion/HG002_GRCh38_ONT-UL_UCSC_20200508.phased.bam; reference -hg38 . **Steps to reproduce:**; - Command: . apptainer exec ; --bind Deepvariant/HG002_HG003_1.5.0 deepvariant_1.5.0.sif /opt/deepvariant/bin/run_deepvariant ; --model_type ONT_R104 ; --ref Homo_sapiens_assembly38.fasta ; --reads HG002_GRCh38_ONT-UL_UCSC_20200508.phased.bam ; --output_vcf HG002_chr1.output.vcf.gz ; --output_gvcf HG002_chr1.output.g.vcf.gz ; --regions chr1 --num_shards 56 --logging_dir chr1 ; --intermediate_results_dir chr1/intermediate_results . - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. Yes, it did work. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/856:122,clear,clear,122,,https://github.com/google/deepvariant/issues/856,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; I tried to test run deepvariant following the quick-start guide at https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-quick-start.md and I got `Fatal Python error: Segmentation fault`. **Setup**; - Operating system: Ubuntu 20.04.6 LTS; - DeepVariant version: r1.6.1; - Installation method (Docker, built from source, etc.): Docker; - Type of data: exact same data in the quick start guide. **Steps to reproduce:**; - Command:; ``` ; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; deepvbuild:latest \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --intermediate_results_dir /output/intermediate_results_dir \; --num_shards=1; ```; - Error trace:; ```; I0906 02:45:46.585311 275767425675280 run_deepvariant.py:519] Re-using the directory for intermediate results in /output/intermediate_results_dir. ***** Intermediate results will be written to /output/intermediate_results_dir in docker. ****. ***** Running the command:*****; time seq 0 0 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ucsc.hg19.chr20.unittest.fasta"" --reads ""/input/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/output/intermediate_results_dir/make_examples.tfrecord@1.gz"" --channels ""insert_size"" --gvcf ""/output/intermediate_results_dir/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. I0906 02:45:51.909050 257960059396112 genomics_reader.py:222] Reading /input/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0906 02:45:51.913105 257960059396112 make_examples_core.py:301] Preparing inputs; I0906 02:45:51.913431 257960059396112 genomics_reader.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/879:177,guid,guide,177,,https://github.com/google/deepvariant/issues/879,2,['guid'],['guide']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**; (A clear and concise description of what the issue is.); `run_deepvariant` is erroring out in the `postprocess_variants` step. **Setup**; - Operating system: Running inside docker image - `google/deepvariant:1.6.0-gpu`; - DeepVariant version: `1.6.0`; - Installation method (Docker, built from source, etc.): Docker image - `google/deepvariant:1.6.0-gpu`; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command: Running the quickstart cmd --; ```; /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/opt/deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta --reads=/opt/deepvariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam --regions ""chr20:10,000,000-10,010,000"" --output_vcf=/opt/deepvariant/quickstart-output/output.vcf.gz --output_gvcf=/opt/deepvariant/quickstart-output/output.g.vcf.gz --intermediate_results_dir /opt/deepvariant/quickstart-output/intermediate_results_dir --num_shards=1 --verbosity=2; ```. - Error trace: (if applicable) In the `postprocess_variants` step; ```; ***** Running the command:*****; time /opt/deepvariant/bin/postprocess_variants --ref ""/opt/deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --infile ""/opt/deepvariant/quickstart-output/intermediate_results_dir/call_variants_output.tfrecord.gz"" --outfile ""/opt/deepvariant/quickstart-output/output.vcf.gz"" --cpus ""1"" --gvcf_outfile ""/opt/deepvariant/quickstart-output/output.g.vcf.gz"" --nonvariant_site_tfrecord_path ""/opt/deepvariant/quickstart-output/intermediate_results_dir/gvcf.tfrecord@1.gz"". 2024-10-31 20:36:34.101345: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PA",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/901:127,clear,clear,127,,https://github.com/google/deepvariant/issues/901,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes; **Describe the issue:**; Although some variants are clearly heterozygous in IGV, deepvariant GT shows a homozygous genotype. **Setup**; - Operating system: linux; - DeepVariant version: 1.6.1; - Installation method : Docker; - Type of data: illumina, WES, hg38. **Steps to reproduce:**; ```; docker run --rm -i \; -v ${ref_dir}:/opt/ref \; -v ${kit_dir}:/opt/kit \; -v ${input_dir}:/opt/sample \; ${deepvariant_docker} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/opt/ref/${ref_fasta}"" \; --reads=""/opt/sample/${input_bam_file}"" \; --regions=""/opt/kit/${kit_bed_file}"" \; --output_vcf=""/opt/sample/${input_bam_file/.bam/.dv.vcf}"" \; --num_shards=""${threads}""; ```. Here are 5 selected variants called by 5 different versions of deepvariant:; ```; v0.10.0 chr12 11353713 . T C 57.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:54:43:27,15:0.348837:57,0,55; v1.1.0 chr12 11353713 . T C 36.6 PASS . GT:GQ:DP:AD:VAF:PL 0/1:34:44:28,15:0.340909:36,0,38; v1.4.0 chr12 11353713 . T C 23.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:21:44:28,15:0.340909:23,0,25; v1.5.0 chr12 11353713 . T C 24.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:17:44:28,15:0.340909:24,0,17; v1.6.1 chr12 11353713 . T C 24.6 PASS . GT:GQ:DP:AD:VAF:PL 1/1:5:44:28,15:0.340909:22,3,0; -----------------------------; v0.10.0 chr3 195779035 . G A 3.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:4:33:24,8:0.242424:1,0,38; v1.1.0 chr3 195779035 . G A 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:41:33:24,8:0.242424:0,43,45; v1.4.0 chr3 195779035 . G A 0 RefCall . GT:GQ:DP:AD:VAF:PL 0/0:23:33:24,8:0.242424:0,32,22; v1.5.0 chr3 195779035 . G A 0.1 RefCall . GT:GQ:DP:AD:VAF:PL ./.:16:33:24,8:0.242424:0,26,16; v1.6.1 chr3 195779035 . G A 13.7 PASS . GT:GQ:DP:AD:VAF:PL 1/1:14:33:24,8:0.242424:13,28,0; -----------------------------; v0.10.0 chr6 159711482 . C T 46.5 PASS . GT:GQ:DP:AD:VAF:PL 0/1:36:70:38,32:0.457143:46,0,36; v1.1.0 chr6 159711482 . C T 6.9 PASS . GT:GQ:DP:A",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/824:151,clear,clearly,151,,https://github.com/google/deepvariant/issues/824,1,['clear'],['clearly']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes; **Describe the issue:**; This isn't a code problem, but rather a documentation issue. I've run DeepVariant via your docker with success. To integrate it with our project I would like to install it via conda. I was able to do that but it isn't clear how to run deep variant. Do you have documentation/examples of what commands to send? . When using docker, we invoke the google/deepvariant:1.6.1 image and send it the command ""/opt/deepvariant/bin/run_deepvariant"" with appropriate arguments. What do we run when using conda? . Note the docs/deepvariant-quick-start.md has examples for docker (very useful and they work with our data) but nothing for conda. **Setup**; - Operating system: linux; - DeepVariant version: 1.5.0 (latest from conda); - Installation method (Docker, built from source, etc.): conda; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. Do you have plans to update conda with the latest deepvariant version? It is still at 1.5.0. Thanks",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/865:342,clear,clear,342,,https://github.com/google/deepvariant/issues/865,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**: yes. **Describe the issue:** ; (A clear and concise description of what the issue is.). Hi, I am trying to set up DeepVariant on our server and would like to use udocker. It runs fine for the make_examples but It gets stuck with call_variants. I get the same error with both my data and the quick start. If I enable intermediate_results_dir, I can actually see the files being generated as expected. Could you please help me? . **Setup**; - Operating system: Red Hat Enterprise Linux 8.6; - DeepVariant version: 1.6.0; - Installation method (Docker, built from source, etc.): Docker (run via udocker); - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) data from the quick start . **Steps to reproduce:**; - Command:. ```; udocker run \; -v ${INPUT_DIR}:""/input"" \; -v ${OUTPUT_DIR}:""/output"" \; DeepVariant \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/""ucsc.hg19.chr20.unittest.fasta"" \; --reads=/input/""NA12878_S1.chr20.10_10p1mb.bam"" \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf=/output/output.g.vcf.gz \; --num_shards=16; ```. - Error trace: (if applicable). ```; ***** Running the command:*****; time /opt/deepvariant/bin/call_variants --outfile ""/tmp/tmpz5qvn8j2/call_variants_output.tfrecord.gz"" --examples ""/tmp/tmpz5qvn8j2/make_examples.tfrecord@16.gz"" --checkpoint ""/opt/models/wgs"". /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorfl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/733:125,clear,clear,125,,https://github.com/google/deepvariant/issues/733,1,['clear'],['clear']
Usability,"**Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/743:120,clear,clear,120,,https://github.com/google/deepvariant/issues/743,1,['clear'],['clear']
Usability,". 27.87% , 0.05% ,python ,libssw.so ,[.] ssw_align; | ; --27.82%--ssw_align; | ; |--14.65%--sw_sse2_word; | ; |--8.32%--sw_sse2_byte; | ; |--2.91%--banded_sw; | ; --1.19%--__memcpy_sse2_unaligned. 14.65% , 14.62% ,python ,libssw.so ,[.] sw_sse2_word; | ; --14.62%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --14.62%--ssw_align; sw_sse2_word. 8.32% , 8.31% ,python ,libssw.so ,[.] sw_sse2_byte; | ; --8.31%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --8.30%--ssw_align; sw_sse2_byte. 4.51% , 0.00% ,python ,[unknown] ,[.] 0x00000000009063e0; |; ---0x9063e0; | ; --3.66%--PyEval_EvalFrameEx; | ; --3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | ; --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | ; --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | ; --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | ; --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | ; --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | ; --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. #### DV 0.5.1. ```; # Samples: 152K of event 'cpu-clock'; # Event count (approx.): 38010500000; #; # Children, Self,Command ,Shared Object ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.33%--PyEval_EvalFrameEx; | | ; | |--31.12%--deepvariant_realigner_python",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:6008,learn,learning,6008,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,".com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:; Yes. **Describe the issue:**; google/deepvariant:1.5.0-gpu google/deepvariant:1.6.1-gpu docker images run as CPU-only because they are using ancient CUDA 11.3.1; Could maintainers build newer docker images with CUDA >=12.4 or at least >=11.8 to be able to use modern cards such as H100 and L40S (CUDA CC = 8.9 and 9.0). **Setup**; - Operating system: RHEL 8.10; - DeepVariant version: 1.5.0-gpu, 1.6.1-gpu; - Installation method (Docker, built from source, etc.): docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Pacbio Revel fresh data. . **Steps to reproduce:**; - Command: docker run --gpus 1 google/deepvariant:1.5.0-gpu or docker run --gpus 1 google/deepvariant:1.6.1-gpu; - Error trace: (if applicable); ...; CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-03 17:21:57.549571: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-07-03 17:21:57.644332: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variabl",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:1064,Learn,Learning,1064,,https://github.com/google/deepvariant/issues/844,1,['Learn'],['Learning']
Usability,".tfrecord.gz""; num_examples: 147448; ```. The training command looks like this:. ```; LR=0.001; BS=1024. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such training run:. ```; I1031 10:55:27.365902 140558597089024 logging_writer.py:48] [0] epoch=0, train/categorical_accuracy=0.91796875, train/categorical_crossentropy=0.6384725570678711, train/f1_het=0.7428571581840515, train/f1_homalt=0.964401364326477, train/f1_homref=0.902255654335022, train/f1_macro=0.; 8698380589485168, train/f1_micro=0.91796875, train/f1_weighted=0.9241795539855957, train/false_negatives=34.0, train/false_positives=14.0, train/learning_rate=9.999999747378752e-06, train/loss=0.6384731531143188, train/precision=0.9406779408454895, train/precision_het=0.702702701091; 7664, train/precision_homalt=0.978723406791687, train/precision_homref=1.0, train/recall=0.8671875, train/recall_het=1.0, train/recall_homalt=0.8789808750152588, train/recall_homref=0.7945205569267273, train/true_negatives=498.0, train/true_positives=222.0; I1031 11:18:53.873582 14055859",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:1858,learn,learning,1858,,https://github.com/google/deepvariant/issues/904,1,['learn'],['learning']
Usability,"0.7.2+data-wes_standard: Failed precondition: /mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?; [[{{node save_1/RestoreV2}} = RestoreV2[dtypes=[DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, ..., DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_FLOAT, DT_INT64], _device=""/job:localhost/replica:0/task:0/device:CPU:0""](_arg_save_1/Const_0_0, save_1/RestoreV2/tensor_names, save_1/RestoreV2/shape_and_slices)]]; ```. This is the script that I am running DeepVariant:. ```; OUTPUT_DIR=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/DeepVariant; REF=/mnt/efs-genome/Ref/hg19.gatk.fasta; BAM=/mnt/efs-genome/Charles_Human/Genos_Exome/Genos_Alignment/82651510240740.mapped.sorted.markdup.realn.recal.bam; MODEL=/mnt/efs-genome/Ref/DeepVariant/DeepVariant-inception_v3-0.7.2+data-wes_standard. ## step #1. LOGDIR=logs; N_SHARDS=4. #mkdir -p ""${LOGDIR}""; #time seq 0 $((N_SHARDS-1)) | \; # parallel --eta --halt 2 --joblog ""${LOGDIR}/log"" --res ""${LOGDIR}"" \; # sudo docker run \; # -v /mnt/efs-genome:/mnt/efs-genome \; # gcr.io/deepvariant-docker/deepvariant \; # /opt/deepvariant/bin/make_examples \; # --mode calling \; # --ref ""${REF}"" \; # --reads ""${BAM}"" \; # --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; # --task {}. ## step #2. CALL_VARIANTS_OUTPUT=""${OUTPUT_DIR}/call_variants_output.tfrecord.gz"". sudo docker run \; -v /mnt/efs-genome:/mnt/efs-genome \; gcr.io/deepvariant-docker/deepvariant \; /opt/deepvariant/bin/call_variants \; --outfile ""${CALL_VARIANTS_OUTPUT}"" \; --examples ""${OUTPUT_DIR}/examples.tfrecord@${N_SHARDS}.gz"" \; --checkpoint ""${MODEL}""; ```. Can you please help me troubleshoot?. I thought it might be something simple, like [this question](https://github.com/google/deepvariant/issues/129). However, that particular solution is not working for me. Thank you very much for your assistance. Sincerely,; Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:13897,simpl,simple,13897,,https://github.com/google/deepvariant/issues/166,1,['simpl'],['simple']
Usability,"2: 89987; # class0: 33161; # class1: 24300. name: ""Shuffle_global""; tfrecord_path: ""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 147448; ```. The training command looks like this:. ```; LR=0.001; BS=1024. apptainer run \; --nv \; -B $WD:/home \; $DV_PATH \; /opt/deepvariant/bin/train \; --config=/home/dv_config.py:base \; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/shuf_test/examples_shuf3_testset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune_test/tune_test_examples_config.pbtxt"" \; --config.num_epochs=1 \; --config.learning_rate=${LR} \; --config.num_validation_examples=0 \; --config.tune_every_steps=2000 \; --experiment_dir=/home/${OUTDIR} \; --strategy=mirrored \; --config.batch_size=${BS} \; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. During other tests I have run training jobs with several other example sets (several times larger), for tens of thousands of steps and multiple epochs, and also using different learning rates and batch sizes. While these things of course make a difference to learning performance, the lower recall for class 0 (hom_ref) remains consistent. . Here are some lines from the log file during one such training run:. ```; I1031 10:55:27.365902 140558597089024 logging_writer.py:48] [0] epoch=0, train/categorical_accuracy=0.91796875, train/categorical_crossentropy=0.6384725570678711, train/f1_het=0.7428571581840515, train/f1_homalt=0.964401364326477, train/f1_homref=0.902255654335022, train/f1_macro=0.; 8698380589485168, train/f1_micro=0.91796875, train/f1_weighted=0.9241795539855957, train/false_negatives=34.0, train/false_positives=14.0, train/learning_rate=9.999999747378752e-06, train/loss=0.6384731531143188, train/precision=0.9406779408454895, train/precision_het=0.702702701091; 7664, train/precision_homalt=0.978723406791687, train/precision_homref=1.0, train/recall=0.8671875, train/recall_het=1",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/904:1776,learn,learning,1776,,https://github.com/google/deepvariant/issues/904,1,['learn'],['learning']
Usability,"3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/python3.6/dist-packages/googleapiclient/discovery.py discovery.patch. CMD [""/opt/",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469:4068,Simpl,SimpleNamespace,4068,,https://github.com/google/deepvariant/issues/469,1,['Simpl'],['SimpleNamespace']
Usability,"4.meta output/models/model.ckpt-22355.meta output/models/model.ckpt-4814.meta; output/models/model.ckpt-13613.meta output/models/model.ckpt-25257.meta output/models/model.ckpt-7724.meta; output/models/model.ckpt-16546.meta output/models/model.ckpt-28168.meta; (dv_venv) [anovak@phoenix-01 trash]$ ls output/models/*metrics; output/models/best_checkpoint.metrics output/models/model.ckpt-28168.metrics output/models/model.ckpt-34008.metrics; output/models/current.metrics output/models/model.ckpt-31078.metrics; ```. But `model_eval` just sits there like this (until a new checkpoint appears):; ```; I0210 17:42:06.700287 139846137329472 checkpoint_utils.py:140] Waiting for new checkpoint at /public/groups/cgl/graph-genomes/anovak/trash/output/models; ```. How do I get the missing `*metrics` files and determine if any of the checkpoints that were missed is actually the best one? Do I need to `touch` some particular files in the directory to get `model_eval` to be interested in them? Is there some other command besides `model_eval` that can process a single particular checkpoint at a time?. **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. It doesn't look like model training is part of the quick start. **Any additional context:**. Eventually I might want a WDL workflow for training DeepVariant, and I'm not sure that managing two simultaneous DV processes in there is going to be worth the engineering required; they'd have to be lumped together into one WDL task and they'd have to always fit simultaneously on one machine. It would be much simpler for me to be able to run the training to the end, and then run all the evaluations afterward to select the best model. But it looks like if I tried that right now `model_eval` would just only evaluate the last checkpoint and always confidently declare it to be the best.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/611:4440,simpl,simpler,4440,,https://github.com/google/deepvariant/issues/611,1,['simpl'],['simpler']
Usability,"5208 . C T 44.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:44:42:22,20:0.47619:44,0,56; Chrom_6 5425277 . G A 32 PASS . GT:GQ:DP:AD:VAF:PL 0/1:32:45:28,17:0.377778:31,0,57; Chrom_6 **5425401** . G T 30.1 PASS . GT:GQ:DP:AD:VAF:PL 0/1:30:60:46,14:0.233333:30,0,50; Chrom_6 **5425402** . G T 18.3 PASS . GT:GQ:DP:AD:VAF:PL 0/1:18:59:46,13:0.220339:18,0,47; Chrom_6 **5425403** . T C 13.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:14:59:46,13:0.220339:13,0,45; Chrom_6 5425421 . T C 6.8 PASS . GT:GQ:DP:AD:VAF:PL 0/1:7:63:50,13:0.206349:5,0,40; Chrom_6 5425460 . G A 8.4 PASS . GT:GQ:DP:AD:VAF:PL 0/1:8:61:48,12:0.196721:7,0,40; Chrom_6 5425539 . A C 14.7 PASS . GT:GQ:DP:AD:VAF:PL 0/1:15:45:36,9:0.2:14,0,45; Chrom_6 5425555 . G A 20.9 PASS . GT:GQ:DP:AD:VAF:PL 0/1:21:44:34,10:0.227273:20,0,50; ```; As you see, the event which is a change of 3 consecutive nucleotides, is called in one sample as a single T to TTC event and in the other sample as 3 SNPs. ; This is annoying if one wants to compare the 2 samples, as simple comparison scripts will not understand those events are in fact identical. . We ran GATK in parallel and GATK calls the event in both sample as T to TTC. . The only difference between the 2 samples is the sequencing coverage which is higher in Sample1. ; I also looked into the gVCF and the same phenomenom happens.; Sample 1:; ```; Chrom_6	5425343	.	G	<*>	0	.	END=5425398	GT:GQ:MIN_DP:PL	0/0:50:104:0,300,2999; Chrom_6	5425399	.	TGG	T,<*>	33.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:33:108:47,58,0:0.537037,0:33,0,61,990,990,990; Chrom_6	5425402	.	G	<*>	0	.	END=5425402	GT:GQ:MIN_DP:PL	0/0:50:106:0,180,2759; Chrom_6	**5425403**	.	T	TTC,<*>	36.2	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:36:105:47,57,0:0.542857,0:36,0,61,990,990,990; Chrom_6	5425404	.	A	<*>	0	.	END=5425420	GT:GQ:MIN_DP:PL	0/0:50:106:0,300,2999; Chrom_6	5425421	.	T	C,<*>	39.4	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:39:105:46,59,0:0.561905,0:39,0,62,990,990,990; Chrom_6	5425422	.	C	<*>	0	.	END=5425459	GT:GQ:MIN_DP:PL	0/0:50:105:0,180,2759; Chrom_6	5425460	.	G	A,",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/202:2184,simpl,simple,2184,,https://github.com/google/deepvariant/issues/202,1,['simpl'],['simple']
Usability,"6737.31s elapsed]; I0218 10:46:36.864049 23456243894080 make_examples_core.py:243] Task 19/64: Skip phasing: len(candidates[main_sample]) is 20526.; I0218 10:48:19.364838 23456243894080 make_examples_core.py:243] Task 2/64: 158555 candidates (173735 examples) [4091.74s elapsed]; I0218 10:48:45.881830 23456243894080 make_examples_core.py:243] Task 2/64: Skip phasing: len(candidates[main_sample]) is 14234.; I0218 10:49:31.045118 23456243894080 make_examples_core.py:243] Task 13/64: 113956 candidates (125182 examples) [6317.67s elapsed]; I0218 10:50:33.895329 23456243894080 make_examples_core.py:243] Task 13/64: Skip phasing: len(candidates[main_sample]) is 18414.; parallel: This job failed:; /opt/deepvariant/bin/make_examples --mode calling --ref /scratch4/path.to.mydir/genomes/c_elegans.PRJNA13758.WS245.genomic.fa --reads /scratch4/path.to.mydir/pbmm2/aln13448198.pbmm2.bam --examples /tmp/tmp1yvr59_z/make_examples.tfrecord@64.gz --add_hp_channel --alt_aligned_pileup diff_channels --max_reads_per_partition 600 --min_mapping_quality 1 --parse_sam_aux_fields --partition_size 25000 --phase_reads --pileup_image_width 199 --norealign_reads --sort_by_haplotypes --track_ref_reads --vsc_min_fraction_indels 0.12 --task 28. real	879m59.515s; user	632m52.969s; sys	6m20.594s; INFO: Cleaning up image... ```. I also ran more jobs using different numbers of cpu and mem using different bam files. One using 48 cpu and --mem-per-cpu=6G simply fizzled without any error message. These jobs are taking considerable core-hours, so troubleshooting is hard. I also wonder if I am using Deepvariant efficiently. On a side note, I got many Deepvariant failures with error messages like:; ```; Detected 1372 oom-kill event(s) in StepId=12049020.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.; ```; This seems to have been resolved by asking for maximum allowable memory. I am still curious about the memory requirement for successfully running Deepvariant.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/614:12736,simpl,simply,12736,,https://github.com/google/deepvariant/issues/614,1,['simpl'],['simply']
Usability,"9%--__memcpy_sse2_unaligned; | ; --1.36%--ssw_init; | ; --0.89%--qP_byte. 27.87% , 0.05% ,python ,libssw.so ,[.] ssw_align; | ; --27.82%--ssw_align; | ; |--14.65%--sw_sse2_word; | ; |--8.32%--sw_sse2_byte; | ; |--2.91%--banded_sw; | ; --1.19%--__memcpy_sse2_unaligned. 14.65% , 14.62% ,python ,libssw.so ,[.] sw_sse2_word; | ; --14.62%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --14.62%--ssw_align; sw_sse2_word. 8.32% , 8.31% ,python ,libssw.so ,[.] sw_sse2_byte; | ; --8.31%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --8.30%--ssw_align; sw_sse2_byte. 4.51% , 0.00% ,python ,[unknown] ,[.] 0x00000000009063e0; |; ---0x9063e0; | ; --3.66%--PyEval_EvalFrameEx; | ; --3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | ; --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | ; --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | ; --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | ; --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | ; --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | ; --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. #### DV 0.5.1. ```; # Samples: 152K of event 'cpu-clock'; # Event count (approx.): 38010500000; #; # Children, Self,Command ,Shared Object ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:5940,learn,learning,5940,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"9%--__memcpy_sse2_unaligned; | ; --1.38%--ssw_init; | ; --0.92%--qP_byte. 28.27% , 0.04% ,python ,libssw.so ,[.] ssw_align; | ; --28.23%--ssw_align; | ; |--14.88%--sw_sse2_word; | ; |--8.45%--sw_sse2_byte; | ; |--2.89%--banded_sw; | ; --1.19%--__memcpy_sse2_unaligned. 14.88% , 14.86% ,python ,libssw.so ,[.] sw_sse2_word; | ; --14.86%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --14.86%--ssw_align; sw_sse2_word. 8.45% , 8.43% ,python ,libssw.so ,[.] sw_sse2_byte; | ; --8.43%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --8.43%--ssw_align; sw_sse2_byte. 4.72% , 0.00% ,python ,[unknown] ,[.] 0x00000000009063e0; |; ---0x9063e0; | ; --3.94%--PyEval_EvalFrameEx; | ; --3.57%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | ; --3.32%--learning::genomics::deepvariant::DeBruijnGraph::Build; | ; --3.02%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | ; --2.63%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | ; --1.89%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | ; --1.60%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | ; --0.56%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. To do this properly would require that the tests be performed on different datasets, and different CPUs on the same Cloud environment - with different distributed scenarios - which would be cost-prohibitive for me. Hop",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:11454,learn,learning,11454,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,":__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. #### DV 0.5.1. ```; # Samples: 152K of event 'cpu-clock'; # Event count (approx.): 38010500000; #; # Children, Self,Command ,Shared Object ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.33%--PyEval_EvalFrameEx; | | ; | |--31.12%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.63%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--28.27%--ssw_align; | | | | ; | | | |--14.88%--sw_sse2_word; | | | | ; | | | |--8.45%--sw_sse2_byte; | | | | ; | | | |--2.89%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.38%--ssw_init; | | | ; | | --0.92%--qP_byte; | | ; | |--3.57%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.32%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --3.02%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.63%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.89%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.60%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.56%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.16%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.12%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.58%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:7600,learn,learning,7600,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,">::_M_find_before_node; ```. #### DV 0.5.1. ```; # Samples: 152K of event 'cpu-clock'; # Event count (approx.): 38010500000; #; # Children, Self,Command ,Shared Object ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.33%--PyEval_EvalFrameEx; | | ; | |--31.12%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.63%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--28.27%--ssw_align; | | | | ; | | | |--14.88%--sw_sse2_word; | | | | ; | | | |--8.45%--sw_sse2_byte; | | | | ; | | | |--2.89%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.38%--ssw_init; | | | ; | | --0.92%--qP_byte; | | ; | |--3.57%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.32%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --3.02%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.63%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.89%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.60%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.56%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.16%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.12%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.58%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap::pyAlleleCounter::wrapAdd_as_add; | | ; | |--0.62%--deepvariant_core_python_sam__rea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:7684,learn,learning,7684,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,A quick start guide issue,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/44:14,guid,guide,14,,https://github.com/google/deepvariant/issues/44,2,['guid'],['guide']
Usability,"Any idea why I can not run the docker?. The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. In the quick start guide https://github.com/google/deepvariant/blob/r0.9/docs/deepvariant-quick-start.md . They show using sudo to run the docker. I do not have sudo permission on this machine. The machine is set up to use the group permission. I do not think this is the issue. . Any suggestions would be greatly appreciated. Andy. ```; (base) -bash-4.2$ groups; giuser kimlab docker; (base) -bash-4.2$ ; ```. ```; docker run -v /public/home/dkim142/quickstart-testdata:/input \; -v /public/home/dkim142/quickstart-output:/output google/deepvariant:0.9.0 \; /opt/deepvariant/bin/run_deepvariant --model_type=WGS \; --ref=/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta \; --reads=/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --regions chr20:10,000,000-10,010,000 \; --output_vcf=/public/home/dkim142/quickstart-output/output.vcf.gz \; --output_gvcf=/public/home/dkim142/quickstart-output/output.g.vcf.gz \; --num_shards=1. ***** Running the command:*****; time seq 0 0 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling \; --ref ""/public/home/dkim142/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta"" --reads ""/public/home/dkim142/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@1.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@1.gz"" --regions ""chr20:10,000,000-10,010,000"" --task {}. 2019-12-08 02:35:44.105906: F tensorflow/core/platform/cpu_feature_guard.cc:37] ; The TensorFlow library was compiled to use AVX instructions, but these aren't available on your machine. real	0m1.146s; user	0m1.709s; sys	0m4.191s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", l",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/248:164,guid,guide,164,,https://github.com/google/deepvariant/issues/248,1,['guid'],['guide']
Usability,"Dears, . I just followed ""DeepVariant quick start"" guide, which was successful. ; Then, I've replaced the reference and reads with my own files and ran following commands but failed to get vcf files. . > OUTPUT_DIR=""${PWD}/quickstart-output""; > INPUT_DIR=""${PWD}/quickstart-testdata""; > mkdir -p ""${OUTPUT_DIR}""; > BIN_VERSION=""0.9.0"". > sudo docker run \; > -v ""${INPUT_DIR}"":""/input"" \; > -v ""${OUTPUT_DIR}:/output"" \; > gcr.io/deepvariant-docker/deepvariant:""${BIN_VERSION}"" \; > /opt/deepvariant/bin/run_deepvariant \; > --model_type=WES \; > --ref=/input/genome.fa \; > --reads=/input/HC3-BC_RG_bwa.bam \; > --regions ""20:10,000,000-10,100,000"" \; > --output_vcf=/output/output.vcf.gz \; > --output_gvcf=/output/output.g.vcf.gz \; > --num_shards=12. Here is a log: ; ```; ***** Running the command:*****; time seq 0 11 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/genome.fa"" --reads ""/input/HC3-BC_RG_bwa.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@12.gz"" --gvcf ""/tmp/deepvariant_tmp_output/gvcf.tfrecord@12.gz"" --regions ""20:10,000,000-10,100,000"" --task {}. I1208 07:03:00.340749 140610455504640 make_examples.py:377] ReadRequirements are: min_mapping_quality: 10; min_base_quality: 10; min_base_quality_mode: ENFORCED_BY_CLIENT. I1208 07:03:00.598805 140610455504640 genomics_reader.py:223] Reading /input/HC3-BC_RG_bwa.bam with NativeSamReader; I1208 07:03:00.619033 140610455504640 make_examples.py:1324] Preparing inputs; I1208 07:03:00.814711 140610455504640 genomics_reader.py:223] Reading /input/HC3-BC_RG_bwa.bam with NativeSamReader; I1208 07:03:00.841161 140610455504640 make_examples.py:1248] Common contigs are [u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9', u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19', u'20', u'21', u'22', u'X', u'Y']; I1208 07:03:00.851603 140610455504640 make_examples.py:1330] Writing examples to /tmp/deepvariant_tmp_output/make_examples.tfrecord-00000-of",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/249:51,guid,guide,51,,https://github.com/google/deepvariant/issues/249,1,['guid'],['guide']
Usability,Deepvariant fails without clear reason - with PacBio data,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/614:26,clear,clear,26,,https://github.com/google/deepvariant/issues/614,2,['clear'],['clear']
Usability,"Deepvariant fails without clear reason. . **Setup**; JHU Rockfish HPC; Singularity 3.8.7; singularity pull docker://google/deepvariant:1.4.0. Problematic data are PacBio (first gen). I have used Deepvariant with Illumina without problem, and I used PEPPER to process Ont data and PacBio Hifi data. I used pbmm2 to align fastq with all PacBio data. Command used to run:; ```; #!/bin/bash; #SBATCH --job-name=deep64_13448198; #SBATCH --time=24:00:00; #SBATCH --nodes=2; #SBATCH --ntasks-per-node=1; #SBATCH --cpus-per-task=32; #SBATCH --mem=0. ml anaconda; conda activate /data/path.to.mydir/deepvariant. singularity run --bind /scratch4/path.to.mydir/:/scratch4/path.to.mydir/ \; docker://google/deepvariant:1.4.0 \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref c_elegans.PRJNA13758.WS245.genomic.fa \; --reads aln13448198.pbmm2.bam \; --output_vcf aln13448198.pbmm2.dv.vcf.gz \; --num_shards 64; ```. Here's a long snippet of slurm output:; ```; INFO: Using cached SIF image; INFO: Converting SIF file to temporary sandbox...; I0217 20:13:14.117354 23456243894080 run_deepvariant.py:342] Re-using the directory for intermediate results in /tmp/tmp1yvr59_z. ***** Intermediate results will be written to /tmp/tmp1yvr59_z in docker. ****. ***** Running the command:*****; time seq 0 63 | parallel -q --halt 2 --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref [snipped]. #[snip]; # this part is likely unimportant. perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back to the standard locale (""C"").; perl: warning: Setting locale failed.; perl: warning: Please check that your locale settings:; 	LANGUAGE = (unset),; 	LC_ALL = (unset),; 	LC_CTYPE = ""C.UTF-8"",; 	LANG = ""en_US.UTF-8""; are supported and installed on your system.; perl: warning: Falling back ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/614:26,clear,clear,26,,https://github.com/google/deepvariant/issues/614,1,['clear'],['clear']
Usability,Feedback on poor results in family variation detection and GIAB dataset evaluation,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/689:0,Feedback,Feedback,0,,https://github.com/google/deepvariant/issues/689,1,['Feedback'],['Feedback']
Usability,Filtering clear false positives / low-confidence sites,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/645:10,clear,clear,10,,https://github.com/google/deepvariant/issues/645,2,['clear'],['clear']
Usability,"Hello DV team, and thanks for creating such a great tool! . I am currently trying to retrain the wgs model for a new species (a fish) however, during training, I see no evaluation statistics (precision, recall, f1) for either het or homalt. Or more specifically they are all 0.0. Eval stats are reported for homref though. I have now tried running the training several times with different hyperparameters but so far still no change at the het or homalt eval stats. . My first, very simple question is thus, are these eval stats truly 0 (i.e. the model is very bad) or is 0.0 some starting value and there are not enough data to calculate them initially? I am warmstarting from the 1.6.1 wgs model so I cant imagine the model is really that bad at calling variants initially, even if in a fish. . **Setup**; Running on a university computing cluster (https://hpc-unibe-ch.github.io/) ; OS: Rocky 9.3 Blue Onyx; GPU: rtx4090 ; Installation: Running from Docker image via singularity; DV version: 1.6.1. **Data**; I am training on examples from 5 individuals, data from Illumina NovaSeq ~20x coverage. ; 17/21 chromosomes used for training (~1.45M examples); 2/21 chromosomes used for tuning (~200k examples); 2/21 chromosomes reserved for testing. ; (Different chromosomes used for train/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --co",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:483,simpl,simple,483,,https://github.com/google/deepvariant/issues/876,1,['simpl'],['simple']
Usability,"Hello Team,. Congratulations on the [nice haplotagging paper](https://www.biorxiv.org/content/10.1101/2023.09.07.556731v1)! I have looked through the paper, algorithm and code, and there are some things that could be made a bit more clear in the paper. Let me know if you would like me to help you with the paper. Best regards,; Paul",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/709:233,clear,clear,233,,https://github.com/google/deepvariant/issues/709,1,['clear'],['clear']
Usability,"Hello team,. First, i really want to thank you for your gigantic effort in building and documenting deepvariant. I personally learned (and still learning) a lot from you. I am interested in training deepvariant on a cluster with no root privileges, so the docker image is not an option for me. Conda is my most efficient way to go, however, I am having the same I am having the exact same error described in issue #137 Is there is any update in regards of this error?. My other question is the training scripts available on the conda build or not? If not, what do you think is the best way to go with training if I have no root privileges? . thank you again!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/139:126,learn,learned,126,,https://github.com/google/deepvariant/issues/139,2,['learn'],"['learned', 'learning']"
Usability,"Hello! . not an issue, a simple suggestion: several persons I know are put off by DeepVariant because they can't get a gVCF with a line per base, basically they would like to deactivate the formation of blocks. Personally, blocks have never worried me and I guess computationally it has its advantages. . But just to tlet you know as a ""user experience"". Cheers and stay safe. DeepVariant is a very awesome tool.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/398:25,simpl,simple,25,,https://github.com/google/deepvariant/issues/398,2,"['simpl', 'user experience']","['simple', 'user experience']"
Usability,"Hello! I've found a performance issue in deepvariant/data_providers.py: `batch()` should be called before `map()`, which could make your program more efficient. Here is [the tensorflow document](https://tensorflow.google.cn/guide/data_performance?hl=zh_cn#vectorized_mapping) to support it. Detailed description is listed below:. - deepvariant/data_providers.py: `dataset.batch(batch_size=batch_size, drop_remainder=True)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L316) should be called before `dataset.map(map_func=self.parse_tfexample, num_parallel_calls=tf.data.AUTOTUNE)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L314).; - deepvariant/data_providers.py: `dataset.batch(batch_size=batch_size)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L364) should be called before `dataset.map(map_func=self.parse_tfexample, num_parallel_calls=tf.data.AUTOTUNE)`[(here)](https://github.com/google/deepvariant/blob/1c1d220f36ac8b9018872adc3d9bcde8ae43d84a/deepvariant/data_providers.py#L362). Besides, you need to check the function called in `map()`(e.g., `self.parse_tfexample` called in `dataset.map()`) whether to be affected or not to make the changed code work properly. For example, if `self.parse_tfexample` needs data with shape (x, y, z) as its input before fix, it would require data with shape (batch_size, x, y, z). Looking forward to your reply. Btw, I am very glad to create a PR to fix it if you are too busy.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/479:224,guid,guide,224,,https://github.com/google/deepvariant/issues/479,1,['guid'],['guide']
Usability,"Hello, . I'm very new to model training and honestly, coding, so thank you for your patience! I'm trying to run my own samples following along with the [advanced training case study](https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md). I've reached the stage where I need to locally shuffle the training examples using the shuffle_tfrecords_beam.py script. I downloaded the latest version of tensorflow (2.15) and was initially getting an error that apache beam was not being recognized, and realized that beam did not install because its latest version (2.54) was incompatible with the current version of numpy (1.26) that was being imported. I uninstalled that new version of numpy in tensorflow and installed an older version that would be compatible (1.24.4), and then was able to install apache beam (2.54). However, now I'm getting even more errors (see below). Do you have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. ; First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, ;",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793:1046,guid,guidance,1046,,https://github.com/google/deepvariant/issues/793,1,['guid'],['guidance']
Usability,"Hello, ; I used DeepVariant with the mosquito trained model, on 11 samples, then performed joint calling with GLnexus. In the following track, in the first sample it calls 2 SNP G/A and T/C (corresponding to the 2 last blue box) in the D2A1 sample. However, in the D5B3 sample the sites are called as 0/0. There is no ""RefCall"" either so I think it means it did not even generate candidates. ; Point to note: I have no idea if there are, or not, SNPs at those 2 loci. But from the alignment, it is not at clear to me why in one case it thought there were SNPs, and in the other case it thought not. . For the same sites, GATK joint caller decided the site was 0/0 for all samples (again, I don't know which one is the true genotype there). The bam I am showing here are the diagnostic bams emitted by DeepVariant, so my understanding is that I see what it saw. ![igv_snapshot](https://user-images.githubusercontent.com/23341393/80101296-7e995c80-8571-11ea-8e3d-37e306442888.png). As you might notice, the coverage depth across my samples is not always identical. So I wonder if it's not simply a question of coverage (though my lowest average coverage value is 30, which is ok I think). . Would you have any clue of what might be happening? . Thank a lot. EDIT: I can get rid of those regions by filtering on QUAL on the GLnexus pVCF, however. But I am still curious, as I might want to keep them and filter them otherwise (not easy to reach a spot where you get read of FP without removing all the TP).",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/303:505,clear,clear,505,,https://github.com/google/deepvariant/issues/303,2,"['clear', 'simpl']","['clear', 'simply']"
Usability,"Hello, ; a rather simple issue, I am following this tutorial ; https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-docker.md. and at this step ; `gsutil -m cp gs://deepvariant/quickstart-testdata/* input/; `; I get; `zsh: no matches found: gs://deepvariant/quickstart-testdata/*`. Thanks a lot",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/61:18,simpl,simple,18,,https://github.com/google/deepvariant/issues/61,1,['simpl'],['simple']
Usability,"Hello, I am trying to install DeepVariant on an IBM Power 8 machine within a docker container. I get the following error during ./build_and_test.sh, which I understand is tied to Intel SSE2 instruction set. `external/libssw/src/ssw.c:38:23: fatal error: emmintrin.h: No such file or directory`. I did `export DV_USE_GCP_OPTIMIZED_TF_WHL=0` from the command line before running the compile. I also changed `DV_COPT_FLAGS` to `--copt=-Wno-sign-compare --copt=-Wno-write-strings` within settings.sh (removing the corei7 option). I am using bazel version '0.15.0-' (settings.sh is changed to reflect this). I am using scikit-learn=0.20 (run-prereq.sh changed to reflect this). pyclif was compiled from source. Is there a way to circumvent this error? The complete error message is as follows. ERROR: /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/external/libssw/BUILD.bazel:11:1: C++ compilation of rule '@libssw//:ssw' failed (Exit 1): gcc failed: error executing command ; (cd /root/.cache/bazel/_bazel_root/8422bf851bfac3671a35809acde131a7/execroot/com_google_deepvariant && \; exec env - \; LD_LIBRARY_PATH=/usr/local/nvidia/lib:/usr/local/nvidia/lib64 \; OMP_NUM_THREADS=1 \; PATH=/root/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin \; PWD=/proc/self/cwd \; PYTHON_BIN_PATH=/usr/bin/python \; PYTHON_LIB_PATH=/usr/local/lib/python2.7/dist-packages \; TF_DOWNLOAD_CLANG=0 \; TF_NEED_CUDA=0 \; TF_NEED_OPENCL_SYCL=0 \; /usr/bin/gcc -U_FORTIFY_SOURCE -fstack-protector -Wall -B/usr/bin -B/usr/bin -Wunused-but-set-parameter -Wno-free-nonheap-object -fno-omit-frame-pointer -g0 -O2 '-D_FORTIFY_SOURCE=1' -DNDEBUG -ffunction; -sections -fdata-sections -MD -MF bazel-out/ppc-opt/bin/external/libssw/_objs/ssw/external/libssw/src/ssw.pic.d -fPIC -iquote external/libssw -iquote bazel-out/ppc-opt/genfiles/external/libssw -iquote ext; ernal/bazel_tools -iquote bazel-out/ppc-opt/genfiles/external/bazel_tools -Wno-maybe-uninitial",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/123:621,learn,learn,621,,https://github.com/google/deepvariant/issues/123,1,['learn'],['learn']
Usability,"Hello, I am trying to make a variant calling analysis with ONT data for Homo Sapiens. However, it is still running even though I started this analysis 6 days ago and it is still making examples. Could anyone help me to understand whether it is normal or I should re-run the analysis? The computer has 64-core Linux. It should be able to run the analysis for nearly one to two days based on my experiences with other variant callers. I would be very happy to get feedbacks from you! ; Thanks a lot!. Deep Variant/ Variant Calling; BIN_VERSION=1.6.1; Installation via Docker; Homo Sapiens Oxford Nanopore Whole Genome; ![Screenshot 2024-05-01 220709](https://github.com/google/deepvariant/assets/74244954/93bf098a-083d-40f5-ba41-9d876a836be3)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/814:462,feedback,feedbacks,462,,https://github.com/google/deepvariant/issues/814,1,['feedback'],['feedbacks']
Usability,"Hello, I have installed all the binaries and ran all the shell scripts to install tensorflow and bazel, but after that I could not follow how to actually train the model or how to identify the snps for my files. I am sorry I am very new to deep learning. Any help would be greatly appreciate",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/63:245,learn,learning,245,,https://github.com/google/deepvariant/issues/63,1,['learn'],['learning']
Usability,"Hello,. I am trying to build DeepVaraint with Tensorflow 2.11.0. I modified ""settings.h"" and ""build-prereq.sh"" to initialize the respective version of bazel and other dependencies. However, I am not able to build successfully. ; Could you please guide me towards upgrading Tensorflow to 2.11.0 and any required modifications to the source code? . Thanks,; Saurabh",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/607:246,guid,guide,246,,https://github.com/google/deepvariant/issues/607,1,['guid'],['guide']
Usability,"Hello,. I have been trying to do some testing of DeepVariant on my own Exome (WES) and WGS data. As a starting point, I was trying to work with calling variants from the .bam file provided for my WES data. I started running from within a Docker container on my local computer but that was taking a long time (and, ultimately, the _make_examples_ step did not run to completion). I started learning more about the AWS options for analysis, and I was able to run the _make_examples_ much quicker (and successfully) on an AWS m5.xlarge ECS instance (although I am admittedly well over the ~25 minutes and $0.20 time/cost mentioned for Google Cloud, just for the _make_examples_, without considering upload/download, long-term storage, etc.). While I was hoping to eventually compare running things on Google Cloud (and I think my experience so far probably helps me ask better questions), I was wondering if you could help me troubleshoot something that I think is probably close to working:. Essentially, I am currently at the **call_variant** step of DeepVariant, with WES data. This is the error message that I am currently receiving:. ```; sudo sh run_deepvariant.sh; I0331 18:31:22.446569 140549764839168 call_variants.py:292] Set KMP_BLOCKTIME to 0; 2019-03-31 18:31:22.486802: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2 AVX512F FMA; 2019-03-31 18:31:22.489180: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.; I0331 18:31:22.527594 140549764839168 modeling.py:351] Initializing model with random parameters; W0331 18:31:22.529449 140549764839168 tf_logging.py:125] Using temporary folder as model directory: /tmp/tmpuBleAQ; I0331 18:31:22.529786 140549764839168 tf_logging.py:115] Using config: {'_save_checkpoints_secs': 1000, '_num_ps_replicas': 0, '_keep_checkpoint",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/166:389,learn,learning,389,,https://github.com/google/deepvariant/issues/166,1,['learn'],['learning']
Usability,"Hello,. I have learned that DeepVariant is three steps at runtime.; To better understand DeepVariant, I want to know if there is a way to parse the output file of call_variants like the output file of make_examples ([Visualizing DeepVariant examples](https://github.com/google/deepvariant/blob/r1.6.1/docs/visualizing_examples.ipynb)), so that it becomes human readable instead of a binary string like below. ```; tf.Tensor(b'\nc2\x01T:\x01AZQ\x12\n\n\x02DP\x12\x04\n\x028\x1c\x12\x0e\n\x02AD\x12\x08\n\x028\x15\n\x028\x04\x12\x12\n\x03VAF\x12\x0b\n\t\x11\x92$I\x92$I\xc2?:\x14\xff\xff\xff\xff\xff\xff\xff\xff\xff\x01\xff\xff\xff\xff\xff\xff\xff\xff\xff\x01J\tHG001_10Xh\xf7Nr\x011\x80\x01\xf6N\x12\x03\n\x01\x00\x1a\x18\x00\x00\x00\x00\xb6\xf8\xef?\x00\x00\x00@\x9e\x0fH?]F\xb5\xb9\x86a$?', shape=(), dtype=string); ```. Sorry for the interruption!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/887:15,learn,learned,15,,https://github.com/google/deepvariant/issues/887,1,['learn'],['learned']
Usability,"Hello,. I have tried following along the guidelines presented under [Best practices for multi-sample variant calling](https://github.com/google/deepvariant/blob/r1.3/docs/trio-merge-case-study.md#best-practices-for-multi-sample-variant-calling-with-deepvariant-wes-trio-demonstration) but am still unclear as to how to run DeepVariant on a large sample size (~200). The example provided only uses a trio and the paper cited doesn't provide any code for their large cohort experiment. Do you have any resources for adjusting the parameters/running the samples in parallel so as to not run all ~200 samples sequentially? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/526:41,guid,guidelines,41,,https://github.com/google/deepvariant/issues/526,1,['guid'],['guidelines']
Usability,"Hello,. some of you might remember me. I know Deepvariant works well in human and in some species like rice, if I recall well. In short, all species with (very) low heterozygousity. I wonder if you see a use for Deepvariant in other species, like, there are marine species that are so ancient, diverse, widespread, you can have 5% heterozygosity, in shorts, SNPs everywhere. In such cases, Deepvariant has a tendency to ""ditch"" apparently at random (Sample1 Chrom3:20456 called, Sample2 same position not called, despite obvious evidence from mapping and support from long reads). Probably because it didn't learn what to do with so many SNPs. You know the issue because of your mosquito blog spot. And I have seen other issues (including mine) talking about that. The issue is to have a gold standard like in human, or trio data like in the mosquito, you need specific conditions, it seems difficult to imagine this could be doable with, let's say, a deep sea coral (just random example, I don't actually know what's their genome like). . Could a synthetic dataset help here? What if we feed Deepvariant a genome we made up based on what we can observed visually? I am aware if we make an error it will learn errors, but I wanted your opinion, because the lack of high quality reference dataset for many species, seems to be a serious limitation for this kind of program. Thanks a lot. Since it's not the first time I bring this out, I understand if you would simply close this. Have a good week everyone.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/872:608,learn,learn,608,,https://github.com/google/deepvariant/issues/872,3,"['learn', 'simpl']","['learn', 'simply']"
Usability,"Hello,; It seems that 1 shard = 1 thread, however at some point in its run it seems that DeepVariant takes more threads N_SHARD is set to 20, no one else is using the computer at this moment and clearly more than 20 threads are taken. ![higdpgpplenjbaao](https://user-images.githubusercontent.com/23341393/74519762-a12a4c00-4f16-11ea-986c-b785044a618b.png). Is it expected?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/271:195,clear,clearly,195,,https://github.com/google/deepvariant/issues/271,1,['clear'],['clearly']
Usability,"Hi (accidentally closed old issue prematurely). Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads bamlink \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; ```; And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:; 1) Why doesn't this work?; 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks!. Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options); But I can't find the equivalent just for the make_examples section",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/149:1208,simpl,simply,1208,,https://github.com/google/deepvariant/issues/149,1,['simpl'],['simply']
Usability,"Hi . I am a student learning about bioinformatics. I am able to us the docker image to run the quick start data, however, I am not able to run on my data. . I used samtools 1.9 to filter out low quality reads as follows. ```; quality=11; samtools view -bSq ${quality} ""${originalBAMFile}"" > ""${filteredBAMFile}""; samtools index ""${filteredBAMFile}""; ```. Note the output from the docker run script bellow is a little misleading. I copied all the data from an aws s3 buck to the local disk before the run. Also, the data is RNA not DNA. I was asked to ""see if it would just work"". I think maybe we have to change Uracil to look like Thymine. . Any suggestions would be greatly appreciated. Happy Holidays. Andy; ```; ubuntu@ip-172-31-1-186:~$ cat nohup.out ; + BIN_VERSION=0.9.0; + s3Root=/data; + INPUT_DIR=/data/aligned; + OUTPUT_DIR=/data/output; + quality=q11; + ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; + reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam; + date +%Y-%m-%d-%H.%M.%S-%Z%n; + dateStamp=2019-12-19-00.20.49-UTC; + output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz; + output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz; + nproc; + num_shards=4; + sudo docker run -v /data/aligned:/input -v /data/output:/output google/deepvariant:0.9.0 /opt/deepvariant/bin/run_deepvariant --model_type=WGS --ref=/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna --reads=/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam --output_vcf=/data/output/2019-12-19-00.20.49-UTC.vcf.gz --output_gvcf=/data/output/2019-12-19-00.20.49-UTC.g.vcf.gz --num_shards=4. ***** Running the command:*****; time seq 0 3 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/data/reference/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna"" --reads ""/data/aligned/84773251_trimmed.AH77TTBBXX_DS-229105_GCCAAT.sorted.rg.final.q11.bam"" --examples ""/tmp/deepvariant_tmp_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/254:20,learn,learning,20,,https://github.com/google/deepvariant/issues/254,1,['learn'],['learning']
Usability,"Hi @pichuan,; I did come across various bits in different documents on how to train custom checkpoint but i don't have a confident on handle on it before I embark. - Can you share the entire commands as an example if i want to re-train using multiple BAMs as input (I saw you are using 18 different BAM to train WGS 1.5); - Do all the generated example files need to live at the same time to perform training? (ie is there a way to do iterative sub-sampling of large BAM and generate examples that get deleted once they are used?; - Do you have a good rule of thumb for how many examples needed? (I saw you are over 350M for WGS 1.5); Thank you for guidance!; -Daniel",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/765:649,guid,guidance,649,,https://github.com/google/deepvariant/issues/765,1,['guid'],['guidance']
Usability,"Hi Again,. Thank you again for your help (on previous issues like #166 and #167). I have successfully run one Exome sample and one WGS sample (on AWS and Google Cloud). The WGS sample has ~250 million reads (paired-end, 150 bp x 150 bp), so it has ~25x coverage. Using the code based upon the [Google Cloud DeepVariant example]( https://cloud.google.com/genomics/docs/tutorials/deepvariant), I could process this 25X WGS samples in almost exactly 24 hours at a cost of approximately $10. On AWS, I probably should have kept more complete notes, but I believe it completed in ~18 hours (with a cost of approximately $15). This includes some cost of storage for the exome sample (and the WGS .fastq.gz files), although read storage would need to be taken into consideration if performing all analysis on the clould. Nevertheless, I previously reported [a much larger number]( https://github.com/google/deepvariant/issues/167#issuecomment-479522653) when I was learning how to use AWS, and these cost figures are now much more similar (as probably should be expected). One reason that I found running each step separately on AWS to be helpful was that I could figure out that I needed to add an extra parameter (`--sample_name VeritasProvided `) for the WGS dataset (for the provided alignment from Veritas) that wasn’t necessary for the Exome dataset (for the provided alignment from Genos). I’ve re-processed each sample locally, so I would also like to compare variant calls from a BWA-MEM alignment. Plus, I would like to make my comparison to AWS as fair as possible. So, here are my thoughts moving forward:. **1a)** I think it is good that you have changed the example WGS run time from [70 minutes]( https://github.com/google/deepvariant/blob/9d24133fc83e0423b3d5cf125a710bbefa864bbb/README.md) minutes to [5 hours](https://github.com/google/deepvariant/blob/r0.8/README.md), but this is still quite different than my own experience (**24 hours**). I believe my upload times for my WGS datasets w",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:958,learn,learning,958,,https://github.com/google/deepvariant/issues/171,1,['learn'],['learning']
Usability,"Hi all,. This is not strictly a DeepVariant issue, but it is an issue I ran into while following the Giraffe case study using exome FASTQs from [here](https://www.internationalgenome.org/data-portal/sample/NA12878) (specifically, `SRR1518158_*.fastq.gz`). I'm running this on a DNAnexus cloud workstation (`mem1_ssd1_v2_x8`) and using the same version of KMC used in the case study. I am trying to run KMC using the following command:. ```; TMPDIR=$(mktemp -d); time ./kmc -k29 -okff -t8 sra.fq.paths ./sra.fq $TMPDIR; ```. where `sra.fq.paths` is the result of `ls SRR1518158_*.fastq.gz > sra.fq.paths`. The error message is simply `Error: unknown exception`. Have you seen this before? I thought I'd ask here before filing an issue with the KMC repo. Thanks,; Samantha",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/767:626,simpl,simply,626,,https://github.com/google/deepvariant/issues/767,1,['simpl'],['simply']
Usability,"Hi all,; I have read the discussion about deep sequencing on issue #62. I have tried to modify three options, downsample_fraction, pileup_image_height, and vsc_min_fraction_snps for our deep sequencing data but it didn't work and output many false-positive calls. Here I want to train a new model for deep sequencing data with rare somatic mutation(MAF~1%) and there is some confusion.; 1) There is a maximum threshold for pileup_height of 362, can I modify it?; 2) There is only one training tutorial with Google cloud platform, is there any guideline for training with the Linux system?; 3) Can Deepvariant be adapted to a somatic mutation caller?; Thanks a lot!; Best regards,; Weiwei",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/308:544,guid,guideline,544,,https://github.com/google/deepvariant/issues/308,1,['guid'],['guideline']
Usability,"Hi guys, . I have been doing some test on DeepVariant genotyping call. I run the suit and got very good results, but as part of experiments I need to generate a cohort VCF (multi-sample). As suggested here on github, I generated GVCF for all my samples, but when I tried to use GATK's CombineGVCFs or GenotypeGVCFs neither worked because they don't recognize the alternative allele `<*>`, instead GATK moved to `<NON_REF>` on the more recent versions.; After identified the problem, I run a simple substitution using `sed` to replace all occurrences of `<*>` for `<NON_REF>` and the commands ran fine. . `zcat SAMPLE.deepvar.g.vcf.gz | sed 's/<*>/<NON_REF>/g' | bgzip -c > SAMPLE.gvcf.gz`. May I suggest this update for your software to keep the compatibility with GATK?. Best,; André Santos",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/83:491,simpl,simple,491,,https://github.com/google/deepvariant/issues/83,1,['simpl'],['simple']
Usability,"Hi guys, . I'm running DeepVariant—pretty much exactly as outlined at https://cloud.google.com/genomics/deepvariant —but I can't seem to get it to work. . My `deepvariant_configuration.yaml` (unmodified from guide) is ; ```; name: deepvariant_pipeline; inputParameters:; - name: PROJECT_ID; - name: OUTPUT_BUCKET; - name: MODEL; - name: DOCKER_IMAGE; - name: DOCKER_IMAGE_GPU; - name: STAGING_FOLDER_NAME; - name: OUTPUT_FILE_NAME; docker:; imageName: gcr.io/deepvariant-docker/deepvariant_runner; cmd: |; ./opt/deepvariant_runner/bin/gcp_deepvariant_runner \; --project ""${PROJECT_ID}"" \; --zones 'us-*' \; --docker_image ""${DOCKER_IMAGE}"" \; --outfile ""${OUTPUT_BUCKET}""/""${OUTPUT_FILE_NAME}"" \; --staging ""${OUTPUT_BUCKET}""/""${STAGING_FOLDER_NAME}"" \; --model ""${MODEL}"" \; --bam gs://deepvariant/quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam \; --ref gs://deepvariant/quickstart-testdata/ucsc.hg19.chr20.unittest.fasta.gz \; --regions ""chr20:10,000,000-10,010,000""; ```. My `runner.sh` (changed `PROJECT_ID`,`OUTPUT_BUCKET`, and `STAGING_FOLDER_NAME`) is; ```; #!/bin/bash; set -euo pipefail; # Set common settings.; PROJECT_ID=udndv-197518 #changed; OUTPUT_BUCKET=gs://udnXXXXXX #changed; STAGING_FOLDER_NAME=staging-folder #changed; OUTPUT_FILE_NAME=output.vcf; # Model for calling whole genome sequencing data.; MODEL=gs://deepvariant/models/DeepVariant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-182548131.data-wgs_standard; # Model for calling exome sequencing data.; # MODEL=gs://deepvariant/models/DeepVariant/0.5.0/DeepVariant-inception_v3-0.5.0+cl-181413382.data-wes_standard; IMAGE_VERSION=0.5.1; DOCKER_IMAGE=gcr.io/deepvariant-docker/deepvariant:""${IMAGE_VERSION}""; DOCKER_IMAGE_GPU=gcr.io/deepvariant-docker/deepvariant_gpu:""${IMAGE_VERSION}"". # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --pipeline-file deepvariant_pipeline.yaml \; --logging ""${OUTPUT_BUCKET}""/runner_logs \; --zones us-west1-b \; --inputs `echo \; PROJECT_ID=""${PROJEC",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/60:208,guid,guide,208,,https://github.com/google/deepvariant/issues/60,1,['guid'],['guide']
Usability,"Hi there,. I've been trying to figure out how to actually run deepvariant in a cluster environment but thus far, the instructions seems a little cryptic to me. ; Is there perhaps a step-by-step guide to running deepvariant on a cluster with a PBS scheduler for instance?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/474:194,guid,guide,194,,https://github.com/google/deepvariant/issues/474,1,['guid'],['guide']
Usability,"Hi! I compiled from source on Ubuntu 14.4. . Tests pass, including `./bazel-bin/deepvariant/make_examples_test`. However, running as simple `make_examples` with downloaded example data fails with confusing (for me) error in realign script. Same script worked fine when I ran pre-compiled binary from the Docker container 🤷‍♂️. ```; input=""./quickstart-testdata"". ./bazel-bin/deepvariant/make_examples \; --ref=$input/ucsc.hg19.chr20.unittest.fasta \; --reads=$input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --examples examples.tfrecord@1.gz \; --mode calling \; --logging_every_n_candidates 10 \; --realign_reads; ```. ```; ./make_examples_demo.sh ; 2019-07-16 17:49:02.877175: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:02.877284 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.117142 139897470359360 make_examples.py:1110] Preparing inputs; 2019-07-16 17:49:03.117644: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.117749 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; I0716 17:49:03.118745 139897470359360 make_examples.py:1034] Common contigs are [u'chr20']; I0716 17:49:03.120177 139897470359360 make_examples.py:1116] Writing examples to examples.tfrecord-00000-of-00001.gz; 2019-07-16 17:49:03.121118: I third_party/nucleus/io/sam_reader.cc:600] Setting HTS_OPT_BLOCK_SIZE to 134217728; 2019-07-16 17:49:03.124279: W third_party/nucleus/io/sam_reader.cc:564] Unrecognized SAM header type, ignoring: ; I0716 17:49:03.124422 139897470359360 genomics_reader.py:218] Reading ./quickstart-testdata/NA12878_S1.chr20.10_10p1mb.bam with NativeSamReader; Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199:133,simpl,simple,133,,https://github.com/google/deepvariant/issues/199,1,['simpl'],['simple']
Usability,"Hi!; I am new in variant detection, especially detection in deep learning. I want to implement the process from bam file to output file for deepening the knowledge . but it's difficlut to find the definite network structure in the code, so, is there a cnn structure like the function,""model.summary()"", result in tensorflow2? That will help me a lot!; thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/328:65,learn,learning,65,,https://github.com/google/deepvariant/issues/328,1,['learn'],['learning']
Usability,"Hi, I had some more quick questions about training a DeepVariant model starting from one of the built-in models. I noticed in the [tutorial](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-training-case-study.md) that there's a `--channels` flag in the `make_examples` command. I was wondering:. 1. Is it possible during the training workflow to input custom BAM tags (e.g. `my_aln ... XA:i:7 XB:i:-2 XC:Z:test`) as features for the model to use during training/calling? For example, `--channels XA, XB, XC`? Or another flag that could serve this sort of purpose?; 2. If it is possible to do so, do the tags need to exist for all alignments, or can the model still take advantage of them when available and otherwise ignore when not present? I'm not an ML expert but think there are some model architectures that can learn/apply even with some missing features.; 3. If possible, would the model be intelligent enough to use `i` and `f` type tags as numerical, and `Z` tags, etc as categorical labels? What sort of encoding would be used for the latter, if allowed?. Sorry if this is covered in some documentation somewhere. If it is, I'd appreciate a link! Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/790:834,learn,learn,834,,https://github.com/google/deepvariant/issues/790,1,['learn'],['learn']
Usability,"Hi, I wanted to try adding some custom channels to experiment with like in the tutorial [here](https://google.github.io/deepvariant/posts/2022-06-09-adding-custom-channels/). I was wondering if there were any developer's notes on setting up an environment for good syntax highlighting, autocomplete, etc. for C++ with this project so I can better understand how to use the Nucleus library. The [build guide](https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-build-test.md) didn't seem to have anything to this effect. I tried using VSCode and CLion Nova, but can't seem to get the autocomplete to work on either. I'm guessing this is because the project is built using the Dockerfile and incorporates a few different languages, so it's not a ""pure"" C++ project. . Could any developers share some tips on their setup, or point me to a developer's guide? Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/792:401,guid,guide,401,,https://github.com/google/deepvariant/issues/792,2,['guid'],['guide']
Usability,"Hi, i am new to deepvariant and kind of learning some programming languages too as a beginner, i try to run these commands on my ubuntu 16.04 and had the following errors, both with Build_and_test.sh, Build-prereq.sh. please i need help on how to fix these errors and get deepvariant running. thank you. [sudo] password for solokopi: ; + source settings.sh; ++ export DV_USE_PREINSTALLED_TF=0; ++ DV_USE_PREINSTALLED_TF=0; ++ export TF_CUDA_CLANG=0; ++ TF_CUDA_CLANG=0; ++ export TF_ENABLE_XLA=0; ++ TF_ENABLE_XLA=0; ++ export TF_NEED_CUDA=0; ++ TF_NEED_CUDA=0; ++ export TF_NEED_GCP=1; ++ TF_NEED_GCP=1; ++ export TF_NEED_GDR=0; ++ TF_NEED_GDR=0; ++ export TF_NEED_HDFS=0; ++ TF_NEED_HDFS=0; ++ export TF_NEED_JEMALLOC=0; ++ TF_NEED_JEMALLOC=0; ++ export TF_NEED_MKL=0; ++ TF_NEED_MKL=0; ++ export TF_NEED_MPI=0; ++ TF_NEED_MPI=0; ++ export TF_NEED_OPENCL=0; ++ TF_NEED_OPENCL=0; ++ export TF_NEED_OPENCL_SYCL=0; ++ TF_NEED_OPENCL_SYCL=0; ++ export TF_NEED_S3=0; ++ TF_NEED_S3=0; ++ export TF_NEED_VERBS=0; ++ TF_NEED_VERBS=0; ++ export TF_CUDA_VERSION=8.0; ++ TF_CUDA_VERSION=8.0; ++ export CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ CUDA_TOOLKIT_PATH=/usr/local/cuda; ++ export TF_CUDNN_VERSION=6; ++ TF_CUDNN_VERSION=6; ++ export CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ CUDNN_INSTALL_PATH=/usr/lib/x86_64-linux-gnu; ++ DV_BAZEL_VERSION=0.15.0; ++ export DEEPVARIANT_BUCKET=gs://deepvariant; ++ DEEPVARIANT_BUCKET=gs://deepvariant; ++ export DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages; ++ DV_PACKAGE_BUCKET_PATH=gs://deepvariant/packages; ++ export DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages; ++ DV_PACKAGE_CURL_PATH=https://storage.googleapis.com/deepvariant/packages; ++ export DV_TF_NIGHTLY_BUILD=0; ++ DV_TF_NIGHTLY_BUILD=0; ++ [[ 0 = \1 ]]; ++ export DV_CPP_TENSORFLOW_TAG=r1.9; ++ DV_CPP_TENSORFLOW_TAG=r1.9; ++ export DV_GCP_OPTIMIZED_TF_WHL_VERSION=1.9.0; ++ DV_GCP_OPTIMIZED_TF_WHL_VERSION=1.9.0; ++ export DV_TENSORFLOW_STANDARD_GPU_WHL_VERSION=1.9",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:40,learn,learning,40,,https://github.com/google/deepvariant/issues/89,1,['learn'],['learning']
Usability,"Hi, it's getting harder to build deepvariant, even using bioconda as everything it moving to python3.7 or higher.; Would it be possible to get the build and Dockerfile updated to 3.7? And/or could you provide some guidance on what is needed?. Using the docker container works perfectly. But I want to add bcftools and samtools (for example) to the container and also have it work on singularity.; thanks,; -B",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/445:214,guid,guidance,214,,https://github.com/google/deepvariant/issues/445,1,['guid'],['guidance']
Usability,"Hi,. I am running the shuffle script (https://raw.githubusercontent.com/google/deepvariant/r1.0/tools/shuffle_tfrecords_beam.py) on some training data. I am wondering how many output files are expected from running the script. When I use DirectRunner, I get a single output file. When I use the SparkRunner I get as many output files as there are input files fitting the pattern (I have noticed this mismatch between spark/direct runner in another situation as [well](https://stackoverflow.com/questions/64450391/apache-beam-beam-flatten-doesnt-flatten-files-with-sparkrunner-but-does-so-wi)). Is this the expected result when using Dataflow runner as well? Basically, I am simply trying to do a sanity check to make sure that the shuffler isn't simply reading in the data and copying it without shuffling, or simply shuffling within each shard. Thanks!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/364:674,simpl,simply,674,,https://github.com/google/deepvariant/issues/364,3,['simpl'],['simply']
Usability,"Hi,. I am trying to train the DeepVariant model in my local machine (presumably not using docker as well). Is there a specific guide on doing that? or do I simply follow https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-training-case-study.md, download the binaries and change all output bucket references in the guide to local directories? . Also, if I want to change the structure and parameters of the neural network for training, where can I impose those changes? (for example reducing the image size from 221x100 to 101x100). . Thank you very much for your help.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/291:127,guid,guide,127,,https://github.com/google/deepvariant/issues/291,3,"['guid', 'simpl']","['guide', 'simply']"
Usability,"Hi,. I followed the guide to retrain DeepVariant in here: https://github.com/google/deepvariant/blob/r1.6.1/docs/deepvariant-training-case-study.md. This is my command to retrain using the default model in s3://deepvariant/deepvariant_training/model/1.6.1_wgs_model/:; ```; time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=s3-mount/deepvariant_training/script/dv_config.py:base \; --config.train_dataset_pbtxt=""${SHUFFLE_DIR}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${SHUFFLE_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=""${GCS_PRETRAINED_WGS_MODEL}"" \; --config.num_epochs=0 \; --config.learning_rate=0.02 \; --config.num_validation_examples=0 \; --experiment_dir=""model_train"" \; --strategy=mirrored \; --config.batch_size=512 \; --debug 'true'; ```. I received an error regarding about the checkpoint: ```No checkpoint found.```; I also attached my log for training step here: ; [train_040224_failed.log](https://github.com/google/deepvariant/files/14844558/train_040224_failed.log). I'm not very clear where I can get the checkpoint file. My understand is that the input for ```experiment_dir``` is created by running this training step, is that right?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/802:20,guid,guide,20,,https://github.com/google/deepvariant/issues/802,2,"['clear', 'guid']","['clear', 'guide']"
Usability,"Hi,. I have been looking for documentation about the quality scores and how to interpret those. I am working on WGS data for human samples derived from patients affected by rare inherited disorders. I am looking to establish a filtering strategy on the raw VCF data, specifically for `RefCall` variants. I assume that simply getting rid of all those calls may reduce sensitivity. Therefore, I would like to figure out a quality score to use to filter these variants to maximize both sensitivity and specificity.; Any suggestion?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/531:318,simpl,simply,318,,https://github.com/google/deepvariant/issues/531,1,['simpl'],['simply']
Usability,"Hi,. I have some thoughts on the **creating-pileup-image** part and really want to try it out. Since I don't have the GPU resources, I followed the **Getting Started with GCP** guide. Q1: How could I retrain my modified forked version of deep variant using gcloud? . Should I change the last parameter in this command?; `gcloud beta compute instances create ""${USER}-deepvariant-quickstart""`. Q2: How could I prepare the trianing data? . You mentioned running make_examples in training mode to get the data in the **Training DeepVariant models** Guide. How do I ""providing the --confident_regions and --truth_variants arguments""? These two terms are too biological for me to understand completely. . Really appreciated if you can provide some help. . Thank you",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/46:177,guid,guide,177,,https://github.com/google/deepvariant/issues/46,2,"['Guid', 'guid']","['Guide', 'guide']"
Usability,"Hi,. I'm testing running DeepVariant on some of our genomic datasets. . I found out through reading the quick start guide that I can download the docker image of Deepvariant and run this docker image on AWS EC2 instance. In the guideline, it uses t2.medium EC2 instance, I tested and was able to run using the test files. This works with t2.medium because the test cases don't go through the first step, which require GPU to make examples. I want to know that for the real cases with bigger memory requirement, what is the **recommended EC2 instance type** I should use in order to run DeepVariant? . Also, if I want to start with fastq sequencing file, is there an existing tool in the docker image to convert from .fastq to .bam?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/696:116,guid,guide,116,,https://github.com/google/deepvariant/issues/696,2,['guid'],"['guide', 'guideline']"
Usability,"Hi,. Is there an argument to specify a bam index file's location in the make_examples step if it is different from the bam file? I am making symlinks for both, then running make_examples like so:. ```; echo ""Start running make_examples...Log will be in the terminal and also to make_examples.log.""; ( time seq 0 $((${numShards}-1)) | \; parallel -k --line-buffer \; /opt/deepvariant/bin/make_examples \; --mode calling \; --ref ${Fasta} \; --reads bamlink \; --examples ""${sample_id}.examples.tfrecord@${numShards}.gz"" \; --gvcf ""${sample_id}.gvcf.tfrecord@${numShards}.gz"" \; --task {} \; ) 2>&1 | tee ""make_examples.log""; ```; And getting this error:. `ValueError: Not found: No index found for bamlink`. A little strange, because the bam index in question is indeed in the same location as the bam file-- these are the linking commands:. ```; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bam bamlink; + ln -s /cromwell_root/fc-.../dir_name/RP-1735/WGS/JN_G2701-1/v2/JN_G2701-1.bai bailink. ```. So two questions:; 1) Why doesn't this work?; 2) Is there a way to specify the index file location separately, or am I going to have to simply copy the two files into a local folder together at the working directory level. This would be somewhat of a pain because the bam is hundreds of GB. Thanks!. Seems like there might be based on [this link] (https://cloud.google.com/genomics/docs/tutorials/deepvariant#additional_configuration_options); But I can't find the equivalent just for the make_examples section",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/148:1165,simpl,simply,1165,,https://github.com/google/deepvariant/issues/148,1,['simpl'],['simply']
Usability,"Hi,. So I am trying a little bit of a peculiar setup in which I am using ONT corrected reads (with [Ratatosk](https://github.com/DecodeGenetics/Ratatosk)) to call variants with DeepVariant 1.1 in difficult to map regions. For that, I am using the default PACBIO model which works pretty well in the non-difficult to map regions from what I've seen so far.; Now in the difficult to map regions, it is a little bit of a mixed bag which I am trying to understand. More specifically, I get variants in my GVCFs such as:; ```; chr1	26740	.	C	<*>	0	.	END=26740	GT:GQ:MIN_DP:PL	./.:0:7:38,0,128; ```; What I do not understand is that the PL values seems to clearly indicate 0/1, yet the genotype is undefined. Also, my understanding of GQ is that it is derived from the PL values as the difference between most likely and second most likely genotypes. Yet, the GQ is 0 in this example. . I would really appreciate if you could shed some light on this. Thank you very much.; Guillaume",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/403:650,clear,clearly,650,,https://github.com/google/deepvariant/issues/403,1,['clear'],['clearly']
Usability,"Hi,. Sorry this is more of a question than an issue but I just want to understand that I am using Deepvariant correctly.; I read in #704 you said ""Direct phasing is happening internally from version 1.4 of DeepVariant, so it's only necessary for DeepTrio (with the additional --use_hp_information flag following whatshap processing), while DeepVariant -> GLnexus should work as is."". I am trying to run trios using Deepvariant/Deeptrio for the first time. With the above statement are you recommending running Trios with DeepVariant -> GLnexus?. Could you also give some guidance as to how we ""or the non-PAR regions of the sex chromosomes (X and Y), we recommend running these providing only the parent who contributed the child's chromosome (e.g. for chromosomeX, only the mother and son samples and for chromosomeY only the father and son samples)."". Does this mean if we have a trio with son we have to remove Dad's X?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/712:571,guid,guidance,571,,https://github.com/google/deepvariant/issues/712,1,['guid'],['guidance']
Usability,"Hi,. Thank you for this great work!; I just wanted to learn about the license under which the Deepvariant model is shared?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/891:54,learn,learn,54,,https://github.com/google/deepvariant/issues/891,1,['learn'],['learn']
Usability,"Hi,. The RNA-seq model in the case-study is available for version 1.4.0, but trying the download by simply replacing with 1.5.0 raises a file not found error. . Is or will there be an RNA-seq model for v1.5.0, or was this only be available for v1.4.0? . ```; # works ; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; # fails; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.5.0/DeepVariant-inception_v3-1.5.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/624:100,simpl,simply,100,,https://github.com/google/deepvariant/issues/624,1,['simpl'],['simply']
Usability,"Hi,; I am running into an error when I try to use make_examples with a training VCF. The error does not occur when I run make_examples in training mode, without a truth VCF, so I believe the error occurs there. I call make examples with the following:; `/opt/deepvariant/bin/make_examples --mode training --ref refs/${ref} --reads ${BAM} --examples ${base}.tfrecord --truth_variants ${TRUTH_VCF} --confident_regions refs/confidence.bed `. I am using a simple test VCF attached here and I get the following error:; `; ValueError: Invalid argument: Invalid interval: reference_name: ""NC_000962.3"" start: -1 end: 22; `; Could you point me towards how to debug this issue? Thank you! ; [test.vcf.gz](https://github.com/google/deepvariant/files/1985941/test.vcf.gz)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/71:452,simpl,simple,452,,https://github.com/google/deepvariant/issues/71,1,['simpl'],['simple']
Usability,"Hi,; I am trying to deepen my understanding of DeepVariant (pun unintended), similar to what is written here (https://github.com/google/deepvariant/issues/306).; If I understand the discussion above correctly, whenever there is an insertion, the insertion itself will not be present in the pileup image, and the preceding pixel will be zero'd out. So if we have a cigar 6M3I1M such as:; ref: ACGCCT T; alt: ACGCCTCCCT; this will actually be represented in the pileup as:; ref: ACGCCTT; alt: ACGCC0T. I visualized the examples you provided in testdata, and on the 3rd example [fig1] (chr20:10001436-10001436) there is a case in which an insertion occurs right after the variant's position. Following the above logic, the center column should be zero'd out. However, it seems to not be the case. What happens instead is that the last channels center column indicates a diversion from the reference genome (a pixel is 'lit' up). So it looks like the bam read is a basic SNP, even though it actually is a length 5 insertion (the bam is an indel and the variant in the vcf is an snp).; In fact, I haven't seen any case of zero'd out pixels which aren't deletions.; Some options are:. 1. Did I not understand the insertions logic correctly? Whas it changed since the above git issue in April?; 2. Am I possibly working with the wrong files? I use the NA12878_S1.chr20.10_10p1mb.bam/.bai bam file with the golden.training_examples.tfrecord for your examples.; Sorry for the long post, I hope I was clear but let me know if more information/explanations are needed.; ![fig1](https://user-images.githubusercontent.com/52149642/98434660-6ab2e380-20da-11eb-887c-4683bb759d1c.png)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/379:1491,clear,clear,1491,,https://github.com/google/deepvariant/issues/379,1,['clear'],['clear']
Usability,"Hi,; I have a question regarding the use of the _**realign_reads**_ parameter for make_examples.py script. I am interested in running DeepVariant for PACBIO long reads data, and I don't want to realign reads before variant calling. It is stated in the official description: _--[no]realign_reads: If True, locally realign reads before calling variants. Reads longer than 500 bp are never realigned.(default: 'true')_, but, in the description for running PACBIO data it is stated that **_Please note, that if you create your own script make_examples must be called with --norealign_reads flag for PacBio long reads._**. I am having trouble understanding the meaning of realign reads parameter, does the TRUE value realign or doesn't realign reads (basically is the parameter NOrealign _reads, or realign reads)? Just want to clear things up so I don't waste time on long runs. Thank you in advance! :)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/437:823,clear,clear,823,,https://github.com/google/deepvariant/issues/437,1,['clear'],['clear']
Usability,I am confusion about your deep learning architecture.,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/801:31,learn,learning,31,,https://github.com/google/deepvariant/issues/801,2,['learn'],['learning']
Usability,I am exploring sharding the `call_variants` step and I came across this comment that I can simply concatenate TF records together with the `cat` Unix utility:. https://github.com/google/deepvariant/issues/49#issuecomment-366848143. I have a couple of questions if you could please answer. *Q1:* Can I also use the `cat` utility to concatenate GVCF output shards?. Suppose I name the `call_variants` outputs: callvariants-00056-of-00064.gz. *Q2:* Could I pass along `callvariants@64.gz` to the `--infile` option of `postprocess_variants`?,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/113:91,simpl,simply,91,,https://github.com/google/deepvariant/issues/113,1,['simpl'],['simply']
Usability,I download the source code and then simply ran:. `./build-prereq.sh`. I got the error: '/tmp/tensorflow-1.4.1.deepvariant_gcp-cp27-none-linux_x86_64.whl'. ![screen shot 2017-12-07 at 11 24 47 am](https://user-images.githubusercontent.com/7627987/33692414-469c85d2-db41-11e7-909e-1a5b53356481.png),MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/5:36,simpl,simply,36,,https://github.com/google/deepvariant/issues/5,1,['simpl'],['simply']
Usability,"I have WGS data (about 200x) and WES data (about 1000x) of the same individual.; Ideally I would like to merge the 2 datasets and run DeepVariant with --model_type=WGS on the merged data and obtain one VCF file. Or is the model behind ""--model_type=WES"" really a different machine learning model (ML) trained on real Exome data?; I could imagine that such a ML model would learn a slightly different sequencing error model specific for sequencing data derived from target enrichment (hybridization probes) as the ones used for WES. Thank you for your advice. **Setup**; - Operating system: Ubuntu 18.04; - DeepVariant version: r0.10; - Installation method (Docker, built from source, etc.): Docker; - Type of data: Illumina WGS and WES data",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/338:281,learn,learning,281,,https://github.com/google/deepvariant/issues/338,2,['learn'],"['learn', 'learning']"
Usability,"I launched a training run, but the evaluation run wasn't launched concurrently. When I launch it, it simply evaluates the final checkpoint, not all the checkpoints in between. Is there an option force evaluation of all checkpoints in model_eval?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/378:101,simpl,simply,101,,https://github.com/google/deepvariant/issues/378,1,['simpl'],['simply']
Usability,"I learned about the the input of Inceptionv3's channel number is 3, but your input's channel is 6 or more. So how do to deal with it? Are you changed the first layer of inceptionv3? . Thank you!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/801:2,learn,learned,2,,https://github.com/google/deepvariant/issues/801,1,['learn'],['learned']
Usability,"I noticed a small typo in the file `docs/deepvariant-details.md` on the first line. The current text is:. ```; f# DeepVariant usage guide; ```. It would be better to remove the leading ""f"":. ```; # DeepVariant usage guide; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/897:132,guid,guide,132,,https://github.com/google/deepvariant/issues/897,2,['guid'],['guide']
Usability,"I ran my DeepVariant and Deeptrio pipeline following the ""quick start"" guidance, and I noticed that in my real trio case analysis, the variants called by Deeptrio outnumbers those called by DeepVariant (especially the RefCalls), for both the parents and child. Why did this happen? And I also noticed that in issue #699 your team recommand to perform trio analysis either through DeepVariant+GLnexus or Deeptrio with truth sets to be compared. I wonder how to use ""truth set"" (and what does the truth set means? like dataset from GIAB?) to check my Deeptrio results? And which method will you consider as the best in both accuracy and time cost in trio analysis? Really appreciate that if your team could answer these questions!. **Setup**; - Operating system: linux; - DeepVariant version: 1.5.0 (in Deeptrio as well); - Installation method: Singularity version; - Type of data: WES",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/704:71,guid,guidance,71,,https://github.com/google/deepvariant/issues/704,1,['guid'],['guidance']
Usability,"I ran this using singularity. I tried to tell the system to read and write files to a folder in my machine's /mnt/ folder. I keep getting an error. After inspecting, it looks like this image has an empty /mnt/ directory that is not writable. This is a problem for us and many users because it is very common to store large amounts of data in the /mnt/ folder on servers that access shared space from a common storage device. Please tell your Dockerfile to ""RUN rm -rf /mnt/"" or something (I'm not a docker expert by any means). The deepvariant docker container clearly does not need /mnt/. **Setup**; - Centos 7; - deepvariant 1.3.0; - Singularity run pulling from here: docker://google/deepvariant:""1.3.0""; - quickstart example. **Steps to reproduce:**; ...please note that /mnt/share is an NFS mount. My server mounts a drive on another machine running nfs.service ; mkdir -p /mnt/share/jasontest; cd /mnt/share/jasontest; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata""; mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; BIN_VERSION=""1.3.0""; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; singularity run -B /usr/lib/locale/:/usr/lib/locale/ docker://google",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/530:561,clear,clearly,561,,https://github.com/google/deepvariant/issues/530,1,['clear'],['clearly']
Usability,"I was trying to follow the quick start guide. While running the run-prereq.sh file, I got; `========== Load config settings.`; `========== [Sun Jan 28 13:47:50 EST 2018] Stage 'Misc setup' starting`; `========== [Sun Jan 28 13:47:50 EST 2018] Stage 'Update package list' starting`; `sudo: apt-get: command not found`; Then, I realize it is because I am running it on mac. Is there any quick fix to this problem?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/44:39,guid,guide,39,,https://github.com/google/deepvariant/issues/44,1,['guid'],['guide']
Usability,"I wonder what is the error model difference between WGS and WES. Is it simply the way coverage varies, or is there any difference in the error rates/types?. Also for the open training [data](https://github.com/google/deepvariant/blob/r0.10/docs/deepvariant-details-training-data.md), I noticed that the BAM files are named *deduplicated.bam. What is the method used to do mark duplication? Is it GATK?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/329:71,simpl,simply,71,,https://github.com/google/deepvariant/issues/329,1,['simpl'],['simply']
Usability,"I'm trying to build a scatter gather implementation of make_calls -> call_variants -> post_processing, without using GNU parallel, and since multiple shards of call_variants, even with num_readers set to 1, increases the system load well beyond the number of cores, I'd like to try to limit this to a 1:1 ratio where one shard produces a system load of 1. Is this possible?. This is essentially what my pipeline looks like today: https://github.com/oskarvid/wdl_deepvariant/blob/master/deepvar-simple-SG.wdl; I say essentially because I've made insignificant changes, like added --num_readers for example. . One way would be to try to combine all tfrecord files into one, because wdl cannot use your method of using all output files from make_calls as input files for call_variants, inputFile@#shards.gz doesn't compute for wdl, and using wdl's normal way of handling multiple input files, i.e ""--examples ${sep="" --examples "" InputFile}"" doesn't work either since call_variants only takes the last ""--examples"" as input when there are many ""--examples"" in the command. Regarding combining the tfrecord files before they're used as input for call_variants, I'm not familiar enough with tensorflow to know if it's at all possible, and a quick google search didn't return anything fruitful. Is it possible to combine many tfrecord files into one?. Is it easier to try to limit the number of threads per process instead of trying to combine the tfrecord files? Or is there a third method that solves this problem better?. And thanks for a great tool!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/49:494,simpl,simple-SG,494,,https://github.com/google/deepvariant/issues/49,1,['simpl'],['simple-SG']
Usability,"I'm trying to output gVCF's via DeepVariant as described by the tutorial here: https://github.com/google/deepvariant/blob/r0.5/docs/deepvariant-gvcf-support.md . Is there a way that I could just modify the gcp_deepvariant_runner.py script (linked below) rather than having to manually run the commands step by step? I have many BAM files to process and running the pipeline manually is intractable. https://github.com/googlegenomics/gcp-deepvariant-runner/blob/master/gcp_deepvariant_runner.py. I'm guessing I would need to fork the gcp-deepvariant-runner repo, edit the python file, then push the new repo to some sort of container registry? Any guidance here would be much appreciated.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/193:647,guid,guidance,647,,https://github.com/google/deepvariant/issues/193,1,['guid'],['guidance']
Usability,"I'm trying to train a deepvariant model with a very simple topology.; After a few thousands of training steps, the logged training loss starts to vibrate around a rather high value. However the performance of saved models still keeps improving on my validation data set.; Why does this happen?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/194:52,simpl,simple,52,,https://github.com/google/deepvariant/issues/194,1,['simpl'],['simple']
Usability,"I've had success following the **Getting started guide** with both CPU and GPU on the example datasets and now I'm trying to run the CPU version on my own data, _C. elegans_, but am getting an error:. ## Submission script for example. ```; #!/bin/bash; #SBATCH --job-name=example_DV; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_example_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/quickstart-testdata""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/cpu-1cpu""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --num_shards=1; ```. ## Submission script for _C. elegans_. ```; #!/bin/bash; #SBATCH --job-name=Celegans_DeepVar; #SBATCH --nodes=1; #SBATCH --ntasks=1; #SBATCH --cpus-per-task=1; #SBATCH --mem=1000; #SBATCH --time=0:20:0; #SBATCH --account=def-mtarailo; #SBATCH --output=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.out; #SBATCH --error=/scratch/moldach/bin/DEEPVARIANT/logs/deepVar_Celegans_%j.err; #SBATCH --mail-type=ALL; #SBATCH --mail-user=moldach@ucalgary.ca. module load singularity. BIN_VERSION=""0.10.0""; INPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/MADDOG""; OUTPUT_DIR=""/scratch/moldach/bin/DEEPVARIANT/celegans""; mkdir -p ""${OUTPUT_DIR}"". # Pull the image.; sin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/292:49,guid,guide,49,,https://github.com/google/deepvariant/issues/292,1,['guid'],['guide']
Usability,"It was great to see haploid support added with version 1.6. However, it took me some time to understand that I had it working correctly as my expectation was that the genotypes would be represented as hemizygous (0 or 1) rather than homozygous (0/0 or 1/1) in the specified regions. Would it be useful to add a clear statement in the documentation regarding the current representation in ""haploid"" regions, and to possibly consider adding an option in postprocess_variants that allowed an alternative output with hemizygous genotypes? I understand that some tools used in downstream applications may have problems with hemizygous genotypes, and therefore the desirability of representing them as homozygous. However there are also downstream applications where true hemizygote representation has significant value. For us this would be the ability to generate accurate AF, AC and AN values after aggregating into multisample vcfs, but I am sure there are others that would benefit from a more accurate representation of genotypes on the true X and Y chromosomes.; Many thanks for considering this and for providing this very useful tool!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/751:311,clear,clear,311,,https://github.com/google/deepvariant/issues/751,1,['clear'],['clear']
Usability,"My goal is to use DeepVariant in a transfer learning application. Is it possible to get the last layer as an embedding output?. If yes:; Is it possible to use it as a Python module to get these embeddings?. Also, is it possible to run DeepVariant in with a VCF input?. Thank you.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/371:44,learn,learning,44,,https://github.com/google/deepvariant/issues/371,1,['learn'],['learning']
Usability,"RE: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-gvcf-support.md. That documents that the GQ value reported in a gVCF reference band is the lowest GQ out of all the positions covered by the reference band -- this is nice and intuitive along with MIN_DP. It would be nice to document how to think about the PL vector in a reference band as well. since it's a vector, it's not so obvious how one would combine information from all the covered positions. In `variant_caller.make_gvcfs()`:. https://github.com/google/deepvariant/blob/ab068c4588a02e2167051bd9e74c0c9579462b51/deepvariant/variant_caller.py#L308-L314. I *think* this code says to fill in the PL just from the *first* position of the reference band; which seems slightly weird (unless please correct me if I misread!). I don't think this is a significant problem, to be clear, since the quantities are in any case not very useful from reference bands, and (I often argue) essentially a waste of storage space. Main interest here is just documenting what in fact occurs!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/714:245,intuit,intuitive,245,,https://github.com/google/deepvariant/issues/714,2,"['clear', 'intuit']","['clear', 'intuitive']"
Usability,"Running several DeepVariant jobs (v0.9.0) on the same server, e.g., by splitting a dataset via the `--regions` option, results in corrupted files (DataLossError) because of non-unique file names for the temp files generated under `/tmp` on the server (confirmed, see below). > I just checked the code, and you're right that the temp file names will be the same:; > https://github.com/google/deepvariant/blob/r0.9/scripts/run_deepvariant.py#L264-L266; > For now, please pass in different `intermediate_results_dir` for each run.; > For example:; > `--intermediate_results_dir=""/tmp/deepvariant_tmp_output/chr1""` for chr1, and so on. > I'll think about how we want to improve this in the future. I can think of a few options for future improvements, such as :. > 1. Use a random name for the internal /tmp files. Given that these are not exposed to the users anyway.; > 2. Use a unique name derived from the output VCF file, instead of calling all temp files the same name. > For now, using the `--intermediate_results_dir` should hopefully resolve your issue. Let me know if it works. If you have a suggestion on what's the best future improvement for better user experience, please let me know. _Originally posted by @pichuan in https://github.com/google/deepvariant/issues/175#issuecomment-560625427_. @pichuan; I think as an immediate step, updating the docs and making this explicit (also the option of using the `intermediate_results_dir`) would be reasonable. Concerning a proper solution, creating a randomly named temp dir for all temp files of the job (which then could have non-random names) seems like a very straightforward way; ideally, this should also take care of the clean-up, e.g.,as it is provided by Python's `tempfile` module. I assume basic functionality like this exists in all relevant programming languages. +Peter",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/242:1158,user experience,user experience,1158,,https://github.com/google/deepvariant/issues/242,1,['user experience'],['user experience']
Usability,Simple suggestion,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/398:0,Simpl,Simple,0,,https://github.com/google/deepvariant/issues/398,1,['Simpl'],['Simple']
Usability,"Thank you for writing such a fantastic tool and I appreciate the effort! I was running DeepVariant on HPC using singularity and encountered the following error:. Traceback (most recent call last):; File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/__init__.py"", line 24, in <module>; from . import multiarray; File ""/home/miniforge3/lib/python3.10/site-packages/numpy/core/multiarray.py"", line 10, in <module>; from . import overrides; File ""/home/miniforge3/miniforge3/lib/python3.10/site-packages/numpy/core/overrides.py"", line 8, in <module>; from numpy.core._multiarray_umath import (; ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. Which happens to resemble a number of the previous issues, like #782, and #132. It seems like at least in my case, the error is due to the fact that PYTHONPATH is set to a local path and passed to singularity, leading to numpy version incompatibility. In my case, I managed to resolve the issue by simply unset PYTHONPATH, and I can imagine that running singularity with --cleanenv may resolve a number of similar issues. . I am sorry if the solution has already be proposed in some previous issues, but I am wondering if this fix can also be mentioned in documentation, as there may be more users having the issue since singularity is pretty much the only option to run containers on HPC without root privileges.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/795:970,simpl,simply,970,,https://github.com/google/deepvariant/issues/795,1,['simpl'],['simply']
Usability,The [blog post on deep variant](https://research.googleblog.com/2017/12/deepvariant-highly-accurate-genomes.html) mentions:. > a deep learning technology to reconstruct the true genome sequence from HTS sequencer data with significantly greater accuracy than previous classical methods. How could I reconstruct the true genome sequence? Would an example of this be using something like gatk with the generated VCF file to correct the assembly?. E.g. https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_gatk_tools_walkers_fasta_FastaAlternateReferenceMaker.php,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/13:134,learn,learning,134,,https://github.com/google/deepvariant/issues/13,1,['learn'],['learning']
Usability,"The code in [Download binaries, models, and test data](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md#download-binaries-models-and-test-data) the of the DV quick start guide ran successfully. However, running [make_examples](https://github.com/google/deepvariant/blob/r0.4/docs/deepvariant-quick-start.md#make_examples) using the quickstart-testdata failed with the following error:; ```; Traceback (most recent call last):; attila-ThinkS:~/tools/deepvariant$ python bin/make_examples.zip \; > --mode calling \; > --ref ""${REF}"" \; > --reads ""${BAM}"" \; > --regions ""chr20:10,000,000-10,010,000"" \; > --examples ""${OUTPUT_DIR}/examples.tfrecord.gz""; File ""/tmp/Bazel.runfiles_pbJgd2/runfiles/genomics/deepvariant/make_examples.py"", line 45, in <module>; from deepvariant import variant_caller; File ""/tmp/Bazel.runfiles_pbJgd2/runfiles/genomics/deepvariant/variant_caller.py"", line 50, in <module>; from deepvariant.python import variant_calling; ImportError: libcrypto.so.1.0.0: cannot open shared object file: No such file or directory; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/41:201,guid,guide,201,,https://github.com/google/deepvariant/issues/41,1,['guid'],['guide']
Usability,"This might be an obvious question but i cannot work it out, what does the -B mean?. For example from your singularity guide: . ```; singularity run **-B** /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; ```. Also, if using a singularity container would you use it like this (fake link to the container):. ```; wget https://containers/deepvariant_1.3.0.sif ; module load singularity. singularity run deepvariant_1.3.0.sif -B --model_type=WES -ref=PolyposisExomeAnalysis/bwa/index/HumanRefSeq/GRCh38_latest_genomic.fna; --reads=PolyposisExomeAnalysis/samtoolssort/{}PE_samtoolssorted.bam; --output_vcf=PolyposisExomeAnalysis/deepvariant/vcf/PE_output.vcf.gz ; --output_gvcf=PolyposisExomeAnalysis/deepvariant/gvcf/PE_output.vcf.gz; --intermediate_results_dir PolyposisExomeAnalysis/deepvariant/intermediateresults/; ```. Sorry, still figuring it out! Thanks, Amy",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/506:118,guid,guide,118,,https://github.com/google/deepvariant/issues/506,1,['guid'],['guide']
Usability,"To whom it may concern,. I have created an openstack instance to run deepvariant 0.9.0 as detailed below. I had no problem running deepvariant in another openstack instance, but I cannot figure out why it will not work in the instance that I newly created. I was wondering if deepvariant developers could provide some guidance towards running deepvariant again. Regards,; Sangjin. ```; ***** Running the command:*****; time seq 0 54 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}. real 0m1.608s; user 0m25.676s; sys 0m23.860s; Traceback (most recent call last):; File ""/opt/deepvariant/bin/run_deepvariant.py"", line 317, in <module>; app.run(main); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 300, in run; _run_main(main, args); File ""/usr/local/lib/python2.7/dist-packages/absl/app.py"", line 251, in _run_main; sys.exit(main(argv)); File ""/opt/deepvariant/bin/run_deepvariant.py"", line 307, in main; subprocess.check_call(command, shell=True, executable='/bin/bash'); File ""/usr/lib/python2.7/subprocess.py"", line 541, in check_call; raise CalledProcessError(retcode, cmd); subprocess.CalledProcessError: Command 'time seq 0 54 | parallel -k --line-buffer /opt/deepvariant/bin/make_examples --mode calling --ref ""/input/ilAriAges1.fasta.bgz"" --reads ""/input/ilAriAges1.m64097_191226_203354.minimap2_asm20.primary_alignments.sorted.bam"" --examples ""/tmp/deepvariant_tmp_output/make_examples.tfrecord@55.gz"" --norealign_reads --vsc_min_fraction_indels ""0.12"" --task {}' returned non-zero exit status 55; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/315:318,guid,guidance,318,,https://github.com/google/deepvariant/issues/315,1,['guid'],['guidance']
Usability,We are interested in training DeepVariant and are wondering the source of the training data for v0.6? Could you direct us towards the BAM files for the replicates listed under WGS models in the [usage guide](https://github.com/google/deepvariant/blob/r0.6/docs/deepvariant-details.md)?. Thank you!,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/80:201,guid,guide,201,,https://github.com/google/deepvariant/issues/80,1,['guid'],['guide']
Usability,"We are trying to customize the WES model to call variants on RNA-Seq reads. . Regardless of how low we set the learning rate or the batch size or saving the intervals, the value of either the values of (TNs/All) and (FNs/All) is set to 0. or the values of (TPs/All) and (FPs/All) is set to zero. . I am not sure I clarified the case enough so here is a sample of the first case:; {'loss': 1.9104841, 'global_step': 0, 'TNs/All': 0.0, 'FNs/All': 0.0, 'F1/Class2': 0.0, 'Recall/Class1': 1.0, 'Recall/All': 1.0, 'Precision/All': 0.49536133, 'Precision/Class2': 0.0, 'Precision/Class1': 0.18457031, 'Accuracy/All': 0.18457031, 'F1/All': 0.6625306, 'FPs/All': 2067.0, 'Recall/Class2': 0.0, 'TPs/All': 2029.0, 'F1/Class1': 0.31162408}. Here is an example of the second case ; {'loss': 1.0544469, 'global_step': 3252, 'TNs/All': 2067.0, 'FNs/All': 2029.0, 'F1/Class2': 0.0, 'Recall/Class1': 0.0, 'Recall/All': 0.0, 'Precision/All': 0.0, 'Precision/Class2': 0.0, 'Precision/Class1': 0.0, 'Accuracy/All': 0.5046387, 'F1/All': 0.0, 'FPs/All': 0.0, 'Recall/Class2': 0.0, 'TPs/All': 0.0, 'F1/Class1': 0.0}. One experiment the first case was the case for all the model from checkpoint-0 until checkpoint-1779 then the second case suddenly appeared till the end of the evaluation. We run shorter experiments and as I mentioned before, no matter how we change the parameters for the learning rate, batch size, or saving the interval, one of the two cases appears since checkpoint-0. Can you guess any reason for such a behavior?",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/203:111,learn,learning,111,,https://github.com/google/deepvariant/issues/203,2,['learn'],['learning']
Usability,"ZA81B/runfiles/com_google_deepvariant/third_party/nucleus/util/io_utils.py:307: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.; Instructions for updating:; Use eager execution and: ; `tf.data.TFRecordDataset(path)`; I0415 07:34:19.549700 140368878327552 model_train.py:193] Running training on DeepVariantInput(name=HG001, input_file_spec=/data/output/training_data/customized_training/training_set_with_label_shuffled/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz, num_examples=33, mode=train with model inception_v3 and tpu False; I0415 07:34:19.550825 140368878327552 model_train.py:196] Batches per epoch 1; I0415 07:34:19.551630 140368878327552 modeling.py:330] Initializing model from checkpoint at /home/models/model.ckpt; I0415 07:34:19.564393 140368878327552 modeling.py:336] The model checkpoint to warm start from has the same number of classes. If this is in training, we will clear excluded_scopes_for_incompatible_shapes so we include everything for warm starting....; I0415 07:34:19.568434 140368878327552 estimator.py:201] Using config: {'_save_checkpoints_secs': 3000, '_session_config': allow_soft_placement: true; graph_options {; rewrite_options {; meta_optimizer_iterations: ONE; }; }; , '_keep_checkpoint_max': 100000, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7faa056a5210>, '_model_dir': '/data/output/trained_model', '_protocol': None, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}; W0415 07:34:19.583559 140368878327552 deprecation.py:323] ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/172:7399,clear,clear,7399,,https://github.com/google/deepvariant/issues/172,1,['clear'],['clear']
Usability,"bject ,Symbol ; 50.33% , 8.80% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--42.49%--PyEval_EvalFrameEx; | | ; | |--30.79%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.34%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--27.87%--ssw_align; | | | | ; | | | |--14.65%--sw_sse2_word; | | | | ; | | | |--8.32%--sw_sse2_byte; | | | | ; | | | |--2.91%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.36%--ssw_init; | | | ; | | --0.89%--qP_byte; | | ; | |--3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.05%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.07%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.63%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap::pyAlleleCounter::wrapAdd_as_add; | | ; | |--0.59%--google::protobuf::python::cmessage::DeepCopy; | | | ; | | --0.58%--google::protobuf::python::cmessage::MergeFrom; | | | ; | | --0.57%--google::protobuf::Message::MergeFrom; | | | ; | | --0.54%--g",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:2322,learn,learning,2322,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"bject ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.33%--PyEval_EvalFrameEx; | | ; | |--31.12%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.63%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--28.27%--ssw_align; | | | | ; | | | |--14.88%--sw_sse2_word; | | | | ; | | | |--8.45%--sw_sse2_byte; | | | | ; | | | |--2.89%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.38%--ssw_init; | | | ; | | --0.92%--qP_byte; | | ; | |--3.57%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.32%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --3.02%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.63%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.89%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.60%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.56%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.16%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.12%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.58%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap::pyAlleleCounter::wrapAdd_as_add; | | ; | |--0.62%--deepvariant_core_python_sam__reader_clifwrap::pySamIterable::wrapNext; | | ; | --0.57%--google::protobuf::python::cmessage::DeepCopy; | | ; | --0.56%--google::protobuf::python::cmessage::MergeFro",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:7848,learn,learning,7848,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"ck'; # Event count (approx.): 38010500000; #; # Children, Self,Command ,Shared Object ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.33%--PyEval_EvalFrameEx; | | ; | |--31.12%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.63%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--28.27%--ssw_align; | | | | ; | | | |--14.88%--sw_sse2_word; | | | | ; | | | |--8.45%--sw_sse2_byte; | | | | ; | | | |--2.89%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.38%--ssw_init; | | | ; | | --0.92%--qP_byte; | | ; | |--3.57%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.32%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --3.02%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.63%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.89%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.60%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.56%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.16%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.12%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.58%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap::pyAlleleCounter::wrapAdd_as_add; | | ; | |--0.62%--deepvariant_core_python_sam__reader_clifwrap::pySamIterable::wrapNext; | | ; | --0.57%--google::protobuf::python::",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:7770,learn,learning,7770,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"ck'; # Event count (approx.): 46604750000; #; # Children, Self,Command ,Shared Object ,Symbol ; 50.33% , 8.80% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--42.49%--PyEval_EvalFrameEx; | | ; | |--30.79%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.34%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--27.87%--ssw_align; | | | | ; | | | |--14.65%--sw_sse2_word; | | | | ; | | | |--8.32%--sw_sse2_byte; | | | | ; | | | |--2.91%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.36%--ssw_init; | | | ; | | --0.89%--qP_byte; | | ; | |--3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.05%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.07%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.63%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap::pyAlleleCounter::wrapAdd_as_add; | | ; | |--0.59%--google::protobuf::python::cmessage::DeepCopy; | | | ; | | --0.58%--google::protobuf::python::cmessage::MergeFrom; ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:2244,learn,learning,2244,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"ded_sw; | ; --1.19%--__memcpy_sse2_unaligned. 14.65% , 14.62% ,python ,libssw.so ,[.] sw_sse2_word; | ; --14.62%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --14.62%--ssw_align; sw_sse2_word. 8.32% , 8.31% ,python ,libssw.so ,[.] sw_sse2_byte; | ; --8.31%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --8.30%--ssw_align; sw_sse2_byte. 4.51% , 0.00% ,python ,[unknown] ,[.] 0x00000000009063e0; |; ---0x9063e0; | ; --3.66%--PyEval_EvalFrameEx; | ; --3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | ; --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | ; --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | ; --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | ; --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | ; --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | ; --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. #### DV 0.5.1. ```; # Samples: 152K of event 'cpu-clock'; # Event count (approx.): 38010500000; #; # Children, Self,Command ,Shared Object ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.33%--PyEval_EvalFrameEx; | | ; | |--31.12%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.63%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--28.27%--ssw_align; | | | | ; |",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:6162,learn,learning,6162,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"e creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:04Z"" level=error msg=""error waiting for container: context canceled""; parallel: This job failed:; docker run -v /home/cwarden/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant /opt/deepvariant/bin/make_examples --mode calling --ref /mnt/cdw-genome/Ref/hg19.gatk.fasta --reads /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/BWA-MEM_realign_TARGET.bam --examples /mnt/cdw-genome/Charles_Human/Genos_Exome/BWA-MEM_Alignment/Genos_BWA-MEM/examples.tfrecord@8.gz --task 0; docker: Error response from daemon: error while creating mount source path '/home/cwarden/cdw-genome': mkdir /home/cwarden/cdw-genome: file exists.; time=""2019-04-13T22:33:05Z"" level=error msg=""error waiting for container: context canceled""; ```. This is admittedly for an alternative Exome alignment (to test the code), but I also have an alternative WGS alignment to test. Also, I changed to name on the file on GitHub (but the content is currently the same). Part of that error message is repeated (for each shard), but I only copied one representative example above, for the repeated part. If I try to run the DeepVariant container in interactive mode (to try and understand what is going on), I get the following message (which is a note, without actually going into interactive mode):; ```; docker run -it -v /home/user/cdw-genome:/mnt/cdw-genome gcr.io/deepvariant-docker/deepvariant; See https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-quick-start.md.; ```; I do have the `gcloud alpha genomics pipelines` example working, so this isn’t absolutely essential for running DeepVariant on Google Cloud. However, if you can help provide me some guidance for running the [linked script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud, I would very much appreciate it. Thank you very much,; Charles",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:7999,guid,guidance,7999,,https://github.com/google/deepvariant/issues/171,1,['guid'],['guidance']
Usability,"el.ckpt \; --number_of_steps=50000 \; --save_interval_secs 300; ```. Here is the run info for just one sample's examples set (only a single chromosome, for testing purposes, from the .run_info.pbtxt file):. ```; labeling_metrics {; n_truth_variant_sites: 3469; n_truth_variant_alleles: 3474; n_candidate_variant_sites: 9778; n_candidate_variant_alleles: 9943; n_non_confident_candidate_variant_sites: 2219; n_true_positive_sites: 3468; n_true_positive_alleles: 3845; n_false_negative_sites: 1; n_false_negative_alleles: 1; n_false_positive_sites: 6309; n_false_positive_alleles: 6469; n_inexact_position_matches: 1; n_exact_position_matches: 3469; n_exact_position_and_allele_matches: 3443; n_exact_position_and_allele_and_genotype_matches: 3443; }; ```. Training runs just fine, with loss starting at ~1.2 and dropping to 0.04. Batch size is relatively small (memory error on the GPU with any larger). Is it simply my patience or is something else going on? I can provide tensorboard stats as well, but taking any model and performing make_examples(calling) -> postprocess results in only refcalls. Thanks, and let me know what other info I can provide. Edit: Here is some of the output from model_eval; ```; Saving dict for global step 0: Accuracy/All = 0.17285156, Accuracy/Indels = 0.078431375, Accuracy/SNPs = 0.19634147, F1/All = 0.39246467, F1/Het = 0.0, F1/HomRef = 0.39246467, F1/HomVar = 0.2947544, FNs/All = 0.0, FNs/Indels = 0.0, FNs/SNPs = 0.0, FPs/All = 774.0, FPs/Indels = 164.0, FPs/SNPs = 610.0, Precision/All = 0.24414062, Precision/Het = 0.0, Precision/HomRef = 0.24414062, Precision/HomVar = 0.17285156, Precision/Indels = 0.19607843, Precision/SNPs = 0.25609756, Recall/All = 1.0, Recall/Het = 0.0, Recall/HomRef = 1.0, Recall/HomVar = 1.0, Recall/Indels = 1.0, Recall/SNPs = 1.0, TNs/All = 0.0, TNs/Indels = 0.0, TNs/SNPs = 0.0, TPs/All = 250.0, TPs/Indels = 40.0, TPs/SNPs = 210.0, global_step = 0, loss = 1.3173751; I1209 06:57:08.677582 46912496317632 estimator.py:2039] Savin",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/251:2293,simpl,simply,2293,,https://github.com/google/deepvariant/issues/251,1,['simpl'],['simply']
Usability,"ely affect things. In any case, below is the top of the call-graph of percent utilization by method (per version):. #### DV 0.4. ```; # Samples: 186K of event 'cpu-clock'; # Event count (approx.): 46604750000; #; # Children, Self,Command ,Shared Object ,Symbol ; 50.33% , 8.80% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--42.49%--PyEval_EvalFrameEx; | | ; | |--30.79%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.34%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--27.87%--ssw_align; | | | | ; | | | |--14.65%--sw_sse2_word; | | | | ; | | | |--8.32%--sw_sse2_byte; | | | | ; | | | |--2.91%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.36%--ssw_init; | | | ; | | --0.89%--qP_byte; | | ; | |--3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.05%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.07%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.63%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:2074,learn,learning,2074,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"epvariant/make_examples.py"", line 1235, in <module>; tf.app.run(); File ""/home/nyakovenko/.local/lib/python2.7/site-packages/tensorflow/python/platform/app.py"", line 125, in run; _sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1225, in main; make_examples_runner(options); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 1127, in make_examples_runner; candidates, examples, gvcfs = region_processor.process(region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 849, in process; self.in_memory_sam_reader.replace_reads(self.region_reads(region)); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/make_examples.py"", line 889, in region_reads; _, reads = self.realigner.realign_reads(reads, region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/realigner.py"", line 606, in realign_reads; self.config.ws_config, self.ref_reader, reads, region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 232, in select_windows; candidates = _candidates_from_reads(config, ref_reader, reads, region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 84, in _candidates_from_reads; region, expanded_region); File ""/tmp/Bazel.runfiles_yOE450/runfiles/com_google_deepvariant/deepvariant/realigner/window_selector.py"", line 152, in _allele_count_linear_selector; allele_counter, model_conf)); TypeError: allele_count_linear_candidates_from_allele_counter() argument counter is not valid for ::learning::genomics::deepvariant::AlleleCounter (deepvariant.python.allelecounter.AlleleCounter instance given): expecting deepvariant.python.allelecounter.AlleleCounter instance, got deepvariant.python.allelecounter.AlleleCounter instance; ```",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/199:3725,learn,learning,3725,,https://github.com/google/deepvariant/issues/199,1,['learn'],['learning']
Usability,"er, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. #### DV 0.5.1. ```; # Samples: 152K of event 'cpu-clock'; # Event count (approx.): 38010500000; #; # Children, Self,Command ,Shared Object ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.33%--PyEval_EvalFrameEx; | | ; | |--31.12%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.63%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--28.27%--ssw_align; | | | | ; | | | |--14.88%--sw_sse2_word; | | | | ; | | | |--8.45%--sw_sse2_byte; | | | | ; | | | |--2.89%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.38%--ssw_init; | | | ; | | --0.92%--qP_byte; | | ; | |--3.57%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.32%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --3.02%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.63%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.89%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.60%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.56%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.16%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.12%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.58%--google::protobuf::Descript",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:7524,learn,learning,7524,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,erenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/regexp.h' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/set.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/simplify.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/stringpiece.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/tostring.cc' contains an error and its package is in error and referenced by '@com_googlesource_code_re2//:re2'; (09:27:18) ERROR: /opt/app/.cache/bazel/_bazel_root/501e9c7e600bb5ec9e98458625ea98f0/external/com_googlesource_code_re2/BUILD:11:1: Target '@com_googlesource_code_re2//:re2/unicode_casefold.cc' contains an error and its package is in error and ,MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/19:14083,simpl,simplify,14083,,https://github.com/google/deepvariant/issues/19,1,['simpl'],['simplify']
Usability,"following command, deepvariant complained that certain libraries are not found which prevented it from using the GPU:. `apptainer run --nv -B /public:/public,/public3:/public3,/public2:/public2,/fast3:/fast3 \; /public/software/deepvariants/1.6.0/gpuver/deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$REF \; --reads=""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam"" \; --regions ""NC_037590.1:200,000-950,000"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --num_shards=2`. Error messages:; `==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available.; Use the NVIDIA Container Toolkit to start this container with GPU support; see; https://docs.nvidia.com/datacenter/cloud-native/ . 2024-01-05 15:52:56.748367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-01-05 15:52:57.864310: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:1367,learn,learning-container-license,1367,,https://github.com/google/deepvariant/issues/761,1,['learn'],['learning-container-license']
Usability,"g into 'clif'...; |S-chain|-<>-10.68.50.55:7890-<><>-20.205.243.166:443-<><>-OK; remote: Enumerating objects: 5846, done.; remote: Counting objects: 100% (700/700), done.; remote: Compressing objects: 100% (111/111), done.; remote: Total 5846 (delta 618), reused 625 (delta 585), pack-reused 5146; Receiving objects: 100% (5846/5846), 1.69 MiB | 1.11 MiB/s, done.; Resolving deltas: 100% (4683/4683), done.; + cd clif; + [[ ! -z 9ec44bde4f7f40de342a1286f84f5b608633a2d7 ]]; + git checkout 9ec44bde4f7f40de342a1286f84f5b608633a2d7; Note: switching to '9ec44bde4f7f40de342a1286f84f5b608633a2d7'. You are in 'detached HEAD' state. You can look around, make experimental; changes and commit them, and you can discard any commits you make in this; state without impacting any branches by switching back to a branch. If you want to create a new branch to retain commits you create, you may; do so (now or later) by using -c with the switch command. Example:. git switch -c <new-branch-name>. Or undo this operation with:. git switch -. Turn off this advice by setting config variable advice.detachedHead to false. HEAD is now at 9ec44bd Replace C++ `#import <...>` with `#include <...>`; + ./INSTALL.sh; +++ dirname ./INSTALL.sh; ++ cd .; ++ pwd; + CLIFSRC_DIR=/root/clif; + BUILD_DIR=/root/clif/build; + declare -a CMAKE_G_FLAG; + declare -a MAKE_PARALLELISM; + which ninja; + CMAKE_G_FLAGS=(); + MAKE_OR_NINJA=make; + MAKE_PARALLELISM=(-j 2); + [[ -r /proc/cpuinfo ]]; ++ cat /proc/cpuinfo; ++ grep -c '^processor'; + N_CPUS=32; + [[ 32 -gt 0 ]]; + MAKE_PARALLELISM=(-j $N_CPUS); + MAKE_INSTALL_PARALLELISM=(${MAKE_PARALLELISM[@]}); + echo 'Using make for the clif backend build.'; Using make for the clif backend build.; + [[ '' =~ ^-?-h ]]; + [[ -n '' ]]; ++ which python3; + PYTHON=/usr/local/bin/python3; + echo -n 'Using Python interpreter: /usr/local/bin/python3'; Using Python interpreter: /usr/local/bin/python3+ [[ '' -eq 1 ]]; + mkdir -p /root/clif/build; + cd /root/clif/build; + cmake -DPYTHO",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/739:2845,undo,undo,2845,,https://github.com/google/deepvariant/issues/739,1,['undo'],['undo']
Usability,"have any advice on which versions of everything I should make sure to have installed correctly before running the shuffle script? Any guidance is very much appreciated. . Not so much a question but I want to confirm my understanding of the pipeline from the tutorial, as again I am very new to this. ; First step is to run deepvariant make_examples in training mode to create training and validation sets. In the documents, these are individual chromosomes, but in theory these could be whole individuals or multiple individuals, is that correct? And then make_examples in training mode should be run multiple times independently for training and validation sets? If for example, I used Chromosome 1 for my training set and Chromosome 2 for my validation set, should those repeated runs be made on different chromosomes, or the same chromosomes? Then finally, once everything is shuffled, run model_train and model_eval. . Thank you very much for your time, and if these questions are answered clearly in a doc already, then I apologize and would appreciate being directed there. . Best, ; Haley . Here is the error traceback: ; `Traceback (most recent call last):; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/__init__.py"", line 23, in <module>; from . import multiarray; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/multiarray.py"", line 10, in <module>; from . import overrides; File ""/project/pbarc/haley.arnold/condaenvs/tensorflow/lib/python3.11/site-packages/numpy/core/overrides.py"", line 6, in <module>; from numpy.core._multiarray_umath import (; ModuleNotFoundError: No module named 'numpy.core._multiarray_umath'. During handling of the above exception, another exception occurred:. Traceback (most recent call last):; File ""/90daydata/pbarc/haley.arnold/AI_Model_Training/Samples/shuffle_tfrecords_beam.py"", line 77, in <module>; import apache_beam as beam; File ""/project/pbarc/haley.arno",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/793:1906,clear,clearly,1906,,https://github.com/google/deepvariant/issues/793,1,['clear'],['clearly']
Usability,"ibssw.so ,[.] sw_sse2_word; | ; --14.62%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --14.62%--ssw_align; sw_sse2_word. 8.32% , 8.31% ,python ,libssw.so ,[.] sw_sse2_byte; | ; --8.31%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --8.30%--ssw_align; sw_sse2_byte. 4.51% , 0.00% ,python ,[unknown] ,[.] 0x00000000009063e0; |; ---0x9063e0; | ; --3.66%--PyEval_EvalFrameEx; | ; --3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | ; --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | ; --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | ; --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | ; --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | ; --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | ; --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. #### DV 0.5.1. ```; # Samples: 152K of event 'cpu-clock'; # Event count (approx.): 38010500000; #; # Children, Self,Command ,Shared Object ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.33%--PyEval_EvalFrameEx; | | ; | |--31.12%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.63%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--28.27%--ssw_align; | | | | ; | | | |--14.88%--sw_sse2_word; | | | | ; | | | |--8.45%--sw_sse2_byte; | |",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:6232,learn,learning,6232,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"ing ancient CUDA 11.3.1; Could maintainers build newer docker images with CUDA >=12.4 or at least >=11.8 to be able to use modern cards such as H100 and L40S (CUDA CC = 8.9 and 9.0). **Setup**; - Operating system: RHEL 8.10; - DeepVariant version: 1.5.0-gpu, 1.6.1-gpu; - Installation method (Docker, built from source, etc.): docker ; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?) Pacbio Revel fresh data. . **Steps to reproduce:**; - Command: docker run --gpus 1 google/deepvariant:1.5.0-gpu or docker run --gpus 1 google/deepvariant:1.6.1-gpu; - Error trace: (if applicable); ...; CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-07-03 17:21:57.549571: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-07-03 17:21:57.644332: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.; 2024-07-03 17:21:58.247052: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/844:1228,learn,learning-container-license,1228,,https://github.com/google/deepvariant/issues/844,1,['learn'],['learning-container-license']
Usability,"isfied: oauth2client>=4.0.0 in /usr/local/lib/python2.7/dist-packages (4.1.2); Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (0.4.4); Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (0.2.2); Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (1.11.0); Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (0.11.3); Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python2.7/dist-packages (from oauth2client>=4.0.0) (3.4.2); Requirement already satisfied: crcmod>=1.7 in /usr/local/lib/python2.7/dist-packages (1.7); Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (1.11.0); Requirement already satisfied: sklearn in /usr/local/lib/python2.7/dist-packages (0.0); Requirement already satisfied: scikit-learn in /usr/local/lib/python2.7/dist-packages (from sklearn) (0.19.2); Requirement already satisfied: pandas in /usr/local/lib/python2.7/dist-packages (0.23.4); Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (1.14.0); Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python2.7/dist-packages (from pandas) (2.7.3); Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas) (2018.5); Requirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil>=2.5.0->pandas) (1.11.0); Requirement already satisfied: psutil in /usr/local/lib/python2.7/dist-packages (5.4.7); Requirement already up-to-date: google-api-python-client in /usr/local/lib/python2.7/dist-packages (1.7.4); Requirement already satisfied, skipping upgrade: httplib2<1dev,>=0.9.2 in /usr/local/lib/python2.7/dist-packages (from google-api-python-client) (0.11.3); Requirement ",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/89:16203,learn,learn,16203,,https://github.com/google/deepvariant/issues/89,1,['learn'],['learn']
Usability,"ll last):; File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/exceptions.py"", line 640, in conda_exception_handler; return_value = func(*args, **kwargs); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main.py"", line 140, in _main; exit_code = args.func(args, p); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/main_install.py"", line 80, in execute; install(args, parser, 'install'); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/cli/install.py"", line 326, in install; execute_actions(actions, index, verbose=not context.quiet); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/plan.py"", line 828, in execute_actions; execute_instructions(plan, index, verbose); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 247, in execute_instructions; cmd(state, arg); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/instructions.py"", line 108, in UNLINKLINKTRANSACTION_CMD; txn.execute(); File ""/exports/applications/apps/SL7/anaconda/5.0.1/lib/python3.6/site-packages/conda/core/link.py"", line 297, in execute; rollback_excs,; conda.CondaMultiError: post-link script failed for package bioconda::deepvariant-0.9.0-py27h7333d49_0; running your command again with `-v` will provide additional information; location of failed script: /PATH/TO/MY/FOLDER/Andrea/myanaconda/deepvariant/bin/.deepvariant-post-link.sh; ==> script messages <==; <None>; ```; Hovewer, since conda installed successfully all the dependencies, I've then tried to download the precompiled binaries and use them, but couldn't find a guide on how to install them.; Is there a page where to find guidelines on how to install the precompiled deepvariant?; If not, is there a way to fix the anaconda environment issue?. Thank you in advance,. Andrea",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/252:4271,guid,guide,4271,,https://github.com/google/deepvariant/issues/252,2,['guid'],"['guide', 'guidelines']"
Usability,"markDup.bam --examples /home/paul/exome-case-study/output/HG002.examples.tfrecord@1.gz --regions /home/paul/exome-case-study/input/data/refseq.coding_exons.b37.extended50.bed --task 0; Illegal instruction (core dumped); $; ```; After digging a bit deeper, I noticed that loading the `pileup_image_native` module was causing this issue. I was curious and looked at the assembly instructions:. ```; $ gdb -ex r --args python -c ""from deepvariant.python import pileup_image_native""; Program received signal SIGILL, Illegal instruction.; 0x00007ffff5d308b4 in google::protobuf::DescriptorPool::Tables::Tables() (); from /home/paul/make-examples/runfiles/genomics/deepvariant/python/../../_solib_k8/libexternal_Sprotobuf_Uarchive_Slibprotobuf.so; (gdb) disassemble $pc,$pc+32; Dump of assembler code from 0x7ffff5d308b4 to 0x7ffff5d308d4:; => 0x00007ffff5d308b4 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+676>: vpxor %xmm0,%xmm0,%xmm0; 0x00007ffff5d308b8 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+680>: lea 0x1b0(%rbx),%rax; 0x00007ffff5d308bf <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+687>: movl $0x0,0x1b0(%rbx); 0x00007ffff5d308c9 <_ZN6google8protobuf14DescriptorPool6TablesC2Ev+697>: movq $0x0,0x1b8(%rbx); End of assembler dump.; (gdb); ```. I noticed the `vpxor` instruction, which made me wonder if my CPU is enabled for AVX, so I proceeded as follows:. ```; $ grep flags /proc/cpuinfo; flags : fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 syscall nx rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc pni pclmulqdq monitor ssse3 cx16 sse4_1 sse4_2 x2apic popcnt aes hypervisor lahf_lm; $; ```. This confirmed for me that I don't have AVX support. So it would be great for the examples that will drive usage and be used by many users to learn from, if the provided libraries are compiled with the bare-minimum of CPU qualities. I think it'll make it a bit easier for many users to adopt this nice pipeline. Thanks,; Paul",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/21:3615,learn,learn,3615,,https://github.com/google/deepvariant/issues/21,1,['learn'],['learn']
Usability,"nd estimated costs) to a program that might be able to help estimate run-time and cost (to possible help with topic **1a)**, **in the long-term**)?. Since `gcp_deepvariant_runner` avoids the possibility of delays between running steps (and has an exist status depending upon whether variant calling was successful), perhaps some sort of optional reporting to an anonymized database could be provided as a parameter for that?. **2)** While I realize it could be considered a cross-post, I am trying to test running each of the 3 steps run separately on Google Cloud (instead of using `gcloud alpha genomics pipelines`). I have some notes on this [Stack Overflow post]( https://stackoverflow.com/questions/55624506/running-docker-on-google-cloud-instance-with-data-in-gcsfuse-mounted-bucket) about the details of my installation and running of Docker on Google Cloud. I suspect there may be some more complications that I need to learn about (in terms of running Docker on Google Cloud, *using data stored in a Google Cloud Bucket*), but the messages that I get are different when using the DeepVariant container versus my own container. So, I thought it might be OK to post a question here. If I try to run [a script]( https://github.com/cwarden45/DTC_Scripts/blob/master/Genos_Exome/run_DeepVariant_3steps.sh) on Google Cloud that is similar to AWS (and based upon the very helpful [DeepVariant Exome Case Study]( https://github.com/google/deepvariant/blob/r0.8/docs/deepvariant-exome-case-study.md)), I get the following error message:. ```; $ sh deepvariant_run_Exome_BWA_MEM_by-step.sh; Reading package lists... Done; Building dependency tree; Reading state information... Done; time is already the newest version (1.7-25.1+b1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.; Reading package lists... Done; Building dependency tree; Reading state information... Done; parallel is already the newest version (20161222-1).; 0 upgraded, 0 newly installed, 0 to remove and 7 not upgr",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/171:4216,learn,learn,4216,,https://github.com/google/deepvariant/issues/171,1,['learn'],['learn']
Usability,"om-8-30720', '--disk-size', '30', '--set', 'MODEL=gs://deepvariant/models/DeepVariant/0.7.1/DeepVariant-inception_v3-0.7.1+data-wgs_standard/', '--set', 'SHARDS=8', '--set', 'CALL_VARIANTS_SHARD_INDEX=0', '--set', 'CALL_VARIANTS_SHARDS=1', '--command', '\n/opt/deepvariant/bin/call_variants\n --examples ""${EXAMPLES}""/examples_output.tfrecord@""${SHARDS}"".gz\n --outfile ""${CALLED_VARIANTS}""/call_variants_output.tfrecord-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARD_INDEX}"")""-of-""$(printf ""%05d"" ""${CALL_VARIANTS_SHARDS}"")"".gz\n --checkpoint ""${MODEL}""/model.ckpt\n --batch_size 512\n']; [12/12/2018 13:33:54 ERROR gcp_deepvariant_runner.py] For more information, consult the worker log at gs://ms_bam/deep_output/stage/logs/call_variants/0; Traceback (most recent call last):; File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 908, in <module>; run(); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 895, in run; _run_call_variants(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 491, in _run_call_variants; _run_call_variants_with_pipelines_api(pipeline_args); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 483, in _run_call_variants_with_pipelines_api; _wait_for_results(threads, results); File ""/opt/deepvariant_runner/src/gcp_deepvariant_runner.py"", line 369, in _wait_for_results; result.get(); File ""/usr/lib/python2.7/multiprocessing/pool.py"", line 572, in get; raise self._value; RuntimeError: Job failed with error ""run"": operation ""projects/ms-deepvariant/operations/23049213423"" failed: executing pipeline: Execution failed: action 4: unexpected exit status 1 was not ignored (reason: FAILED_PRECONDITION). 5. The script is run from a GCP VM instance.; 6. I have a standard setup with increased CPUs to 1025. . It is not clear to me what the issues are and what to make of the errors? ; I am sorry If this is not the correct forum to post in - please let me know where to seek help if not! ; Cheers, C",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/129:9448,clear,clear,9448,,https://github.com/google/deepvariant/issues/129,1,['clear'],['clear']
Usability,"orted from `googleapiclient.discovery`. The issue appears to be the [python3.3 _ _ init _ _.py trap](http://python-notes.curiousefficiency.org/en/latest/python_concepts/import_traps.html#the-init-py-trap) where one python module is blocking another from being found. In the python path there is a `google` module with an `__init__.py` found here, `/tmp/Bazel.runfiles_461ld2s6/runfiles/com_google_protobuf/python/google/__init__.py`, while running. That may be blocking the discovery of `/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py`. **Work Around**. I think configuring Bazel to avoid the issue is probably the right way to fix this, but I worked around the issue by patching `googleapiclient.discovery` with the following patch:. ```; 49c49,59; < import google.api_core.client_options; ---; > ; > # Mega hack to avoid init.py trap of google/init.py which is somewhere on the path; > # Make a namespace to hold our module; > import types; > google = types.SimpleNamespace(); > google.api_core = types.SimpleNamespace(); > # Directly import our module into the namespace; > import importlib.util; > spec = importlib.util.spec_from_file_location(""google.api_core.client_options"", ""/usr/local/lib/python3.6/dist-packages/google/api_core/client_options.py""); > google.api_core.client_options = importlib.util.module_from_spec(spec); > spec.loader.exec_module(google.api_core.client_options); ```; This manually imports the required module, which only works because we know the path won't change in our docker image and we know `googleapiclient.discovery` only uses `client_options.py`. Finally, make a new docker image with this patch by calling it discovery.patch and using this Dockerfile:. ```; ARG VERSION=1.1.0. FROM google/deepvariant:""${VERSION}""-gpu. RUN python3.6 -m pip install --upgrade pip; RUN python3.6 -m pip install --upgrade --force-reinstall cloud-tpu-client. WORKDIR /opt/deepvariant. COPY discovery.patch /opt/deepvariant/; RUN patch /usr/local/lib/pytho",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/469:4023,Simpl,SimpleNamespace,4023,,https://github.com/google/deepvariant/issues/469,1,['Simpl'],['SimpleNamespace']
Usability,"python programs that used gpu with success.; I also managed to run the CPU version with deepvariant with singularity with success. ; However when running deepvariant on a gpu node with the following command, deepvariant complained that certain libraries are not found which prevented it from using the GPU:. `apptainer run --nv -B /public:/public,/public3:/public3,/public2:/public2,/fast3:/fast3 \; /public/software/deepvariants/1.6.0/gpuver/deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=$REF \; --reads=""/public2/courses/ec3121/shareddata/Pomacea_canaliculata/wgs/FSL10-M.bam"" \; --regions ""NC_037590.1:200,000-950,000"" \; --output_vcf=${OUTPUT_DIR}/output.vcf.gz \; --output_gvcf=${OUTPUT_DIR}/output.g.vcf.gz \; --num_shards=2`. Error messages:; `==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. WARNING: The NVIDIA Driver was not detected. GPU functionality will not be available.; Use the NVIDIA Container Toolkit to start this container with GPU support; see; https://docs.nvidia.com/datacenter/cloud-native/ . 2024-01-05 15:52:56.748367: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F AVX512_VNNI FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-01-05 15:52:57.864310: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/761:1203,Learn,Learning,1203,,https://github.com/google/deepvariant/issues/761,1,['Learn'],['Learning']
Usability,"r_gpu:/output \; deepvariant_1.6.0-gpu.sif \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf=output_apptainer_gpu/HG001.apptainer.gpu.output.vcf.gz \; --output_gvcf=output_apptainer_gpu/HG001.apptainer.gpu.output.g.vcf.gz \; --num_shards=$(nproc) \; --customized_model=input/weights-51-0.995354.ckpt; INFO: /usr/local/etc/singularity/ exists; cleanup by system administrator is not complete (see https://apptainer.org/docs/admin/latest/singularity_migration.html). ==========; == CUDA ==; ==========. CUDA Version 11.3.1. Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved. This container image and its contents are governed by the NVIDIA Deep Learning Container License.; By pulling and using the container, you accept the terms and conditions of this license:; https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license. A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience. 2024-02-17 23:31:25.687399: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations: AVX2 AVX512F FMA; To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.; 2024-02-17 23:31:39.809521: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libcublas.so.12: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64:/.singularity.d/libs; 2024-02-17 23:31:39.810043: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with Ten",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/774:2796,learn,learning-container-license,2796,,https://github.com/google/deepvariant/issues/774,1,['learn'],['learning-container-license']
Usability,"rain/tune/test across samples - see below). <img width=""1437"" alt=""Screenshot 2024-08-07 at 09 30 23"" src=""https://github.com/user-attachments/assets/3178e87a-8cf7-47cb-84a2-0a84d15c958f"">. **Shuffling**; Performed downsampling=0.5.; Shuffled globally across samples, chromosomes and downsampling. . **Command**. My latest training run was like so:. ```; apptainer run ; --nv ; -B $WD:/home ; $DV_PATH ; /opt/deepvariant/bin/train ; --config=/home/dv_config.py:base ; --config.train_dataset_pbtxt=""/home/examples_shuffled/train/All_samples_training_examples.dataset_config.pbtxt"" ; --config.tune_dataset_pbtxt=""/home/examples_shuffled/tune/All_samples_tune_examples.dataset_config.pbtxt"". ; --config.num_epochs=1 ; --config.learning_rate=0.0001 ; --config.num_validation_examples=0 ; --config.tune_every_steps=2000 ; --experiment_dir=/home/${OUTDIR} ; --strategy=mirrored ; --config.batch_size=64 ; --config.init_checkpoint=""/home/model_wgs_v1.6.1/deepvariant.wgs.ckpt""; ```. Though previous runs had higher learning rates (0.01) and batch sizes (128). Training proceeds as follows:. Training Examples: 1454377; Batch Size: 64; Epochs: 1; Steps per epoch: 22724; Steps per tune: 3162; Num train steps: 22724. **Log file**. Here is the top of the log file, including some warnings in case they are relevant:. ```; /usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning:. TensorFlow Addons (TFA) has ended development and introduction of new features.; TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.; Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). For more information see: https://github.com/tensorflow/addons/issues/2807. warnings.warn(; 2024-08-28 10:40:42.588215: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:2290,learn,learning,2290,,https://github.com/google/deepvariant/issues/876,1,['learn'],['learning']
Usability,"rity, but I got the same error. I've tried to find a solution through various online resources, but nothing has helped so far. `Traceback (most recent call last):; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/train.py"", line 532, in <module>; app.run(main); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/absl_py/absl/app.py"", line 312, in run; _run_main(main, args); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/absl_py/absl/app.py"", line 258, in _run_main; sys.exit(main(argv)); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/train.py"", line 518, in main; train(FLAGS.config); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/train.py"", line 121, in train; tune_dataset_config = data_providers.read_dataset_config(; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_deepvariant/deepvariant/data_providers.py"", line 634, in read_dataset_config; dataset_config = text_format.Parse(; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 648, in Parse; return ParseLines(text.split(b'\n' if isinstance(text, bytes) else u'\n'),; File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 722, in ParseLines; return parser.ParseLines(lines, message); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 776, in ParseLines; self._ParseOrMerge(lines, message); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 804, in _ParseOrMerge; self._MergeField(tokenizer, message); File ""/tmp/Bazel.runfiles_ebq8nvgq/runfiles/com_google_protobuf/python/google/protobuf/text_format.py"", line 894, in _MergeField; raise tokenizer.ParseErrorPreviousToken(; google.protobuf.text_format.ParseError: 13:1 : Message type ""learning.genomics.deepvariant.DeepVariantDatasetConfig"" has no field named ""s2"".`",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/837:2515,learn,learning,2515,,https://github.com/google/deepvariant/issues/837,1,['learn'],['learning']
Usability,"roject ${PROJECT_ID} \; --zones us-west2-* \; --docker_image ${DOCKER_IMAGE} \; --outfile ${OUTPUT_BUCKET}/${OUTPUT_FILE_NAME} \; --staging ${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME} \; --model ${MODEL} \; --bam gs://mbh-bam-files1/HR090610.final.bam \; --bai gs://mbh-bam-files1/HR090610.final.bam.bai \; --ref gs://mbh-bam-files1/GCA_000001405.28_GRCh38.p13_genomic.fa \; --shards 224 \; --make_examples_workers 7 \; --make_examples_cores_per_worker 32 \; --make_examples_ram_per_worker_gb 60 \; --make_examples_disk_per_worker_gb 200 \; --call_variants_workers 7 \; --call_variants_cores_per_worker 32 \; --call_variants_ram_per_worker_gb 60 \; --call_variants_disk_per_worker_gb 200 \; --gcsfuse""; # Run the pipeline.; gcloud alpha genomics pipelines run \; --project ""${PROJECT_ID}"" \; --service-account-scopes=""https://www.googleapis.com/auth/cloud-platform"" \; --logging ""${OUTPUT_BUCKET}/${STAGING_FOLDER_NAME}/runner_logs_$(date +%Y%m%d_%H%M%S).log"" \; --regions us-west2 \; --docker-image gcr.io/cloud-lifesciences/gcp-deepvariant-runner \; --command-line ""${COMMAND}"". The log file is attached, but part of it is also pasted below. Is it saying that there is a mismatch between the .fai and .fa files for the reference or between the reference and the bam file? The .fai file was created from the .fa file using samtools index command. . ValueError: Reference contigs span 3270284521 bases but only 0 bases (0.00%) were found in common among our input files. Check that the sources were created on a common genome reference build. Contig matches were: ""CM000663.2"" is 248956422 bp and IS MISSING, ""KI270706.1"" is 175055 bp and IS MISSING, ""KI270707.1"" is 32032 bp and IS MISSING, ""KI270708.1"" is 127682 bp and IS MISSING, ""KI270709.1"" is 66860 bp and IS MISSING, ""KI270710.1"" is 40176 bp and IS MISSING... Any feedback would be appreciated. Thanks, -Matt. [staging_folder1_logs_make_examples_7.txt](https://github.com/google/deepvariant/files/3700231/staging_folder1_logs_make_examples_7.txt)",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/225:2556,feedback,feedback,2556,,https://github.com/google/deepvariant/issues/225,1,['feedback'],['feedback']
Usability,"rolled through and verified that is the only read. . here is the content of the dad's VCF for that region (the mom's is actually very similar):; ```; chr8	75144980	.	CT	C	44.7	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:44:34:16,18:0.529412:44,0,53; chr8	75144983	.	T	TG	49.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:49:35:18,17:0.485714:49,0,64; ```; note that that is the single-base del and the insertion that occurs in only 1 read. Since the mom's is the same, maybe there's something akin to realignment going on, but; by contrast, here is the kid's (seemingly more sensible) VCF for that region:; ```; chr8	75144981	.	T	A	71.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:66:27:11,16:0.592593:71,0,67; chr8	75144982	.	A	T	63.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:61:27:11,16:0.592593:63,0,63; chr8	75144983	.	T	G	67.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:66:27:11,16:0.592593:67,0,71; ```; here is the content of the gvcf for dad:; ```; chr8	75144980	.	CT	C,<*>	44.7	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:44:34:16,18,0:0.529412,0:44,0,53,990,990,990; chr8	75144982	.	A	<*>	0	.	END=75144982	GT:GQ:MIN_DP:PL	0/0:48:16:0,48,479; chr8	75144983	.	T	TG,<*>	49.5	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:49:35:18,17,0:0.485714,0:49,0,64,990,990,990; chr8	75144984	.	G	<*>	0	.	END=75145000	GT:GQ:MIN_DP:PL	0/0:50:31:0,105,1049; ```; and kid:; ```; chr8	75144981	.	T	A,<*>	71.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:66:27:11,16,0:0.592593,0:71,0,67,990,990,990; chr8	75144982	.	A	T,<*>	63.6	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:61:27:11,16,0:0.592593,0:63,0,63,990,990,990; chr8	75144983	.	T	G,<*>	67.9	PASS	.	GT:GQ:DP:AD:VAF:PL	0/1:66:27:11,16,0:0.592593,0:67,0,71,990,990,990; chr8	75144984	.	G	<*>	0	.	END=75145000	GT:GQ:MIN_DP:PL	0/0:50:25:0,75,749; ```; I am attaching a small sam for kid and dad aligned to hg38; [kid.sam.gz](https://github.com/google/deepvariant/files/4206576/kid.sam.gz); [dad.sam.gz](https://github.com/google/deepvariant/files/4206577/dad.sam.gz). I have other scenarios, but this one is one that seems clearly a deep variant issue and not a problem with glnexus.",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/272:2866,clear,clearly,2866,,https://github.com/google/deepvariant/issues/272,1,['clear'],['clearly']
Usability,"running call variants on ONT data using the command . `run_pepper_margin_deepvariant call_variant -b ./Sample/alignments/GRCh38/41195.minimap2.bam -f /data/Homo_sapiens_assembly38.fasta -o Sample -t 4 -s Sample --phased_output --ont_r9_guppy5_sup; `. no errors but it's stuck at Starting Candidate Finding and the CPU is at 0% for hours. Can I stop it and resume? why there is no error? will it ever finish?. ....; [12-24-2023 05:47:47] INFO: SUMMARY PROCESSED 7280/7280.; [12-24-2023 05:47:47] INFO: THREAD 0 FINISHED SUCCESSFULLY.; [12-24-2023 05:54:33] INFO: FINISHED PREDICTION; [12-24-2023 05:54:33] INFO: ELAPSED TIME: 867 Min 41 Sec; [12-24-2023 05:54:33] INFO: PREDICTION FINISHED SUCCESSFULLY.; [12-24-2023 05:54:33] INFO: TOTAL ELAPSED TIME FOR INFERENCE: 867 Min 44 Sec; [12-24-2023 05:54:33] INFO: STEP 3/3 FINDING CANDIDATES; [12-24-2023 05:54:33] INFO: OUTPUT: 41195/pepper/; [12-24-2023 05:55:25] INFO: STARTING CANDIDATE FINDING. - Operating system: Linux; - DeepVariant version: kishwars/pepper_deepvariant r0.8; - Installation method (Docker, built from source, etc.): Docker; ONT long reads",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/758:356,resume,resume,356,,https://github.com/google/deepvariant/issues/758,1,['resume'],['resume']
Usability,"ssw_init; | ; --0.92%--qP_byte. 28.27% , 0.04% ,python ,libssw.so ,[.] ssw_align; | ; --28.23%--ssw_align; | ; |--14.88%--sw_sse2_word; | ; |--8.45%--sw_sse2_byte; | ; |--2.89%--banded_sw; | ; --1.19%--__memcpy_sse2_unaligned. 14.88% , 14.86% ,python ,libssw.so ,[.] sw_sse2_word; | ; --14.86%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --14.86%--ssw_align; sw_sse2_word. 8.45% , 8.43% ,python ,libssw.so ,[.] sw_sse2_byte; | ; --8.43%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --8.43%--ssw_align; sw_sse2_byte. 4.72% , 0.00% ,python ,[unknown] ,[.] 0x00000000009063e0; |; ---0x9063e0; | ; --3.94%--PyEval_EvalFrameEx; | ; --3.57%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | ; --3.32%--learning::genomics::deepvariant::DeBruijnGraph::Build; | ; --3.02%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | ; --2.63%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | ; --1.89%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | ; --1.60%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | ; --0.56%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. To do this properly would require that the tests be performed on different datasets, and different CPUs on the same Cloud environment - with different distributed scenarios - which would be cost-prohibitive for me. Hope it helps and have a great weekend!; Paul",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:11522,learn,learning,11522,,https://github.com/google/deepvariant/issues/50,4,['learn'],['learning']
Usability,"tart to have enough material to get long ONT precise reads (the very last one that is accurate at 99.99%, just like a giant PCR); those are much less likely to mismap, and Clair3 seems extremely powerful and precise to call. Therefore, we could consider the call from long reads as a ""truth set"". . My point is if I give deep variant the ONT ""truth set"" and then the mapping of short illumina reads. Could it be retrained to understand the mapping and calling issues with this kind of genome? I don't have a ""rule"" such as Mendelian violation because my organism is clonal. Therefore, the only possibility of having a ""truth set"" is to trust long reads mapping (Sanger sequencing doesn't work well either; we don't know why). . Is it something doable? I could use other things, such as Python random forests, as suggested by a colleague, but since you have spent a lot of time trying to help me, before, I found it gentlemanly to ask if we can use Deepvariant.; I think the answer is ""Yes, I could"". To be quick ... I have 25 datasets of high-coverage Illumina data. And I'm not sure I will get those datasets resequenced in long reads with enough coverage and N50 (for another reason we don't understand well, DNA extraction is harrowing in rotifers; it often fails or yields highly damaged DNA). Therefore, if I could retrain a model to use the Illumina datasets, that would be great. . My worries are: what does it take in terms of hardware to retrain Deepvariant? I don't have access to a huge GPU. . I found this tutorial: https://github.com/google/deepvariant/blob/r1.5/docs/deepvariant-training-case-study.md . but I am not sure if it is adapted to my case, streamlined, or can be done here, if I understand well this example relies on using Google machines, right?. EDIT: to be perfectly clear it seems to me I need some discussion to understand what you take as a truth set and how you define a bed file with the confidence region. I also would like to know if everything can be done locally",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/716:2022,clear,clear,2022,,https://github.com/google/deepvariant/issues/716,1,['clear'],['clear']
Usability,"tion by method (per version):. #### DV 0.4. ```; # Samples: 186K of event 'cpu-clock'; # Event count (approx.): 46604750000; #; # Children, Self,Command ,Shared Object ,Symbol ; 50.33% , 8.80% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--42.49%--PyEval_EvalFrameEx; | | ; | |--30.79%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.34%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--27.87%--ssw_align; | | | | ; | | | |--14.65%--sw_sse2_word; | | | | ; | | | |--8.32%--sw_sse2_byte; | | | | ; | | | |--2.91%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.36%--ssw_init; | | | ; | | --0.89%--qP_byte; | | ; | |--3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.05%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.07%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.63%--google::protobuf::Descriptor::FindFieldByName; | | ; | |--0.70%--deepvariant_python_allelecounter_clifwrap::pyAlleleCounter::wrapAdd_as_add; | | ; | |--0.59%--google::protobuf::python::cmessa",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:2158,learn,learning,2158,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"tune/loss=0.5603554248809814, tune/precision_1=0.9923615455627441, tune/precision_het=0.0, tune/precision_homalt=0.0, tune/precision_homref=1.0, tune/recall_1=0.9912189841270447, tune/recall_het=0.0, tune/recall_homalt=0.0, tune/recall_homref=0.9912189841270447, tune/true_negatives_1=403192.0, tune/true_positives_1=200591.0; I0829 08:30:42.590469 140318776715072 train.py:471] Skipping checkpoint with tune/f1_weighted=0.99583185 < previous best tune/f1_weighted=0.99845344; I0829 08:30:42.595992 140305134778112 logging_writer.py:48] [13993] tune/early_stopping=7; I0829 08:30:46.123329 140318776715072 local.py:41] Setting work unit notes: 0.0 steps/s, 61.6% (13994/22724), ETA: 8d4h11m; I0829 08:30:46.125013 140305134778112 logging_writer.py:48] [13994] steps_per_sec=0.0123604; I0829 08:30:46.125087 140305134778112 logging_writer.py:48] [13994] uptime=78596.1; I0829 08:31:07.673585 140305134778112 logging_writer.py:48] [14000] epoch=0, train/categorical_accuracy=1.0, train/categorical_crossentropy=0.5519920587539673, train/f1_het=0.0, train/f1_homalt=0.0, train/f1_homref=1.0, train/f1_macro=0.3333333432674408, train/f1_micro=1.0, train/f1_weighted=1.0, train/false_negatives=0.0, train/false_positives=0.0, train/learning_rate=9.999999747378752e-05, train/loss=0.551992654800415, train/precision=1.0, train/precision_het=0.0, train/precision_homalt=0.0, train/precision_homref=1.0, train/recall=1.0, train/recall_het=0.0, train/recall_homalt=0.0, train/recall_homref=1.0, train/true_negatives=12800.0, train/true_positives=6400.0; ```. I am new to Deep Learning and am struggling to decide whether something is wrong with my training approach/scripts or whether the model just needs more time / different hyperparams. Given the number of examples, I can only run 1 epoch at a time before I hit the 24hr cluster wall-time limit. So I have only trained for around 30,000 steps in total across 2 epochs so far (starting from last checkpoint after 1st epoch). . All advice much appreciated!",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/876:14051,Learn,Learning,14051,,https://github.com/google/deepvariant/issues/876,1,['Learn'],['Learning']
Usability,"ve_reader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 255, in _native_reader; return NativeSamReader(input_path, **kwargs); File ""/tmp/Bazel.runfiles_rBHpvo/runfiles/com_google_deepvariant/third_party/nucleus/io/sam.py"", line 232, in __init__; use_original_base_quality_scores=use_original_base_quality_scores); ValueError: Not found: Could not open /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam; parallel: This job failed:; sudo docker run -v /home/root:/home/root gcr.io/deepvariant-docker/deepvariant:0.8.0 /opt/deepvariant/bin/make_examples --mode training --ref /home/chenyangwang600/training-case-study/input/data/ucsc_hg19.fa --reads /home/chenyangwang600/training-case-study/input/data/BGISEQ_PE100_NA12878.sorted.bam --examples /home/chenyangwang600/training-case-study/output/validation_set.with_label.tfrecord@8.gz --truth_variants /home/chenyangwang600/training-case-study/input/data/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz --confident_regions /home/chenyangwang600/training-case-study/input/data/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed --task 1 --regions 'chr21 chr22'. real 0m4.444s; user 0m0.318s; sys 0m0.216s`. I thought I followed the instructions in the guide(Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data) except that I used a 8vCPUs with ; `gcloud beta compute instances create ""cpu-eight"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-1604-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-8"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""`. and set variables; N_SHARDS=""8"". I tried to use another VM but also failed. How can I solve this issue?. Thanks,; Yang",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/184:3057,guid,guide,3057,,https://github.com/google/deepvariant/issues/184,1,['guid'],['guide']
Usability,"y - and there seem to be some slight increases in `0.5.1`, which might cumulatively affect things. In any case, below is the top of the call-graph of percent utilization by method (per version):. #### DV 0.4. ```; # Samples: 186K of event 'cpu-clock'; # Event count (approx.): 46604750000; #; # Children, Self,Command ,Shared Object ,Symbol ; 50.33% , 8.80% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--42.49%--PyEval_EvalFrameEx; | | ; | |--30.79%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.34%--StripedSmithWaterman::Aligner::Align; | | | ; | | |--27.87%--ssw_align; | | | | ; | | | |--14.65%--sw_sse2_word; | | | | ; | | | |--8.32%--sw_sse2_byte; | | | | ; | | | |--2.91%--banded_sw; | | | | ; | | | --1.19%--__memcpy_sse2_unaligned; | | | ; | | --1.36%--ssw_init; | | | ; | | --0.89%--qP_byte; | | ; | |--3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | | | ; | | --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | | | ; | | --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | | | ; | | --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | | | ; | | --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | | | ; | | --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | | | ; | | --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; | | ; | |--3.05%--google::protobuf::python::cmessage::GetAttr; | | | ; | | |--1.07%--google::protobuf::python::cmessage::InternalGetScalar; | | | ; | | --0.63%--google::protobuf::Descript",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:1998,learn,learning,1998,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
Usability,"| ; |--14.65%--sw_sse2_word; | ; |--8.32%--sw_sse2_byte; | ; |--2.91%--banded_sw; | ; --1.19%--__memcpy_sse2_unaligned. 14.65% , 14.62% ,python ,libssw.so ,[.] sw_sse2_word; | ; --14.62%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --14.62%--ssw_align; sw_sse2_word. 8.32% , 8.31% ,python ,libssw.so ,[.] sw_sse2_byte; | ; --8.31%--0x9060a0; PyEval_EvalFrameEx; deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; StripedSmithWaterman::Aligner::Align; | ; --8.30%--ssw_align; sw_sse2_byte. 4.51% , 0.00% ,python ,[unknown] ,[.] 0x00000000009063e0; |; ---0x9063e0; | ; --3.66%--PyEval_EvalFrameEx; | ; --3.30%--deepvariant_realigner_python_debruijn__graph_clifwrap::wrapBuild_as_build; | ; --3.04%--learning::genomics::deepvariant::DeBruijnGraph::Build; | ; --2.73%--learning::genomics::deepvariant::DeBruijnGraph::DeBruijnGraph; | ; --2.41%--learning::genomics::deepvariant::DeBruijnGraph::AddEdgesForRead; | ; --1.75%--learning::genomics::deepvariant::DeBruijnGraph::AddEdge; | ; --1.46%--learning::genomics::deepvariant::DeBruijnGraph::EnsureVertex; | ; --0.50%--std::_Hashtable<tensorflow::StringPiece, std::pair<tensorflow::StringPiece const, void*>, std::allocator<std::pair<tensorflow::StringPiece const, void*> >, std::__detail::_Select1st, std::equal_to<tensorflow::StringPiece>, tensorflow::StringPieceHasher, std::__detail::_Mod_range_hashing, std::__detail::_Default_ranged_hash, std::__detail::_Prime_rehash_policy, std::__detail::_Hashtable_traits<true, false, true> >::_M_find_before_node; ```. #### DV 0.5.1. ```; # Samples: 152K of event 'cpu-clock'; # Event count (approx.): 38010500000; #; # Children, Self,Command ,Shared Object ,Symbol ; 51.45% , 9.13% ,python ,python2.7 ,[.] PyEval_EvalFrameEx; | ; |--43.33%--PyEval_EvalFrameEx; | | ; | |--31.12%--deepvariant_realigner_python_ssw_clifwrap::pyAligner::wrapAlign_as_align; | | | ; | | --30.63%--StripedSm",MatchSource.ISSUE,google,deepvariant,v1.6.1,https://github.com/google/deepvariant/issues/50:6084,learn,learning,6084,,https://github.com/google/deepvariant/issues/50,1,['learn'],['learning']
